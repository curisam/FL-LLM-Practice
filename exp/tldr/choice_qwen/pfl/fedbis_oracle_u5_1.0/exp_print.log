2025-10-10 16:38:05 (root:426) INFO: [logger] file handler -> exp/tldr/choice_qwen/pfl/fedbis_oracle_u5_1.0/exp_print.log
2025-10-10 16:38:05 (root:51) INFO: [main] outdir=exp/tldr/choice_qwen/pfl/fedbis_oracle_u5_1.0/
2025-10-10 16:38:29 (federatedscope.core.data.base_translator:234) INFO: Main process: Completion file found. Skipping generation.
2025-10-10 16:39:11 (federatedscope.core.data.base_translator:264) INFO: [Final Split Summary][loaded][server=0][rank=0/4] Train=92858, Val=33082, Test=50715, Total=176655
2025-10-10 16:39:11 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=1][rank=0/4] Train=2793, Val=146, Test=40, Total=2979
2025-10-10 16:39:11 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=2][rank=0/4] Train=214, Val=11, Test=40, Total=265
2025-10-10 16:39:11 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=3][rank=0/4] Train=691, Val=36, Test=40, Total=767
2025-10-10 16:39:11 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=4][rank=0/4] Train=213, Val=11, Test=40, Total=264
2025-10-10 16:39:11 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=5][rank=0/4] Train=285, Val=14, Test=40, Total=339
2025-10-10 16:39:11 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=6][rank=0/4] Train=2547, Val=134, Test=40, Total=2721
2025-10-10 16:39:11 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=7][rank=0/4] Train=1088, Val=57, Test=40, Total=1185
2025-10-10 16:39:11 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=8][rank=0/4] Train=1316, Val=69, Test=40, Total=1425
2025-10-10 16:39:11 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=9][rank=0/4] Train=3572, Val=188, Test=40, Total=3800
2025-10-10 16:39:11 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=10][rank=0/4] Train=1209, Val=63, Test=40, Total=1312
2025-10-10 16:39:11 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=11][rank=0/4] Train=621, Val=32, Test=40, Total=693
2025-10-10 16:39:11 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=12][rank=0/4] Train=2605, Val=137, Test=40, Total=2782
2025-10-10 16:39:11 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=13][rank=0/4] Train=1372, Val=72, Test=40, Total=1484
2025-10-10 16:39:11 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=14][rank=0/4] Train=3055, Val=160, Test=40, Total=3255
2025-10-10 16:39:11 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=15][rank=0/4] Train=14550, Val=200, Test=40, Total=14790
2025-10-10 16:39:11 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=16][rank=0/4] Train=2589, Val=136, Test=40, Total=2765
2025-10-10 16:39:11 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=17][rank=0/4] Train=5883, Val=200, Test=40, Total=6123
2025-10-10 16:39:11 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=18][rank=0/4] Train=2576, Val=135, Test=40, Total=2751
2025-10-10 16:39:11 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=19][rank=0/4] Train=2102, Val=110, Test=40, Total=2252
2025-10-10 16:39:11 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=20][rank=0/4] Train=2399, Val=126, Test=40, Total=2565
2025-10-10 16:39:11 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=21][rank=0/4] Train=2915, Val=153, Test=40, Total=3108
2025-10-10 16:39:11 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=22][rank=0/4] Train=224, Val=11, Test=40, Total=275
2025-10-10 16:39:11 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=23][rank=0/4] Train=583, Val=30, Test=40, Total=653
2025-10-10 16:39:11 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=24][rank=0/4] Train=4944, Val=200, Test=40, Total=5184
2025-10-10 16:39:11 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=25][rank=0/4] Train=4647, Val=200, Test=40, Total=4887
2025-10-10 16:39:11 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=26][rank=0/4] Train=3063, Val=161, Test=40, Total=3264
2025-10-10 16:39:11 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=27][rank=0/4] Train=2342, Val=123, Test=40, Total=2505
2025-10-10 16:39:11 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=28][rank=0/4] Train=1434, Val=75, Test=40, Total=1549
2025-10-10 16:39:11 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=29][rank=0/4] Train=6191, Val=200, Test=40, Total=6431
2025-10-10 16:39:11 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=30][rank=0/4] Train=3247, Val=170, Test=40, Total=3457
2025-10-10 16:39:11 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=31][rank=0/4] Train=3679, Val=193, Test=40, Total=3912
2025-10-10 16:39:11 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=32][rank=0/4] Train=2144, Val=112, Test=40, Total=2296
2025-10-10 16:39:11 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=33][rank=0/4] Train=1409, Val=74, Test=40, Total=1523
2025-10-10 16:39:11 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=34][rank=0/4] Train=4486, Val=200, Test=40, Total=4726
2025-10-10 16:39:11 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=35][rank=0/4] Train=4736, Val=200, Test=40, Total=4976
2025-10-10 16:39:11 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=36][rank=0/4] Train=1030, Val=54, Test=40, Total=1124
2025-10-10 16:39:11 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=37][rank=0/4] Train=4273, Val=200, Test=40, Total=4513
2025-10-10 16:39:11 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=38][rank=0/4] Train=6171, Val=200, Test=40, Total=6411
2025-10-10 16:39:11 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=39][rank=0/4] Train=1594, Val=83, Test=40, Total=1717
2025-10-10 16:39:11 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=40][rank=0/4] Train=4005, Val=200, Test=40, Total=4245
2025-10-10 16:39:11 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=41][rank=0/4] Train=2275, Val=119, Test=40, Total=2434
2025-10-10 16:39:11 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=42][rank=0/4] Train=5772, Val=200, Test=40, Total=6012
2025-10-10 16:39:11 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=43][rank=0/4] Train=1694, Val=89, Test=40, Total=1823
2025-10-10 16:39:11 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=44][rank=0/4] Train=7916, Val=200, Test=40, Total=8156
2025-10-10 16:39:11 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=45][rank=0/4] Train=1901, Val=100, Test=40, Total=2041
2025-10-10 16:39:11 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=46][rank=0/4] Train=2100, Val=110, Test=40, Total=2250
2025-10-10 16:39:11 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=47][rank=0/4] Train=2812, Val=147, Test=40, Total=2999
2025-10-10 16:39:11 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=48][rank=0/4] Train=880, Val=46, Test=40, Total=966
2025-10-10 16:39:11 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=49][rank=0/4] Train=2521, Val=132, Test=40, Total=2693
2025-10-10 16:39:11 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=50][rank=0/4] Train=2527, Val=133, Test=40, Total=2700
2025-10-10 16:39:11 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=51][rank=0/4] Train=1580, Val=83, Test=40, Total=1703
2025-10-10 16:39:11 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=52][rank=0/4] Train=3589, Val=188, Test=40, Total=3817
2025-10-10 16:39:11 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=53][rank=0/4] Train=6791, Val=200, Test=40, Total=7031
2025-10-10 16:39:13 (federatedscope.core.auxiliaries.utils:175) INFO: The device information file is not provided
2025-10-10 16:39:13 (federatedscope.core.auxiliaries.model_builder:139) WARNING: The input shape is None. Please specify the `data.input_shape`(a tuple) or give the representative data to `get_model` if necessary
2025-10-10 16:39:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-build][rank=0] tok_len=151643 | base=Qwen2ForCausalLM | in_emb=(Embedding) num=151646 ptr=139866006413376 | out_emb=(Linear) num=151646 ptr=139866006413376 | lora_ptr=None
2025-10-10 16:39:28 (federatedscope.llm.model.model_builder:188) INFO: [Warmup-Init] loaded from checkpoints_1.0_oracle/final_tldr_choice_qwen_fedbis_oracle_u5_round_210.ckpt (round=210) | missing=291 unexpected=0
2025-10-10 16:39:28 (federatedscope.core.fed_runner:211) INFO: Server has been set up ... 
2025-10-10 16:39:30 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 16:39:32 (federatedscope.core.fed_runner:275) INFO: Client 1 has been set up ... 
2025-10-10 16:39:32 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 16:39:35 (federatedscope.core.fed_runner:275) INFO: Client 2 has been set up ... 
2025-10-10 16:39:35 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 16:39:37 (federatedscope.core.fed_runner:275) INFO: Client 3 has been set up ... 
2025-10-10 16:39:37 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 16:39:40 (federatedscope.core.fed_runner:275) INFO: Client 4 has been set up ... 
2025-10-10 16:39:40 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 16:39:43 (federatedscope.core.fed_runner:275) INFO: Client 5 has been set up ... 
2025-10-10 16:39:43 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 16:39:45 (federatedscope.core.fed_runner:275) INFO: Client 6 has been set up ... 
2025-10-10 16:39:46 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 16:39:48 (federatedscope.core.fed_runner:275) INFO: Client 7 has been set up ... 
2025-10-10 16:39:48 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 16:39:51 (federatedscope.core.fed_runner:275) INFO: Client 8 has been set up ... 
2025-10-10 16:39:51 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 16:39:54 (federatedscope.core.fed_runner:275) INFO: Client 9 has been set up ... 
2025-10-10 16:39:54 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 16:39:56 (federatedscope.core.fed_runner:275) INFO: Client 10 has been set up ... 
2025-10-10 16:39:56 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 16:39:59 (federatedscope.core.fed_runner:275) INFO: Client 11 has been set up ... 
2025-10-10 16:39:59 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 16:40:01 (federatedscope.core.fed_runner:275) INFO: Client 12 has been set up ... 
2025-10-10 16:40:02 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 16:40:05 (federatedscope.core.fed_runner:275) INFO: Client 13 has been set up ... 
2025-10-10 16:40:05 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 16:40:08 (federatedscope.core.fed_runner:275) INFO: Client 14 has been set up ... 
2025-10-10 16:40:08 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 16:40:11 (federatedscope.core.fed_runner:275) INFO: Client 15 has been set up ... 
2025-10-10 16:40:11 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 16:40:14 (federatedscope.core.fed_runner:275) INFO: Client 16 has been set up ... 
2025-10-10 16:40:14 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 16:40:16 (federatedscope.core.fed_runner:275) INFO: Client 17 has been set up ... 
2025-10-10 16:40:16 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 16:40:19 (federatedscope.core.fed_runner:275) INFO: Client 18 has been set up ... 
2025-10-10 16:40:19 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 16:40:22 (federatedscope.core.fed_runner:275) INFO: Client 19 has been set up ... 
2025-10-10 16:40:22 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 16:40:24 (federatedscope.core.fed_runner:275) INFO: Client 20 has been set up ... 
2025-10-10 16:40:24 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 16:40:27 (federatedscope.core.fed_runner:275) INFO: Client 21 has been set up ... 
2025-10-10 16:40:27 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 16:40:30 (federatedscope.core.fed_runner:275) INFO: Client 22 has been set up ... 
2025-10-10 16:40:30 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 16:40:32 (federatedscope.core.fed_runner:275) INFO: Client 23 has been set up ... 
2025-10-10 16:40:32 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 16:40:35 (federatedscope.core.fed_runner:275) INFO: Client 24 has been set up ... 
2025-10-10 16:40:35 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 16:40:38 (federatedscope.core.fed_runner:275) INFO: Client 25 has been set up ... 
2025-10-10 16:40:38 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 16:40:42 (federatedscope.core.fed_runner:275) INFO: Client 26 has been set up ... 
2025-10-10 16:40:42 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 16:40:44 (federatedscope.core.fed_runner:275) INFO: Client 27 has been set up ... 
2025-10-10 16:40:44 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 16:40:47 (federatedscope.core.fed_runner:275) INFO: Client 28 has been set up ... 
2025-10-10 16:40:47 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 16:40:50 (federatedscope.core.fed_runner:275) INFO: Client 29 has been set up ... 
2025-10-10 16:40:50 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 16:40:52 (federatedscope.core.fed_runner:275) INFO: Client 30 has been set up ... 
2025-10-10 16:40:52 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 16:40:55 (federatedscope.core.fed_runner:275) INFO: Client 31 has been set up ... 
2025-10-10 16:40:55 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 16:40:58 (federatedscope.core.fed_runner:275) INFO: Client 32 has been set up ... 
2025-10-10 16:40:58 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 16:41:00 (federatedscope.core.fed_runner:275) INFO: Client 33 has been set up ... 
2025-10-10 16:41:01 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 16:41:03 (federatedscope.core.fed_runner:275) INFO: Client 34 has been set up ... 
2025-10-10 16:41:04 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 16:41:06 (federatedscope.core.fed_runner:275) INFO: Client 35 has been set up ... 
2025-10-10 16:41:06 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 16:41:09 (federatedscope.core.fed_runner:275) INFO: Client 36 has been set up ... 
2025-10-10 16:41:09 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 16:41:11 (federatedscope.core.fed_runner:275) INFO: Client 37 has been set up ... 
2025-10-10 16:41:12 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 16:41:14 (federatedscope.core.fed_runner:275) INFO: Client 38 has been set up ... 
2025-10-10 16:41:14 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 16:41:18 (federatedscope.core.fed_runner:275) INFO: Client 39 has been set up ... 
2025-10-10 16:41:18 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 16:41:21 (federatedscope.core.fed_runner:275) INFO: Client 40 has been set up ... 
2025-10-10 16:41:21 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 16:41:23 (federatedscope.core.fed_runner:275) INFO: Client 41 has been set up ... 
2025-10-10 16:41:23 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 16:41:26 (federatedscope.core.fed_runner:275) INFO: Client 42 has been set up ... 
2025-10-10 16:41:26 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 16:41:29 (federatedscope.core.fed_runner:275) INFO: Client 43 has been set up ... 
2025-10-10 16:41:29 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 16:41:32 (federatedscope.core.fed_runner:275) INFO: Client 44 has been set up ... 
2025-10-10 16:41:32 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 16:41:34 (federatedscope.core.fed_runner:275) INFO: Client 45 has been set up ... 
2025-10-10 16:41:34 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 16:41:37 (federatedscope.core.fed_runner:275) INFO: Client 46 has been set up ... 
2025-10-10 16:41:37 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 16:41:40 (federatedscope.core.fed_runner:275) INFO: Client 47 has been set up ... 
2025-10-10 16:41:40 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 16:41:42 (federatedscope.core.fed_runner:275) INFO: Client 48 has been set up ... 
2025-10-10 16:41:43 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 16:41:45 (federatedscope.core.fed_runner:275) INFO: Client 49 has been set up ... 
2025-10-10 16:41:45 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 16:41:48 (federatedscope.core.fed_runner:275) INFO: Client 50 has been set up ... 
2025-10-10 16:41:48 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 16:41:50 (federatedscope.core.fed_runner:275) INFO: Client 51 has been set up ... 
2025-10-10 16:41:51 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 16:41:53 (federatedscope.core.fed_runner:275) INFO: Client 52 has been set up ... 
2025-10-10 16:41:53 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 16:41:57 (federatedscope.core.fed_runner:275) INFO: Client 53 has been set up ... 
2025-10-10 16:41:57 (federatedscope.core.trainers.trainer:569) INFO: Model meta-info: <class 'federatedscope.llm.model.adapter_builder.AdapterModel'>.
2025-10-10 16:41:57 (federatedscope.core.trainers.trainer:584) INFO: Num of original para names: 2016.
2025-10-10 16:41:57 (federatedscope.core.trainers.trainer:585) INFO: Num of original trainable para names: 2306.
2025-10-10 16:41:57 (federatedscope.core.trainers.trainer:587) INFO: Num of preserved para names in local update: 2016. 
Preserved para names in local update: {'base_model.model.model.layers.10.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.Adapter_1.weight'}.
2025-10-10 16:41:57 (federatedscope.core.trainers.trainer:591) INFO: Num of filtered para names in local update: 0. 
Filtered para names in local update: set().
2025-10-10 16:41:57 (federatedscope.core.trainers.trainer:599) INFO: After register default hooks,
	the hooks_in_train is:
	{
	  "on_fit_start": [
	    "_hook_on_fit_start_numerical_precision",
	    "_hook_on_data_parallel_init",
	    "_hook_on_fit_start_init",
	    "_hook_on_fit_start_calculate_model_size"
	  ],
	  "on_batch_start": [
	    "_hook_on_batch_start_init"
	  ],
	  "on_batch_forward": [
	    "_hook_on_batch_forward",
	    "_hook_on_batch_forward_regularizer",
	    "_hook_on_batch_forward_flop_count"
	  ],
	  "on_batch_backward": [
	    "_hook_on_batch_backward"
	  ],
	  "on_batch_end": [
	    "_hook_on_batch_end"
	  ],
	  "on_fit_end": [
	    "_hook_on_fit_end",
	    "_hook_on_fit_end_free_space"
	  ]
	};
	the hooks_in_eval is:
            t{
	  "on_fit_start": [
	    "_hook_on_fit_start_numerical_precision",
	    "_hook_on_data_parallel_init",
	    "_hook_on_fit_start_init"
	  ],
	  "on_batch_start": [
	    "_hook_on_batch_start_init"
	  ],
	  "on_batch_forward": [
	    "_hook_on_batch_forward"
	  ],
	  "on_batch_end": [
	    "_hook_on_batch_end"
	  ],
	  "on_fit_end": [
	    "_hook_on_fit_end",
	    "_hook_on_fit_end_free_space"
	  ]
	}
2025-10-10 16:41:57 (federatedscope.llm.llm_local.server:200) INFO: Waited all clients join, start now...
2025-10-10 16:41:57 (federatedscope.llm.llm_local.server:217) INFO: ----------- Starting training (Round #0) -------------
2025-10-10 16:42:04 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 16:42:05 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 16:42:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:42:06 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 16:42:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:42:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:42:10 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 16:42:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 16:42:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:42:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:42:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 16:42:13 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=141.923401, avg_loss=0.709617, seen=200, correct=108, accuracy=0.540000
2025-10-10 16:42:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:42:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:42:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:42:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1802MB allocated=1776MB
2025-10-10 16:42:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:42:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:42:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:42:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:42:16 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.310680, avg_loss=0.707767, seen=40, correct=21, accuracy=0.525000
2025-10-10 16:42:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:42:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:42:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:42:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1802MB allocated=1776MB
2025-10-10 16:42:17 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.525000
2025-10-10 16:42:18 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 16:42:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1979, total=7916)
2025-10-10 16:42:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:42:18 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 16:42:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:42:18 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 16:42:18 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=990, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 16:42:26 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 16:42:26 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 16:42:26 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 16:42:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 16:42:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:42:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:42:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 16:42:29 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=141.316757, avg_loss=0.706584, seen=200, correct=93, accuracy=0.465000
2025-10-10 16:42:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:42:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:42:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:42:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1866MB allocated=1801MB
2025-10-10 16:42:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:42:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:42:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:42:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:42:33 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.131432, avg_loss=0.678286, seen=40, correct=22, accuracy=0.550000
2025-10-10 16:42:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:42:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:42:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:42:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1866MB allocated=1801MB
2025-10-10 16:42:36 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.550000
2025-10-10 16:42:44 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 16:42:44 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 16:42:44 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 16:42:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 16:42:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:42:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:42:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 16:42:47 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=141.997192, avg_loss=0.709986, seen=200, correct=109, accuracy=0.545000
2025-10-10 16:42:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:42:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:42:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:42:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1866MB allocated=1801MB
2025-10-10 16:42:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:42:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:42:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:42:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:42:50 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.109463, avg_loss=0.702737, seen=40, correct=21, accuracy=0.525000
2025-10-10 16:42:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:42:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:42:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:42:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1866MB allocated=1801MB
2025-10-10 16:42:52 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.550000, curr=0.525000
2025-10-10 16:43:00 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 16:43:00 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 16:43:00 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 16:43:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 16:43:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:43:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:43:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 16:43:03 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=141.904114, avg_loss=0.709521, seen=200, correct=109, accuracy=0.545000
2025-10-10 16:43:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:43:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:43:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:43:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1866MB allocated=1801MB
2025-10-10 16:43:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:43:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:43:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:43:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:43:07 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.715881, avg_loss=0.692897, seen=40, correct=21, accuracy=0.525000
2025-10-10 16:43:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:43:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:43:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:43:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1866MB allocated=1801MB
2025-10-10 16:43:09 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.550000, curr=0.525000
2025-10-10 16:43:18 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 16:43:18 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 16:43:18 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 16:43:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 16:43:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:43:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:43:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 16:43:21 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=142.553528, avg_loss=0.712768, seen=200, correct=101, accuracy=0.505000
2025-10-10 16:43:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:43:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:43:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:43:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1866MB allocated=1801MB
2025-10-10 16:43:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:43:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:43:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:43:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:43:23 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.564402, avg_loss=0.639110, seen=40, correct=29, accuracy=0.725000
2025-10-10 16:43:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:43:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:43:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:43:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1866MB allocated=1801MB
2025-10-10 16:43:25 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.725000
2025-10-10 16:43:32 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 16:43:32 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 16:43:32 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 16:43:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 16:43:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:43:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:43:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 16:43:35 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=142.687073, avg_loss=0.713435, seen=200, correct=103, accuracy=0.515000
2025-10-10 16:43:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:43:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:43:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:43:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1866MB allocated=1801MB
2025-10-10 16:43:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:43:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:43:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:43:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:43:39 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.224016, avg_loss=0.630600, seen=40, correct=27, accuracy=0.675000
2025-10-10 16:43:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:43:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:43:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:43:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1866MB allocated=1801MB
2025-10-10 16:43:40 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.725000, curr=0.675000
2025-10-10 16:43:48 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 16:43:48 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 16:43:48 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 16:43:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 16:43:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:43:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:43:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 16:43:51 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=140.642380, avg_loss=0.703212, seen=200, correct=102, accuracy=0.510000
2025-10-10 16:43:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:43:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:43:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:43:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1866MB allocated=1801MB
2025-10-10 16:43:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:43:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:43:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:43:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:43:54 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.907963, avg_loss=0.647699, seen=40, correct=23, accuracy=0.575000
2025-10-10 16:43:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:43:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:43:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:43:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1866MB allocated=1801MB
2025-10-10 16:43:56 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.725000, curr=0.575000
2025-10-10 16:44:03 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 16:44:03 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 16:44:03 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 16:44:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 16:44:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:44:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:44:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 16:44:06 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=140.348068, avg_loss=0.701740, seen=200, correct=104, accuracy=0.520000
2025-10-10 16:44:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:44:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:44:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:44:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1866MB allocated=1801MB
2025-10-10 16:44:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:44:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:44:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:44:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:44:10 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.577866, avg_loss=0.664447, seen=40, correct=21, accuracy=0.525000
2025-10-10 16:44:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:44:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:44:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:44:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1866MB allocated=1801MB
2025-10-10 16:44:12 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.725000, curr=0.525000
2025-10-10 16:44:21 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 16:44:21 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 16:44:21 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 16:44:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 16:44:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:44:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:44:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 16:44:24 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=140.966583, avg_loss=0.704833, seen=200, correct=102, accuracy=0.510000
2025-10-10 16:44:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:44:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:44:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:44:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1866MB allocated=1801MB
2025-10-10 16:44:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:44:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:44:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:44:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:44:28 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.820095, avg_loss=0.670502, seen=40, correct=20, accuracy=0.500000
2025-10-10 16:44:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:44:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:44:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:44:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1866MB allocated=1801MB
2025-10-10 16:44:29 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/25), best=0.725000, curr=0.500000
2025-10-10 16:44:37 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 16:44:37 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 16:44:37 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 16:44:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 16:44:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:44:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:44:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 16:44:40 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=140.591278, avg_loss=0.702956, seen=200, correct=104, accuracy=0.520000
2025-10-10 16:44:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:44:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:44:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:44:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1866MB allocated=1801MB
2025-10-10 16:44:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:44:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:44:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:44:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:44:44 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.926979, avg_loss=0.673174, seen=40, correct=19, accuracy=0.475000
2025-10-10 16:44:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:44:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:44:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:44:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1866MB allocated=1801MB
2025-10-10 16:44:45 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/25), best=0.725000, curr=0.475000
2025-10-10 16:44:53 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 16:44:53 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 16:44:53 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 16:44:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 16:44:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:44:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:44:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 16:44:56 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=140.496475, avg_loss=0.702482, seen=200, correct=103, accuracy=0.515000
2025-10-10 16:44:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:44:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:44:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:44:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1866MB allocated=1801MB
2025-10-10 16:44:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:44:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:44:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:45:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:45:01 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.967388, avg_loss=0.649185, seen=40, correct=23, accuracy=0.575000
2025-10-10 16:45:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:45:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:45:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:45:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1866MB allocated=1801MB
2025-10-10 16:45:02 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/25), best=0.725000, curr=0.575000
2025-10-10 16:45:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 16:45:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 16:45:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:45:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:45:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1866MB allocated=1801MB
2025-10-10 16:45:03 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #44', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 16:45:03 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #44', 'Round': 0, 'Results_raw': {}}
2025-10-10 16:45:03 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 16:45:04 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 16:45:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:45:05 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 16:45:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:45:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:45:06 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 16:45:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-10-10 16:45:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:45:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:45:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-10 16:45:07 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=74, loss_sum=50.897537, avg_loss=0.687805, seen=74, correct=46, accuracy=0.621622
2025-10-10 16:45:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:45:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:45:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:45:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1842MB allocated=1793MB
2025-10-10 16:45:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:45:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:45:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:45:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:45:11 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.230709, avg_loss=0.680768, seen=40, correct=23, accuracy=0.575000
2025-10-10 16:45:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:45:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:45:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:45:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1842MB allocated=1793MB
2025-10-10 16:45:13 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-10 16:45:13 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 16:45:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=353, total=1409)
2025-10-10 16:45:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:45:13 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 16:45:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:45:13 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 16:45:13 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=177, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 16:45:21 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 16:45:21 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 16:45:21 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 16:45:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-10-10 16:45:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:45:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:45:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-10 16:45:22 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=74, loss_sum=52.050327, avg_loss=0.703383, seen=74, correct=46, accuracy=0.621622
2025-10-10 16:45:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:45:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:45:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:45:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1870MB allocated=1810MB
2025-10-10 16:45:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:45:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:45:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:45:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:45:26 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.458729, avg_loss=0.686468, seen=40, correct=25, accuracy=0.625000
2025-10-10 16:45:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:45:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:45:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:45:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1870MB allocated=1810MB
2025-10-10 16:45:28 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-10 16:45:35 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 16:45:35 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 16:45:35 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 16:45:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-10-10 16:45:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:45:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:45:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-10 16:45:37 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=74, loss_sum=52.532784, avg_loss=0.709902, seen=74, correct=39, accuracy=0.527027
2025-10-10 16:45:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:45:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:45:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:45:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1870MB allocated=1810MB
2025-10-10 16:45:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:45:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:45:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:45:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:45:41 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.906944, avg_loss=0.672674, seen=40, correct=23, accuracy=0.575000
2025-10-10 16:45:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:45:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:45:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:45:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1870MB allocated=1810MB
2025-10-10 16:45:43 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.625000, curr=0.575000
2025-10-10 16:45:52 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 16:45:52 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 16:45:52 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 16:45:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-10-10 16:45:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:45:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:45:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-10 16:45:54 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=74, loss_sum=52.352787, avg_loss=0.707470, seen=74, correct=45, accuracy=0.608108
2025-10-10 16:45:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:45:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:45:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:45:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1870MB allocated=1810MB
2025-10-10 16:45:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:45:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:45:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:45:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:45:57 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.804255, avg_loss=0.695106, seen=40, correct=24, accuracy=0.600000
2025-10-10 16:45:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:45:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:45:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:45:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1870MB allocated=1810MB
2025-10-10 16:45:59 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.625000, curr=0.600000
2025-10-10 16:46:07 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 16:46:07 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 16:46:07 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 16:46:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-10-10 16:46:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:46:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:46:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-10 16:46:08 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=74, loss_sum=52.857281, avg_loss=0.714288, seen=74, correct=45, accuracy=0.608108
2025-10-10 16:46:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:46:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:46:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:46:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1870MB allocated=1810MB
2025-10-10 16:46:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:46:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:46:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:46:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:46:11 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.337505, avg_loss=0.708438, seen=40, correct=23, accuracy=0.575000
2025-10-10 16:46:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:46:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:46:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:46:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1870MB allocated=1810MB
2025-10-10 16:46:13 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.625000, curr=0.575000
2025-10-10 16:46:20 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 16:46:20 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 16:46:20 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 16:46:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-10-10 16:46:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:46:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:46:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-10 16:46:22 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=74, loss_sum=53.577564, avg_loss=0.724021, seen=74, correct=37, accuracy=0.500000
2025-10-10 16:46:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:46:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:46:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:46:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1870MB allocated=1810MB
2025-10-10 16:46:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:46:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:46:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:46:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:46:25 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.835983, avg_loss=0.670900, seen=40, correct=23, accuracy=0.575000
2025-10-10 16:46:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:46:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:46:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:46:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1870MB allocated=1810MB
2025-10-10 16:46:27 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/25), best=0.625000, curr=0.575000
2025-10-10 16:46:35 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 16:46:35 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 16:46:35 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 16:46:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-10-10 16:46:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:46:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:46:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-10 16:46:36 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=74, loss_sum=52.803932, avg_loss=0.713567, seen=74, correct=38, accuracy=0.513514
2025-10-10 16:46:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:46:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:46:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:46:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1870MB allocated=1810MB
2025-10-10 16:46:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:46:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:46:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:46:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:46:40 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.907320, avg_loss=0.672683, seen=40, correct=22, accuracy=0.550000
2025-10-10 16:46:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:46:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:46:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:46:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1870MB allocated=1810MB
2025-10-10 16:46:42 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/25), best=0.625000, curr=0.550000
2025-10-10 16:46:51 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 16:46:51 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 16:46:51 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 16:46:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-10-10 16:46:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:46:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:46:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-10 16:46:52 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=74, loss_sum=51.949814, avg_loss=0.702025, seen=74, correct=44, accuracy=0.594595
2025-10-10 16:46:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:46:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:46:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:46:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1870MB allocated=1810MB
2025-10-10 16:46:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:46:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:46:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:46:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:46:57 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.357386, avg_loss=0.683935, seen=40, correct=24, accuracy=0.600000
2025-10-10 16:46:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:46:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:46:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:46:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1870MB allocated=1810MB
2025-10-10 16:46:59 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/25), best=0.625000, curr=0.600000
2025-10-10 16:47:07 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 16:47:07 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 16:47:07 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 16:47:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-10-10 16:47:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:47:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:47:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-10 16:47:09 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=74, loss_sum=51.725800, avg_loss=0.698997, seen=74, correct=44, accuracy=0.594595
2025-10-10 16:47:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:47:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:47:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:47:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1870MB allocated=1810MB
2025-10-10 16:47:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:47:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:47:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:47:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:47:12 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.260036, avg_loss=0.681501, seen=40, correct=24, accuracy=0.600000
2025-10-10 16:47:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:47:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:47:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:47:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1870MB allocated=1810MB
2025-10-10 16:47:14 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/25), best=0.625000, curr=0.600000
2025-10-10 16:47:23 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 16:47:23 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 16:47:23 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 16:47:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-10-10 16:47:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:47:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:47:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-10 16:47:24 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=74, loss_sum=52.172722, avg_loss=0.705037, seen=74, correct=39, accuracy=0.527027
2025-10-10 16:47:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:47:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:47:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:47:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1870MB allocated=1810MB
2025-10-10 16:47:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:47:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:47:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:47:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:47:27 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.844648, avg_loss=0.671116, seen=40, correct=23, accuracy=0.575000
2025-10-10 16:47:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:47:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:47:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:47:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1870MB allocated=1810MB
2025-10-10 16:47:29 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/25), best=0.625000, curr=0.575000
2025-10-10 16:47:36 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 16:47:36 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 16:47:36 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 16:47:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-10-10 16:47:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:47:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:47:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-10 16:47:37 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=74, loss_sum=52.687126, avg_loss=0.711988, seen=74, correct=39, accuracy=0.527027
2025-10-10 16:47:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:47:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:47:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:47:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1870MB allocated=1810MB
2025-10-10 16:47:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:47:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:47:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:47:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:47:41 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.996500, avg_loss=0.674913, seen=40, correct=22, accuracy=0.550000
2025-10-10 16:47:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:47:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:47:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:47:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1870MB allocated=1810MB
2025-10-10 16:47:43 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=9/25), best=0.625000, curr=0.550000
2025-10-10 16:47:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 16:47:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 16:47:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:47:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:47:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1870MB allocated=1810MB
2025-10-10 16:47:44 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #33', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 16:47:44 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #33', 'Round': 0, 'Results_raw': {}}
2025-10-10 16:47:44 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 16:47:45 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 16:47:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:47:45 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 16:47:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:47:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:47:46 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 16:47:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-10-10 16:47:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:47:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:47:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-10-10 16:47:48 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=83, loss_sum=59.875145, avg_loss=0.721387, seen=83, correct=44, accuracy=0.530120
2025-10-10 16:47:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:47:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:47:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:47:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1842MB allocated=1801MB
2025-10-10 16:47:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:47:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:47:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:47:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:47:52 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.608780, avg_loss=0.690219, seen=40, correct=24, accuracy=0.600000
2025-10-10 16:47:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:47:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:47:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:47:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1842MB allocated=1801MB
2025-10-10 16:47:54 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-10 16:47:54 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 16:47:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=399, total=1594)
2025-10-10 16:47:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:47:54 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 16:47:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:47:54 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 16:47:54 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=200, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 16:48:02 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 16:48:02 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 16:48:02 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 16:48:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-10-10 16:48:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:48:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:48:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-10-10 16:48:03 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=83, loss_sum=60.427246, avg_loss=0.728039, seen=83, correct=47, accuracy=0.566265
2025-10-10 16:48:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:48:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:48:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:48:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1868MB allocated=1818MB
2025-10-10 16:48:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:48:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:48:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:48:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:48:06 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.161072, avg_loss=0.679027, seen=40, correct=24, accuracy=0.600000
2025-10-10 16:48:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:48:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:48:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:48:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1868MB allocated=1818MB
2025-10-10 16:48:08 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-10 16:48:17 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 16:48:17 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 16:48:17 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 16:48:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-10-10 16:48:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:48:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:48:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-10-10 16:48:18 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=83, loss_sum=60.300468, avg_loss=0.726512, seen=83, correct=45, accuracy=0.542169
2025-10-10 16:48:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:48:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:48:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:48:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1868MB allocated=1818MB
2025-10-10 16:48:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:48:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:48:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:48:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:48:21 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.155642, avg_loss=0.678891, seen=40, correct=22, accuracy=0.550000
2025-10-10 16:48:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:48:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:48:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:48:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1868MB allocated=1818MB
2025-10-10 16:48:23 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.600000, curr=0.550000
2025-10-10 16:48:32 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 16:48:32 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 16:48:32 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 16:48:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-10-10 16:48:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:48:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:48:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-10-10 16:48:33 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=83, loss_sum=60.881950, avg_loss=0.733517, seen=83, correct=46, accuracy=0.554217
2025-10-10 16:48:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:48:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:48:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:48:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1868MB allocated=1818MB
2025-10-10 16:48:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:48:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:48:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:48:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:48:36 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.360439, avg_loss=0.684011, seen=40, correct=24, accuracy=0.600000
2025-10-10 16:48:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:48:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:48:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:48:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1868MB allocated=1818MB
2025-10-10 16:48:37 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-10 16:48:45 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 16:48:45 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 16:48:45 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 16:48:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-10-10 16:48:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:48:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:48:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-10-10 16:48:47 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=83, loss_sum=60.621162, avg_loss=0.730375, seen=83, correct=47, accuracy=0.566265
2025-10-10 16:48:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:48:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:48:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:48:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1868MB allocated=1818MB
2025-10-10 16:48:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:48:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:48:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:48:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:48:51 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.924124, avg_loss=0.673103, seen=40, correct=23, accuracy=0.575000
2025-10-10 16:48:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:48:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:48:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:48:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1868MB allocated=1818MB
2025-10-10 16:48:52 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.600000, curr=0.575000
2025-10-10 16:49:01 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 16:49:01 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 16:49:01 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 16:49:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-10-10 16:49:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:49:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:49:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-10-10 16:49:02 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=83, loss_sum=60.880043, avg_loss=0.733494, seen=83, correct=45, accuracy=0.542169
2025-10-10 16:49:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:49:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:49:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:49:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1868MB allocated=1818MB
2025-10-10 16:49:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:49:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:49:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:49:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:49:06 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.704247, avg_loss=0.667606, seen=40, correct=24, accuracy=0.600000
2025-10-10 16:49:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:49:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:49:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:49:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1868MB allocated=1818MB
2025-10-10 16:49:08 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-10 16:49:15 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 16:49:15 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 16:49:15 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 16:49:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-10-10 16:49:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:49:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:49:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-10-10 16:49:16 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=83, loss_sum=60.118633, avg_loss=0.724321, seen=83, correct=45, accuracy=0.542169
2025-10-10 16:49:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:49:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:49:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:49:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1868MB allocated=1818MB
2025-10-10 16:49:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:49:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:49:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:49:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:49:19 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.842768, avg_loss=0.671069, seen=40, correct=22, accuracy=0.550000
2025-10-10 16:49:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:49:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:49:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:49:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1868MB allocated=1818MB
2025-10-10 16:49:20 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.600000, curr=0.550000
2025-10-10 16:49:28 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 16:49:28 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 16:49:28 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 16:49:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-10-10 16:49:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:49:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:49:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-10-10 16:49:30 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=83, loss_sum=58.798889, avg_loss=0.708420, seen=83, correct=46, accuracy=0.554217
2025-10-10 16:49:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:49:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:49:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:49:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1868MB allocated=1818MB
2025-10-10 16:49:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:49:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:49:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:49:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:49:34 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.949799, avg_loss=0.673745, seen=40, correct=22, accuracy=0.550000
2025-10-10 16:49:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:49:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:49:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:49:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1868MB allocated=1818MB
2025-10-10 16:49:35 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.600000, curr=0.550000
2025-10-10 16:49:43 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 16:49:43 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 16:49:43 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 16:49:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-10-10 16:49:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:49:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:49:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-10-10 16:49:44 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=83, loss_sum=59.735096, avg_loss=0.719700, seen=83, correct=45, accuracy=0.542169
2025-10-10 16:49:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:49:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:49:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:49:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1868MB allocated=1818MB
2025-10-10 16:49:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:49:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:49:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:49:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:49:48 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.918608, avg_loss=0.672965, seen=40, correct=23, accuracy=0.575000
2025-10-10 16:49:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:49:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:49:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:49:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1868MB allocated=1818MB
2025-10-10 16:49:49 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.600000, curr=0.575000
2025-10-10 16:49:57 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 16:49:57 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 16:49:57 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 16:49:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-10-10 16:49:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:49:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:49:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-10-10 16:49:59 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=83, loss_sum=59.839699, avg_loss=0.720960, seen=83, correct=44, accuracy=0.530120
2025-10-10 16:49:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:49:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:50:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:50:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1868MB allocated=1818MB
2025-10-10 16:50:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:50:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:50:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:50:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:50:03 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.841061, avg_loss=0.671027, seen=40, correct=22, accuracy=0.550000
2025-10-10 16:50:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:50:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:50:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:50:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1868MB allocated=1818MB
2025-10-10 16:50:05 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/25), best=0.600000, curr=0.550000
2025-10-10 16:50:14 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 16:50:14 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 16:50:14 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 16:50:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-10-10 16:50:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:50:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:50:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-10-10 16:50:16 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=83, loss_sum=59.840214, avg_loss=0.720966, seen=83, correct=44, accuracy=0.530120
2025-10-10 16:50:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:50:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:50:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:50:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1868MB allocated=1818MB
2025-10-10 16:50:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:50:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:50:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:50:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:50:19 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.043652, avg_loss=0.676091, seen=40, correct=25, accuracy=0.625000
2025-10-10 16:50:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:50:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:50:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:50:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1868MB allocated=1818MB
2025-10-10 16:50:20 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-10 16:50:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 16:50:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 16:50:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:50:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:50:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1868MB allocated=1818MB
2025-10-10 16:50:21 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #39', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 16:50:21 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #39', 'Round': 0, 'Results_raw': {}}
2025-10-10 16:50:21 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 16:50:22 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 16:50:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:50:22 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 16:50:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:50:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:50:23 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 16:50:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 16:50:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:50:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:50:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 16:50:26 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=125.297165, avg_loss=0.626486, seen=200, correct=134, accuracy=0.670000
2025-10-10 16:50:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:50:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:50:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:50:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1842MB allocated=1810MB
2025-10-10 16:50:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:50:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:50:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:50:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:50:28 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=22.784267, avg_loss=0.569607, seen=40, correct=29, accuracy=0.725000
2025-10-10 16:50:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:50:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:50:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:50:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1842MB allocated=1810MB
2025-10-10 16:50:30 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.725000
2025-10-10 16:50:30 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 16:50:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1122, total=4486)
2025-10-10 16:50:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:50:30 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 16:50:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:50:30 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 16:50:30 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=561, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 16:50:37 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 16:50:37 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 16:50:37 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 16:50:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 16:50:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:50:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:50:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 16:50:40 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=129.599030, avg_loss=0.647995, seen=200, correct=125, accuracy=0.625000
2025-10-10 16:50:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:50:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:50:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:50:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1874MB allocated=1826MB
2025-10-10 16:50:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:50:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:50:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:50:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:50:44 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=22.250906, avg_loss=0.556273, seen=40, correct=27, accuracy=0.675000
2025-10-10 16:50:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:50:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:50:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:50:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1874MB allocated=1826MB
2025-10-10 16:50:45 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.725000, curr=0.675000
2025-10-10 16:50:53 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 16:50:53 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 16:50:53 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 16:50:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 16:50:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:50:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:50:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 16:50:56 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=129.003113, avg_loss=0.645016, seen=200, correct=122, accuracy=0.610000
2025-10-10 16:50:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:50:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:50:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:50:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1874MB allocated=1826MB
2025-10-10 16:50:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:50:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:50:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:50:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:50:59 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.932917, avg_loss=0.623323, seen=40, correct=28, accuracy=0.700000
2025-10-10 16:50:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:50:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:50:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:51:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1874MB allocated=1826MB
2025-10-10 16:51:00 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.725000, curr=0.700000
2025-10-10 16:51:08 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 16:51:08 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 16:51:08 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 16:51:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 16:51:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:51:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:51:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 16:51:11 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=129.555634, avg_loss=0.647778, seen=200, correct=121, accuracy=0.605000
2025-10-10 16:51:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:51:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:51:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:51:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1874MB allocated=1826MB
2025-10-10 16:51:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:51:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:51:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:51:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:51:15 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.786766, avg_loss=0.619669, seen=40, correct=28, accuracy=0.700000
2025-10-10 16:51:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:51:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:51:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:51:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1874MB allocated=1826MB
2025-10-10 16:51:17 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.725000, curr=0.700000
2025-10-10 16:51:25 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 16:51:25 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 16:51:25 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 16:51:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 16:51:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:51:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:51:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 16:51:28 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=126.544601, avg_loss=0.632723, seen=200, correct=130, accuracy=0.650000
2025-10-10 16:51:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:51:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:51:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:51:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1874MB allocated=1826MB
2025-10-10 16:51:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:51:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:51:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:51:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:51:31 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=22.535744, avg_loss=0.563394, seen=40, correct=30, accuracy=0.750000
2025-10-10 16:51:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:51:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:51:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:51:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1874MB allocated=1826MB
2025-10-10 16:51:33 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.750000
2025-10-10 16:51:41 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 16:51:41 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 16:51:41 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 16:51:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 16:51:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:51:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:51:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 16:51:44 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=127.719421, avg_loss=0.638597, seen=200, correct=124, accuracy=0.620000
2025-10-10 16:51:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:51:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:51:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:51:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1874MB allocated=1826MB
2025-10-10 16:51:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:51:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:51:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:51:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:51:47 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.840021, avg_loss=0.596001, seen=40, correct=29, accuracy=0.725000
2025-10-10 16:51:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:51:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:51:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:51:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1874MB allocated=1826MB
2025-10-10 16:51:48 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.750000, curr=0.725000
2025-10-10 16:51:57 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 16:51:57 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 16:51:57 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 16:51:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 16:51:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:51:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:52:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 16:52:00 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=128.043900, avg_loss=0.640219, seen=200, correct=122, accuracy=0.610000
2025-10-10 16:52:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:52:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:52:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:52:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1874MB allocated=1826MB
2025-10-10 16:52:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:52:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:52:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:52:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:52:04 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.942347, avg_loss=0.598559, seen=40, correct=29, accuracy=0.725000
2025-10-10 16:52:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:52:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:52:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:52:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1874MB allocated=1826MB
2025-10-10 16:52:06 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.750000, curr=0.725000
2025-10-10 16:52:13 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 16:52:13 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 16:52:13 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 16:52:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 16:52:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:52:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:52:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 16:52:16 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=127.099426, avg_loss=0.635497, seen=200, correct=131, accuracy=0.655000
2025-10-10 16:52:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:52:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:52:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:52:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1874MB allocated=1826MB
2025-10-10 16:52:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:52:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:52:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:52:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:52:20 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=22.965500, avg_loss=0.574137, seen=40, correct=30, accuracy=0.750000
2025-10-10 16:52:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:52:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:52:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:52:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1874MB allocated=1826MB
2025-10-10 16:52:22 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.750000
2025-10-10 16:52:31 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 16:52:31 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 16:52:31 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 16:52:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 16:52:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:52:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:52:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 16:52:34 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=127.710083, avg_loss=0.638550, seen=200, correct=125, accuracy=0.625000
2025-10-10 16:52:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:52:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:52:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:52:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1874MB allocated=1826MB
2025-10-10 16:52:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:52:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:52:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:52:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:52:38 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.722576, avg_loss=0.593064, seen=40, correct=29, accuracy=0.725000
2025-10-10 16:52:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:52:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:52:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:52:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1874MB allocated=1826MB
2025-10-10 16:52:39 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.750000, curr=0.725000
2025-10-10 16:52:47 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 16:52:47 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 16:52:47 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 16:52:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 16:52:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:52:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:52:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 16:52:50 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=126.990433, avg_loss=0.634952, seen=200, correct=126, accuracy=0.630000
2025-10-10 16:52:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:52:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:52:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:52:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1874MB allocated=1826MB
2025-10-10 16:52:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:52:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:52:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:52:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:52:54 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.924614, avg_loss=0.598115, seen=40, correct=28, accuracy=0.700000
2025-10-10 16:52:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:52:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:52:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:52:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1874MB allocated=1826MB
2025-10-10 16:52:55 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.750000, curr=0.700000
2025-10-10 16:53:03 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 16:53:03 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 16:53:03 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 16:53:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 16:53:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:53:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:53:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 16:53:06 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=125.683655, avg_loss=0.628418, seen=200, correct=129, accuracy=0.645000
2025-10-10 16:53:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:53:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:53:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:53:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1874MB allocated=1826MB
2025-10-10 16:53:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:53:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:53:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:53:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:53:09 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=22.602776, avg_loss=0.565069, seen=40, correct=29, accuracy=0.725000
2025-10-10 16:53:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:53:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:53:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:53:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1874MB allocated=1826MB
2025-10-10 16:53:11 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.750000, curr=0.725000
2025-10-10 16:53:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 16:53:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 16:53:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:53:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:53:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1874MB allocated=1826MB
2025-10-10 16:53:12 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #34', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 16:53:12 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #34', 'Round': 0, 'Results_raw': {}}
2025-10-10 16:53:12 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 16:53:13 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 16:53:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:53:14 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 16:53:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:53:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:53:15 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 16:53:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-10-10 16:53:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:53:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:53:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-10-10 16:53:18 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=137, loss_sum=87.667885, avg_loss=0.639912, seen=137, correct=87, accuracy=0.635036
2025-10-10 16:53:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:53:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:53:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:53:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1862MB allocated=1818MB
2025-10-10 16:53:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:53:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:53:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:53:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:53:22 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.309948, avg_loss=0.607749, seen=40, correct=26, accuracy=0.650000
2025-10-10 16:53:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:53:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:53:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:53:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1862MB allocated=1818MB
2025-10-10 16:53:23 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.650000
2025-10-10 16:53:23 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 16:53:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=652, total=2605)
2025-10-10 16:53:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:53:23 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 16:53:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:53:23 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 16:53:23 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=326, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 16:53:31 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 16:53:31 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 16:53:31 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 16:53:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-10-10 16:53:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:53:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:53:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-10-10 16:53:33 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=137, loss_sum=87.371895, avg_loss=0.637751, seen=137, correct=88, accuracy=0.642336
2025-10-10 16:53:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:53:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:53:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:53:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1896MB allocated=1835MB
2025-10-10 16:53:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:53:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:53:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:53:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:53:37 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.285791, avg_loss=0.632145, seen=40, correct=23, accuracy=0.575000
2025-10-10 16:53:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:53:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:53:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:53:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1896MB allocated=1835MB
2025-10-10 16:53:38 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.650000, curr=0.575000
2025-10-10 16:53:46 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 16:53:46 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 16:53:46 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 16:53:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-10-10 16:53:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:53:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:53:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-10-10 16:53:48 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=137, loss_sum=87.548073, avg_loss=0.639037, seen=137, correct=88, accuracy=0.642336
2025-10-10 16:53:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:53:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:53:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:53:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1896MB allocated=1835MB
2025-10-10 16:53:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:53:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:53:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:53:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:53:52 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.881393, avg_loss=0.597035, seen=40, correct=27, accuracy=0.675000
2025-10-10 16:53:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:53:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:53:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:53:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1896MB allocated=1835MB
2025-10-10 16:53:53 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.675000
2025-10-10 16:54:01 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 16:54:01 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 16:54:01 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 16:54:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-10-10 16:54:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:54:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:54:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-10-10 16:54:03 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=137, loss_sum=87.373940, avg_loss=0.637766, seen=137, correct=87, accuracy=0.635036
2025-10-10 16:54:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:54:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:54:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:54:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1896MB allocated=1835MB
2025-10-10 16:54:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:54:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:54:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:54:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:54:06 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.271797, avg_loss=0.606795, seen=40, correct=26, accuracy=0.650000
2025-10-10 16:54:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:54:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:54:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:54:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1896MB allocated=1835MB
2025-10-10 16:54:08 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.675000, curr=0.650000
2025-10-10 16:54:17 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 16:54:17 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 16:54:17 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 16:54:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-10-10 16:54:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:54:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:54:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-10-10 16:54:19 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=137, loss_sum=87.309349, avg_loss=0.637295, seen=137, correct=86, accuracy=0.627737
2025-10-10 16:54:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:54:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:54:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:54:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1896MB allocated=1835MB
2025-10-10 16:54:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:54:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:54:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:54:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:54:23 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.174480, avg_loss=0.604362, seen=40, correct=26, accuracy=0.650000
2025-10-10 16:54:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:54:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:54:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:54:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1896MB allocated=1835MB
2025-10-10 16:54:24 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.675000, curr=0.650000
2025-10-10 16:54:31 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 16:54:31 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 16:54:31 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 16:54:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-10-10 16:54:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:54:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:54:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-10-10 16:54:33 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=137, loss_sum=87.821838, avg_loss=0.641035, seen=137, correct=87, accuracy=0.635036
2025-10-10 16:54:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:54:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:54:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:54:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1896MB allocated=1835MB
2025-10-10 16:54:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:54:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:54:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:54:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:54:36 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.549541, avg_loss=0.613739, seen=40, correct=25, accuracy=0.625000
2025-10-10 16:54:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:54:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:54:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:54:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1896MB allocated=1835MB
2025-10-10 16:54:38 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.675000, curr=0.625000
2025-10-10 16:54:45 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 16:54:45 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 16:54:45 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 16:54:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-10-10 16:54:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:54:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:54:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-10-10 16:54:47 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=137, loss_sum=89.938843, avg_loss=0.656488, seen=137, correct=83, accuracy=0.605839
2025-10-10 16:54:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:54:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:54:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:54:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1896MB allocated=1835MB
2025-10-10 16:54:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:54:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:54:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:54:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:54:51 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.507130, avg_loss=0.612678, seen=40, correct=26, accuracy=0.650000
2025-10-10 16:54:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:54:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:54:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:54:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1896MB allocated=1835MB
2025-10-10 16:54:53 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/25), best=0.675000, curr=0.650000
2025-10-10 16:55:02 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 16:55:02 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 16:55:02 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 16:55:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-10-10 16:55:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:55:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:55:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-10-10 16:55:04 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=137, loss_sum=86.017181, avg_loss=0.627863, seen=137, correct=85, accuracy=0.620438
2025-10-10 16:55:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:55:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:55:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:55:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1896MB allocated=1835MB
2025-10-10 16:55:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:55:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:55:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:55:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:55:08 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.684326, avg_loss=0.617108, seen=40, correct=26, accuracy=0.650000
2025-10-10 16:55:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:55:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:55:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:55:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1896MB allocated=1835MB
2025-10-10 16:55:10 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/25), best=0.675000, curr=0.650000
2025-10-10 16:55:17 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 16:55:17 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 16:55:17 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 16:55:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-10-10 16:55:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:55:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:55:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-10-10 16:55:19 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=137, loss_sum=87.258194, avg_loss=0.636921, seen=137, correct=88, accuracy=0.642336
2025-10-10 16:55:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:55:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:55:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:55:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1896MB allocated=1835MB
2025-10-10 16:55:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:55:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:55:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:55:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:55:22 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.514809, avg_loss=0.637870, seen=40, correct=24, accuracy=0.600000
2025-10-10 16:55:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:55:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:55:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:55:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1896MB allocated=1835MB
2025-10-10 16:55:24 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/25), best=0.675000, curr=0.600000
2025-10-10 16:55:32 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 16:55:32 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 16:55:32 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 16:55:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-10-10 16:55:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:55:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:55:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-10-10 16:55:34 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=137, loss_sum=87.211235, avg_loss=0.636578, seen=137, correct=89, accuracy=0.649635
2025-10-10 16:55:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:55:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:55:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:55:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1896MB allocated=1835MB
2025-10-10 16:55:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:55:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:55:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:55:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:55:37 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.340363, avg_loss=0.633509, seen=40, correct=25, accuracy=0.625000
2025-10-10 16:55:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:55:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:55:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:55:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1896MB allocated=1835MB
2025-10-10 16:55:38 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/25), best=0.675000, curr=0.625000
2025-10-10 16:55:46 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 16:55:46 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 16:55:46 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 16:55:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-10-10 16:55:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:55:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:55:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-10-10 16:55:48 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=137, loss_sum=86.676933, avg_loss=0.632678, seen=137, correct=87, accuracy=0.635036
2025-10-10 16:55:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:55:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:55:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:55:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1896MB allocated=1835MB
2025-10-10 16:55:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:55:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:55:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:55:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:55:52 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.957977, avg_loss=0.598949, seen=40, correct=28, accuracy=0.700000
2025-10-10 16:55:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:55:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:55:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:55:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1896MB allocated=1835MB
2025-10-10 16:55:53 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.700000
2025-10-10 16:55:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 16:55:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 16:55:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:55:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:55:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1896MB allocated=1835MB
2025-10-10 16:55:55 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #12', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 16:55:55 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #12', 'Round': 0, 'Results_raw': {}}
2025-10-10 16:55:55 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 16:55:56 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 16:55:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:55:56 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 16:55:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:55:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:55:57 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 16:55:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-10-10 16:55:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:55:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:55:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-10-10 16:55:58 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=36, loss_sum=26.040943, avg_loss=0.723360, seen=36, correct=17, accuracy=0.472222
2025-10-10 16:55:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:55:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:55:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:56:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1862MB allocated=1826MB
2025-10-10 16:56:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:56:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:56:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:56:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:56:01 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.269566, avg_loss=0.706739, seen=40, correct=21, accuracy=0.525000
2025-10-10 16:56:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:56:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:56:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:56:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1862MB allocated=1826MB
2025-10-10 16:56:03 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.525000
2025-10-10 16:56:03 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 16:56:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=173, total=691)
2025-10-10 16:56:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:56:03 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 16:56:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:56:03 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 16:56:03 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=87, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 16:56:12 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 16:56:12 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 16:56:12 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 16:56:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-10-10 16:56:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:56:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:56:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-10-10 16:56:12 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=36, loss_sum=24.581551, avg_loss=0.682821, seen=36, correct=21, accuracy=0.583333
2025-10-10 16:56:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:56:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:56:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:56:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1890MB allocated=1843MB
2025-10-10 16:56:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:56:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:56:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:56:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:56:16 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.418993, avg_loss=0.685475, seen=40, correct=23, accuracy=0.575000
2025-10-10 16:56:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:56:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:56:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:56:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1890MB allocated=1843MB
2025-10-10 16:56:18 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-10 16:56:26 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 16:56:26 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 16:56:26 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 16:56:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-10-10 16:56:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:56:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:56:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-10-10 16:56:27 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=36, loss_sum=24.268732, avg_loss=0.674131, seen=36, correct=21, accuracy=0.583333
2025-10-10 16:56:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:56:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:56:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:56:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1890MB allocated=1843MB
2025-10-10 16:56:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:56:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:56:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:56:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:56:31 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.683220, avg_loss=0.692080, seen=40, correct=23, accuracy=0.575000
2025-10-10 16:56:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:56:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:56:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:56:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1890MB allocated=1843MB
2025-10-10 16:56:32 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-10 16:56:39 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 16:56:39 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 16:56:39 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 16:56:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-10-10 16:56:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:56:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:56:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-10-10 16:56:40 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=36, loss_sum=25.169586, avg_loss=0.699155, seen=36, correct=25, accuracy=0.694444
2025-10-10 16:56:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:56:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:56:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:56:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1890MB allocated=1843MB
2025-10-10 16:56:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:56:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:56:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:56:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:56:44 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.853060, avg_loss=0.696326, seen=40, correct=21, accuracy=0.525000
2025-10-10 16:56:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:56:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:56:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:56:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1890MB allocated=1843MB
2025-10-10 16:56:45 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.575000, curr=0.525000
2025-10-10 16:56:53 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 16:56:53 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 16:56:53 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 16:56:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-10-10 16:56:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:56:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:56:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-10-10 16:56:54 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=36, loss_sum=25.369158, avg_loss=0.704699, seen=36, correct=21, accuracy=0.583333
2025-10-10 16:56:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:56:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:56:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:56:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1890MB allocated=1843MB
2025-10-10 16:56:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:56:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:56:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:56:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:56:58 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.819897, avg_loss=0.695497, seen=40, correct=22, accuracy=0.550000
2025-10-10 16:56:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:56:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:56:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:57:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1890MB allocated=1843MB
2025-10-10 16:57:00 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.575000, curr=0.550000
2025-10-10 16:57:09 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 16:57:09 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 16:57:09 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 16:57:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-10-10 16:57:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:57:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:57:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-10-10 16:57:09 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=36, loss_sum=25.997574, avg_loss=0.722155, seen=36, correct=21, accuracy=0.583333
2025-10-10 16:57:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:57:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:57:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:57:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1890MB allocated=1843MB
2025-10-10 16:57:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:57:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:57:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:57:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:57:14 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.011555, avg_loss=0.700289, seen=40, correct=22, accuracy=0.550000
2025-10-10 16:57:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:57:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:57:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:57:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1890MB allocated=1843MB
2025-10-10 16:57:16 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.575000, curr=0.550000
2025-10-10 16:57:23 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 16:57:23 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 16:57:23 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 16:57:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-10-10 16:57:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:57:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:57:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-10-10 16:57:24 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=36, loss_sum=24.693520, avg_loss=0.685931, seen=36, correct=24, accuracy=0.666667
2025-10-10 16:57:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:57:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:57:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:57:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1890MB allocated=1843MB
2025-10-10 16:57:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:57:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:57:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:57:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:57:28 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.579588, avg_loss=0.689490, seen=40, correct=24, accuracy=0.600000
2025-10-10 16:57:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:57:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:57:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:57:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1890MB allocated=1843MB
2025-10-10 16:57:30 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-10 16:57:37 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 16:57:37 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 16:57:37 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 16:57:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-10-10 16:57:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:57:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:57:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-10-10 16:57:38 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=36, loss_sum=24.672935, avg_loss=0.685359, seen=36, correct=22, accuracy=0.611111
2025-10-10 16:57:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:57:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:57:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:57:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1890MB allocated=1843MB
2025-10-10 16:57:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:57:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:57:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:57:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:57:41 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.421993, avg_loss=0.685550, seen=40, correct=24, accuracy=0.600000
2025-10-10 16:57:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:57:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:57:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:57:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1890MB allocated=1843MB
2025-10-10 16:57:42 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-10 16:57:50 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 16:57:50 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 16:57:50 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 16:57:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-10-10 16:57:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:57:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:57:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-10-10 16:57:51 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=36, loss_sum=25.834629, avg_loss=0.717629, seen=36, correct=20, accuracy=0.555556
2025-10-10 16:57:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:57:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:57:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:57:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1890MB allocated=1843MB
2025-10-10 16:57:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:57:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:57:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:57:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:57:54 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.852371, avg_loss=0.696309, seen=40, correct=21, accuracy=0.525000
2025-10-10 16:57:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:57:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:57:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:57:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1890MB allocated=1843MB
2025-10-10 16:57:55 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.600000, curr=0.525000
2025-10-10 16:58:03 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 16:58:03 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 16:58:03 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 16:58:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-10-10 16:58:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:58:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:58:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-10-10 16:58:04 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=36, loss_sum=26.199747, avg_loss=0.727771, seen=36, correct=16, accuracy=0.444444
2025-10-10 16:58:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:58:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:58:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:58:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1890MB allocated=1843MB
2025-10-10 16:58:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:58:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:58:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:58:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:58:07 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.776897, avg_loss=0.694422, seen=40, correct=22, accuracy=0.550000
2025-10-10 16:58:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:58:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:58:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:58:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1890MB allocated=1843MB
2025-10-10 16:58:09 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.600000, curr=0.550000
2025-10-10 16:58:16 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 16:58:16 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 16:58:16 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 16:58:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-10-10 16:58:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:58:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:58:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-10-10 16:58:17 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=36, loss_sum=25.706242, avg_loss=0.714062, seen=36, correct=21, accuracy=0.583333
2025-10-10 16:58:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:58:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:58:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:58:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1890MB allocated=1843MB
2025-10-10 16:58:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:58:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:58:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:58:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:58:20 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.714514, avg_loss=0.692863, seen=40, correct=20, accuracy=0.500000
2025-10-10 16:58:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:58:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:58:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:58:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1890MB allocated=1843MB
2025-10-10 16:58:21 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.600000, curr=0.500000
2025-10-10 16:58:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 16:58:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 16:58:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:58:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:58:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1890MB allocated=1843MB
2025-10-10 16:58:22 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #3', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 16:58:22 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #3', 'Round': 0, 'Results_raw': {}}
2025-10-10 16:58:22 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 16:58:23 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 16:58:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:58:23 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 16:58:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:58:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:58:24 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 16:58:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-10-10 16:58:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:58:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:58:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-10 16:58:26 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=112, loss_sum=76.217812, avg_loss=0.680516, seen=112, correct=59, accuracy=0.526786
2025-10-10 16:58:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:58:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:58:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:58:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1862MB allocated=1835MB
2025-10-10 16:58:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:58:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:58:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:58:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:58:30 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.877985, avg_loss=0.746950, seen=40, correct=17, accuracy=0.425000
2025-10-10 16:58:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:58:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:58:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:58:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1862MB allocated=1835MB
2025-10-10 16:58:31 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.425000
2025-10-10 16:58:31 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 16:58:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=536, total=2144)
2025-10-10 16:58:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:58:32 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 16:58:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:58:32 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 16:58:32 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=268, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 16:58:39 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 16:58:39 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 16:58:39 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 16:58:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-10-10 16:58:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:58:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:58:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-10 16:58:41 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=112, loss_sum=73.845428, avg_loss=0.659334, seen=112, correct=72, accuracy=0.642857
2025-10-10 16:58:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:58:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:58:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:58:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1884MB allocated=1851MB
2025-10-10 16:58:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:58:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:58:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:58:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:58:44 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.340469, avg_loss=0.708512, seen=40, correct=22, accuracy=0.550000
2025-10-10 16:58:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:58:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:58:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:58:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1884MB allocated=1851MB
2025-10-10 16:58:46 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.550000
2025-10-10 16:58:53 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 16:58:53 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 16:58:53 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 16:58:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-10-10 16:58:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:58:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:58:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-10 16:58:55 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=112, loss_sum=73.825226, avg_loss=0.659154, seen=112, correct=63, accuracy=0.562500
2025-10-10 16:58:55 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:58:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:58:56 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:58:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1884MB allocated=1851MB
2025-10-10 16:58:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:58:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:58:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:58:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:58:58 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.798985, avg_loss=0.719975, seen=40, correct=19, accuracy=0.475000
2025-10-10 16:58:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:58:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:58:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:58:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1884MB allocated=1851MB
2025-10-10 16:58:59 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.550000, curr=0.475000
2025-10-10 16:59:07 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 16:59:07 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 16:59:07 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 16:59:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-10-10 16:59:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:59:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:59:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-10 16:59:09 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=112, loss_sum=74.807777, avg_loss=0.667927, seen=112, correct=61, accuracy=0.544643
2025-10-10 16:59:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:59:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:59:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:59:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1884MB allocated=1851MB
2025-10-10 16:59:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:59:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:59:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:59:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:59:13 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.021393, avg_loss=0.725535, seen=40, correct=19, accuracy=0.475000
2025-10-10 16:59:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:59:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:59:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:59:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1884MB allocated=1851MB
2025-10-10 16:59:14 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.550000, curr=0.475000
2025-10-10 16:59:22 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 16:59:22 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 16:59:22 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 16:59:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-10-10 16:59:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:59:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:59:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-10 16:59:24 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=112, loss_sum=73.430832, avg_loss=0.655632, seen=112, correct=63, accuracy=0.562500
2025-10-10 16:59:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:59:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:59:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:59:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1884MB allocated=1851MB
2025-10-10 16:59:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:59:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:59:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:59:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:59:28 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.489899, avg_loss=0.712247, seen=40, correct=20, accuracy=0.500000
2025-10-10 16:59:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:59:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:59:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:59:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1884MB allocated=1851MB
2025-10-10 16:59:29 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.550000, curr=0.500000
2025-10-10 16:59:37 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 16:59:37 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 16:59:37 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 16:59:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-10-10 16:59:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:59:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:59:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-10 16:59:38 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=112, loss_sum=72.579453, avg_loss=0.648031, seen=112, correct=71, accuracy=0.633929
2025-10-10 16:59:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:59:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:59:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:59:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1884MB allocated=1851MB
2025-10-10 16:59:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:59:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:59:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:59:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:59:42 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.539797, avg_loss=0.688495, seen=40, correct=24, accuracy=0.600000
2025-10-10 16:59:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:59:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:59:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:59:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1884MB allocated=1851MB
2025-10-10 16:59:43 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-10 16:59:52 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 16:59:52 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 16:59:52 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 16:59:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-10-10 16:59:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:59:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:59:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-10 16:59:54 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=112, loss_sum=74.451790, avg_loss=0.664748, seen=112, correct=69, accuracy=0.616071
2025-10-10 16:59:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:59:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:59:56 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:59:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1884MB allocated=1851MB
2025-10-10 16:59:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 16:59:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:59:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 16:59:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 16:59:58 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.554115, avg_loss=0.688853, seen=40, correct=25, accuracy=0.625000
2025-10-10 16:59:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 16:59:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 16:59:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 16:59:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1884MB allocated=1851MB
2025-10-10 16:59:59 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-10 17:00:08 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 17:00:08 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 17:00:08 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 17:00:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-10-10 17:00:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:00:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:00:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-10 17:00:10 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=112, loss_sum=71.954857, avg_loss=0.642454, seen=112, correct=73, accuracy=0.651786
2025-10-10 17:00:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:00:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:00:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:00:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1884MB allocated=1851MB
2025-10-10 17:00:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:00:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:00:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:00:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:00:14 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.714870, avg_loss=0.692872, seen=40, correct=24, accuracy=0.600000
2025-10-10 17:00:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:00:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:00:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:00:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1884MB allocated=1851MB
2025-10-10 17:00:16 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.625000, curr=0.600000
2025-10-10 17:00:23 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 17:00:23 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 17:00:23 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 17:00:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-10-10 17:00:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:00:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:00:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-10 17:00:25 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=112, loss_sum=72.574989, avg_loss=0.647991, seen=112, correct=67, accuracy=0.598214
2025-10-10 17:00:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:00:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:00:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:00:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1884MB allocated=1851MB
2025-10-10 17:00:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:00:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:00:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:00:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:00:29 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.572996, avg_loss=0.714325, seen=40, correct=20, accuracy=0.500000
2025-10-10 17:00:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:00:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:00:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:00:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1884MB allocated=1851MB
2025-10-10 17:00:31 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.625000, curr=0.500000
2025-10-10 17:00:39 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 17:00:39 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 17:00:39 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 17:00:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-10-10 17:00:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:00:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:00:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-10 17:00:41 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=112, loss_sum=73.653175, avg_loss=0.657618, seen=112, correct=66, accuracy=0.589286
2025-10-10 17:00:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:00:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:00:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:00:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1884MB allocated=1851MB
2025-10-10 17:00:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:00:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:00:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:00:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:00:45 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.889648, avg_loss=0.747241, seen=40, correct=17, accuracy=0.425000
2025-10-10 17:00:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:00:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:00:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:00:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1884MB allocated=1851MB
2025-10-10 17:00:47 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.625000, curr=0.425000
2025-10-10 17:00:54 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 17:00:54 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 17:00:54 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 17:00:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-10-10 17:00:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:00:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:00:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-10 17:00:56 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=112, loss_sum=75.195580, avg_loss=0.671389, seen=112, correct=59, accuracy=0.526786
2025-10-10 17:00:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:00:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:00:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:00:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1884MB allocated=1851MB
2025-10-10 17:00:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:00:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:00:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:01:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:01:00 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.357565, avg_loss=0.758939, seen=40, correct=19, accuracy=0.475000
2025-10-10 17:01:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:01:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:01:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:01:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1884MB allocated=1851MB
2025-10-10 17:01:01 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/25), best=0.625000, curr=0.475000
2025-10-10 17:01:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 17:01:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 17:01:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:01:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:01:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1884MB allocated=1851MB
2025-10-10 17:01:02 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #32', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 17:01:02 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #32', 'Round': 0, 'Results_raw': {}}
2025-10-10 17:01:02 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 17:01:03 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 17:01:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:01:04 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 17:01:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:01:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:01:05 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 17:01:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:01:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:01:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:01:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:01:08 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=132.815201, avg_loss=0.664076, seen=200, correct=110, accuracy=0.550000
2025-10-10 17:01:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:01:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:01:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:01:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1882MB allocated=1843MB
2025-10-10 17:01:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:01:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:01:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:01:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:01:12 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.653435, avg_loss=0.641336, seen=40, correct=27, accuracy=0.675000
2025-10-10 17:01:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:01:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:01:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:01:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1882MB allocated=1843MB
2025-10-10 17:01:14 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.675000
2025-10-10 17:01:14 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 17:01:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1443, total=5772)
2025-10-10 17:01:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:01:14 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 17:01:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:01:14 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 17:01:14 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=722, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 17:01:22 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 17:01:22 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 17:01:22 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 17:01:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:01:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:01:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:01:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:01:25 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=131.900085, avg_loss=0.659500, seen=200, correct=115, accuracy=0.575000
2025-10-10 17:01:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:01:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:01:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:01:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1922MB allocated=1860MB
2025-10-10 17:01:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:01:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:01:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:01:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:01:28 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.475649, avg_loss=0.636891, seen=40, correct=27, accuracy=0.675000
2025-10-10 17:01:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:01:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:01:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:01:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1922MB allocated=1860MB
2025-10-10 17:01:29 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.675000
2025-10-10 17:01:38 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 17:01:38 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 17:01:38 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 17:01:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:01:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:01:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:01:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:01:41 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=131.889084, avg_loss=0.659445, seen=200, correct=114, accuracy=0.570000
2025-10-10 17:01:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:01:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:01:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:01:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1922MB allocated=1860MB
2025-10-10 17:01:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:01:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:01:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:01:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:01:44 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.265400, avg_loss=0.631635, seen=40, correct=26, accuracy=0.650000
2025-10-10 17:01:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:01:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:01:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:01:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1922MB allocated=1860MB
2025-10-10 17:01:46 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.675000, curr=0.650000
2025-10-10 17:01:53 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 17:01:53 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 17:01:53 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 17:01:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:01:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:01:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:01:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:01:56 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=133.238556, avg_loss=0.666193, seen=200, correct=117, accuracy=0.585000
2025-10-10 17:01:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:01:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:01:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:01:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1922MB allocated=1860MB
2025-10-10 17:01:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:01:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:01:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:01:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:01:59 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.102722, avg_loss=0.627568, seen=40, correct=27, accuracy=0.675000
2025-10-10 17:01:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:01:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:01:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:02:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1922MB allocated=1860MB
2025-10-10 17:02:00 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.675000
2025-10-10 17:02:08 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 17:02:08 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 17:02:08 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 17:02:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:02:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:02:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:02:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:02:11 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=131.738907, avg_loss=0.658695, seen=200, correct=116, accuracy=0.580000
2025-10-10 17:02:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:02:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:02:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:02:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1922MB allocated=1860MB
2025-10-10 17:02:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:02:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:02:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:02:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:02:14 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.887138, avg_loss=0.622178, seen=40, correct=26, accuracy=0.650000
2025-10-10 17:02:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:02:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:02:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:02:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1922MB allocated=1860MB
2025-10-10 17:02:16 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.675000, curr=0.650000
2025-10-10 17:02:24 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 17:02:24 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 17:02:24 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 17:02:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:02:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:02:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:02:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:02:27 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=131.821396, avg_loss=0.659107, seen=200, correct=116, accuracy=0.580000
2025-10-10 17:02:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:02:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:02:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:02:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1922MB allocated=1860MB
2025-10-10 17:02:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:02:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:02:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:02:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:02:31 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.501904, avg_loss=0.612548, seen=40, correct=25, accuracy=0.625000
2025-10-10 17:02:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:02:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:02:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:02:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1922MB allocated=1860MB
2025-10-10 17:02:32 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.675000, curr=0.625000
2025-10-10 17:02:41 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 17:02:41 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 17:02:41 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 17:02:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:02:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:02:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:02:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:02:44 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=131.620773, avg_loss=0.658104, seen=200, correct=118, accuracy=0.590000
2025-10-10 17:02:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:02:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:02:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:02:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1922MB allocated=1860MB
2025-10-10 17:02:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:02:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:02:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:02:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:02:47 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.327469, avg_loss=0.608187, seen=40, correct=26, accuracy=0.650000
2025-10-10 17:02:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:02:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:02:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:02:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1922MB allocated=1860MB
2025-10-10 17:02:48 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.675000, curr=0.650000
2025-10-10 17:02:55 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 17:02:55 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 17:02:55 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 17:02:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:02:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:02:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:02:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:02:58 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=135.343384, avg_loss=0.676717, seen=200, correct=120, accuracy=0.600000
2025-10-10 17:02:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:02:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:02:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:02:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1922MB allocated=1860MB
2025-10-10 17:03:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:03:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:03:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:03:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:03:01 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.469431, avg_loss=0.611736, seen=40, correct=27, accuracy=0.675000
2025-10-10 17:03:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:03:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:03:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:03:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1922MB allocated=1860MB
2025-10-10 17:03:02 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.675000
2025-10-10 17:03:11 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 17:03:11 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 17:03:11 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 17:03:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:03:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:03:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:03:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:03:14 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=131.314453, avg_loss=0.656572, seen=200, correct=121, accuracy=0.605000
2025-10-10 17:03:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:03:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:03:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:03:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1922MB allocated=1860MB
2025-10-10 17:03:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:03:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:03:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:03:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:03:18 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.171993, avg_loss=0.604300, seen=40, correct=25, accuracy=0.625000
2025-10-10 17:03:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:03:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:03:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:03:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1922MB allocated=1860MB
2025-10-10 17:03:19 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.675000, curr=0.625000
2025-10-10 17:03:27 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 17:03:27 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 17:03:27 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 17:03:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:03:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:03:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:03:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:03:30 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=130.891068, avg_loss=0.654455, seen=200, correct=123, accuracy=0.615000
2025-10-10 17:03:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:03:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:03:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:03:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1922MB allocated=1860MB
2025-10-10 17:03:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:03:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:03:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:03:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:03:35 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.133286, avg_loss=0.603332, seen=40, correct=25, accuracy=0.625000
2025-10-10 17:03:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:03:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:03:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:03:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1922MB allocated=1860MB
2025-10-10 17:03:36 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.675000, curr=0.625000
2025-10-10 17:03:45 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 17:03:45 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 17:03:45 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 17:03:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:03:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:03:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:03:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:03:48 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=132.280609, avg_loss=0.661403, seen=200, correct=120, accuracy=0.600000
2025-10-10 17:03:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:03:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:03:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:03:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1922MB allocated=1860MB
2025-10-10 17:03:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:03:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:03:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:03:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:03:52 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.302376, avg_loss=0.607559, seen=40, correct=25, accuracy=0.625000
2025-10-10 17:03:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:03:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:03:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:03:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1922MB allocated=1860MB
2025-10-10 17:03:53 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.675000, curr=0.625000
2025-10-10 17:03:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 17:03:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 17:03:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:03:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:03:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1922MB allocated=1860MB
2025-10-10 17:03:54 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #42', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 17:03:54 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #42', 'Round': 0, 'Results_raw': {}}
2025-10-10 17:03:55 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 17:03:55 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 17:03:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:03:56 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 17:03:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:03:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:03:57 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 17:03:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-10-10 17:03:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:03:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:03:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-10-10 17:03:59 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=170, loss_sum=113.298225, avg_loss=0.666460, seen=170, correct=99, accuracy=0.582353
2025-10-10 17:03:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:04:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:04:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:04:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1882MB allocated=1851MB
2025-10-10 17:04:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:04:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:04:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:04:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:04:03 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.487013, avg_loss=0.687175, seen=40, correct=27, accuracy=0.675000
2025-10-10 17:04:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:04:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:04:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:04:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1882MB allocated=1851MB
2025-10-10 17:04:04 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.675000
2025-10-10 17:04:04 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 17:04:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=812, total=3247)
2025-10-10 17:04:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:04:04 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 17:04:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:04:04 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 17:04:04 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=406, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 17:04:12 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 17:04:12 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 17:04:12 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 17:04:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-10-10 17:04:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:04:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:04:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-10-10 17:04:15 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=170, loss_sum=112.943703, avg_loss=0.664375, seen=170, correct=107, accuracy=0.629412
2025-10-10 17:04:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:04:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:04:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:04:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1906MB allocated=1868MB
2025-10-10 17:04:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:04:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:04:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:04:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:04:18 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.513304, avg_loss=0.662833, seen=40, correct=20, accuracy=0.500000
2025-10-10 17:04:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:04:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:04:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:04:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1906MB allocated=1868MB
2025-10-10 17:04:19 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.675000, curr=0.500000
2025-10-10 17:04:26 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 17:04:26 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 17:04:27 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 17:04:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-10-10 17:04:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:04:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:04:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-10-10 17:04:29 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=170, loss_sum=112.597809, avg_loss=0.662340, seen=170, correct=107, accuracy=0.629412
2025-10-10 17:04:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:04:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:04:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:04:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1906MB allocated=1868MB
2025-10-10 17:04:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:04:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:04:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:04:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:04:33 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.544212, avg_loss=0.688605, seen=40, correct=25, accuracy=0.625000
2025-10-10 17:04:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:04:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:04:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:04:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1906MB allocated=1868MB
2025-10-10 17:04:34 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.675000, curr=0.625000
2025-10-10 17:04:42 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 17:04:42 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 17:04:42 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 17:04:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-10-10 17:04:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:04:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:04:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-10-10 17:04:45 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=170, loss_sum=112.392700, avg_loss=0.661134, seen=170, correct=106, accuracy=0.623529
2025-10-10 17:04:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:04:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:04:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:04:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1906MB allocated=1868MB
2025-10-10 17:04:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:04:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:04:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:04:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:04:49 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.986811, avg_loss=0.674670, seen=40, correct=22, accuracy=0.550000
2025-10-10 17:04:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:04:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:04:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:04:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1906MB allocated=1868MB
2025-10-10 17:04:50 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.675000, curr=0.550000
2025-10-10 17:04:58 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 17:04:58 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 17:04:58 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 17:04:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-10-10 17:04:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:04:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:05:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-10-10 17:05:01 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=170, loss_sum=119.043663, avg_loss=0.700257, seen=170, correct=92, accuracy=0.541176
2025-10-10 17:05:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:05:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:05:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:05:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1906MB allocated=1868MB
2025-10-10 17:05:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:05:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:05:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:05:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:05:04 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.513008, avg_loss=0.662825, seen=40, correct=21, accuracy=0.525000
2025-10-10 17:05:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:05:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:05:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:05:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1906MB allocated=1868MB
2025-10-10 17:05:05 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/25), best=0.675000, curr=0.525000
2025-10-10 17:05:13 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 17:05:13 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 17:05:13 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 17:05:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-10-10 17:05:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:05:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:05:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-10-10 17:05:15 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=170, loss_sum=112.083916, avg_loss=0.659317, seen=170, correct=112, accuracy=0.658824
2025-10-10 17:05:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:05:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:05:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:05:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1906MB allocated=1868MB
2025-10-10 17:05:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:05:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:05:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:05:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:05:18 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.764425, avg_loss=0.669111, seen=40, correct=23, accuracy=0.575000
2025-10-10 17:05:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:05:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:05:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:05:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1906MB allocated=1868MB
2025-10-10 17:05:20 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/25), best=0.675000, curr=0.575000
2025-10-10 17:05:28 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 17:05:28 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 17:05:28 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 17:05:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-10-10 17:05:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:05:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:05:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-10-10 17:05:31 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=170, loss_sum=111.926941, avg_loss=0.658394, seen=170, correct=104, accuracy=0.611765
2025-10-10 17:05:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:05:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:05:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:05:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1906MB allocated=1868MB
2025-10-10 17:05:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:05:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:05:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:05:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:05:35 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.382067, avg_loss=0.684552, seen=40, correct=25, accuracy=0.625000
2025-10-10 17:05:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:05:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:05:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:05:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1906MB allocated=1868MB
2025-10-10 17:05:36 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/25), best=0.675000, curr=0.625000
2025-10-10 17:05:44 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 17:05:44 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 17:05:44 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 17:05:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-10-10 17:05:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:05:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:05:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-10-10 17:05:47 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=170, loss_sum=111.831001, avg_loss=0.657829, seen=170, correct=108, accuracy=0.635294
2025-10-10 17:05:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:05:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:05:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:05:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1906MB allocated=1868MB
2025-10-10 17:05:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:05:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:05:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:05:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:05:50 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.468237, avg_loss=0.686706, seen=40, correct=23, accuracy=0.575000
2025-10-10 17:05:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:05:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:05:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:05:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1906MB allocated=1868MB
2025-10-10 17:05:52 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/25), best=0.675000, curr=0.575000
2025-10-10 17:06:01 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 17:06:01 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 17:06:01 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 17:06:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-10-10 17:06:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:06:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:06:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-10-10 17:06:03 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=170, loss_sum=113.409645, avg_loss=0.667116, seen=170, correct=107, accuracy=0.629412
2025-10-10 17:06:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:06:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:06:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:06:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1906MB allocated=1868MB
2025-10-10 17:06:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:06:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:06:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:06:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:06:07 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.546789, avg_loss=0.663670, seen=40, correct=19, accuracy=0.475000
2025-10-10 17:06:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:06:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:06:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:06:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1906MB allocated=1868MB
2025-10-10 17:06:09 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/25), best=0.675000, curr=0.475000
2025-10-10 17:06:16 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 17:06:16 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 17:06:16 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 17:06:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-10-10 17:06:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:06:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:06:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-10-10 17:06:19 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=170, loss_sum=112.228989, avg_loss=0.660171, seen=170, correct=104, accuracy=0.611765
2025-10-10 17:06:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:06:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:06:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:06:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1906MB allocated=1868MB
2025-10-10 17:06:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:06:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:06:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:06:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:06:22 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.287289, avg_loss=0.682182, seen=40, correct=25, accuracy=0.625000
2025-10-10 17:06:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:06:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:06:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:06:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1906MB allocated=1868MB
2025-10-10 17:06:24 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=9/25), best=0.675000, curr=0.625000
2025-10-10 17:06:31 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 17:06:31 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 17:06:31 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 17:06:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-10-10 17:06:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:06:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:06:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-10-10 17:06:34 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=170, loss_sum=113.360641, avg_loss=0.666827, seen=170, correct=95, accuracy=0.558824
2025-10-10 17:06:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:06:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:06:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:06:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1906MB allocated=1868MB
2025-10-10 17:06:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:06:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:06:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:06:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:06:37 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.731302, avg_loss=0.693283, seen=40, correct=26, accuracy=0.650000
2025-10-10 17:06:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:06:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:06:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:06:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1906MB allocated=1868MB
2025-10-10 17:06:39 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=10/25), best=0.675000, curr=0.650000
2025-10-10 17:06:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 17:06:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 17:06:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:06:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:06:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1906MB allocated=1868MB
2025-10-10 17:06:40 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #30', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 17:06:40 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #30', 'Round': 0, 'Results_raw': {}}
2025-10-10 17:06:40 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 17:06:41 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 17:06:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:06:41 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 17:06:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:06:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:06:42 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 17:06:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-10-10 17:06:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:06:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:06:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-10-10 17:06:44 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=123, loss_sum=82.836990, avg_loss=0.673471, seen=123, correct=71, accuracy=0.577236
2025-10-10 17:06:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:06:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:06:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:06:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1882MB allocated=1860MB
2025-10-10 17:06:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:06:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:06:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:06:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:06:48 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.064512, avg_loss=0.751613, seen=40, correct=18, accuracy=0.450000
2025-10-10 17:06:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:06:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:06:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:06:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1882MB allocated=1860MB
2025-10-10 17:06:49 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.450000
2025-10-10 17:06:49 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 17:06:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=586, total=2342)
2025-10-10 17:06:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:06:50 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 17:06:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:06:50 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 17:06:50 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=293, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 17:06:57 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 17:06:57 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 17:06:57 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 17:06:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-10-10 17:06:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:06:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:06:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-10-10 17:06:59 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=123, loss_sum=82.725006, avg_loss=0.672561, seen=123, correct=69, accuracy=0.560976
2025-10-10 17:06:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:06:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:07:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:07:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1916MB allocated=1877MB
2025-10-10 17:07:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:07:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:07:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:07:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:07:03 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.952152, avg_loss=0.748804, seen=40, correct=19, accuracy=0.475000
2025-10-10 17:07:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:07:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:07:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:07:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1916MB allocated=1877MB
2025-10-10 17:07:04 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.475000
2025-10-10 17:07:11 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 17:07:11 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 17:07:11 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 17:07:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-10-10 17:07:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:07:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:07:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-10-10 17:07:13 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=123, loss_sum=82.445389, avg_loss=0.670288, seen=123, correct=72, accuracy=0.585366
2025-10-10 17:07:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:07:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:07:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:07:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1916MB allocated=1877MB
2025-10-10 17:07:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:07:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:07:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:07:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:07:16 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.202560, avg_loss=0.730064, seen=40, correct=19, accuracy=0.475000
2025-10-10 17:07:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:07:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:07:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:07:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1916MB allocated=1877MB
2025-10-10 17:07:17 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.475000
2025-10-10 17:07:25 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 17:07:25 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 17:07:25 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 17:07:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-10-10 17:07:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:07:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:07:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-10-10 17:07:27 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=123, loss_sum=82.258553, avg_loss=0.668769, seen=123, correct=70, accuracy=0.569106
2025-10-10 17:07:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:07:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:07:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:07:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1916MB allocated=1877MB
2025-10-10 17:07:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:07:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:07:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:07:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:07:31 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.419470, avg_loss=0.710487, seen=40, correct=19, accuracy=0.475000
2025-10-10 17:07:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:07:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:07:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:07:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1916MB allocated=1877MB
2025-10-10 17:07:32 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.475000
2025-10-10 17:07:40 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 17:07:40 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 17:07:40 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 17:07:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-10-10 17:07:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:07:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:07:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-10-10 17:07:43 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=123, loss_sum=81.858917, avg_loss=0.665520, seen=123, correct=77, accuracy=0.626016
2025-10-10 17:07:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:07:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:07:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:07:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1916MB allocated=1877MB
2025-10-10 17:07:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:07:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:07:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:07:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:07:46 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.172747, avg_loss=0.679319, seen=40, correct=23, accuracy=0.575000
2025-10-10 17:07:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:07:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:07:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:07:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1916MB allocated=1877MB
2025-10-10 17:07:47 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-10 17:07:55 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 17:07:55 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 17:07:55 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 17:07:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-10-10 17:07:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:07:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:07:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-10-10 17:07:57 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=123, loss_sum=81.859390, avg_loss=0.665523, seen=123, correct=75, accuracy=0.609756
2025-10-10 17:07:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:07:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:07:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:07:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1916MB allocated=1877MB
2025-10-10 17:07:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:07:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:07:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:08:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:08:01 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.311354, avg_loss=0.707784, seen=40, correct=21, accuracy=0.525000
2025-10-10 17:08:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:08:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:08:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:08:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1916MB allocated=1877MB
2025-10-10 17:08:02 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.575000, curr=0.525000
2025-10-10 17:08:10 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 17:08:10 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 17:08:10 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 17:08:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-10-10 17:08:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:08:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:08:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-10-10 17:08:12 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=123, loss_sum=82.329330, avg_loss=0.669344, seen=123, correct=74, accuracy=0.601626
2025-10-10 17:08:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:08:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:08:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:08:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1916MB allocated=1877MB
2025-10-10 17:08:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:08:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:08:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:08:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:08:16 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.279917, avg_loss=0.706998, seen=40, correct=19, accuracy=0.475000
2025-10-10 17:08:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:08:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:08:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:08:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1916MB allocated=1877MB
2025-10-10 17:08:17 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.575000, curr=0.475000
2025-10-10 17:08:26 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 17:08:26 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 17:08:26 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 17:08:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-10-10 17:08:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:08:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:08:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-10-10 17:08:28 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=123, loss_sum=81.915794, avg_loss=0.665982, seen=123, correct=70, accuracy=0.569106
2025-10-10 17:08:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:08:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:08:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:08:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1916MB allocated=1877MB
2025-10-10 17:08:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:08:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:08:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:08:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:08:31 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.272116, avg_loss=0.681803, seen=40, correct=24, accuracy=0.600000
2025-10-10 17:08:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:08:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:08:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:08:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1916MB allocated=1877MB
2025-10-10 17:08:32 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-10 17:08:39 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 17:08:39 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 17:08:39 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 17:08:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-10-10 17:08:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:08:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:08:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-10-10 17:08:41 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=123, loss_sum=81.960068, avg_loss=0.666342, seen=123, correct=77, accuracy=0.626016
2025-10-10 17:08:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:08:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:08:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:08:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1916MB allocated=1877MB
2025-10-10 17:08:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:08:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:08:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:08:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:08:46 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.468222, avg_loss=0.661706, seen=40, correct=24, accuracy=0.600000
2025-10-10 17:08:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:08:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:08:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:08:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1916MB allocated=1877MB
2025-10-10 17:08:47 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-10 17:08:56 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 17:08:56 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 17:08:56 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 17:08:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-10-10 17:08:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:08:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:08:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-10-10 17:08:58 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=123, loss_sum=80.910126, avg_loss=0.657806, seen=123, correct=81, accuracy=0.658537
2025-10-10 17:08:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:08:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:08:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:09:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1916MB allocated=1877MB
2025-10-10 17:09:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:09:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:09:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:09:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:09:02 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.886806, avg_loss=0.672170, seen=40, correct=24, accuracy=0.600000
2025-10-10 17:09:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:09:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:09:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:09:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1916MB allocated=1877MB
2025-10-10 17:09:03 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-10 17:09:12 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 17:09:12 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 17:09:12 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 17:09:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-10-10 17:09:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:09:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:09:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-10-10 17:09:14 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=123, loss_sum=81.459106, avg_loss=0.662269, seen=123, correct=71, accuracy=0.577236
2025-10-10 17:09:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:09:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:09:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:09:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1916MB allocated=1877MB
2025-10-10 17:09:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:09:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:09:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:09:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:09:18 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.649553, avg_loss=0.666239, seen=40, correct=21, accuracy=0.525000
2025-10-10 17:09:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:09:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:09:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:09:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1916MB allocated=1877MB
2025-10-10 17:09:19 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.600000, curr=0.525000
2025-10-10 17:09:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 17:09:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 17:09:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:09:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:09:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1916MB allocated=1877MB
2025-10-10 17:09:21 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #27', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 17:09:21 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #27', 'Round': 0, 'Results_raw': {}}
2025-10-10 17:09:21 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 17:09:21 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 17:09:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:09:22 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 17:09:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:09:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:09:23 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 17:09:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-10-10 17:09:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:09:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:09:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-10 17:09:24 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=14, loss_sum=9.504007, avg_loss=0.678858, seen=14, correct=9, accuracy=0.642857
2025-10-10 17:09:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:09:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:09:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:09:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1902MB allocated=1868MB
2025-10-10 17:09:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:09:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:09:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:09:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:09:27 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.874849, avg_loss=0.721871, seen=40, correct=19, accuracy=0.475000
2025-10-10 17:09:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:09:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:09:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:09:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1902MB allocated=1868MB
2025-10-10 17:09:30 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.475000
2025-10-10 17:09:30 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 17:09:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=72, total=285)
2025-10-10 17:09:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:09:30 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 17:09:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:09:30 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 17:09:30 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=36, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 17:09:38 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 17:09:38 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 17:09:38 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 17:09:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-10-10 17:09:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:09:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:09:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-10 17:09:38 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=14, loss_sum=9.652563, avg_loss=0.689469, seen=14, correct=7, accuracy=0.500000
2025-10-10 17:09:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:09:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:09:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:09:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1946MB allocated=1885MB
2025-10-10 17:09:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:09:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:09:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:09:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:09:43 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.683258, avg_loss=0.742081, seen=40, correct=18, accuracy=0.450000
2025-10-10 17:09:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:09:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:09:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:09:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1946MB allocated=1885MB
2025-10-10 17:09:44 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.475000, curr=0.450000
2025-10-10 17:09:51 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 17:09:51 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 17:09:51 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 17:09:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-10-10 17:09:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:09:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:09:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-10 17:09:51 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=14, loss_sum=9.066975, avg_loss=0.647641, seen=14, correct=8, accuracy=0.571429
2025-10-10 17:09:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:09:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:09:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:09:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1946MB allocated=1885MB
2025-10-10 17:09:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:09:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:09:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:09:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:09:55 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.558586, avg_loss=0.638965, seen=40, correct=27, accuracy=0.675000
2025-10-10 17:09:55 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:09:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:09:56 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:09:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1946MB allocated=1885MB
2025-10-10 17:09:56 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.675000
2025-10-10 17:10:04 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 17:10:04 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 17:10:04 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 17:10:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-10-10 17:10:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:10:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:10:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-10 17:10:05 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=14, loss_sum=8.933996, avg_loss=0.638143, seen=14, correct=10, accuracy=0.714286
2025-10-10 17:10:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:10:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:10:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:10:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1946MB allocated=1885MB
2025-10-10 17:10:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:10:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:10:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:10:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:10:08 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.608795, avg_loss=0.665220, seen=40, correct=24, accuracy=0.600000
2025-10-10 17:10:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:10:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:10:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:10:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1946MB allocated=1885MB
2025-10-10 17:10:09 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.675000, curr=0.600000
2025-10-10 17:10:17 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 17:10:17 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 17:10:17 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 17:10:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-10-10 17:10:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:10:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:10:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-10 17:10:18 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=14, loss_sum=9.047533, avg_loss=0.646252, seen=14, correct=11, accuracy=0.785714
2025-10-10 17:10:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:10:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:10:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:10:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1946MB allocated=1885MB
2025-10-10 17:10:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:10:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:10:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:10:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:10:21 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.117428, avg_loss=0.677936, seen=40, correct=22, accuracy=0.550000
2025-10-10 17:10:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:10:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:10:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:10:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1946MB allocated=1885MB
2025-10-10 17:10:23 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.675000, curr=0.550000
2025-10-10 17:10:30 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 17:10:30 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 17:10:30 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 17:10:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-10-10 17:10:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:10:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:10:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-10 17:10:31 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=14, loss_sum=9.134740, avg_loss=0.652481, seen=14, correct=10, accuracy=0.714286
2025-10-10 17:10:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:10:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:10:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:10:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1946MB allocated=1885MB
2025-10-10 17:10:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:10:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:10:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:10:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:10:33 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.934757, avg_loss=0.673369, seen=40, correct=24, accuracy=0.600000
2025-10-10 17:10:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:10:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:10:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:10:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1946MB allocated=1885MB
2025-10-10 17:10:35 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.675000, curr=0.600000
2025-10-10 17:10:42 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 17:10:42 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 17:10:42 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 17:10:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-10-10 17:10:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:10:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:10:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-10 17:10:42 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=14, loss_sum=9.272276, avg_loss=0.662305, seen=14, correct=8, accuracy=0.571429
2025-10-10 17:10:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:10:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:10:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:10:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1946MB allocated=1885MB
2025-10-10 17:10:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:10:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:10:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:10:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:10:45 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.799738, avg_loss=0.669993, seen=40, correct=24, accuracy=0.600000
2025-10-10 17:10:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:10:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:10:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:10:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1946MB allocated=1885MB
2025-10-10 17:10:46 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/25), best=0.675000, curr=0.600000
2025-10-10 17:10:53 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 17:10:53 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 17:10:53 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 17:10:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-10-10 17:10:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:10:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:10:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-10 17:10:54 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=14, loss_sum=8.987438, avg_loss=0.641960, seen=14, correct=10, accuracy=0.714286
2025-10-10 17:10:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:10:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:10:56 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:10:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1946MB allocated=1885MB
2025-10-10 17:10:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:10:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:10:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:10:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:10:58 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.655758, avg_loss=0.641394, seen=40, correct=26, accuracy=0.650000
2025-10-10 17:10:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:10:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:10:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:11:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1946MB allocated=1885MB
2025-10-10 17:11:00 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/25), best=0.675000, curr=0.650000
2025-10-10 17:11:08 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 17:11:08 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 17:11:08 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 17:11:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-10-10 17:11:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:11:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:11:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-10 17:11:09 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=14, loss_sum=9.263960, avg_loss=0.661711, seen=14, correct=8, accuracy=0.571429
2025-10-10 17:11:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:11:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:11:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:11:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1946MB allocated=1885MB
2025-10-10 17:11:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:11:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:11:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:11:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:11:13 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.772175, avg_loss=0.669304, seen=40, correct=24, accuracy=0.600000
2025-10-10 17:11:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:11:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:11:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:11:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1946MB allocated=1885MB
2025-10-10 17:11:15 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/25), best=0.675000, curr=0.600000
2025-10-10 17:11:22 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 17:11:22 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 17:11:22 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 17:11:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-10-10 17:11:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:11:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:11:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-10 17:11:23 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=14, loss_sum=8.943186, avg_loss=0.638799, seen=14, correct=10, accuracy=0.714286
2025-10-10 17:11:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:11:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:11:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:11:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1946MB allocated=1885MB
2025-10-10 17:11:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:11:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:11:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:11:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:11:26 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.063585, avg_loss=0.626590, seen=40, correct=26, accuracy=0.650000
2025-10-10 17:11:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:11:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:11:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:11:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1946MB allocated=1885MB
2025-10-10 17:11:28 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/25), best=0.675000, curr=0.650000
2025-10-10 17:11:36 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 17:11:36 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 17:11:36 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 17:11:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-10-10 17:11:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:11:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:11:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-10 17:11:36 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=14, loss_sum=9.266851, avg_loss=0.661918, seen=14, correct=8, accuracy=0.571429
2025-10-10 17:11:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:11:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:11:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:11:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1946MB allocated=1885MB
2025-10-10 17:11:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:11:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:11:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:11:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:11:41 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.293051, avg_loss=0.657326, seen=40, correct=25, accuracy=0.625000
2025-10-10 17:11:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:11:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:11:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:11:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1946MB allocated=1885MB
2025-10-10 17:11:42 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/25), best=0.675000, curr=0.625000
2025-10-10 17:11:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 17:11:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 17:11:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:11:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:11:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1946MB allocated=1885MB
2025-10-10 17:11:43 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #5', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 17:11:43 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #5', 'Round': 0, 'Results_raw': {}}
2025-10-10 17:11:43 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 17:11:44 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 17:11:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:11:45 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 17:11:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:11:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:11:46 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 17:11:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-10-10 17:11:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:11:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:11:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-10 17:11:46 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=32, loss_sum=21.704006, avg_loss=0.678250, seen=32, correct=18, accuracy=0.562500
2025-10-10 17:11:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:11:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:11:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:11:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1902MB allocated=1877MB
2025-10-10 17:11:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:11:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:11:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:11:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:11:49 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.896837, avg_loss=0.772421, seen=40, correct=19, accuracy=0.475000
2025-10-10 17:11:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:11:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:11:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:11:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1902MB allocated=1877MB
2025-10-10 17:11:51 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.475000
2025-10-10 17:11:51 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 17:11:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=156, total=621)
2025-10-10 17:11:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:11:52 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 17:11:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:11:52 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 17:11:52 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=78, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 17:12:01 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 17:12:01 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 17:12:01 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 17:12:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-10-10 17:12:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:12:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:12:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-10 17:12:02 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=32, loss_sum=21.676331, avg_loss=0.677385, seen=32, correct=20, accuracy=0.625000
2025-10-10 17:12:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:12:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:12:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:12:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1942MB allocated=1893MB
2025-10-10 17:12:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:12:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:12:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:12:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:12:05 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=31.217094, avg_loss=0.780427, seen=40, correct=20, accuracy=0.500000
2025-10-10 17:12:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:12:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:12:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:12:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1942MB allocated=1893MB
2025-10-10 17:12:06 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.500000
2025-10-10 17:12:13 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 17:12:13 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 17:12:13 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 17:12:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-10-10 17:12:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:12:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:12:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-10 17:12:14 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=32, loss_sum=21.911079, avg_loss=0.684721, seen=32, correct=20, accuracy=0.625000
2025-10-10 17:12:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:12:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:12:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:12:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1942MB allocated=1893MB
2025-10-10 17:12:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:12:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:12:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:12:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:12:17 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.467854, avg_loss=0.711696, seen=40, correct=20, accuracy=0.500000
2025-10-10 17:12:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:12:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:12:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:12:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1942MB allocated=1893MB
2025-10-10 17:12:19 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.500000
2025-10-10 17:12:27 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 17:12:27 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 17:12:27 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 17:12:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-10-10 17:12:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:12:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:12:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-10 17:12:27 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=32, loss_sum=22.415382, avg_loss=0.700481, seen=32, correct=18, accuracy=0.562500
2025-10-10 17:12:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:12:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:12:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:12:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1942MB allocated=1893MB
2025-10-10 17:12:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:12:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:12:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:12:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:12:32 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=32.938362, avg_loss=0.823459, seen=40, correct=16, accuracy=0.400000
2025-10-10 17:12:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:12:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:12:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:12:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1942MB allocated=1893MB
2025-10-10 17:12:33 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.500000, curr=0.400000
2025-10-10 17:12:42 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 17:12:42 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 17:12:42 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 17:12:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-10-10 17:12:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:12:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:12:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-10 17:12:43 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=32, loss_sum=21.542099, avg_loss=0.673191, seen=32, correct=21, accuracy=0.656250
2025-10-10 17:12:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:12:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:12:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:12:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1942MB allocated=1893MB
2025-10-10 17:12:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:12:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:12:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:12:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:12:47 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.283211, avg_loss=0.707080, seen=40, correct=19, accuracy=0.475000
2025-10-10 17:12:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:12:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:12:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:12:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1942MB allocated=1893MB
2025-10-10 17:12:49 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.500000, curr=0.475000
2025-10-10 17:12:57 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 17:12:57 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 17:12:57 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 17:12:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-10-10 17:12:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:12:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:12:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-10 17:12:58 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=32, loss_sum=21.447392, avg_loss=0.670231, seen=32, correct=22, accuracy=0.687500
2025-10-10 17:12:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:12:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:12:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:13:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1942MB allocated=1893MB
2025-10-10 17:13:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:13:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:13:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:13:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:13:01 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.993059, avg_loss=0.699826, seen=40, correct=19, accuracy=0.475000
2025-10-10 17:13:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:13:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:13:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:13:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1942MB allocated=1893MB
2025-10-10 17:13:03 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.500000, curr=0.475000
2025-10-10 17:13:11 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 17:13:11 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 17:13:11 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 17:13:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-10-10 17:13:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:13:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:13:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-10 17:13:12 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=32, loss_sum=21.446505, avg_loss=0.670203, seen=32, correct=21, accuracy=0.656250
2025-10-10 17:13:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:13:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:13:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:13:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1942MB allocated=1893MB
2025-10-10 17:13:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:13:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:13:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:13:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:13:16 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.901993, avg_loss=0.697550, seen=40, correct=19, accuracy=0.475000
2025-10-10 17:13:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:13:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:13:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:13:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1942MB allocated=1893MB
2025-10-10 17:13:17 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/25), best=0.500000, curr=0.475000
2025-10-10 17:13:25 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 17:13:25 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 17:13:25 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 17:13:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-10-10 17:13:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:13:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:13:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-10 17:13:25 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=32, loss_sum=22.487785, avg_loss=0.702743, seen=32, correct=17, accuracy=0.531250
2025-10-10 17:13:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:13:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:13:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:13:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1942MB allocated=1893MB
2025-10-10 17:13:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:13:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:13:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:13:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:13:29 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=31.002827, avg_loss=0.775071, seen=40, correct=18, accuracy=0.450000
2025-10-10 17:13:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:13:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:13:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:13:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1942MB allocated=1893MB
2025-10-10 17:13:30 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/25), best=0.500000, curr=0.450000
2025-10-10 17:13:38 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 17:13:38 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 17:13:38 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 17:13:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-10-10 17:13:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:13:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:13:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-10 17:13:39 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=32, loss_sum=21.561243, avg_loss=0.673789, seen=32, correct=20, accuracy=0.625000
2025-10-10 17:13:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:13:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:13:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:13:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1942MB allocated=1893MB
2025-10-10 17:13:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:13:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:13:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:13:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:13:42 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.410603, avg_loss=0.685265, seen=40, correct=19, accuracy=0.475000
2025-10-10 17:13:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:13:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:13:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:13:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1942MB allocated=1893MB
2025-10-10 17:13:43 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/25), best=0.500000, curr=0.475000
2025-10-10 17:13:51 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 17:13:51 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 17:13:51 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 17:13:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-10-10 17:13:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:13:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:13:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-10 17:13:51 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=32, loss_sum=21.711491, avg_loss=0.678484, seen=32, correct=20, accuracy=0.625000
2025-10-10 17:13:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:13:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:13:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:13:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1942MB allocated=1893MB
2025-10-10 17:13:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:13:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:13:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:13:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:13:55 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.406761, avg_loss=0.710169, seen=40, correct=20, accuracy=0.500000
2025-10-10 17:13:55 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:13:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:13:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:13:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1942MB allocated=1893MB
2025-10-10 17:13:56 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.500000
2025-10-10 17:14:05 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 17:14:05 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 17:14:05 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 17:14:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-10-10 17:14:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:14:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:14:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-10 17:14:06 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=32, loss_sum=21.453547, avg_loss=0.670423, seen=32, correct=19, accuracy=0.593750
2025-10-10 17:14:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:14:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:14:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:14:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1942MB allocated=1893MB
2025-10-10 17:14:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:14:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:14:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:14:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:14:10 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.132650, avg_loss=0.703316, seen=40, correct=19, accuracy=0.475000
2025-10-10 17:14:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:14:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:14:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:14:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1942MB allocated=1893MB
2025-10-10 17:14:11 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.500000, curr=0.475000
2025-10-10 17:14:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 17:14:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 17:14:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:14:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:14:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1942MB allocated=1893MB
2025-10-10 17:14:12 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #11', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 17:14:12 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #11', 'Round': 0, 'Results_raw': {}}
2025-10-10 17:14:12 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 17:14:13 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 17:14:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:14:14 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 17:14:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:14:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:14:15 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 17:14:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-10-10 17:14:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:14:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:14:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-10 17:14:16 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=75, loss_sum=52.581291, avg_loss=0.701084, seen=75, correct=42, accuracy=0.560000
2025-10-10 17:14:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:14:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:14:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:14:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1922MB allocated=1885MB
2025-10-10 17:14:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:14:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:14:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:14:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:14:20 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.743656, avg_loss=0.668591, seen=40, correct=22, accuracy=0.550000
2025-10-10 17:14:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:14:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:14:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:14:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1922MB allocated=1885MB
2025-10-10 17:14:21 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.550000
2025-10-10 17:14:21 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 17:14:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=359, total=1434)
2025-10-10 17:14:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:14:22 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 17:14:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:14:22 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 17:14:22 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=180, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 17:14:30 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 17:14:30 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 17:14:30 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 17:14:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-10-10 17:14:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:14:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:14:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-10 17:14:31 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=75, loss_sum=52.802917, avg_loss=0.704039, seen=75, correct=46, accuracy=0.613333
2025-10-10 17:14:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:14:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:14:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:14:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1944MB allocated=1902MB
2025-10-10 17:14:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:14:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:14:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:14:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:14:35 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.388382, avg_loss=0.684710, seen=40, correct=23, accuracy=0.575000
2025-10-10 17:14:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:14:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:14:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:14:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1944MB allocated=1902MB
2025-10-10 17:14:38 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-10 17:14:46 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 17:14:46 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 17:14:46 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 17:14:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-10-10 17:14:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:14:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:14:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-10 17:14:47 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=75, loss_sum=53.412560, avg_loss=0.712167, seen=75, correct=44, accuracy=0.586667
2025-10-10 17:14:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:14:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:14:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:14:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1944MB allocated=1902MB
2025-10-10 17:14:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:14:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:14:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:14:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:14:50 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.112988, avg_loss=0.677825, seen=40, correct=23, accuracy=0.575000
2025-10-10 17:14:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:14:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:14:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:14:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1944MB allocated=1902MB
2025-10-10 17:14:52 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-10 17:14:59 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 17:14:59 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 17:14:59 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 17:15:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-10-10 17:15:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:15:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:15:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-10 17:15:01 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=75, loss_sum=52.651989, avg_loss=0.702027, seen=75, correct=44, accuracy=0.586667
2025-10-10 17:15:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:15:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:15:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:15:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1944MB allocated=1902MB
2025-10-10 17:15:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:15:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:15:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:15:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:15:05 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.159264, avg_loss=0.678982, seen=40, correct=23, accuracy=0.575000
2025-10-10 17:15:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:15:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:15:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:15:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1944MB allocated=1902MB
2025-10-10 17:15:07 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-10 17:15:16 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 17:15:16 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 17:15:16 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 17:15:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-10-10 17:15:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:15:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:15:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-10 17:15:17 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=75, loss_sum=51.804512, avg_loss=0.690727, seen=75, correct=45, accuracy=0.600000
2025-10-10 17:15:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:15:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:15:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:15:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1944MB allocated=1902MB
2025-10-10 17:15:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:15:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:15:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:15:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:15:20 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.177208, avg_loss=0.679430, seen=40, correct=25, accuracy=0.625000
2025-10-10 17:15:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:15:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:15:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:15:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1944MB allocated=1902MB
2025-10-10 17:15:21 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-10 17:15:31 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 17:15:31 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 17:15:31 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 17:15:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-10-10 17:15:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:15:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:15:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-10 17:15:32 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=75, loss_sum=51.480282, avg_loss=0.686404, seen=75, correct=45, accuracy=0.600000
2025-10-10 17:15:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:15:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:15:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:15:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1944MB allocated=1902MB
2025-10-10 17:15:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:15:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:15:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:15:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:15:35 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.463842, avg_loss=0.661596, seen=40, correct=24, accuracy=0.600000
2025-10-10 17:15:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:15:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:15:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:15:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1944MB allocated=1902MB
2025-10-10 17:15:37 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.625000, curr=0.600000
2025-10-10 17:15:45 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 17:15:45 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 17:15:45 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 17:15:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-10-10 17:15:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:15:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:15:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-10 17:15:46 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=75, loss_sum=52.383083, avg_loss=0.698441, seen=75, correct=44, accuracy=0.586667
2025-10-10 17:15:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:15:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:15:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:15:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1944MB allocated=1902MB
2025-10-10 17:15:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:15:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:15:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:15:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:15:49 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.102697, avg_loss=0.652567, seen=40, correct=24, accuracy=0.600000
2025-10-10 17:15:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:15:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:15:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:15:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1944MB allocated=1902MB
2025-10-10 17:15:51 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.625000, curr=0.600000
2025-10-10 17:16:00 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 17:16:00 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 17:16:00 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 17:16:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-10-10 17:16:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:16:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:16:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-10 17:16:01 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=75, loss_sum=51.898479, avg_loss=0.691980, seen=75, correct=45, accuracy=0.600000
2025-10-10 17:16:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:16:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:16:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:16:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1944MB allocated=1902MB
2025-10-10 17:16:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:16:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:16:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:16:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:16:05 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.122715, avg_loss=0.653068, seen=40, correct=23, accuracy=0.575000
2025-10-10 17:16:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:16:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:16:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:16:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1944MB allocated=1902MB
2025-10-10 17:16:07 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.625000, curr=0.575000
2025-10-10 17:16:15 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 17:16:15 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 17:16:15 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 17:16:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-10-10 17:16:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:16:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:16:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-10 17:16:16 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=75, loss_sum=51.048111, avg_loss=0.680641, seen=75, correct=44, accuracy=0.586667
2025-10-10 17:16:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:16:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:16:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:16:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1944MB allocated=1902MB
2025-10-10 17:16:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:16:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:16:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:16:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:16:20 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.807812, avg_loss=0.670195, seen=40, correct=23, accuracy=0.575000
2025-10-10 17:16:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:16:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:16:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:16:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1944MB allocated=1902MB
2025-10-10 17:16:22 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/25), best=0.625000, curr=0.575000
2025-10-10 17:16:29 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 17:16:29 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 17:16:29 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 17:16:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-10-10 17:16:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:16:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:16:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-10 17:16:30 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=75, loss_sum=50.853447, avg_loss=0.678046, seen=75, correct=45, accuracy=0.600000
2025-10-10 17:16:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:16:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:16:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:16:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1944MB allocated=1902MB
2025-10-10 17:16:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:16:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:16:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:16:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:16:34 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.334747, avg_loss=0.658369, seen=40, correct=25, accuracy=0.625000
2025-10-10 17:16:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:16:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:16:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:16:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1944MB allocated=1902MB
2025-10-10 17:16:36 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-10 17:16:45 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 17:16:45 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 17:16:45 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 17:16:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-10-10 17:16:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:16:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:16:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-10 17:16:46 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=75, loss_sum=50.363129, avg_loss=0.671508, seen=75, correct=46, accuracy=0.613333
2025-10-10 17:16:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:16:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:16:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:16:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1944MB allocated=1902MB
2025-10-10 17:16:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:16:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:16:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:16:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:16:51 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.162710, avg_loss=0.654068, seen=40, correct=24, accuracy=0.600000
2025-10-10 17:16:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:16:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:16:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:16:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1944MB allocated=1902MB
2025-10-10 17:16:53 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.625000, curr=0.600000
2025-10-10 17:16:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 17:16:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 17:16:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:16:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:16:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1944MB allocated=1902MB
2025-10-10 17:16:54 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #28', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 17:16:54 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #28', 'Round': 0, 'Results_raw': {}}
2025-10-10 17:16:54 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 17:16:55 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 17:16:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:16:55 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 17:16:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:16:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:16:57 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 17:16:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:16:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:16:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:17:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:17:00 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=127.861801, avg_loss=0.639309, seen=200, correct=131, accuracy=0.655000
2025-10-10 17:17:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:17:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:17:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:17:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1922MB allocated=1893MB
2025-10-10 17:17:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:17:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:17:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:17:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:17:03 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.136963, avg_loss=0.653424, seen=40, correct=24, accuracy=0.600000
2025-10-10 17:17:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:17:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:17:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:17:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1922MB allocated=1893MB
2025-10-10 17:17:05 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-10 17:17:05 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 17:17:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1698, total=6791)
2025-10-10 17:17:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:17:05 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 17:17:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:17:05 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 17:17:05 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=849, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 17:17:12 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 17:17:12 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 17:17:12 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 17:17:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:17:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:17:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:17:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:17:15 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=128.211884, avg_loss=0.641059, seen=200, correct=125, accuracy=0.625000
2025-10-10 17:17:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:17:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:17:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:17:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1954MB allocated=1910MB
2025-10-10 17:17:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:17:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:17:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:17:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:17:18 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.695347, avg_loss=0.642384, seen=40, correct=25, accuracy=0.625000
2025-10-10 17:17:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:17:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:17:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:17:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1954MB allocated=1910MB
2025-10-10 17:17:21 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-10 17:17:29 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 17:17:29 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 17:17:29 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 17:17:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:17:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:17:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:17:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:17:32 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=128.461365, avg_loss=0.642307, seen=200, correct=126, accuracy=0.630000
2025-10-10 17:17:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:17:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:17:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:17:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1954MB allocated=1910MB
2025-10-10 17:17:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:17:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:17:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:17:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:17:35 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.470938, avg_loss=0.636773, seen=40, correct=25, accuracy=0.625000
2025-10-10 17:17:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:17:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:17:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:17:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1954MB allocated=1910MB
2025-10-10 17:17:36 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-10 17:17:45 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 17:17:45 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 17:17:45 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 17:17:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:17:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:17:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:17:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:17:48 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=127.194138, avg_loss=0.635971, seen=200, correct=136, accuracy=0.680000
2025-10-10 17:17:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:17:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:17:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:17:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1954MB allocated=1910MB
2025-10-10 17:17:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:17:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:17:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:17:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:17:52 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.919096, avg_loss=0.647977, seen=40, correct=24, accuracy=0.600000
2025-10-10 17:17:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:17:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:17:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:17:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1954MB allocated=1910MB
2025-10-10 17:17:54 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.625000, curr=0.600000
2025-10-10 17:18:02 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 17:18:02 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 17:18:02 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 17:18:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:18:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:18:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:18:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:18:05 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=127.377388, avg_loss=0.636887, seen=200, correct=129, accuracy=0.645000
2025-10-10 17:18:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:18:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:18:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:18:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1954MB allocated=1910MB
2025-10-10 17:18:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:18:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:18:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:18:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:18:09 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.491934, avg_loss=0.637298, seen=40, correct=25, accuracy=0.625000
2025-10-10 17:18:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:18:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:18:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:18:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1954MB allocated=1910MB
2025-10-10 17:18:10 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-10 17:18:18 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 17:18:18 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 17:18:18 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 17:18:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:18:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:18:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:18:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:18:21 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=127.633965, avg_loss=0.638170, seen=200, correct=125, accuracy=0.625000
2025-10-10 17:18:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:18:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:18:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:18:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1954MB allocated=1910MB
2025-10-10 17:18:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:18:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:18:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:18:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:18:24 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.774405, avg_loss=0.644360, seen=40, correct=25, accuracy=0.625000
2025-10-10 17:18:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:18:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:18:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:18:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1954MB allocated=1910MB
2025-10-10 17:18:26 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-10 17:18:34 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 17:18:34 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 17:18:34 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 17:18:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:18:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:18:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:18:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:18:37 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=127.185234, avg_loss=0.635926, seen=200, correct=131, accuracy=0.655000
2025-10-10 17:18:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:18:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:18:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:18:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1954MB allocated=1910MB
2025-10-10 17:18:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:18:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:18:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:18:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:18:40 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.995764, avg_loss=0.649894, seen=40, correct=23, accuracy=0.575000
2025-10-10 17:18:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:18:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:18:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:18:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1954MB allocated=1910MB
2025-10-10 17:18:41 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.625000, curr=0.575000
2025-10-10 17:18:48 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 17:18:48 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 17:18:48 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 17:18:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:18:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:18:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:18:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:18:51 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=126.718117, avg_loss=0.633591, seen=200, correct=132, accuracy=0.660000
2025-10-10 17:18:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:18:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:18:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:18:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1954MB allocated=1910MB
2025-10-10 17:18:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:18:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:18:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:18:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:18:55 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.908010, avg_loss=0.647700, seen=40, correct=26, accuracy=0.650000
2025-10-10 17:18:55 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:18:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:18:56 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:18:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1954MB allocated=1910MB
2025-10-10 17:18:57 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.650000
2025-10-10 17:19:05 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 17:19:05 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 17:19:05 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 17:19:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:19:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:19:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:19:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:19:08 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=128.077744, avg_loss=0.640389, seen=200, correct=129, accuracy=0.645000
2025-10-10 17:19:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:19:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:19:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:19:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1954MB allocated=1910MB
2025-10-10 17:19:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:19:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:19:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:19:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:19:11 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.113426, avg_loss=0.652836, seen=40, correct=24, accuracy=0.600000
2025-10-10 17:19:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:19:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:19:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:19:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1954MB allocated=1910MB
2025-10-10 17:19:12 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.650000, curr=0.600000
2025-10-10 17:19:22 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 17:19:22 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 17:19:22 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 17:19:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:19:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:19:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:19:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:19:25 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=128.470764, avg_loss=0.642354, seen=200, correct=132, accuracy=0.660000
2025-10-10 17:19:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:19:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:19:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:19:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1954MB allocated=1910MB
2025-10-10 17:19:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:19:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:19:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:19:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:19:28 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.334789, avg_loss=0.658370, seen=40, correct=25, accuracy=0.625000
2025-10-10 17:19:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:19:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:19:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:19:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1954MB allocated=1910MB
2025-10-10 17:19:30 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.650000, curr=0.625000
2025-10-10 17:19:39 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 17:19:39 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 17:19:39 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 17:19:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:19:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:19:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:19:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:19:42 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=127.660530, avg_loss=0.638303, seen=200, correct=131, accuracy=0.655000
2025-10-10 17:19:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:19:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:19:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:19:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1954MB allocated=1910MB
2025-10-10 17:19:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:19:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:19:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:19:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:19:45 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.016562, avg_loss=0.650414, seen=40, correct=26, accuracy=0.650000
2025-10-10 17:19:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:19:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:19:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:19:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1954MB allocated=1910MB
2025-10-10 17:19:48 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.650000
2025-10-10 17:19:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 17:19:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 17:19:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:19:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:19:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1954MB allocated=1910MB
2025-10-10 17:19:49 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #53', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 17:19:49 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #53', 'Round': 0, 'Results_raw': {}}
2025-10-10 17:19:49 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 17:19:50 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 17:19:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:19:50 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 17:19:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:19:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:19:52 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 17:19:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-10-10 17:19:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:19:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:19:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-10-10 17:19:55 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=193, loss_sum=140.280182, avg_loss=0.726840, seen=193, correct=88, accuracy=0.455959
2025-10-10 17:19:55 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:19:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:19:56 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:19:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1942MB allocated=1902MB
2025-10-10 17:19:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:19:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:19:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:19:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:19:58 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.342590, avg_loss=0.683565, seen=40, correct=24, accuracy=0.600000
2025-10-10 17:19:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:19:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:19:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:19:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1942MB allocated=1902MB
2025-10-10 17:19:59 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-10 17:19:59 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 17:19:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=920, total=3679)
2025-10-10 17:19:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:19:59 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 17:19:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:19:59 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 17:19:59 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=460, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 17:20:09 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 17:20:09 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 17:20:09 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 17:20:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-10-10 17:20:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:20:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:20:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-10-10 17:20:12 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=193, loss_sum=136.144913, avg_loss=0.705414, seen=193, correct=91, accuracy=0.471503
2025-10-10 17:20:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:20:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:20:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:20:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1976MB allocated=1919MB
2025-10-10 17:20:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:20:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:20:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:20:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:20:16 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.911907, avg_loss=0.672798, seen=40, correct=23, accuracy=0.575000
2025-10-10 17:20:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:20:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:20:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:20:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1976MB allocated=1919MB
2025-10-10 17:20:18 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.600000, curr=0.575000
2025-10-10 17:20:26 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 17:20:26 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 17:20:26 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 17:20:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-10-10 17:20:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:20:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:20:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-10-10 17:20:29 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=193, loss_sum=132.578705, avg_loss=0.686936, seen=193, correct=113, accuracy=0.585492
2025-10-10 17:20:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:20:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:20:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:20:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1976MB allocated=1919MB
2025-10-10 17:20:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:20:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:20:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:20:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:20:32 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.104462, avg_loss=0.677612, seen=40, correct=21, accuracy=0.525000
2025-10-10 17:20:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:20:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:20:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:20:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1976MB allocated=1919MB
2025-10-10 17:20:33 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.600000, curr=0.525000
2025-10-10 17:20:41 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 17:20:41 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 17:20:41 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 17:20:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-10-10 17:20:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:20:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:20:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-10-10 17:20:44 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=193, loss_sum=136.499420, avg_loss=0.707251, seen=193, correct=95, accuracy=0.492228
2025-10-10 17:20:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:20:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:20:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:20:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1976MB allocated=1919MB
2025-10-10 17:20:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:20:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:20:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:20:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:20:47 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.962490, avg_loss=0.674062, seen=40, correct=24, accuracy=0.600000
2025-10-10 17:20:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:20:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:20:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:20:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1976MB allocated=1919MB
2025-10-10 17:20:48 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-10 17:20:57 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 17:20:57 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 17:20:57 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 17:20:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-10-10 17:20:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:20:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:21:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-10-10 17:21:00 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=193, loss_sum=134.126572, avg_loss=0.694956, seen=193, correct=106, accuracy=0.549223
2025-10-10 17:21:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:21:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:21:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:21:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1976MB allocated=1919MB
2025-10-10 17:21:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:21:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:21:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:21:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:21:04 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.964949, avg_loss=0.674124, seen=40, correct=26, accuracy=0.650000
2025-10-10 17:21:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:21:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:21:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:21:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1976MB allocated=1919MB
2025-10-10 17:21:05 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.650000
2025-10-10 17:21:13 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 17:21:13 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 17:21:13 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 17:21:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-10-10 17:21:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:21:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:21:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-10-10 17:21:16 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=193, loss_sum=131.484833, avg_loss=0.681269, seen=193, correct=119, accuracy=0.616580
2025-10-10 17:21:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:21:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:21:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:21:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1976MB allocated=1919MB
2025-10-10 17:21:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:21:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:21:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:21:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:21:19 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.473284, avg_loss=0.686832, seen=40, correct=23, accuracy=0.575000
2025-10-10 17:21:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:21:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:21:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:21:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1976MB allocated=1919MB
2025-10-10 17:21:20 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.650000, curr=0.575000
2025-10-10 17:21:27 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 17:21:27 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 17:21:27 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 17:21:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-10-10 17:21:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:21:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:21:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-10-10 17:21:30 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=193, loss_sum=132.590225, avg_loss=0.686996, seen=193, correct=110, accuracy=0.569948
2025-10-10 17:21:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:21:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:21:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:21:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1976MB allocated=1919MB
2025-10-10 17:21:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:21:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:21:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:21:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:21:34 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.021744, avg_loss=0.675544, seen=40, correct=21, accuracy=0.525000
2025-10-10 17:21:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:21:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:21:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:21:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1976MB allocated=1919MB
2025-10-10 17:21:35 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.650000, curr=0.525000
2025-10-10 17:21:43 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 17:21:43 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 17:21:43 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 17:21:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-10-10 17:21:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:21:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:21:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-10-10 17:21:46 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=193, loss_sum=138.195236, avg_loss=0.716037, seen=193, correct=92, accuracy=0.476684
2025-10-10 17:21:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:21:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:21:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:21:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1976MB allocated=1919MB
2025-10-10 17:21:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:21:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:21:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:21:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:21:49 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.630905, avg_loss=0.665773, seen=40, correct=23, accuracy=0.575000
2025-10-10 17:21:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:21:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:21:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:21:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1976MB allocated=1919MB
2025-10-10 17:21:51 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.650000, curr=0.575000
2025-10-10 17:21:59 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 17:21:59 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 17:21:59 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 17:21:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-10-10 17:21:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:21:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:22:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-10-10 17:22:02 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=193, loss_sum=135.884094, avg_loss=0.704063, seen=193, correct=102, accuracy=0.528497
2025-10-10 17:22:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:22:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:22:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:22:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1976MB allocated=1919MB
2025-10-10 17:22:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:22:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:22:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:22:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:22:06 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.703487, avg_loss=0.667587, seen=40, correct=24, accuracy=0.600000
2025-10-10 17:22:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:22:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:22:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:22:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1976MB allocated=1919MB
2025-10-10 17:22:08 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/25), best=0.650000, curr=0.600000
2025-10-10 17:22:16 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 17:22:16 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 17:22:16 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 17:22:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-10-10 17:22:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:22:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:22:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-10-10 17:22:19 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=193, loss_sum=132.748352, avg_loss=0.687815, seen=193, correct=110, accuracy=0.569948
2025-10-10 17:22:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:22:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:22:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:22:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1976MB allocated=1919MB
2025-10-10 17:22:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:22:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:22:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:22:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:22:23 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.869419, avg_loss=0.671735, seen=40, correct=23, accuracy=0.575000
2025-10-10 17:22:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:22:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:22:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:22:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1976MB allocated=1919MB
2025-10-10 17:22:24 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/25), best=0.650000, curr=0.575000
2025-10-10 17:22:32 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 17:22:32 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 17:22:32 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 17:22:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-10-10 17:22:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:22:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:22:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-10-10 17:22:35 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=193, loss_sum=132.371216, avg_loss=0.685861, seen=193, correct=112, accuracy=0.580311
2025-10-10 17:22:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:22:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:22:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:22:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1976MB allocated=1919MB
2025-10-10 17:22:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:22:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:22:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:22:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:22:38 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.764755, avg_loss=0.669119, seen=40, correct=24, accuracy=0.600000
2025-10-10 17:22:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:22:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:22:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:22:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1976MB allocated=1919MB
2025-10-10 17:22:40 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/25), best=0.650000, curr=0.600000
2025-10-10 17:22:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 17:22:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 17:22:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:22:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:22:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1976MB allocated=1919MB
2025-10-10 17:22:41 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #31', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 17:22:41 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #31', 'Round': 0, 'Results_raw': {}}
2025-10-10 17:22:41 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 17:22:42 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 17:22:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:22:43 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 17:22:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:22:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:22:44 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 17:22:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:22:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:22:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:22:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:22:48 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=130.486816, avg_loss=0.652434, seen=200, correct=122, accuracy=0.610000
2025-10-10 17:22:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:22:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:22:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:22:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1942MB allocated=1910MB
2025-10-10 17:22:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:22:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:22:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:22:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:22:51 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=22.824255, avg_loss=0.570606, seen=40, correct=30, accuracy=0.750000
2025-10-10 17:22:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:22:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:22:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:22:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1942MB allocated=1910MB
2025-10-10 17:22:53 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.750000
2025-10-10 17:22:53 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 17:22:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1543, total=6171)
2025-10-10 17:22:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:22:53 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 17:22:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:22:53 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 17:22:53 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=772, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 17:23:01 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 17:23:01 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 17:23:01 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 17:23:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:23:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:23:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:23:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:23:04 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=130.240875, avg_loss=0.651204, seen=200, correct=121, accuracy=0.605000
2025-10-10 17:23:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:23:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:23:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:23:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1970MB allocated=1927MB
2025-10-10 17:23:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:23:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:23:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:23:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:23:08 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=22.155670, avg_loss=0.553892, seen=40, correct=30, accuracy=0.750000
2025-10-10 17:23:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:23:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:23:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:23:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1970MB allocated=1927MB
2025-10-10 17:23:10 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.750000
2025-10-10 17:23:19 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 17:23:19 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 17:23:19 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 17:23:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:23:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:23:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:23:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:23:22 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=131.216934, avg_loss=0.656085, seen=200, correct=124, accuracy=0.620000
2025-10-10 17:23:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:23:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:23:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:23:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1970MB allocated=1927MB
2025-10-10 17:23:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:23:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:23:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:23:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:23:26 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=22.294243, avg_loss=0.557356, seen=40, correct=30, accuracy=0.750000
2025-10-10 17:23:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:23:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:23:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:23:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1970MB allocated=1927MB
2025-10-10 17:23:27 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.750000
2025-10-10 17:23:34 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 17:23:34 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 17:23:34 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 17:23:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:23:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:23:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:23:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:23:37 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=133.797928, avg_loss=0.668990, seen=200, correct=125, accuracy=0.625000
2025-10-10 17:23:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:23:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:23:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:23:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1970MB allocated=1927MB
2025-10-10 17:23:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:23:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:23:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:23:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:23:41 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=22.816128, avg_loss=0.570403, seen=40, correct=29, accuracy=0.725000
2025-10-10 17:23:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:23:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:23:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:23:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1970MB allocated=1927MB
2025-10-10 17:23:42 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.750000, curr=0.725000
2025-10-10 17:23:49 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 17:23:49 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 17:23:49 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 17:23:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:23:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:23:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:23:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:23:52 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=132.815475, avg_loss=0.664077, seen=200, correct=123, accuracy=0.615000
2025-10-10 17:23:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:23:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:23:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:23:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1970MB allocated=1927MB
2025-10-10 17:23:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:23:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:23:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:23:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:23:56 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=22.570913, avg_loss=0.564273, seen=40, correct=27, accuracy=0.675000
2025-10-10 17:23:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:23:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:23:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:23:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1970MB allocated=1927MB
2025-10-10 17:23:57 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.750000, curr=0.675000
2025-10-10 17:24:05 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 17:24:05 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 17:24:05 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 17:24:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:24:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:24:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:24:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:24:08 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=131.856522, avg_loss=0.659283, seen=200, correct=124, accuracy=0.620000
2025-10-10 17:24:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:24:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:24:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:24:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1970MB allocated=1927MB
2025-10-10 17:24:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:24:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:24:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:24:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:24:11 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=22.742456, avg_loss=0.568561, seen=40, correct=27, accuracy=0.675000
2025-10-10 17:24:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:24:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:24:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:24:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1970MB allocated=1927MB
2025-10-10 17:24:13 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.750000, curr=0.675000
2025-10-10 17:24:21 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 17:24:21 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 17:24:21 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 17:24:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:24:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:24:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:24:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:24:24 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=131.368500, avg_loss=0.656842, seen=200, correct=124, accuracy=0.620000
2025-10-10 17:24:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:24:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:24:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:24:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1970MB allocated=1927MB
2025-10-10 17:24:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:24:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:24:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:24:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:24:28 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=22.774813, avg_loss=0.569370, seen=40, correct=27, accuracy=0.675000
2025-10-10 17:24:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:24:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:24:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:24:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1970MB allocated=1927MB
2025-10-10 17:24:30 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/25), best=0.750000, curr=0.675000
2025-10-10 17:24:38 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 17:24:38 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 17:24:38 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 17:24:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:24:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:24:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:24:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:24:41 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=130.827362, avg_loss=0.654137, seen=200, correct=124, accuracy=0.620000
2025-10-10 17:24:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:24:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:24:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:24:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1970MB allocated=1927MB
2025-10-10 17:24:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:24:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:24:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:24:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:24:46 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=22.485331, avg_loss=0.562133, seen=40, correct=28, accuracy=0.700000
2025-10-10 17:24:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:24:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:24:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:24:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1970MB allocated=1927MB
2025-10-10 17:24:47 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/25), best=0.750000, curr=0.700000
2025-10-10 17:24:55 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 17:24:55 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 17:24:55 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 17:24:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:24:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:24:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:24:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:24:58 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=131.172791, avg_loss=0.655864, seen=200, correct=125, accuracy=0.625000
2025-10-10 17:24:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:24:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:24:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:25:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1970MB allocated=1927MB
2025-10-10 17:25:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:25:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:25:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:25:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:25:01 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=22.557365, avg_loss=0.563934, seen=40, correct=28, accuracy=0.700000
2025-10-10 17:25:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:25:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:25:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:25:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1970MB allocated=1927MB
2025-10-10 17:25:03 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/25), best=0.750000, curr=0.700000
2025-10-10 17:25:11 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 17:25:11 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 17:25:11 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 17:25:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:25:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:25:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:25:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:25:14 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=130.539307, avg_loss=0.652697, seen=200, correct=129, accuracy=0.645000
2025-10-10 17:25:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:25:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:25:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:25:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1970MB allocated=1927MB
2025-10-10 17:25:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:25:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:25:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:25:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:25:18 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=22.461967, avg_loss=0.561549, seen=40, correct=28, accuracy=0.700000
2025-10-10 17:25:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:25:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:25:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:25:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1970MB allocated=1927MB
2025-10-10 17:25:19 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/25), best=0.750000, curr=0.700000
2025-10-10 17:25:28 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 17:25:28 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 17:25:28 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 17:25:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:25:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:25:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:25:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:25:31 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=133.705093, avg_loss=0.668525, seen=200, correct=125, accuracy=0.625000
2025-10-10 17:25:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:25:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:25:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:25:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1970MB allocated=1927MB
2025-10-10 17:25:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:25:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:25:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:25:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:25:35 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=22.987436, avg_loss=0.574686, seen=40, correct=28, accuracy=0.700000
2025-10-10 17:25:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:25:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:25:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:25:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1970MB allocated=1927MB
2025-10-10 17:25:37 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/25), best=0.750000, curr=0.700000
2025-10-10 17:25:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 17:25:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 17:25:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:25:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:25:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1970MB allocated=1927MB
2025-10-10 17:25:38 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #38', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 17:25:38 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #38', 'Round': 0, 'Results_raw': {}}
2025-10-10 17:25:38 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 17:25:39 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 17:25:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:25:39 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 17:25:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:25:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:25:40 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 17:25:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-10-10 17:25:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:25:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:25:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-10 17:25:41 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=30, loss_sum=17.678846, avg_loss=0.589295, seen=30, correct=22, accuracy=0.733333
2025-10-10 17:25:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:25:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:25:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:25:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1942MB allocated=1919MB
2025-10-10 17:25:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:25:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:25:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:25:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:25:44 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.027613, avg_loss=0.600690, seen=40, correct=23, accuracy=0.575000
2025-10-10 17:25:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:25:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:25:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:25:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1942MB allocated=1919MB
2025-10-10 17:25:46 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-10 17:25:46 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 17:25:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=146, total=583)
2025-10-10 17:25:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:25:46 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 17:25:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:25:46 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 17:25:46 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=73, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 17:25:54 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 17:25:54 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 17:25:54 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 17:25:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-10-10 17:25:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:25:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:25:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-10 17:25:55 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=30, loss_sum=18.582521, avg_loss=0.619417, seen=30, correct=17, accuracy=0.566667
2025-10-10 17:25:55 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:25:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:25:56 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:25:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1964MB allocated=1935MB
2025-10-10 17:25:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:25:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:25:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:25:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:25:59 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.561213, avg_loss=0.614030, seen=40, correct=25, accuracy=0.625000
2025-10-10 17:25:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:25:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:25:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:26:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1964MB allocated=1935MB
2025-10-10 17:26:00 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-10 17:26:08 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 17:26:08 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 17:26:08 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 17:26:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-10-10 17:26:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:26:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:26:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-10 17:26:09 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=30, loss_sum=17.605799, avg_loss=0.586860, seen=30, correct=21, accuracy=0.700000
2025-10-10 17:26:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:26:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:26:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:26:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1964MB allocated=1935MB
2025-10-10 17:26:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:26:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:26:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:26:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:26:12 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.283443, avg_loss=0.582086, seen=40, correct=24, accuracy=0.600000
2025-10-10 17:26:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:26:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:26:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:26:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1964MB allocated=1935MB
2025-10-10 17:26:13 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.625000, curr=0.600000
2025-10-10 17:26:21 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 17:26:21 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 17:26:21 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 17:26:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-10-10 17:26:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:26:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:26:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-10 17:26:22 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=30, loss_sum=17.403444, avg_loss=0.580115, seen=30, correct=20, accuracy=0.666667
2025-10-10 17:26:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:26:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:26:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:26:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1964MB allocated=1935MB
2025-10-10 17:26:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:26:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:26:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:26:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:26:25 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.070026, avg_loss=0.576751, seen=40, correct=25, accuracy=0.625000
2025-10-10 17:26:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:26:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:26:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:26:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1964MB allocated=1935MB
2025-10-10 17:26:26 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-10 17:26:34 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 17:26:34 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 17:26:34 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 17:26:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-10-10 17:26:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:26:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:26:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-10 17:26:35 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=30, loss_sum=17.400940, avg_loss=0.580031, seen=30, correct=20, accuracy=0.666667
2025-10-10 17:26:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:26:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:26:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:26:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1964MB allocated=1935MB
2025-10-10 17:26:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:26:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:26:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:26:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:26:39 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.197298, avg_loss=0.579932, seen=40, correct=25, accuracy=0.625000
2025-10-10 17:26:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:26:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:26:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:26:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1964MB allocated=1935MB
2025-10-10 17:26:40 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-10 17:26:49 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 17:26:49 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 17:26:49 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 17:26:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-10-10 17:26:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:26:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:26:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-10 17:26:49 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=30, loss_sum=17.421120, avg_loss=0.580704, seen=30, correct=21, accuracy=0.700000
2025-10-10 17:26:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:26:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:26:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:26:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1964MB allocated=1935MB
2025-10-10 17:26:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:26:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:26:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:26:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:26:53 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.375422, avg_loss=0.584386, seen=40, correct=26, accuracy=0.650000
2025-10-10 17:26:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:26:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:26:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:26:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1964MB allocated=1935MB
2025-10-10 17:26:55 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.650000
2025-10-10 17:27:03 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 17:27:03 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 17:27:03 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 17:27:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-10-10 17:27:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:27:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:27:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-10 17:27:04 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=30, loss_sum=17.559629, avg_loss=0.585321, seen=30, correct=22, accuracy=0.733333
2025-10-10 17:27:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:27:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:27:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:27:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1964MB allocated=1935MB
2025-10-10 17:27:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:27:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:27:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:27:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:27:08 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=22.837858, avg_loss=0.570946, seen=40, correct=27, accuracy=0.675000
2025-10-10 17:27:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:27:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:27:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:27:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1964MB allocated=1935MB
2025-10-10 17:27:09 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.675000
2025-10-10 17:27:18 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 17:27:18 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 17:27:18 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 17:27:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-10-10 17:27:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:27:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:27:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-10 17:27:19 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=30, loss_sum=17.462042, avg_loss=0.582068, seen=30, correct=20, accuracy=0.666667
2025-10-10 17:27:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:27:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:27:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:27:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1964MB allocated=1935MB
2025-10-10 17:27:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:27:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:27:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:27:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:27:23 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=22.932522, avg_loss=0.573313, seen=40, correct=26, accuracy=0.650000
2025-10-10 17:27:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:27:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:27:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:27:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1964MB allocated=1935MB
2025-10-10 17:27:24 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.675000, curr=0.650000
2025-10-10 17:27:33 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 17:27:33 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 17:27:33 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 17:27:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-10-10 17:27:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:27:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:27:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-10 17:27:34 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=30, loss_sum=17.352287, avg_loss=0.578410, seen=30, correct=22, accuracy=0.733333
2025-10-10 17:27:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:27:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:27:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:27:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1964MB allocated=1935MB
2025-10-10 17:27:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:27:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:27:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:27:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:27:38 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=22.819439, avg_loss=0.570486, seen=40, correct=26, accuracy=0.650000
2025-10-10 17:27:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:27:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:27:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:27:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1964MB allocated=1935MB
2025-10-10 17:27:39 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.675000, curr=0.650000
2025-10-10 17:27:48 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 17:27:48 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 17:27:48 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 17:27:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-10-10 17:27:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:27:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:27:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-10 17:27:49 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=30, loss_sum=17.369884, avg_loss=0.578996, seen=30, correct=21, accuracy=0.700000
2025-10-10 17:27:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:27:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:27:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:27:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1964MB allocated=1935MB
2025-10-10 17:27:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:27:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:27:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:27:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:27:52 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=22.964550, avg_loss=0.574114, seen=40, correct=26, accuracy=0.650000
2025-10-10 17:27:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:27:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:27:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:27:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1964MB allocated=1935MB
2025-10-10 17:27:54 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.675000, curr=0.650000
2025-10-10 17:28:02 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 17:28:02 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 17:28:02 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 17:28:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-10-10 17:28:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:28:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:28:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-10 17:28:03 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=30, loss_sum=17.234394, avg_loss=0.574480, seen=30, correct=21, accuracy=0.700000
2025-10-10 17:28:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:28:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:28:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:28:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1964MB allocated=1935MB
2025-10-10 17:28:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:28:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:28:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:28:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:28:06 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=22.480627, avg_loss=0.562016, seen=40, correct=30, accuracy=0.750000
2025-10-10 17:28:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:28:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:28:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:28:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1964MB allocated=1935MB
2025-10-10 17:28:08 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.750000
2025-10-10 17:28:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 17:28:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 17:28:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:28:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:28:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1964MB allocated=1935MB
2025-10-10 17:28:10 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #23', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 17:28:10 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #23', 'Round': 0, 'Results_raw': {}}
2025-10-10 17:28:10 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 17:28:11 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 17:28:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:28:11 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 17:28:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:28:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:28:12 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 17:28:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-10-10 17:28:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:28:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:28:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-10 17:28:14 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=69, loss_sum=44.820103, avg_loss=0.649567, seen=69, correct=45, accuracy=0.652174
2025-10-10 17:28:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:28:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:28:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:28:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1962MB allocated=1927MB
2025-10-10 17:28:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:28:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:28:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:28:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:28:17 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.608967, avg_loss=0.715224, seen=40, correct=20, accuracy=0.500000
2025-10-10 17:28:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:28:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:28:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:28:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1962MB allocated=1927MB
2025-10-10 17:28:19 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.500000
2025-10-10 17:28:19 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 17:28:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=329, total=1316)
2025-10-10 17:28:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:28:19 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 17:28:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:28:19 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 17:28:19 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=165, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 17:28:28 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 17:28:28 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 17:28:28 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 17:28:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-10-10 17:28:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:28:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:28:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-10 17:28:29 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=69, loss_sum=43.931896, avg_loss=0.636694, seen=69, correct=40, accuracy=0.579710
2025-10-10 17:28:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:28:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:28:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:28:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1990MB allocated=1944MB
2025-10-10 17:28:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:28:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:28:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:28:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:28:33 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.150780, avg_loss=0.728769, seen=40, correct=25, accuracy=0.625000
2025-10-10 17:28:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:28:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:28:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:28:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1990MB allocated=1944MB
2025-10-10 17:28:35 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-10 17:28:42 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 17:28:42 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 17:28:42 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 17:28:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-10-10 17:28:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:28:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:28:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-10 17:28:44 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=69, loss_sum=42.503143, avg_loss=0.615988, seen=69, correct=40, accuracy=0.579710
2025-10-10 17:28:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:28:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:28:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:28:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1990MB allocated=1944MB
2025-10-10 17:28:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:28:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:28:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:28:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:28:48 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.131006, avg_loss=0.728275, seen=40, correct=24, accuracy=0.600000
2025-10-10 17:28:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:28:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:28:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:28:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1990MB allocated=1944MB
2025-10-10 17:28:49 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.625000, curr=0.600000
2025-10-10 17:28:57 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 17:28:57 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 17:28:57 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 17:28:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-10-10 17:28:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:28:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:28:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-10 17:28:59 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=69, loss_sum=44.039574, avg_loss=0.638255, seen=69, correct=43, accuracy=0.623188
2025-10-10 17:28:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:28:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:29:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:29:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1990MB allocated=1944MB
2025-10-10 17:29:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:29:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:29:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:29:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:29:03 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.632603, avg_loss=0.740815, seen=40, correct=20, accuracy=0.500000
2025-10-10 17:29:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:29:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:29:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:29:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1990MB allocated=1944MB
2025-10-10 17:29:04 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.625000, curr=0.500000
2025-10-10 17:29:14 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 17:29:14 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 17:29:14 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 17:29:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-10-10 17:29:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:29:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:29:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-10 17:29:15 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=69, loss_sum=44.499279, avg_loss=0.644917, seen=69, correct=39, accuracy=0.565217
2025-10-10 17:29:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:29:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:29:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:29:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1990MB allocated=1944MB
2025-10-10 17:29:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:29:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:29:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:29:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:29:18 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.223694, avg_loss=0.755592, seen=40, correct=20, accuracy=0.500000
2025-10-10 17:29:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:29:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:29:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:29:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1990MB allocated=1944MB
2025-10-10 17:29:19 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.625000, curr=0.500000
2025-10-10 17:29:27 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 17:29:27 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 17:29:27 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 17:29:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-10-10 17:29:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:29:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:29:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-10 17:29:28 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=69, loss_sum=45.078621, avg_loss=0.653313, seen=69, correct=38, accuracy=0.550725
2025-10-10 17:29:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:29:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:29:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:29:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1990MB allocated=1944MB
2025-10-10 17:29:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:29:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:29:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:29:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:29:33 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.518627, avg_loss=0.762966, seen=40, correct=19, accuracy=0.475000
2025-10-10 17:29:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:29:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:29:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:29:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1990MB allocated=1944MB
2025-10-10 17:29:34 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/25), best=0.625000, curr=0.475000
2025-10-10 17:29:42 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 17:29:42 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 17:29:42 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 17:29:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-10-10 17:29:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:29:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:29:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-10 17:29:43 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=69, loss_sum=45.822090, avg_loss=0.664088, seen=69, correct=35, accuracy=0.507246
2025-10-10 17:29:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:29:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:29:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:29:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1990MB allocated=1944MB
2025-10-10 17:29:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:29:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:29:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:29:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:29:47 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=31.762079, avg_loss=0.794052, seen=40, correct=20, accuracy=0.500000
2025-10-10 17:29:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:29:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:29:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:29:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1990MB allocated=1944MB
2025-10-10 17:29:49 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/25), best=0.625000, curr=0.500000
2025-10-10 17:29:56 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 17:29:56 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 17:29:56 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 17:29:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-10-10 17:29:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:29:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:29:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-10 17:29:57 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=69, loss_sum=45.567711, avg_loss=0.660402, seen=69, correct=45, accuracy=0.652174
2025-10-10 17:29:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:29:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:29:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:29:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1990MB allocated=1944MB
2025-10-10 17:29:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:29:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:29:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:30:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:30:01 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.789129, avg_loss=0.719728, seen=40, correct=20, accuracy=0.500000
2025-10-10 17:30:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:30:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:30:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:30:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1990MB allocated=1944MB
2025-10-10 17:30:02 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/25), best=0.625000, curr=0.500000
2025-10-10 17:30:11 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 17:30:11 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 17:30:11 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 17:30:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-10-10 17:30:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:30:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:30:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-10 17:30:12 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=69, loss_sum=45.002037, avg_loss=0.652203, seen=69, correct=36, accuracy=0.521739
2025-10-10 17:30:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:30:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:30:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:30:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1990MB allocated=1944MB
2025-10-10 17:30:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:30:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:30:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:30:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:30:15 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.616148, avg_loss=0.740404, seen=40, correct=21, accuracy=0.525000
2025-10-10 17:30:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:30:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:30:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:30:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1990MB allocated=1944MB
2025-10-10 17:30:16 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/25), best=0.625000, curr=0.525000
2025-10-10 17:30:23 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 17:30:23 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 17:30:23 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 17:30:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-10-10 17:30:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:30:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:30:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-10 17:30:24 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=69, loss_sum=45.322624, avg_loss=0.656850, seen=69, correct=36, accuracy=0.521739
2025-10-10 17:30:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:30:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:30:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:30:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1990MB allocated=1944MB
2025-10-10 17:30:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:30:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:30:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:30:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:30:27 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.206800, avg_loss=0.755170, seen=40, correct=18, accuracy=0.450000
2025-10-10 17:30:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:30:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:30:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:30:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1990MB allocated=1944MB
2025-10-10 17:30:29 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/25), best=0.625000, curr=0.450000
2025-10-10 17:30:36 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 17:30:36 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 17:30:36 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 17:30:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-10-10 17:30:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:30:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:30:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-10 17:30:37 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=69, loss_sum=45.233643, avg_loss=0.655560, seen=69, correct=43, accuracy=0.623188
2025-10-10 17:30:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:30:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:30:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:30:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1990MB allocated=1944MB
2025-10-10 17:30:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:30:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:30:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:30:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:30:41 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.930550, avg_loss=0.723264, seen=40, correct=21, accuracy=0.525000
2025-10-10 17:30:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:30:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:30:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:30:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1990MB allocated=1944MB
2025-10-10 17:30:42 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=9/25), best=0.625000, curr=0.525000
2025-10-10 17:30:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 17:30:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 17:30:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:30:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:30:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1990MB allocated=1944MB
2025-10-10 17:30:43 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #8', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 17:30:43 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #8', 'Round': 0, 'Results_raw': {}}
2025-10-10 17:30:43 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 17:30:44 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 17:30:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:30:45 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 17:30:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:30:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:30:46 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 17:30:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:30:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:30:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:30:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:30:49 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=130.421936, avg_loss=0.652110, seen=200, correct=127, accuracy=0.635000
2025-10-10 17:30:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:30:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:30:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:30:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1962MB allocated=1935MB
2025-10-10 17:30:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:30:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:30:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:30:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:30:52 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.350039, avg_loss=0.683751, seen=40, correct=23, accuracy=0.575000
2025-10-10 17:30:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:30:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:30:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:30:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1962MB allocated=1935MB
2025-10-10 17:30:54 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-10 17:30:54 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 17:30:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3638, total=14550)
2025-10-10 17:30:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:30:54 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 17:30:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:30:54 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 17:30:54 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=1819, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 17:31:03 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 17:31:03 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 17:31:03 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 17:31:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:31:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:31:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:31:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:31:06 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=129.497345, avg_loss=0.647487, seen=200, correct=119, accuracy=0.595000
2025-10-10 17:31:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:31:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:31:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:31:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1952MB
2025-10-10 17:31:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:31:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:31:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:31:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:31:10 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.881025, avg_loss=0.722026, seen=40, correct=23, accuracy=0.575000
2025-10-10 17:31:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:31:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:31:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:31:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1952MB
2025-10-10 17:31:12 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-10 17:31:21 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 17:31:21 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 17:31:21 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 17:31:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:31:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:31:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:31:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:31:24 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=135.165680, avg_loss=0.675828, seen=200, correct=126, accuracy=0.630000
2025-10-10 17:31:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:31:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:31:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:31:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1952MB
2025-10-10 17:31:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:31:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:31:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:31:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:31:27 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.429302, avg_loss=0.685733, seen=40, correct=23, accuracy=0.575000
2025-10-10 17:31:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:31:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:31:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:31:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1952MB
2025-10-10 17:31:29 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-10 17:31:38 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 17:31:38 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 17:31:38 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 17:31:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:31:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:31:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:31:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:31:41 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=131.157806, avg_loss=0.655789, seen=200, correct=125, accuracy=0.625000
2025-10-10 17:31:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:31:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:31:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:31:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1952MB
2025-10-10 17:31:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:31:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:31:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:31:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:31:44 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.589535, avg_loss=0.689738, seen=40, correct=22, accuracy=0.550000
2025-10-10 17:31:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:31:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:31:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:31:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1952MB
2025-10-10 17:31:45 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.575000, curr=0.550000
2025-10-10 17:31:54 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 17:31:54 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 17:31:54 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 17:31:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:31:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:31:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:31:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:31:57 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=129.227478, avg_loss=0.646137, seen=200, correct=125, accuracy=0.625000
2025-10-10 17:31:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:31:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:31:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:31:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1952MB
2025-10-10 17:32:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:32:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:32:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:32:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:32:01 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.362059, avg_loss=0.709051, seen=40, correct=22, accuracy=0.550000
2025-10-10 17:32:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:32:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:32:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:32:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1952MB
2025-10-10 17:32:02 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.575000, curr=0.550000
2025-10-10 17:32:10 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 17:32:10 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 17:32:10 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 17:32:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:32:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:32:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:32:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:32:13 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=130.166351, avg_loss=0.650832, seen=200, correct=120, accuracy=0.600000
2025-10-10 17:32:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:32:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:32:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:32:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1952MB
2025-10-10 17:32:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:32:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:32:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:32:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:32:16 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.946173, avg_loss=0.673654, seen=40, correct=22, accuracy=0.550000
2025-10-10 17:32:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:32:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:32:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:32:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1952MB
2025-10-10 17:32:17 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.575000, curr=0.550000
2025-10-10 17:32:25 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 17:32:25 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 17:32:25 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 17:32:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:32:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:32:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:32:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:32:28 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=132.512939, avg_loss=0.662565, seen=200, correct=126, accuracy=0.630000
2025-10-10 17:32:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:32:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:32:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:32:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1952MB
2025-10-10 17:32:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:32:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:32:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:32:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:32:31 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.996593, avg_loss=0.674915, seen=40, correct=24, accuracy=0.600000
2025-10-10 17:32:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:32:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:32:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:32:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1952MB
2025-10-10 17:32:34 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-10 17:32:41 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 17:32:41 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 17:32:41 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 17:32:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:32:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:32:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:32:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:32:44 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=128.149902, avg_loss=0.640750, seen=200, correct=119, accuracy=0.595000
2025-10-10 17:32:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:32:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:32:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:32:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1952MB
2025-10-10 17:32:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:32:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:32:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:32:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:32:49 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.907331, avg_loss=0.697683, seen=40, correct=24, accuracy=0.600000
2025-10-10 17:32:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:32:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:32:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:32:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1952MB
2025-10-10 17:32:50 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-10 17:32:58 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 17:32:58 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 17:32:58 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 17:32:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:32:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:32:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:33:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:33:01 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=127.651886, avg_loss=0.638259, seen=200, correct=124, accuracy=0.620000
2025-10-10 17:33:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:33:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:33:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:33:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1952MB
2025-10-10 17:33:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:33:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:33:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:33:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:33:05 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.318371, avg_loss=0.682959, seen=40, correct=25, accuracy=0.625000
2025-10-10 17:33:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:33:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:33:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:33:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1952MB
2025-10-10 17:33:06 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-10 17:33:15 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 17:33:15 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 17:33:15 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 17:33:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:33:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:33:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:33:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:33:18 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=131.920288, avg_loss=0.659601, seen=200, correct=128, accuracy=0.640000
2025-10-10 17:33:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:33:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:33:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:33:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1952MB
2025-10-10 17:33:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:33:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:33:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:33:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:33:21 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.005955, avg_loss=0.675149, seen=40, correct=22, accuracy=0.550000
2025-10-10 17:33:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:33:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:33:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:33:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1952MB
2025-10-10 17:33:22 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.625000, curr=0.550000
2025-10-10 17:33:31 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 17:33:31 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 17:33:31 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 17:33:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:33:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:33:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:33:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:33:34 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=127.352272, avg_loss=0.636761, seen=200, correct=127, accuracy=0.635000
2025-10-10 17:33:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:33:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:33:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:33:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1952MB
2025-10-10 17:33:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:33:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:33:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:33:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:33:38 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.763859, avg_loss=0.694096, seen=40, correct=23, accuracy=0.575000
2025-10-10 17:33:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:33:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:33:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:33:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1952MB
2025-10-10 17:33:40 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.625000, curr=0.575000
2025-10-10 17:33:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 17:33:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 17:33:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:33:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:33:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1952MB
2025-10-10 17:33:41 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #15', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 17:33:41 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #15', 'Round': 0, 'Results_raw': {}}
2025-10-10 17:33:41 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 17:33:42 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 17:33:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:33:42 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 17:33:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:33:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:33:44 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 17:33:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:33:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:33:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:33:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:33:47 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=125.705429, avg_loss=0.628527, seen=200, correct=129, accuracy=0.645000
2025-10-10 17:33:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:33:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:33:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:33:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1982MB allocated=1944MB
2025-10-10 17:33:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:33:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:33:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:33:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:33:51 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.164967, avg_loss=0.629124, seen=40, correct=26, accuracy=0.650000
2025-10-10 17:33:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:33:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:33:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:33:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1982MB allocated=1944MB
2025-10-10 17:33:52 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.650000
2025-10-10 17:33:52 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 17:33:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1184, total=4736)
2025-10-10 17:33:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:33:52 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 17:33:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:33:52 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 17:33:52 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=592, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 17:34:01 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 17:34:01 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 17:34:01 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 17:34:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:34:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:34:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:34:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:34:04 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=124.456490, avg_loss=0.622282, seen=200, correct=130, accuracy=0.650000
2025-10-10 17:34:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:34:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:34:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:34:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2016MB allocated=1961MB
2025-10-10 17:34:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:34:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:34:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:34:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:34:07 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.797667, avg_loss=0.619942, seen=40, correct=27, accuracy=0.675000
2025-10-10 17:34:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:34:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:34:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:34:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2016MB allocated=1961MB
2025-10-10 17:34:10 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.675000
2025-10-10 17:34:18 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 17:34:18 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 17:34:18 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 17:34:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:34:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:34:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:34:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:34:21 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=124.876167, avg_loss=0.624381, seen=200, correct=131, accuracy=0.655000
2025-10-10 17:34:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:34:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:34:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:34:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2016MB allocated=1961MB
2025-10-10 17:34:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:34:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:34:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:34:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:34:25 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.850246, avg_loss=0.621256, seen=40, correct=27, accuracy=0.675000
2025-10-10 17:34:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:34:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:34:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:34:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2016MB allocated=1961MB
2025-10-10 17:34:27 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.675000
2025-10-10 17:34:35 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 17:34:35 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 17:34:35 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 17:34:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:34:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:34:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:34:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:34:38 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=121.267746, avg_loss=0.606339, seen=200, correct=131, accuracy=0.655000
2025-10-10 17:34:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:34:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:34:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:34:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2016MB allocated=1961MB
2025-10-10 17:34:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:34:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:34:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:34:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:34:43 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.567680, avg_loss=0.614192, seen=40, correct=27, accuracy=0.675000
2025-10-10 17:34:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:34:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:34:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:34:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2016MB allocated=1961MB
2025-10-10 17:34:44 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.675000
2025-10-10 17:34:51 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 17:34:51 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 17:34:51 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 17:34:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:34:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:34:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:34:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:34:54 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=122.268341, avg_loss=0.611342, seen=200, correct=130, accuracy=0.650000
2025-10-10 17:34:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:34:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:34:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:34:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2016MB allocated=1961MB
2025-10-10 17:34:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:34:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:34:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:34:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:34:58 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.418268, avg_loss=0.610457, seen=40, correct=28, accuracy=0.700000
2025-10-10 17:34:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:34:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:34:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:34:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2016MB allocated=1961MB
2025-10-10 17:34:59 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.700000
2025-10-10 17:35:07 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 17:35:07 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 17:35:07 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 17:35:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:35:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:35:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:35:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:35:10 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=122.707428, avg_loss=0.613537, seen=200, correct=132, accuracy=0.660000
2025-10-10 17:35:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:35:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:35:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:35:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2016MB allocated=1961MB
2025-10-10 17:35:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:35:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:35:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:35:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:35:15 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.433174, avg_loss=0.610829, seen=40, correct=28, accuracy=0.700000
2025-10-10 17:35:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:35:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:35:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:35:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2016MB allocated=1961MB
2025-10-10 17:35:16 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.700000
2025-10-10 17:35:24 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 17:35:24 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 17:35:24 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 17:35:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:35:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:35:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:35:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:35:27 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=125.616714, avg_loss=0.628084, seen=200, correct=129, accuracy=0.645000
2025-10-10 17:35:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:35:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:35:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:35:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2016MB allocated=1961MB
2025-10-10 17:35:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:35:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:35:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:35:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:35:30 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.992176, avg_loss=0.624804, seen=40, correct=26, accuracy=0.650000
2025-10-10 17:35:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:35:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:35:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:35:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2016MB allocated=1961MB
2025-10-10 17:35:32 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.700000, curr=0.650000
2025-10-10 17:35:40 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 17:35:40 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 17:35:40 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 17:35:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:35:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:35:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:35:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:35:43 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=123.469482, avg_loss=0.617347, seen=200, correct=132, accuracy=0.660000
2025-10-10 17:35:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:35:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:35:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:35:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2016MB allocated=1961MB
2025-10-10 17:35:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:35:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:35:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:35:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:35:47 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.700754, avg_loss=0.617519, seen=40, correct=27, accuracy=0.675000
2025-10-10 17:35:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:35:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:35:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:35:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2016MB allocated=1961MB
2025-10-10 17:35:49 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.700000, curr=0.675000
2025-10-10 17:35:57 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 17:35:57 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 17:35:57 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 17:35:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:35:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:35:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:36:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:36:00 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=121.754196, avg_loss=0.608771, seen=200, correct=128, accuracy=0.640000
2025-10-10 17:36:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:36:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:36:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:36:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2016MB allocated=1961MB
2025-10-10 17:36:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:36:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:36:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:36:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:36:04 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.328861, avg_loss=0.608222, seen=40, correct=27, accuracy=0.675000
2025-10-10 17:36:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:36:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:36:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:36:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2016MB allocated=1961MB
2025-10-10 17:36:07 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.700000, curr=0.675000
2025-10-10 17:36:14 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 17:36:14 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 17:36:14 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 17:36:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:36:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:36:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:36:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:36:17 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=120.770088, avg_loss=0.603850, seen=200, correct=133, accuracy=0.665000
2025-10-10 17:36:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:36:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:36:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:36:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2016MB allocated=1961MB
2025-10-10 17:36:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:36:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:36:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:36:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:36:19 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.588104, avg_loss=0.614703, seen=40, correct=28, accuracy=0.700000
2025-10-10 17:36:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:36:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:36:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:36:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2016MB allocated=1961MB
2025-10-10 17:36:22 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.700000
2025-10-10 17:36:29 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 17:36:29 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 17:36:29 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 17:36:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:36:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:36:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:36:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:36:32 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=122.613281, avg_loss=0.613066, seen=200, correct=131, accuracy=0.655000
2025-10-10 17:36:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:36:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:36:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:36:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2016MB allocated=1961MB
2025-10-10 17:36:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:36:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:36:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:36:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:36:37 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.682224, avg_loss=0.617056, seen=40, correct=26, accuracy=0.650000
2025-10-10 17:36:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:36:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:36:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:36:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2016MB allocated=1961MB
2025-10-10 17:36:38 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.700000, curr=0.650000
2025-10-10 17:36:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 17:36:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 17:36:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:36:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:36:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2016MB allocated=1961MB
2025-10-10 17:36:39 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #35', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 17:36:39 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #35', 'Round': 0, 'Results_raw': {}}
2025-10-10 17:36:39 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 17:36:40 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 17:36:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:36:41 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 17:36:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:36:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:36:42 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 17:36:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-10-10 17:36:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:36:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:36:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-10-10 17:36:44 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=132, loss_sum=86.099205, avg_loss=0.652267, seen=132, correct=91, accuracy=0.689394
2025-10-10 17:36:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:36:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:36:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:36:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1982MB allocated=1952MB
2025-10-10 17:36:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:36:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:36:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:36:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:36:47 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.445295, avg_loss=0.711132, seen=40, correct=21, accuracy=0.525000
2025-10-10 17:36:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:36:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:36:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:36:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1982MB allocated=1952MB
2025-10-10 17:36:49 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.525000
2025-10-10 17:36:49 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 17:36:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=631, total=2521)
2025-10-10 17:36:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:36:49 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 17:36:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:36:49 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 17:36:49 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=316, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 17:36:58 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 17:36:58 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 17:36:58 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 17:36:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-10-10 17:36:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:36:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:37:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-10-10 17:37:01 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=132, loss_sum=86.504669, avg_loss=0.655338, seen=132, correct=88, accuracy=0.666667
2025-10-10 17:37:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:37:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:37:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:37:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2026MB allocated=1969MB
2025-10-10 17:37:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:37:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:37:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:37:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:37:04 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.447502, avg_loss=0.711188, seen=40, correct=25, accuracy=0.625000
2025-10-10 17:37:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:37:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:37:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:37:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2026MB allocated=1969MB
2025-10-10 17:37:05 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-10 17:37:13 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 17:37:13 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 17:37:13 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 17:37:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-10-10 17:37:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:37:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:37:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-10-10 17:37:15 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=132, loss_sum=90.101562, avg_loss=0.682588, seen=132, correct=82, accuracy=0.621212
2025-10-10 17:37:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:37:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:37:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:37:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2026MB allocated=1969MB
2025-10-10 17:37:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:37:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:37:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:37:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:37:19 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.103867, avg_loss=0.752597, seen=40, correct=23, accuracy=0.575000
2025-10-10 17:37:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:37:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:37:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:37:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2026MB allocated=1969MB
2025-10-10 17:37:20 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.625000, curr=0.575000
2025-10-10 17:37:29 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 17:37:29 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 17:37:29 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 17:37:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-10-10 17:37:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:37:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:37:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-10-10 17:37:31 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=132, loss_sum=86.183411, avg_loss=0.652905, seen=132, correct=88, accuracy=0.666667
2025-10-10 17:37:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:37:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:37:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:37:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2026MB allocated=1969MB
2025-10-10 17:37:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:37:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:37:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:37:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:37:36 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.723591, avg_loss=0.718090, seen=40, correct=22, accuracy=0.550000
2025-10-10 17:37:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:37:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:37:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:37:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2026MB allocated=1969MB
2025-10-10 17:37:37 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.625000, curr=0.550000
2025-10-10 17:37:45 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 17:37:45 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 17:37:45 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 17:37:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-10-10 17:37:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:37:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:37:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-10-10 17:37:47 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=132, loss_sum=86.055481, avg_loss=0.651935, seen=132, correct=86, accuracy=0.651515
2025-10-10 17:37:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:37:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:37:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:37:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2026MB allocated=1969MB
2025-10-10 17:37:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:37:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:37:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:37:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:37:51 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.827829, avg_loss=0.695696, seen=40, correct=25, accuracy=0.625000
2025-10-10 17:37:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:37:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:37:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:37:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2026MB allocated=1969MB
2025-10-10 17:37:52 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-10 17:37:59 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 17:37:59 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 17:37:59 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 17:37:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-10-10 17:37:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:37:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:38:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-10-10 17:38:01 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=132, loss_sum=85.915787, avg_loss=0.650877, seen=132, correct=86, accuracy=0.651515
2025-10-10 17:38:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:38:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:38:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:38:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2026MB allocated=1969MB
2025-10-10 17:38:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:38:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:38:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:38:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:38:05 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.150082, avg_loss=0.703752, seen=40, correct=24, accuracy=0.600000
2025-10-10 17:38:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:38:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:38:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:38:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2026MB allocated=1969MB
2025-10-10 17:38:07 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.625000, curr=0.600000
2025-10-10 17:38:15 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 17:38:15 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 17:38:15 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 17:38:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-10-10 17:38:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:38:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:38:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-10-10 17:38:17 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=132, loss_sum=87.030655, avg_loss=0.659323, seen=132, correct=84, accuracy=0.636364
2025-10-10 17:38:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:38:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:38:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:38:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2026MB allocated=1969MB
2025-10-10 17:38:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:38:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:38:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:38:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:38:21 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.526951, avg_loss=0.713174, seen=40, correct=22, accuracy=0.550000
2025-10-10 17:38:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:38:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:38:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:38:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2026MB allocated=1969MB
2025-10-10 17:38:22 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.625000, curr=0.550000
2025-10-10 17:38:31 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 17:38:31 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 17:38:31 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 17:38:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-10-10 17:38:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:38:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:38:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-10-10 17:38:33 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=132, loss_sum=86.631401, avg_loss=0.656298, seen=132, correct=86, accuracy=0.651515
2025-10-10 17:38:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:38:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:38:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:38:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2026MB allocated=1969MB
2025-10-10 17:38:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:38:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:38:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:38:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:38:38 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.033968, avg_loss=0.700849, seen=40, correct=26, accuracy=0.650000
2025-10-10 17:38:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:38:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:38:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:38:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2026MB allocated=1969MB
2025-10-10 17:38:39 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.650000
2025-10-10 17:38:47 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 17:38:47 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 17:38:47 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 17:38:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-10-10 17:38:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:38:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:38:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-10-10 17:38:49 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=132, loss_sum=87.161552, avg_loss=0.660315, seen=132, correct=82, accuracy=0.621212
2025-10-10 17:38:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:38:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:38:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:38:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2026MB allocated=1969MB
2025-10-10 17:38:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:38:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:38:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:38:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:38:53 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.402546, avg_loss=0.710064, seen=40, correct=26, accuracy=0.650000
2025-10-10 17:38:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:38:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:38:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:38:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2026MB allocated=1969MB
2025-10-10 17:38:54 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.650000
2025-10-10 17:39:03 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 17:39:03 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 17:39:03 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 17:39:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-10-10 17:39:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:39:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:39:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-10-10 17:39:05 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=132, loss_sum=85.365540, avg_loss=0.646709, seen=132, correct=88, accuracy=0.666667
2025-10-10 17:39:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:39:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:39:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:39:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2026MB allocated=1969MB
2025-10-10 17:39:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:39:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:39:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:39:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:39:08 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.935259, avg_loss=0.698381, seen=40, correct=24, accuracy=0.600000
2025-10-10 17:39:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:39:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:39:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:39:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2026MB allocated=1969MB
2025-10-10 17:39:09 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.650000, curr=0.600000
2025-10-10 17:39:18 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 17:39:18 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 17:39:18 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 17:39:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-10-10 17:39:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:39:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:39:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-10-10 17:39:20 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=132, loss_sum=85.810669, avg_loss=0.650081, seen=132, correct=87, accuracy=0.659091
2025-10-10 17:39:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:39:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:39:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:39:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2026MB allocated=1969MB
2025-10-10 17:39:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:39:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:39:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:39:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:39:25 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.739399, avg_loss=0.693485, seen=40, correct=23, accuracy=0.575000
2025-10-10 17:39:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:39:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:39:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:39:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2026MB allocated=1969MB
2025-10-10 17:39:26 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.650000, curr=0.575000
2025-10-10 17:39:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 17:39:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 17:39:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:39:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:39:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2026MB allocated=1969MB
2025-10-10 17:39:27 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #49', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 17:39:27 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #49', 'Round': 0, 'Results_raw': {}}
2025-10-10 17:39:27 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 17:39:28 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 17:39:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:39:29 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 17:39:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:39:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:39:30 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 17:39:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-10 17:39:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:39:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:39:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-10 17:39:32 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=70.419891, avg_loss=0.640181, seen=110, correct=70, accuracy=0.636364
2025-10-10 17:39:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:39:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:39:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:39:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2002MB allocated=1961MB
2025-10-10 17:39:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:39:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:39:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:39:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:39:36 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.936428, avg_loss=0.748411, seen=40, correct=22, accuracy=0.550000
2025-10-10 17:39:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:39:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:39:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:39:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2002MB allocated=1961MB
2025-10-10 17:39:38 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.550000
2025-10-10 17:39:38 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 17:39:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=526, total=2102)
2025-10-10 17:39:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:39:38 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 17:39:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:39:38 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 17:39:38 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=263, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 17:39:47 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 17:39:47 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 17:39:47 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 17:39:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-10 17:39:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:39:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:39:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-10 17:39:49 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=70.698868, avg_loss=0.642717, seen=110, correct=70, accuracy=0.636364
2025-10-10 17:39:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:39:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:39:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:39:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2044MB allocated=1977MB
2025-10-10 17:39:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:39:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:39:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:39:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:39:53 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.029387, avg_loss=0.725735, seen=40, correct=22, accuracy=0.550000
2025-10-10 17:39:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:39:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:39:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:39:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2044MB allocated=1977MB
2025-10-10 17:39:54 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.550000
2025-10-10 17:40:03 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 17:40:03 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 17:40:03 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 17:40:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-10 17:40:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:40:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:40:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-10 17:40:05 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=69.387756, avg_loss=0.630798, seen=110, correct=71, accuracy=0.645455
2025-10-10 17:40:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:40:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:40:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:40:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2044MB allocated=1977MB
2025-10-10 17:40:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:40:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:40:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:40:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:40:08 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=31.170219, avg_loss=0.779255, seen=40, correct=21, accuracy=0.525000
2025-10-10 17:40:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:40:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:40:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:40:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2044MB allocated=1977MB
2025-10-10 17:40:10 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.550000, curr=0.525000
2025-10-10 17:40:18 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 17:40:18 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 17:40:18 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 17:40:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-10 17:40:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:40:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:40:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-10 17:40:20 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=69.431442, avg_loss=0.631195, seen=110, correct=68, accuracy=0.618182
2025-10-10 17:40:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:40:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:40:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:40:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2044MB allocated=1977MB
2025-10-10 17:40:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:40:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:40:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:40:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:40:25 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.352100, avg_loss=0.758803, seen=40, correct=23, accuracy=0.575000
2025-10-10 17:40:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:40:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:40:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:40:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2044MB allocated=1977MB
2025-10-10 17:40:26 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-10 17:40:34 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 17:40:34 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 17:40:34 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 17:40:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-10 17:40:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:40:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:40:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-10 17:40:36 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=68.774330, avg_loss=0.625221, seen=110, correct=70, accuracy=0.636364
2025-10-10 17:40:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:40:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:40:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:40:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2044MB allocated=1977MB
2025-10-10 17:40:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:40:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:40:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:40:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:40:39 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.813633, avg_loss=0.745341, seen=40, correct=22, accuracy=0.550000
2025-10-10 17:40:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:40:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:40:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:40:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2044MB allocated=1977MB
2025-10-10 17:40:41 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.575000, curr=0.550000
2025-10-10 17:40:49 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 17:40:49 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 17:40:49 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 17:40:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-10 17:40:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:40:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:40:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-10 17:40:51 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=70.490639, avg_loss=0.640824, seen=110, correct=72, accuracy=0.654545
2025-10-10 17:40:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:40:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:40:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:40:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2044MB allocated=1977MB
2025-10-10 17:40:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:40:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:40:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:40:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:40:54 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.035778, avg_loss=0.700894, seen=40, correct=26, accuracy=0.650000
2025-10-10 17:40:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:40:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:40:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:40:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2044MB allocated=1977MB
2025-10-10 17:40:56 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.650000
2025-10-10 17:41:05 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 17:41:05 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 17:41:05 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 17:41:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-10 17:41:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:41:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:41:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-10 17:41:07 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=68.982605, avg_loss=0.627115, seen=110, correct=72, accuracy=0.654545
2025-10-10 17:41:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:41:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:41:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:41:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2044MB allocated=1977MB
2025-10-10 17:41:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:41:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:41:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:41:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:41:11 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.425684, avg_loss=0.735642, seen=40, correct=23, accuracy=0.575000
2025-10-10 17:41:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:41:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:41:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:41:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2044MB allocated=1977MB
2025-10-10 17:41:12 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.650000, curr=0.575000
2025-10-10 17:41:19 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 17:41:19 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 17:41:19 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 17:41:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-10 17:41:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:41:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:41:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-10 17:41:21 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=68.553932, avg_loss=0.623218, seen=110, correct=73, accuracy=0.663636
2025-10-10 17:41:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:41:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:41:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:41:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2044MB allocated=1977MB
2025-10-10 17:41:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:41:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:41:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:41:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:41:26 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.391649, avg_loss=0.734791, seen=40, correct=24, accuracy=0.600000
2025-10-10 17:41:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:41:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:41:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:41:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2044MB allocated=1977MB
2025-10-10 17:41:28 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.650000, curr=0.600000
2025-10-10 17:41:36 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 17:41:36 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 17:41:36 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 17:41:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-10 17:41:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:41:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:41:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-10 17:41:38 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=68.677605, avg_loss=0.624342, seen=110, correct=72, accuracy=0.654545
2025-10-10 17:41:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:41:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:41:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:41:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2044MB allocated=1977MB
2025-10-10 17:41:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:41:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:41:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:41:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:41:41 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.199787, avg_loss=0.704995, seen=40, correct=26, accuracy=0.650000
2025-10-10 17:41:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:41:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:41:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:41:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2044MB allocated=1977MB
2025-10-10 17:41:43 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.650000
2025-10-10 17:41:51 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 17:41:51 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 17:41:51 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 17:41:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-10 17:41:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:41:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:41:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-10 17:41:53 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=68.267982, avg_loss=0.620618, seen=110, correct=71, accuracy=0.645455
2025-10-10 17:41:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:41:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:41:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:41:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2044MB allocated=1977MB
2025-10-10 17:41:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:41:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:41:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:41:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:41:56 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.133104, avg_loss=0.703328, seen=40, correct=27, accuracy=0.675000
2025-10-10 17:41:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:41:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:41:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:41:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2044MB allocated=1977MB
2025-10-10 17:41:58 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.675000
2025-10-10 17:42:05 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 17:42:05 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 17:42:05 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 17:42:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-10 17:42:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:42:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:42:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-10 17:42:07 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=67.895020, avg_loss=0.617227, seen=110, correct=67, accuracy=0.609091
2025-10-10 17:42:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:42:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:42:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:42:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2044MB allocated=1977MB
2025-10-10 17:42:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:42:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:42:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:42:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:42:11 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.584936, avg_loss=0.739623, seen=40, correct=23, accuracy=0.575000
2025-10-10 17:42:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:42:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:42:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:42:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2044MB allocated=1977MB
2025-10-10 17:42:13 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.675000, curr=0.575000
2025-10-10 17:42:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 17:42:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 17:42:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:42:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:42:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2044MB allocated=1977MB
2025-10-10 17:42:14 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #19', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 17:42:14 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #19', 'Round': 0, 'Results_raw': {}}
2025-10-10 17:42:14 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 17:42:15 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 17:42:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:42:15 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 17:42:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:42:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:42:17 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 17:42:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-10-10 17:42:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:42:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:42:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-10-10 17:42:19 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=83, loss_sum=56.034283, avg_loss=0.675112, seen=83, correct=51, accuracy=0.614458
2025-10-10 17:42:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:42:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:42:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:42:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2002MB allocated=1969MB
2025-10-10 17:42:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:42:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:42:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:42:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:42:23 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.346407, avg_loss=0.633660, seen=40, correct=29, accuracy=0.725000
2025-10-10 17:42:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:42:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:42:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:42:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2002MB allocated=1969MB
2025-10-10 17:42:24 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.725000
2025-10-10 17:42:24 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 17:42:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=395, total=1580)
2025-10-10 17:42:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:42:25 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 17:42:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:42:25 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 17:42:25 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=198, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 17:42:33 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 17:42:33 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 17:42:33 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 17:42:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-10-10 17:42:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:42:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:42:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-10-10 17:42:34 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=83, loss_sum=55.970623, avg_loss=0.674345, seen=83, correct=51, accuracy=0.614458
2025-10-10 17:42:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:42:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:42:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:42:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2038MB allocated=1986MB
2025-10-10 17:42:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:42:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:42:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:42:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:42:38 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.655018, avg_loss=0.616375, seen=40, correct=29, accuracy=0.725000
2025-10-10 17:42:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:42:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:42:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:42:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2038MB allocated=1986MB
2025-10-10 17:42:40 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.725000
2025-10-10 17:42:48 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 17:42:48 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 17:42:48 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 17:42:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-10-10 17:42:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:42:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:42:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-10-10 17:42:49 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=83, loss_sum=55.551300, avg_loss=0.669293, seen=83, correct=52, accuracy=0.626506
2025-10-10 17:42:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:42:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:42:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:42:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2038MB allocated=1986MB
2025-10-10 17:42:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:42:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:42:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:42:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:42:54 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.003071, avg_loss=0.650077, seen=40, correct=29, accuracy=0.725000
2025-10-10 17:42:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:42:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:42:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:42:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2038MB allocated=1986MB
2025-10-10 17:42:55 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.725000
2025-10-10 17:43:01 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 17:43:01 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 17:43:01 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 17:43:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-10-10 17:43:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:43:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:43:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-10-10 17:43:03 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=83, loss_sum=55.603653, avg_loss=0.669924, seen=83, correct=51, accuracy=0.614458
2025-10-10 17:43:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:43:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:43:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:43:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2038MB allocated=1986MB
2025-10-10 17:43:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:43:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:43:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:43:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:43:07 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.520420, avg_loss=0.638011, seen=40, correct=31, accuracy=0.775000
2025-10-10 17:43:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:43:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:43:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:43:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2038MB allocated=1986MB
2025-10-10 17:43:08 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.775000
2025-10-10 17:43:16 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 17:43:16 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 17:43:16 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 17:43:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-10-10 17:43:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:43:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:43:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-10-10 17:43:17 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=83, loss_sum=55.107090, avg_loss=0.663941, seen=83, correct=53, accuracy=0.638554
2025-10-10 17:43:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:43:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:43:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:43:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2038MB allocated=1986MB
2025-10-10 17:43:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:43:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:43:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:43:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:43:22 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.927519, avg_loss=0.648188, seen=40, correct=29, accuracy=0.725000
2025-10-10 17:43:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:43:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:43:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:43:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2038MB allocated=1986MB
2025-10-10 17:43:24 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.775000, curr=0.725000
2025-10-10 17:43:33 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 17:43:33 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 17:43:33 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 17:43:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-10-10 17:43:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:43:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:43:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-10-10 17:43:34 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=83, loss_sum=55.513428, avg_loss=0.668836, seen=83, correct=51, accuracy=0.614458
2025-10-10 17:43:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:43:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:43:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:43:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2038MB allocated=1986MB
2025-10-10 17:43:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:43:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:43:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:43:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:43:39 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.881180, avg_loss=0.622029, seen=40, correct=29, accuracy=0.725000
2025-10-10 17:43:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:43:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:43:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:43:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2038MB allocated=1986MB
2025-10-10 17:43:41 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.775000, curr=0.725000
2025-10-10 17:43:50 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 17:43:50 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 17:43:50 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 17:43:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-10-10 17:43:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:43:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:43:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-10-10 17:43:51 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=83, loss_sum=55.862171, avg_loss=0.673038, seen=83, correct=52, accuracy=0.626506
2025-10-10 17:43:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:43:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:43:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:43:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2038MB allocated=1986MB
2025-10-10 17:43:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:43:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:43:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:43:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:43:56 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.582327, avg_loss=0.614558, seen=40, correct=28, accuracy=0.700000
2025-10-10 17:43:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:43:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:43:56 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:43:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2038MB allocated=1986MB
2025-10-10 17:43:58 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.775000, curr=0.700000
2025-10-10 17:44:06 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 17:44:06 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 17:44:06 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 17:44:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-10-10 17:44:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:44:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:44:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-10-10 17:44:08 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=83, loss_sum=54.566757, avg_loss=0.657431, seen=83, correct=53, accuracy=0.638554
2025-10-10 17:44:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:44:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:44:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:44:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2038MB allocated=1986MB
2025-10-10 17:44:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:44:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:44:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:44:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:44:11 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.514883, avg_loss=0.612872, seen=40, correct=29, accuracy=0.725000
2025-10-10 17:44:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:44:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:44:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:44:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2038MB allocated=1986MB
2025-10-10 17:44:14 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/25), best=0.775000, curr=0.725000
2025-10-10 17:44:22 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 17:44:22 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 17:44:22 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 17:44:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-10-10 17:44:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:44:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:44:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-10-10 17:44:24 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=83, loss_sum=54.241753, avg_loss=0.653515, seen=83, correct=55, accuracy=0.662651
2025-10-10 17:44:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:44:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:44:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:44:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2038MB allocated=1986MB
2025-10-10 17:44:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:44:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:44:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:44:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:44:28 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.083477, avg_loss=0.627087, seen=40, correct=28, accuracy=0.700000
2025-10-10 17:44:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:44:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:44:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:44:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2038MB allocated=1986MB
2025-10-10 17:44:30 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/25), best=0.775000, curr=0.700000
2025-10-10 17:44:39 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 17:44:39 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 17:44:39 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 17:44:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-10-10 17:44:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:44:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:44:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-10-10 17:44:41 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=83, loss_sum=54.615028, avg_loss=0.658012, seen=83, correct=52, accuracy=0.626506
2025-10-10 17:44:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:44:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:44:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:44:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2038MB allocated=1986MB
2025-10-10 17:44:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:44:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:44:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:44:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:44:47 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.023775, avg_loss=0.625594, seen=40, correct=28, accuracy=0.700000
2025-10-10 17:44:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:44:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:44:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:44:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2038MB allocated=1986MB
2025-10-10 17:44:50 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/25), best=0.775000, curr=0.700000
2025-10-10 17:44:59 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 17:44:59 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 17:44:59 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 17:44:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-10-10 17:44:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:44:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:45:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-10-10 17:45:00 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=83, loss_sum=56.724899, avg_loss=0.683433, seen=83, correct=44, accuracy=0.530120
2025-10-10 17:45:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:45:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:45:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:45:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2038MB allocated=1986MB
2025-10-10 17:45:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:45:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:45:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:45:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:45:04 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.597084, avg_loss=0.614927, seen=40, correct=27, accuracy=0.675000
2025-10-10 17:45:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:45:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:45:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:45:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2038MB allocated=1986MB
2025-10-10 17:45:06 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/25), best=0.775000, curr=0.675000
2025-10-10 17:45:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 17:45:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 17:45:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:45:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:45:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2038MB allocated=1986MB
2025-10-10 17:45:07 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #51', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 17:45:07 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #51', 'Round': 0, 'Results_raw': {}}
2025-10-10 17:45:08 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 17:45:09 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 17:45:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:45:09 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 17:45:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:45:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:45:10 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 17:45:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-10-10 17:45:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:45:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:45:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-10-10 17:45:12 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=54, loss_sum=36.608608, avg_loss=0.677937, seen=54, correct=29, accuracy=0.537037
2025-10-10 17:45:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:45:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:45:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:45:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2002MB allocated=1977MB
2025-10-10 17:45:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:45:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:45:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:45:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:45:16 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.029079, avg_loss=0.700727, seen=40, correct=21, accuracy=0.525000
2025-10-10 17:45:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:45:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:45:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:45:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2002MB allocated=1977MB
2025-10-10 17:45:18 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.525000
2025-10-10 17:45:18 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 17:45:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=258, total=1030)
2025-10-10 17:45:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:45:19 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 17:45:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:45:19 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 17:45:19 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=129, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 17:45:27 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 17:45:27 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 17:45:27 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 17:45:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-10-10 17:45:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:45:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:45:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-10-10 17:45:28 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=54, loss_sum=37.266403, avg_loss=0.690119, seen=54, correct=29, accuracy=0.537037
2025-10-10 17:45:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:45:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:45:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:45:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2032MB allocated=1994MB
2025-10-10 17:45:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:45:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:45:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:45:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:45:33 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.652679, avg_loss=0.716317, seen=40, correct=19, accuracy=0.475000
2025-10-10 17:45:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:45:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:45:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:45:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2032MB allocated=1994MB
2025-10-10 17:45:35 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.525000, curr=0.475000
2025-10-10 17:45:44 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 17:45:44 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 17:45:44 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 17:45:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-10-10 17:45:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:45:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:45:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-10-10 17:45:45 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=54, loss_sum=36.996174, avg_loss=0.685114, seen=54, correct=31, accuracy=0.574074
2025-10-10 17:45:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:45:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:45:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:45:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2032MB allocated=1994MB
2025-10-10 17:45:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:45:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:45:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:45:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:45:48 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.648975, avg_loss=0.716224, seen=40, correct=19, accuracy=0.475000
2025-10-10 17:45:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:45:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:45:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:45:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2032MB allocated=1994MB
2025-10-10 17:45:50 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.525000, curr=0.475000
2025-10-10 17:45:58 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 17:45:58 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 17:45:58 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 17:45:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-10-10 17:45:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:45:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:45:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-10-10 17:45:59 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=54, loss_sum=36.422131, avg_loss=0.674484, seen=54, correct=27, accuracy=0.500000
2025-10-10 17:45:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:45:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:46:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:46:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2032MB allocated=1994MB
2025-10-10 17:46:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:46:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:46:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:46:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:46:02 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.761806, avg_loss=0.694045, seen=40, correct=21, accuracy=0.525000
2025-10-10 17:46:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:46:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:46:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:46:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2032MB allocated=1994MB
2025-10-10 17:46:04 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.525000
2025-10-10 17:46:13 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 17:46:13 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 17:46:13 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 17:46:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-10-10 17:46:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:46:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:46:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-10-10 17:46:14 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=54, loss_sum=36.139107, avg_loss=0.669243, seen=54, correct=27, accuracy=0.500000
2025-10-10 17:46:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:46:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:46:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:46:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2032MB allocated=1994MB
2025-10-10 17:46:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:46:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:46:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:46:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:46:19 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.915794, avg_loss=0.672895, seen=40, correct=23, accuracy=0.575000
2025-10-10 17:46:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:46:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:46:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:46:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2032MB allocated=1994MB
2025-10-10 17:46:21 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-10 17:46:29 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 17:46:29 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 17:46:29 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 17:46:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-10-10 17:46:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:46:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:46:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-10-10 17:46:30 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=54, loss_sum=36.495998, avg_loss=0.675852, seen=54, correct=29, accuracy=0.537037
2025-10-10 17:46:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:46:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:46:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:46:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2032MB allocated=1994MB
2025-10-10 17:46:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:46:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:46:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:46:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:46:34 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.656393, avg_loss=0.666410, seen=40, correct=21, accuracy=0.525000
2025-10-10 17:46:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:46:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:46:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:46:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2032MB allocated=1994MB
2025-10-10 17:46:37 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.575000, curr=0.525000
2025-10-10 17:46:45 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 17:46:45 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 17:46:45 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 17:46:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-10-10 17:46:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:46:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:46:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-10-10 17:46:46 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=54, loss_sum=36.877392, avg_loss=0.682915, seen=54, correct=27, accuracy=0.500000
2025-10-10 17:46:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:46:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:46:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:46:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2032MB allocated=1994MB
2025-10-10 17:46:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:46:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:46:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:46:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:46:49 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.436670, avg_loss=0.685917, seen=40, correct=21, accuracy=0.525000
2025-10-10 17:46:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:46:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:46:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:46:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2032MB allocated=1994MB
2025-10-10 17:46:52 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.575000, curr=0.525000
2025-10-10 17:47:00 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 17:47:00 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 17:47:00 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 17:47:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-10-10 17:47:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:47:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:47:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-10-10 17:47:01 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=54, loss_sum=37.666252, avg_loss=0.697523, seen=54, correct=27, accuracy=0.500000
2025-10-10 17:47:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:47:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:47:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:47:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2032MB allocated=1994MB
2025-10-10 17:47:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:47:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:47:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:47:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:47:06 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.157452, avg_loss=0.703936, seen=40, correct=22, accuracy=0.550000
2025-10-10 17:47:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:47:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:47:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:47:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2032MB allocated=1994MB
2025-10-10 17:47:08 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.575000, curr=0.550000
2025-10-10 17:47:16 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 17:47:16 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 17:47:17 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 17:47:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-10-10 17:47:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:47:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:47:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-10-10 17:47:18 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=54, loss_sum=38.236038, avg_loss=0.708075, seen=54, correct=29, accuracy=0.537037
2025-10-10 17:47:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:47:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:47:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:47:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2032MB allocated=1994MB
2025-10-10 17:47:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:47:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:47:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:47:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:47:21 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.441921, avg_loss=0.736048, seen=40, correct=21, accuracy=0.525000
2025-10-10 17:47:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:47:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:47:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:47:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2032MB allocated=1994MB
2025-10-10 17:47:23 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/25), best=0.575000, curr=0.525000
2025-10-10 17:47:32 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 17:47:32 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 17:47:32 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 17:47:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-10-10 17:47:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:47:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:47:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-10-10 17:47:33 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=54, loss_sum=36.820118, avg_loss=0.681854, seen=54, correct=27, accuracy=0.500000
2025-10-10 17:47:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:47:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:47:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:47:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2032MB allocated=1994MB
2025-10-10 17:47:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:47:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:47:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:47:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:47:38 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.633781, avg_loss=0.690845, seen=40, correct=21, accuracy=0.525000
2025-10-10 17:47:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:47:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:47:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:47:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2032MB allocated=1994MB
2025-10-10 17:47:40 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/25), best=0.575000, curr=0.525000
2025-10-10 17:47:49 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 17:47:49 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 17:47:49 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 17:47:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-10-10 17:47:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:47:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:47:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-10-10 17:47:50 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=54, loss_sum=36.546417, avg_loss=0.676786, seen=54, correct=29, accuracy=0.537037
2025-10-10 17:47:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:47:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:47:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:47:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2032MB allocated=1994MB
2025-10-10 17:47:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:47:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:47:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:47:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:47:53 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.766891, avg_loss=0.669172, seen=40, correct=21, accuracy=0.525000
2025-10-10 17:47:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:47:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:47:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:47:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2032MB allocated=1994MB
2025-10-10 17:47:55 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/25), best=0.575000, curr=0.525000
2025-10-10 17:47:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 17:47:55 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 17:47:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:47:56 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:47:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2032MB allocated=1994MB
2025-10-10 17:47:57 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #36', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 17:47:57 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #36', 'Round': 0, 'Results_raw': {}}
2025-10-10 17:47:57 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 17:47:58 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 17:47:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:47:59 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 17:47:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:47:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:48:00 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 17:48:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-10-10 17:48:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:48:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:48:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 17:48:02 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=136, loss_sum=85.708786, avg_loss=0.630212, seen=136, correct=89, accuracy=0.654412
2025-10-10 17:48:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:48:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:48:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:48:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2022MB allocated=1986MB
2025-10-10 17:48:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:48:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:48:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:48:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:48:05 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.581896, avg_loss=0.589547, seen=40, correct=25, accuracy=0.625000
2025-10-10 17:48:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:48:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:48:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:48:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2022MB allocated=1986MB
2025-10-10 17:48:08 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-10 17:48:08 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 17:48:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=648, total=2589)
2025-10-10 17:48:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:48:08 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 17:48:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:48:08 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 17:48:08 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=324, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 17:48:16 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 17:48:16 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 17:48:16 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 17:48:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-10-10 17:48:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:48:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:48:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 17:48:18 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=136, loss_sum=86.015572, avg_loss=0.632467, seen=136, correct=90, accuracy=0.661765
2025-10-10 17:48:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:48:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:48:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:48:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2066MB allocated=2003MB
2025-10-10 17:48:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:48:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:48:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:48:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:48:22 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.341434, avg_loss=0.583536, seen=40, correct=25, accuracy=0.625000
2025-10-10 17:48:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:48:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:48:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:48:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2066MB allocated=2003MB
2025-10-10 17:48:24 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-10 17:48:31 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 17:48:31 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 17:48:31 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 17:48:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-10-10 17:48:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:48:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:48:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 17:48:33 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=136, loss_sum=88.381081, avg_loss=0.649861, seen=136, correct=94, accuracy=0.691176
2025-10-10 17:48:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:48:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:48:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:48:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2066MB allocated=2003MB
2025-10-10 17:48:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:48:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:48:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:48:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:48:37 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.206461, avg_loss=0.655162, seen=40, correct=23, accuracy=0.575000
2025-10-10 17:48:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:48:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:48:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:48:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2066MB allocated=2003MB
2025-10-10 17:48:40 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.625000, curr=0.575000
2025-10-10 17:48:48 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 17:48:48 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 17:48:48 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 17:48:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-10-10 17:48:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:48:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:48:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 17:48:50 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=136, loss_sum=87.407249, avg_loss=0.642700, seen=136, correct=94, accuracy=0.691176
2025-10-10 17:48:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:48:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:48:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:48:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2066MB allocated=2003MB
2025-10-10 17:48:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:48:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:48:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:48:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:48:54 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.578829, avg_loss=0.639471, seen=40, correct=26, accuracy=0.650000
2025-10-10 17:48:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:48:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:48:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:48:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2066MB allocated=2003MB
2025-10-10 17:48:57 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.650000
2025-10-10 17:49:05 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 17:49:05 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 17:49:05 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 17:49:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-10-10 17:49:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:49:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:49:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 17:49:07 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=136, loss_sum=85.534348, avg_loss=0.628929, seen=136, correct=92, accuracy=0.676471
2025-10-10 17:49:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:49:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:49:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:49:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2066MB allocated=2003MB
2025-10-10 17:49:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:49:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:49:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:49:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:49:11 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.581543, avg_loss=0.589539, seen=40, correct=24, accuracy=0.600000
2025-10-10 17:49:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:49:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:49:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:49:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2066MB allocated=2003MB
2025-10-10 17:49:14 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.650000, curr=0.600000
2025-10-10 17:49:24 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 17:49:24 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 17:49:24 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 17:49:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-10-10 17:49:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:49:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:49:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 17:49:26 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=136, loss_sum=85.334236, avg_loss=0.627458, seen=136, correct=92, accuracy=0.676471
2025-10-10 17:49:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:49:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:49:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:49:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2066MB allocated=2003MB
2025-10-10 17:49:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:49:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:49:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:49:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:49:30 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.298069, avg_loss=0.607452, seen=40, correct=25, accuracy=0.625000
2025-10-10 17:49:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:49:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:49:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:49:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2066MB allocated=2003MB
2025-10-10 17:49:33 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.650000, curr=0.625000
2025-10-10 17:49:41 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 17:49:41 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 17:49:41 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 17:49:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-10-10 17:49:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:49:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:49:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 17:49:43 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=136, loss_sum=85.202934, avg_loss=0.626492, seen=136, correct=93, accuracy=0.683824
2025-10-10 17:49:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:49:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:49:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:49:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2066MB allocated=2003MB
2025-10-10 17:49:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:49:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:49:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:49:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:49:47 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.151379, avg_loss=0.578784, seen=40, correct=24, accuracy=0.600000
2025-10-10 17:49:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:49:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:49:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:49:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2066MB allocated=2003MB
2025-10-10 17:49:50 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.650000, curr=0.600000
2025-10-10 17:49:58 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 17:49:58 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 17:49:58 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 17:49:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-10-10 17:49:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:49:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:50:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 17:50:00 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=136, loss_sum=85.400192, avg_loss=0.627943, seen=136, correct=90, accuracy=0.661765
2025-10-10 17:50:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:50:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:50:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:50:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2066MB allocated=2003MB
2025-10-10 17:50:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:50:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:50:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:50:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:50:02 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.130802, avg_loss=0.578270, seen=40, correct=26, accuracy=0.650000
2025-10-10 17:50:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:50:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:50:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:50:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2066MB allocated=2003MB
2025-10-10 17:50:05 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.650000
2025-10-10 17:50:14 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 17:50:14 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 17:50:14 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 17:50:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-10-10 17:50:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:50:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:50:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 17:50:16 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=136, loss_sum=84.189407, avg_loss=0.619040, seen=136, correct=93, accuracy=0.683824
2025-10-10 17:50:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:50:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:50:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:50:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2066MB allocated=2003MB
2025-10-10 17:50:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:50:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:50:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:50:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:50:19 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.568981, avg_loss=0.589225, seen=40, correct=26, accuracy=0.650000
2025-10-10 17:50:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:50:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:50:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:50:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2066MB allocated=2003MB
2025-10-10 17:50:21 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.650000
2025-10-10 17:50:30 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 17:50:30 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 17:50:30 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 17:50:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-10-10 17:50:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:50:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:50:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 17:50:33 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=136, loss_sum=83.679634, avg_loss=0.615291, seen=136, correct=92, accuracy=0.676471
2025-10-10 17:50:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:50:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:50:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:50:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2066MB allocated=2003MB
2025-10-10 17:50:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:50:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:50:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:50:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:50:36 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.357197, avg_loss=0.583930, seen=40, correct=26, accuracy=0.650000
2025-10-10 17:50:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:50:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:50:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:50:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2066MB allocated=2003MB
2025-10-10 17:50:38 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.650000
2025-10-10 17:50:47 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 17:50:47 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 17:50:47 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 17:50:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-10-10 17:50:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:50:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:50:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 17:50:49 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=136, loss_sum=84.148064, avg_loss=0.618736, seen=136, correct=95, accuracy=0.698529
2025-10-10 17:50:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:50:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:50:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:50:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2066MB allocated=2003MB
2025-10-10 17:50:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:50:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:50:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:50:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:50:53 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.776691, avg_loss=0.594417, seen=40, correct=27, accuracy=0.675000
2025-10-10 17:50:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:50:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:50:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:50:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2066MB allocated=2003MB
2025-10-10 17:50:55 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.675000
2025-10-10 17:50:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 17:50:55 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 17:50:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:50:56 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:50:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2066MB allocated=2003MB
2025-10-10 17:50:57 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #16', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 17:50:57 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #16', 'Round': 0, 'Results_raw': {}}
2025-10-10 17:50:57 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 17:50:58 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 17:50:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:50:59 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 17:50:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:50:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:51:00 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 17:51:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-10-10 17:51:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:51:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:51:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 17:51:03 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=134, loss_sum=87.761772, avg_loss=0.654939, seen=134, correct=87, accuracy=0.649254
2025-10-10 17:51:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:51:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:51:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:51:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2022MB allocated=1994MB
2025-10-10 17:51:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:51:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:51:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:51:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:51:06 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.540478, avg_loss=0.713512, seen=40, correct=21, accuracy=0.525000
2025-10-10 17:51:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:51:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:51:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:51:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2022MB allocated=1994MB
2025-10-10 17:51:08 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.525000
2025-10-10 17:51:08 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 17:51:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=637, total=2547)
2025-10-10 17:51:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:51:08 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 17:51:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:51:08 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 17:51:08 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=319, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 17:51:16 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 17:51:16 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 17:51:16 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 17:51:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-10-10 17:51:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:51:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:51:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 17:51:18 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=134, loss_sum=92.378853, avg_loss=0.689394, seen=134, correct=72, accuracy=0.537313
2025-10-10 17:51:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:51:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:51:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:51:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2056MB allocated=2011MB
2025-10-10 17:51:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:51:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:51:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:51:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:51:22 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.104166, avg_loss=0.752604, seen=40, correct=24, accuracy=0.600000
2025-10-10 17:51:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:51:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:51:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:51:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2056MB allocated=2011MB
2025-10-10 17:51:24 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-10 17:51:33 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 17:51:33 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 17:51:33 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 17:51:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-10-10 17:51:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:51:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:51:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 17:51:35 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=134, loss_sum=87.011139, avg_loss=0.649337, seen=134, correct=86, accuracy=0.641791
2025-10-10 17:51:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:51:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:51:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:51:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2056MB allocated=2011MB
2025-10-10 17:51:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:51:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:51:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:51:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:51:39 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.762861, avg_loss=0.719072, seen=40, correct=22, accuracy=0.550000
2025-10-10 17:51:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:51:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:51:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:51:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2056MB allocated=2011MB
2025-10-10 17:51:41 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.600000, curr=0.550000
2025-10-10 17:51:49 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 17:51:49 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 17:51:49 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 17:51:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-10-10 17:51:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:51:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:51:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 17:51:51 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=134, loss_sum=86.696320, avg_loss=0.646987, seen=134, correct=90, accuracy=0.671642
2025-10-10 17:51:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:51:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:51:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:51:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2056MB allocated=2011MB
2025-10-10 17:51:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:51:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:51:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:51:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:51:56 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.846600, avg_loss=0.696165, seen=40, correct=20, accuracy=0.500000
2025-10-10 17:51:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:51:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:51:56 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:51:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2056MB allocated=2011MB
2025-10-10 17:51:58 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.600000, curr=0.500000
2025-10-10 17:52:06 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 17:52:06 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 17:52:06 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 17:52:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-10-10 17:52:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:52:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:52:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 17:52:08 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=134, loss_sum=86.525444, avg_loss=0.645712, seen=134, correct=90, accuracy=0.671642
2025-10-10 17:52:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:52:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:52:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:52:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2056MB allocated=2011MB
2025-10-10 17:52:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:52:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:52:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:52:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:52:13 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.091301, avg_loss=0.702283, seen=40, correct=20, accuracy=0.500000
2025-10-10 17:52:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:52:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:52:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:52:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2056MB allocated=2011MB
2025-10-10 17:52:15 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.600000, curr=0.500000
2025-10-10 17:52:24 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 17:52:24 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 17:52:24 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 17:52:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-10-10 17:52:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:52:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:52:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 17:52:26 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=134, loss_sum=86.679703, avg_loss=0.646863, seen=134, correct=88, accuracy=0.656716
2025-10-10 17:52:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:52:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:52:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:52:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2056MB allocated=2011MB
2025-10-10 17:52:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:52:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:52:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:52:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:52:30 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.967426, avg_loss=0.699186, seen=40, correct=21, accuracy=0.525000
2025-10-10 17:52:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:52:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:52:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:52:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2056MB allocated=2011MB
2025-10-10 17:52:32 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/25), best=0.600000, curr=0.525000
2025-10-10 17:52:39 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 17:52:39 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 17:52:39 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 17:52:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-10-10 17:52:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:52:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:52:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 17:52:41 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=134, loss_sum=87.992722, avg_loss=0.656662, seen=134, correct=80, accuracy=0.597015
2025-10-10 17:52:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:52:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:52:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:52:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2056MB allocated=2011MB
2025-10-10 17:52:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:52:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:52:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:52:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:52:44 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.079792, avg_loss=0.701995, seen=40, correct=21, accuracy=0.525000
2025-10-10 17:52:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:52:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:52:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:52:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2056MB allocated=2011MB
2025-10-10 17:52:47 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/25), best=0.600000, curr=0.525000
2025-10-10 17:52:55 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 17:52:55 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 17:52:55 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 17:52:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-10-10 17:52:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:52:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:52:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 17:52:57 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=134, loss_sum=86.232811, avg_loss=0.643528, seen=134, correct=91, accuracy=0.679104
2025-10-10 17:52:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:52:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:52:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:53:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2056MB allocated=2011MB
2025-10-10 17:53:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:53:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:53:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:53:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:53:01 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.580853, avg_loss=0.689521, seen=40, correct=21, accuracy=0.525000
2025-10-10 17:53:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:53:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:53:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:53:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2056MB allocated=2011MB
2025-10-10 17:53:04 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/25), best=0.600000, curr=0.525000
2025-10-10 17:53:13 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 17:53:13 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 17:53:13 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 17:53:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-10-10 17:53:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:53:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:53:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 17:53:15 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=134, loss_sum=85.592079, avg_loss=0.638747, seen=134, correct=88, accuracy=0.656716
2025-10-10 17:53:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:53:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:53:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:53:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2056MB allocated=2011MB
2025-10-10 17:53:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:53:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:53:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:53:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:53:18 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.649426, avg_loss=0.691236, seen=40, correct=20, accuracy=0.500000
2025-10-10 17:53:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:53:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:53:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:53:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2056MB allocated=2011MB
2025-10-10 17:53:21 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/25), best=0.600000, curr=0.500000
2025-10-10 17:53:29 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 17:53:29 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 17:53:29 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 17:53:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-10-10 17:53:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:53:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:53:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 17:53:31 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=134, loss_sum=86.020386, avg_loss=0.641943, seen=134, correct=88, accuracy=0.656716
2025-10-10 17:53:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:53:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:53:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:53:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2056MB allocated=2011MB
2025-10-10 17:53:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:53:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:53:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:53:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:53:35 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.458935, avg_loss=0.686473, seen=40, correct=21, accuracy=0.525000
2025-10-10 17:53:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:53:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:53:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:53:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2056MB allocated=2011MB
2025-10-10 17:53:38 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/25), best=0.600000, curr=0.525000
2025-10-10 17:53:46 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 17:53:46 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 17:53:46 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 17:53:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-10-10 17:53:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:53:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:53:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 17:53:48 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=134, loss_sum=86.151085, avg_loss=0.642919, seen=134, correct=89, accuracy=0.664179
2025-10-10 17:53:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:53:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:53:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:53:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2056MB allocated=2011MB
2025-10-10 17:53:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:53:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:53:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:53:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:53:52 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.925724, avg_loss=0.698143, seen=40, correct=19, accuracy=0.475000
2025-10-10 17:53:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:53:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:53:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:53:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2056MB allocated=2011MB
2025-10-10 17:53:55 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=9/25), best=0.600000, curr=0.475000
2025-10-10 17:53:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 17:53:55 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 17:53:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:53:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:53:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2056MB allocated=2011MB
2025-10-10 17:53:56 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #6', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 17:53:56 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #6', 'Round': 0, 'Results_raw': {}}
2025-10-10 17:53:56 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 17:53:57 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 17:53:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:53:58 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 17:53:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:53:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:53:59 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 17:53:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:53:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:54:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:54:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:54:03 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=132.566864, avg_loss=0.662834, seen=200, correct=119, accuracy=0.595000
2025-10-10 17:54:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:54:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:54:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:54:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2042MB allocated=2003MB
2025-10-10 17:54:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:54:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:54:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:54:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:54:07 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.610863, avg_loss=0.615272, seen=40, correct=28, accuracy=0.700000
2025-10-10 17:54:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:54:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:54:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:54:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2042MB allocated=2003MB
2025-10-10 17:54:10 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.700000
2025-10-10 17:54:10 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 17:54:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1548, total=6191)
2025-10-10 17:54:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:54:10 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 17:54:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:54:10 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 17:54:10 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=774, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 17:54:18 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 17:54:18 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 17:54:18 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 17:54:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:54:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:54:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:54:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:54:21 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=134.534271, avg_loss=0.672671, seen=200, correct=123, accuracy=0.615000
2025-10-10 17:54:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:54:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:54:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:54:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2080MB allocated=2019MB
2025-10-10 17:54:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:54:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:54:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:54:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:54:25 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.608944, avg_loss=0.640224, seen=40, correct=26, accuracy=0.650000
2025-10-10 17:54:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:54:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:54:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:54:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2080MB allocated=2019MB
2025-10-10 17:54:28 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.700000, curr=0.650000
2025-10-10 17:54:36 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 17:54:36 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 17:54:36 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 17:54:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:54:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:54:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:54:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:54:39 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=137.865494, avg_loss=0.689327, seen=200, correct=116, accuracy=0.580000
2025-10-10 17:54:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:54:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:54:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:54:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2080MB allocated=2019MB
2025-10-10 17:54:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:54:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:54:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:54:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:54:43 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.346127, avg_loss=0.583653, seen=40, correct=26, accuracy=0.650000
2025-10-10 17:54:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:54:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:54:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:54:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2080MB allocated=2019MB
2025-10-10 17:54:45 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.700000, curr=0.650000
2025-10-10 17:54:54 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 17:54:54 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 17:54:54 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 17:54:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:54:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:54:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:54:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:54:57 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=132.534271, avg_loss=0.662671, seen=200, correct=118, accuracy=0.590000
2025-10-10 17:54:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:54:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:54:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:54:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2080MB allocated=2019MB
2025-10-10 17:54:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:54:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:54:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:55:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:55:00 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.813366, avg_loss=0.595334, seen=40, correct=30, accuracy=0.750000
2025-10-10 17:55:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:55:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:55:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:55:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2080MB allocated=2019MB
2025-10-10 17:55:02 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.750000
2025-10-10 17:55:11 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 17:55:11 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 17:55:11 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 17:55:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:55:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:55:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:55:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:55:14 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=131.362518, avg_loss=0.656813, seen=200, correct=122, accuracy=0.610000
2025-10-10 17:55:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:55:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:55:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:55:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2080MB allocated=2019MB
2025-10-10 17:55:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:55:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:55:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:55:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:55:19 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.345470, avg_loss=0.608637, seen=40, correct=30, accuracy=0.750000
2025-10-10 17:55:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:55:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:55:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:55:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2080MB allocated=2019MB
2025-10-10 17:55:21 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.750000
2025-10-10 17:55:28 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 17:55:28 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 17:55:28 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 17:55:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:55:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:55:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:55:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:55:31 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=132.145111, avg_loss=0.660726, seen=200, correct=116, accuracy=0.580000
2025-10-10 17:55:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:55:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:55:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:55:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2080MB allocated=2019MB
2025-10-10 17:55:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:55:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:55:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:55:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:55:34 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.698963, avg_loss=0.617474, seen=40, correct=28, accuracy=0.700000
2025-10-10 17:55:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:55:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:55:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:55:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2080MB allocated=2019MB
2025-10-10 17:55:37 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.750000, curr=0.700000
2025-10-10 17:55:45 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 17:55:45 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 17:55:45 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 17:55:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:55:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:55:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:55:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:55:48 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=131.275925, avg_loss=0.656380, seen=200, correct=119, accuracy=0.595000
2025-10-10 17:55:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:55:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:55:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:55:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2080MB allocated=2019MB
2025-10-10 17:55:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:55:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:55:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:55:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:55:51 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.641779, avg_loss=0.616044, seen=40, correct=28, accuracy=0.700000
2025-10-10 17:55:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:55:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:55:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:55:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2080MB allocated=2019MB
2025-10-10 17:55:54 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.750000, curr=0.700000
2025-10-10 17:56:02 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 17:56:02 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 17:56:02 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 17:56:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:56:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:56:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:56:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:56:05 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=131.830170, avg_loss=0.659151, seen=200, correct=119, accuracy=0.595000
2025-10-10 17:56:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:56:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:56:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:56:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2080MB allocated=2019MB
2025-10-10 17:56:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:56:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:56:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:56:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:56:09 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.156605, avg_loss=0.628915, seen=40, correct=29, accuracy=0.725000
2025-10-10 17:56:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:56:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:56:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:56:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2080MB allocated=2019MB
2025-10-10 17:56:10 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.750000, curr=0.725000
2025-10-10 17:56:20 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 17:56:20 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 17:56:20 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 17:56:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:56:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:56:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:56:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:56:23 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=131.485840, avg_loss=0.657429, seen=200, correct=118, accuracy=0.590000
2025-10-10 17:56:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:56:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:56:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:56:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2080MB allocated=2019MB
2025-10-10 17:56:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:56:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:56:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:56:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:56:27 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.136518, avg_loss=0.603413, seen=40, correct=28, accuracy=0.700000
2025-10-10 17:56:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:56:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:56:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:56:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2080MB allocated=2019MB
2025-10-10 17:56:29 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/25), best=0.750000, curr=0.700000
2025-10-10 17:56:37 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 17:56:37 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 17:56:37 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 17:56:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:56:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:56:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:56:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:56:40 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=132.309662, avg_loss=0.661548, seen=200, correct=121, accuracy=0.605000
2025-10-10 17:56:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:56:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:56:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:56:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2080MB allocated=2019MB
2025-10-10 17:56:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:56:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:56:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:56:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:56:44 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.226807, avg_loss=0.630670, seen=40, correct=28, accuracy=0.700000
2025-10-10 17:56:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:56:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:56:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:56:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2080MB allocated=2019MB
2025-10-10 17:56:46 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/25), best=0.750000, curr=0.700000
2025-10-10 17:56:53 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 17:56:53 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 17:56:53 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 17:56:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:56:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:56:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:56:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:56:56 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=132.713028, avg_loss=0.663565, seen=200, correct=121, accuracy=0.605000
2025-10-10 17:56:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:56:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:56:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:56:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2080MB allocated=2019MB
2025-10-10 17:56:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:56:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:56:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:56:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:56:59 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.632275, avg_loss=0.640807, seen=40, correct=30, accuracy=0.750000
2025-10-10 17:56:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:56:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:57:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:57:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2080MB allocated=2019MB
2025-10-10 17:57:02 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.750000
2025-10-10 17:57:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 17:57:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 17:57:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:57:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:57:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2080MB allocated=2019MB
2025-10-10 17:57:03 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #29', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 17:57:03 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #29', 'Round': 0, 'Results_raw': {}}
2025-10-10 17:57:03 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 17:57:04 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 17:57:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:57:05 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 17:57:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:57:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:57:06 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 17:57:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:57:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:57:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:57:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:57:10 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=121.188095, avg_loss=0.605940, seen=200, correct=135, accuracy=0.675000
2025-10-10 17:57:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:57:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:57:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:57:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2042MB allocated=2011MB
2025-10-10 17:57:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:57:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:57:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:57:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:57:14 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.544338, avg_loss=0.663608, seen=40, correct=26, accuracy=0.650000
2025-10-10 17:57:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:57:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:57:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:57:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2042MB allocated=2011MB
2025-10-10 17:57:15 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.650000
2025-10-10 17:57:15 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 17:57:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1471, total=5883)
2025-10-10 17:57:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:57:16 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 17:57:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:57:16 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 17:57:16 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=736, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 17:57:24 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 17:57:24 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 17:57:24 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 17:57:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:57:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:57:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:57:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:57:27 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=121.845154, avg_loss=0.609226, seen=200, correct=127, accuracy=0.635000
2025-10-10 17:57:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:57:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:57:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:57:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2078MB allocated=2028MB
2025-10-10 17:57:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:57:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:57:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:57:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:57:31 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.152504, avg_loss=0.678813, seen=40, correct=22, accuracy=0.550000
2025-10-10 17:57:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:57:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:57:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:57:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2078MB allocated=2028MB
2025-10-10 17:57:32 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.650000, curr=0.550000
2025-10-10 17:57:40 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 17:57:40 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 17:57:40 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 17:57:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:57:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:57:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:57:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:57:43 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=119.781456, avg_loss=0.598907, seen=200, correct=135, accuracy=0.675000
2025-10-10 17:57:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:57:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:57:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:57:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2078MB allocated=2028MB
2025-10-10 17:57:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:57:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:57:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:57:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:57:46 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.893682, avg_loss=0.672342, seen=40, correct=25, accuracy=0.625000
2025-10-10 17:57:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:57:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:57:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:57:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2078MB allocated=2028MB
2025-10-10 17:57:48 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.650000, curr=0.625000
2025-10-10 17:57:56 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 17:57:56 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 17:57:56 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 17:57:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:57:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:57:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:57:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:57:59 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=119.249603, avg_loss=0.596248, seen=200, correct=136, accuracy=0.680000
2025-10-10 17:57:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:57:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:58:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:58:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2078MB allocated=2028MB
2025-10-10 17:58:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:58:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:58:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:58:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:58:03 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.272348, avg_loss=0.681809, seen=40, correct=26, accuracy=0.650000
2025-10-10 17:58:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:58:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:58:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:58:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2078MB allocated=2028MB
2025-10-10 17:58:05 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.650000
2025-10-10 17:58:15 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 17:58:15 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 17:58:15 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 17:58:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:58:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:58:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:58:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:58:18 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=123.821625, avg_loss=0.619108, seen=200, correct=128, accuracy=0.640000
2025-10-10 17:58:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:58:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:58:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:58:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2078MB allocated=2028MB
2025-10-10 17:58:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:58:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:58:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:58:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:58:22 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.214729, avg_loss=0.705368, seen=40, correct=22, accuracy=0.550000
2025-10-10 17:58:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:58:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:58:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:58:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2078MB allocated=2028MB
2025-10-10 17:58:24 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.650000, curr=0.550000
2025-10-10 17:58:33 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 17:58:33 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 17:58:33 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 17:58:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:58:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:58:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:58:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:58:36 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=119.492645, avg_loss=0.597463, seen=200, correct=137, accuracy=0.685000
2025-10-10 17:58:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:58:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:58:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:58:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2078MB allocated=2028MB
2025-10-10 17:58:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:58:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:58:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:58:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:58:39 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.514212, avg_loss=0.687855, seen=40, correct=26, accuracy=0.650000
2025-10-10 17:58:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:58:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:58:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:58:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2078MB allocated=2028MB
2025-10-10 17:58:41 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.650000
2025-10-10 17:58:50 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 17:58:50 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 17:58:50 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 17:58:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:58:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:58:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:58:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:58:53 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=122.070877, avg_loss=0.610354, seen=200, correct=130, accuracy=0.650000
2025-10-10 17:58:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:58:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:58:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:58:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2078MB allocated=2028MB
2025-10-10 17:58:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:58:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:58:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:58:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:58:58 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.917252, avg_loss=0.697931, seen=40, correct=22, accuracy=0.550000
2025-10-10 17:58:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:58:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:58:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:59:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2078MB allocated=2028MB
2025-10-10 17:59:00 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.650000, curr=0.550000
2025-10-10 17:59:10 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 17:59:10 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 17:59:10 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 17:59:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:59:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:59:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:59:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:59:13 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=120.734833, avg_loss=0.603674, seen=200, correct=132, accuracy=0.660000
2025-10-10 17:59:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:59:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:59:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:59:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2078MB allocated=2028MB
2025-10-10 17:59:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:59:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:59:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:59:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:59:17 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.948547, avg_loss=0.698714, seen=40, correct=20, accuracy=0.500000
2025-10-10 17:59:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:59:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:59:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:59:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2078MB allocated=2028MB
2025-10-10 17:59:20 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.650000, curr=0.500000
2025-10-10 17:59:28 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 17:59:28 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 17:59:28 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 17:59:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:59:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:59:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:59:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:59:31 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=120.394638, avg_loss=0.601973, seen=200, correct=134, accuracy=0.670000
2025-10-10 17:59:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:59:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:59:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:59:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2078MB allocated=2028MB
2025-10-10 17:59:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:59:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:59:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:59:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:59:36 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.418842, avg_loss=0.685471, seen=40, correct=26, accuracy=0.650000
2025-10-10 17:59:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:59:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:59:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:59:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2078MB allocated=2028MB
2025-10-10 17:59:38 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.650000
2025-10-10 17:59:45 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 17:59:45 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 17:59:45 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 17:59:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 17:59:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:59:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:59:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 17:59:48 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=119.732269, avg_loss=0.598661, seen=200, correct=133, accuracy=0.665000
2025-10-10 17:59:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:59:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:59:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:59:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2078MB allocated=2028MB
2025-10-10 17:59:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 17:59:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:59:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 17:59:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 17:59:51 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.339392, avg_loss=0.683485, seen=40, correct=25, accuracy=0.625000
2025-10-10 17:59:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 17:59:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 17:59:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 17:59:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2078MB allocated=2028MB
2025-10-10 17:59:54 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.650000, curr=0.625000
2025-10-10 18:00:03 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 18:00:03 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 18:00:03 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 18:00:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 18:00:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:00:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:00:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 18:00:06 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=127.816093, avg_loss=0.639080, seen=200, correct=124, accuracy=0.620000
2025-10-10 18:00:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:00:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:00:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:00:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2078MB allocated=2028MB
2025-10-10 18:00:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:00:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:00:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:00:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:00:10 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.516973, avg_loss=0.712924, seen=40, correct=22, accuracy=0.550000
2025-10-10 18:00:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:00:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:00:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:00:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2078MB allocated=2028MB
2025-10-10 18:00:12 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.650000, curr=0.550000
2025-10-10 18:00:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 18:00:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 18:00:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:00:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:00:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2078MB allocated=2028MB
2025-10-10 18:00:13 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #17', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 18:00:13 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #17', 'Round': 0, 'Results_raw': {}}
2025-10-10 18:00:13 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 18:00:15 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 18:00:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:00:15 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 18:00:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:00:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:00:17 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 18:00:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-10 18:00:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:00:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:00:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-10 18:00:19 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=75.368561, avg_loss=0.685169, seen=110, correct=62, accuracy=0.563636
2025-10-10 18:00:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:00:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:00:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:00:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2042MB allocated=2019MB
2025-10-10 18:00:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:00:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:00:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:00:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:00:23 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=21.934479, avg_loss=0.548362, seen=40, correct=27, accuracy=0.675000
2025-10-10 18:00:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:00:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:00:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:00:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2042MB allocated=2019MB
2025-10-10 18:00:25 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.675000
2025-10-10 18:00:25 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 18:00:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=525, total=2100)
2025-10-10 18:00:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:00:26 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 18:00:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:00:26 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 18:00:26 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=263, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 18:00:34 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 18:00:34 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 18:00:34 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 18:00:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-10 18:00:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:00:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:00:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-10 18:00:36 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=77.425789, avg_loss=0.703871, seen=110, correct=64, accuracy=0.581818
2025-10-10 18:00:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:00:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:00:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:00:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2074MB allocated=2036MB
2025-10-10 18:00:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:00:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:00:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:00:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:00:40 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.376133, avg_loss=0.584403, seen=40, correct=28, accuracy=0.700000
2025-10-10 18:00:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:00:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:00:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:00:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2074MB allocated=2036MB
2025-10-10 18:00:43 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.700000
2025-10-10 18:00:51 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 18:00:51 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 18:00:51 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 18:00:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-10 18:00:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:00:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:00:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-10 18:00:53 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=76.384918, avg_loss=0.694408, seen=110, correct=65, accuracy=0.590909
2025-10-10 18:00:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:00:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:00:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:00:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2074MB allocated=2036MB
2025-10-10 18:00:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:00:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:00:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:00:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:00:55 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=22.153320, avg_loss=0.553833, seen=40, correct=25, accuracy=0.625000
2025-10-10 18:00:55 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:00:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:00:56 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:00:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2074MB allocated=2036MB
2025-10-10 18:00:58 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.700000, curr=0.625000
2025-10-10 18:01:06 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 18:01:06 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 18:01:06 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 18:01:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-10 18:01:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:01:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:01:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-10 18:01:08 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=75.182800, avg_loss=0.683480, seen=110, correct=67, accuracy=0.609091
2025-10-10 18:01:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:01:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:01:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:01:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2074MB allocated=2036MB
2025-10-10 18:01:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:01:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:01:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:01:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:01:12 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=22.059475, avg_loss=0.551487, seen=40, correct=26, accuracy=0.650000
2025-10-10 18:01:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:01:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:01:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:01:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2074MB allocated=2036MB
2025-10-10 18:01:15 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.700000, curr=0.650000
2025-10-10 18:01:23 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 18:01:23 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 18:01:23 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 18:01:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-10 18:01:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:01:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:01:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-10 18:01:25 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=74.716217, avg_loss=0.679238, seen=110, correct=66, accuracy=0.600000
2025-10-10 18:01:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:01:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:01:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:01:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2074MB allocated=2036MB
2025-10-10 18:01:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:01:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:01:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:01:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:01:29 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=22.211544, avg_loss=0.555289, seen=40, correct=27, accuracy=0.675000
2025-10-10 18:01:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:01:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:01:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:01:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2074MB allocated=2036MB
2025-10-10 18:01:31 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.700000, curr=0.675000
2025-10-10 18:01:39 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 18:01:39 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 18:01:39 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 18:01:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-10 18:01:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:01:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:01:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-10 18:01:41 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=75.022156, avg_loss=0.682020, seen=110, correct=63, accuracy=0.572727
2025-10-10 18:01:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:01:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:01:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:01:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2074MB allocated=2036MB
2025-10-10 18:01:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:01:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:01:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:01:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:01:46 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=22.535185, avg_loss=0.563380, seen=40, correct=28, accuracy=0.700000
2025-10-10 18:01:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:01:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:01:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:01:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2074MB allocated=2036MB
2025-10-10 18:01:48 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.700000
2025-10-10 18:01:57 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 18:01:57 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 18:01:57 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 18:01:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-10 18:01:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:01:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:01:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-10 18:01:58 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=74.311234, avg_loss=0.675557, seen=110, correct=65, accuracy=0.590909
2025-10-10 18:01:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:01:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:01:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:02:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2074MB allocated=2036MB
2025-10-10 18:02:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:02:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:02:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:02:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:02:01 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=22.762611, avg_loss=0.569065, seen=40, correct=28, accuracy=0.700000
2025-10-10 18:02:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:02:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:02:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:02:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2074MB allocated=2036MB
2025-10-10 18:02:03 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.700000
2025-10-10 18:02:12 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 18:02:12 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 18:02:12 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 18:02:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-10 18:02:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:02:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:02:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-10 18:02:14 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=74.417763, avg_loss=0.676525, seen=110, correct=64, accuracy=0.581818
2025-10-10 18:02:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:02:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:02:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:02:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2074MB allocated=2036MB
2025-10-10 18:02:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:02:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:02:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:02:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:02:18 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=22.094646, avg_loss=0.552366, seen=40, correct=27, accuracy=0.675000
2025-10-10 18:02:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:02:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:02:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:02:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2074MB allocated=2036MB
2025-10-10 18:02:20 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.700000, curr=0.675000
2025-10-10 18:02:28 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 18:02:28 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 18:02:28 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 18:02:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-10 18:02:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:02:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:02:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-10 18:02:30 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=75.447456, avg_loss=0.685886, seen=110, correct=62, accuracy=0.563636
2025-10-10 18:02:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:02:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:02:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:02:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2074MB allocated=2036MB
2025-10-10 18:02:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:02:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:02:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:02:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:02:33 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=22.019714, avg_loss=0.550493, seen=40, correct=28, accuracy=0.700000
2025-10-10 18:02:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:02:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:02:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:02:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2074MB allocated=2036MB
2025-10-10 18:02:35 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.700000
2025-10-10 18:02:43 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 18:02:43 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 18:02:43 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 18:02:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-10 18:02:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:02:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:02:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-10 18:02:45 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=73.823273, avg_loss=0.671121, seen=110, correct=65, accuracy=0.590909
2025-10-10 18:02:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:02:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:02:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:02:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2074MB allocated=2036MB
2025-10-10 18:02:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:02:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:02:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:02:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:02:48 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=22.050400, avg_loss=0.551260, seen=40, correct=27, accuracy=0.675000
2025-10-10 18:02:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:02:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:02:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:02:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2074MB allocated=2036MB
2025-10-10 18:02:50 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.700000, curr=0.675000
2025-10-10 18:02:58 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 18:02:58 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 18:02:58 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 18:02:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-10 18:02:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:02:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:03:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-10 18:03:00 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=73.994400, avg_loss=0.672676, seen=110, correct=67, accuracy=0.609091
2025-10-10 18:03:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:03:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:03:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:03:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2074MB allocated=2036MB
2025-10-10 18:03:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:03:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:03:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:03:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:03:04 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=22.182537, avg_loss=0.554563, seen=40, correct=28, accuracy=0.700000
2025-10-10 18:03:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:03:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:03:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:03:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2074MB allocated=2036MB
2025-10-10 18:03:05 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.700000
2025-10-10 18:03:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 18:03:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 18:03:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:03:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:03:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2074MB allocated=2036MB
2025-10-10 18:03:06 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #46', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 18:03:06 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #46', 'Round': 0, 'Results_raw': {}}
2025-10-10 18:03:07 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 18:03:07 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 18:03:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:03:08 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 18:03:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:03:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:03:09 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 18:03:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-10-10 18:03:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:03:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:03:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-10-10 18:03:13 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=153, loss_sum=103.550873, avg_loss=0.676803, seen=153, correct=94, accuracy=0.614379
2025-10-10 18:03:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:03:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:03:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:03:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2062MB allocated=2028MB
2025-10-10 18:03:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:03:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:03:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:03:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:03:17 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.278868, avg_loss=0.706972, seen=40, correct=23, accuracy=0.575000
2025-10-10 18:03:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:03:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:03:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:03:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2062MB allocated=2028MB
2025-10-10 18:03:20 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-10 18:03:20 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 18:03:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=729, total=2915)
2025-10-10 18:03:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:03:20 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 18:03:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:03:20 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 18:03:20 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=365, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 18:03:29 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 18:03:29 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 18:03:29 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 18:03:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-10-10 18:03:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:03:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:03:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-10-10 18:03:31 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=153, loss_sum=105.240128, avg_loss=0.687844, seen=153, correct=90, accuracy=0.588235
2025-10-10 18:03:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:03:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:03:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:03:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2092MB allocated=2044MB
2025-10-10 18:03:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:03:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:03:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:03:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:03:34 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.006811, avg_loss=0.675170, seen=40, correct=22, accuracy=0.550000
2025-10-10 18:03:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:03:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:03:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:03:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2092MB allocated=2044MB
2025-10-10 18:03:37 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.575000, curr=0.550000
2025-10-10 18:03:45 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 18:03:45 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 18:03:45 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 18:03:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-10-10 18:03:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:03:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:03:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-10-10 18:03:48 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=153, loss_sum=105.134033, avg_loss=0.687151, seen=153, correct=88, accuracy=0.575163
2025-10-10 18:03:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:03:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:03:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:03:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2092MB allocated=2044MB
2025-10-10 18:03:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:03:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:03:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:03:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:03:51 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.495382, avg_loss=0.687385, seen=40, correct=22, accuracy=0.550000
2025-10-10 18:03:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:03:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:03:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:03:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2092MB allocated=2044MB
2025-10-10 18:03:54 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.575000, curr=0.550000
2025-10-10 18:04:03 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 18:04:03 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 18:04:03 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 18:04:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-10-10 18:04:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:04:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:04:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-10-10 18:04:06 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=153, loss_sum=104.162125, avg_loss=0.680798, seen=153, correct=88, accuracy=0.575163
2025-10-10 18:04:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:04:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:04:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:04:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2092MB allocated=2044MB
2025-10-10 18:04:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:04:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:04:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:04:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:04:10 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.796764, avg_loss=0.719919, seen=40, correct=21, accuracy=0.525000
2025-10-10 18:04:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:04:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:04:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:04:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2092MB allocated=2044MB
2025-10-10 18:04:13 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.575000, curr=0.525000
2025-10-10 18:04:22 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 18:04:22 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 18:04:22 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 18:04:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-10-10 18:04:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:04:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:04:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-10-10 18:04:24 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=153, loss_sum=104.076279, avg_loss=0.680237, seen=153, correct=90, accuracy=0.588235
2025-10-10 18:04:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:04:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:04:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:04:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2092MB allocated=2044MB
2025-10-10 18:04:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:04:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:04:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:04:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:04:27 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.811813, avg_loss=0.695295, seen=40, correct=22, accuracy=0.550000
2025-10-10 18:04:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:04:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:04:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:04:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2092MB allocated=2044MB
2025-10-10 18:04:30 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/25), best=0.575000, curr=0.550000
2025-10-10 18:04:39 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 18:04:39 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 18:04:39 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 18:04:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-10-10 18:04:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:04:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:04:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-10-10 18:04:41 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=153, loss_sum=104.465286, avg_loss=0.682780, seen=153, correct=87, accuracy=0.568627
2025-10-10 18:04:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:04:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:04:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:04:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2092MB allocated=2044MB
2025-10-10 18:04:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:04:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:04:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:04:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:04:44 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.682154, avg_loss=0.667054, seen=40, correct=25, accuracy=0.625000
2025-10-10 18:04:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:04:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:04:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:04:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2092MB allocated=2044MB
2025-10-10 18:04:47 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-10 18:04:54 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 18:04:54 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 18:04:54 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 18:04:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-10-10 18:04:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:04:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:04:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-10-10 18:04:57 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=153, loss_sum=103.279320, avg_loss=0.675028, seen=153, correct=85, accuracy=0.555556
2025-10-10 18:04:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:04:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:04:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:05:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2092MB allocated=2044MB
2025-10-10 18:05:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:05:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:05:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:05:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:05:01 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.009556, avg_loss=0.675239, seen=40, correct=26, accuracy=0.650000
2025-10-10 18:05:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:05:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:05:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:05:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2092MB allocated=2044MB
2025-10-10 18:05:04 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.650000
2025-10-10 18:05:12 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 18:05:12 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 18:05:12 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 18:05:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-10-10 18:05:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:05:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:05:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-10-10 18:05:15 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=153, loss_sum=104.459526, avg_loss=0.682742, seen=153, correct=80, accuracy=0.522876
2025-10-10 18:05:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:05:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:05:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:05:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2092MB allocated=2044MB
2025-10-10 18:05:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:05:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:05:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:05:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:05:18 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.312262, avg_loss=0.657807, seen=40, correct=24, accuracy=0.600000
2025-10-10 18:05:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:05:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:05:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:05:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2092MB allocated=2044MB
2025-10-10 18:05:21 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.650000, curr=0.600000
2025-10-10 18:05:29 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 18:05:29 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 18:05:29 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 18:05:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-10-10 18:05:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:05:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:05:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-10-10 18:05:32 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=153, loss_sum=104.928474, avg_loss=0.685807, seen=153, correct=83, accuracy=0.542484
2025-10-10 18:05:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:05:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:05:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:05:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2092MB allocated=2044MB
2025-10-10 18:05:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:05:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:05:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:05:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:05:35 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.804062, avg_loss=0.645102, seen=40, correct=26, accuracy=0.650000
2025-10-10 18:05:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:05:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:05:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:05:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2092MB allocated=2044MB
2025-10-10 18:05:38 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.650000
2025-10-10 18:05:47 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 18:05:47 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 18:05:47 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 18:05:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-10-10 18:05:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:05:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:05:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-10-10 18:05:49 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=153, loss_sum=103.530434, avg_loss=0.676670, seen=153, correct=91, accuracy=0.594771
2025-10-10 18:05:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:05:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:05:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:05:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2092MB allocated=2044MB
2025-10-10 18:05:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:05:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:05:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:05:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:05:52 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.044157, avg_loss=0.701104, seen=40, correct=20, accuracy=0.500000
2025-10-10 18:05:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:05:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:05:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:05:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2092MB allocated=2044MB
2025-10-10 18:05:55 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.650000, curr=0.500000
2025-10-10 18:06:03 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 18:06:03 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 18:06:03 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 18:06:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-10-10 18:06:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:06:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:06:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-10-10 18:06:05 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=153, loss_sum=104.010216, avg_loss=0.679805, seen=153, correct=87, accuracy=0.568627
2025-10-10 18:06:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:06:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:06:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:06:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2092MB allocated=2044MB
2025-10-10 18:06:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:06:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:06:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:06:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:06:09 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.939545, avg_loss=0.723489, seen=40, correct=18, accuracy=0.450000
2025-10-10 18:06:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:06:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:06:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:06:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2092MB allocated=2044MB
2025-10-10 18:06:12 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.650000, curr=0.450000
2025-10-10 18:06:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 18:06:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 18:06:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:06:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:06:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2092MB allocated=2044MB
2025-10-10 18:06:13 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #21', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 18:06:13 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #21', 'Round': 0, 'Results_raw': {}}
2025-10-10 18:06:13 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 18:06:14 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 18:06:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:06:15 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 18:06:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:06:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:06:16 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 18:06:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-10-10 18:06:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:06:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:06:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-10 18:06:19 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=147, loss_sum=97.674622, avg_loss=0.664453, seen=147, correct=83, accuracy=0.564626
2025-10-10 18:06:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:06:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:06:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:06:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2062MB allocated=2036MB
2025-10-10 18:06:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:06:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:06:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:06:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:06:22 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.694645, avg_loss=0.617366, seen=40, correct=27, accuracy=0.675000
2025-10-10 18:06:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:06:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:06:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:06:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2062MB allocated=2036MB
2025-10-10 18:06:24 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.675000
2025-10-10 18:06:24 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 18:06:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=703, total=2812)
2025-10-10 18:06:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:06:24 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 18:06:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:06:24 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 18:06:24 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=352, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 18:06:34 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 18:06:34 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 18:06:34 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 18:06:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-10-10 18:06:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:06:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:06:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-10 18:06:36 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=147, loss_sum=101.337341, avg_loss=0.689370, seen=147, correct=86, accuracy=0.585034
2025-10-10 18:06:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:06:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:06:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:06:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2088MB allocated=2053MB
2025-10-10 18:06:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:06:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:06:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:06:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:06:41 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.052311, avg_loss=0.626308, seen=40, correct=26, accuracy=0.650000
2025-10-10 18:06:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:06:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:06:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:06:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2088MB allocated=2053MB
2025-10-10 18:06:43 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.675000, curr=0.650000
2025-10-10 18:06:52 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 18:06:52 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 18:06:52 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 18:06:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-10-10 18:06:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:06:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:06:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-10 18:06:54 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=147, loss_sum=99.171509, avg_loss=0.674636, seen=147, correct=84, accuracy=0.571429
2025-10-10 18:06:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:06:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:06:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:06:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2088MB allocated=2053MB
2025-10-10 18:06:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:06:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:06:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:06:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:06:58 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.806252, avg_loss=0.620156, seen=40, correct=26, accuracy=0.650000
2025-10-10 18:06:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:06:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:06:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:07:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2088MB allocated=2053MB
2025-10-10 18:07:00 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.675000, curr=0.650000
2025-10-10 18:07:09 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 18:07:09 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 18:07:09 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 18:07:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-10-10 18:07:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:07:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:07:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-10 18:07:11 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=147, loss_sum=98.935585, avg_loss=0.673031, seen=147, correct=83, accuracy=0.564626
2025-10-10 18:07:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:07:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:07:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:07:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2088MB allocated=2053MB
2025-10-10 18:07:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:07:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:07:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:07:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:07:15 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.220440, avg_loss=0.630511, seen=40, correct=26, accuracy=0.650000
2025-10-10 18:07:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:07:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:07:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:07:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2088MB allocated=2053MB
2025-10-10 18:07:17 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.675000, curr=0.650000
2025-10-10 18:07:25 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 18:07:25 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 18:07:25 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 18:07:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-10-10 18:07:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:07:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:07:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-10 18:07:27 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=147, loss_sum=99.288879, avg_loss=0.675435, seen=147, correct=90, accuracy=0.612245
2025-10-10 18:07:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:07:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:07:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:07:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2088MB allocated=2053MB
2025-10-10 18:07:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:07:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:07:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:07:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:07:32 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.205383, avg_loss=0.630135, seen=40, correct=23, accuracy=0.575000
2025-10-10 18:07:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:07:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:07:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:07:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2088MB allocated=2053MB
2025-10-10 18:07:34 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/25), best=0.675000, curr=0.575000
2025-10-10 18:07:42 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 18:07:42 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 18:07:42 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 18:07:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-10-10 18:07:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:07:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:07:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-10 18:07:44 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=147, loss_sum=98.789230, avg_loss=0.672036, seen=147, correct=79, accuracy=0.537415
2025-10-10 18:07:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:07:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:07:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:07:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2088MB allocated=2053MB
2025-10-10 18:07:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:07:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:07:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:07:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:07:49 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.822001, avg_loss=0.620550, seen=40, correct=28, accuracy=0.700000
2025-10-10 18:07:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:07:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:07:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:07:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2088MB allocated=2053MB
2025-10-10 18:07:51 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.700000
2025-10-10 18:08:00 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 18:08:00 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 18:08:00 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 18:08:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-10-10 18:08:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:08:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:08:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-10 18:08:02 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=147, loss_sum=98.198830, avg_loss=0.668019, seen=147, correct=84, accuracy=0.571429
2025-10-10 18:08:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:08:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:08:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:08:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2088MB allocated=2053MB
2025-10-10 18:08:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:08:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:08:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:08:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:08:06 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.452002, avg_loss=0.611300, seen=40, correct=27, accuracy=0.675000
2025-10-10 18:08:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:08:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:08:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:08:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2088MB allocated=2053MB
2025-10-10 18:08:08 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.700000, curr=0.675000
2025-10-10 18:08:16 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 18:08:16 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 18:08:16 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 18:08:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-10-10 18:08:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:08:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:08:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-10 18:08:19 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=147, loss_sum=97.836876, avg_loss=0.665557, seen=147, correct=86, accuracy=0.585034
2025-10-10 18:08:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:08:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:08:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:08:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2088MB allocated=2053MB
2025-10-10 18:08:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:08:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:08:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:08:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:08:23 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.902996, avg_loss=0.622575, seen=40, correct=24, accuracy=0.600000
2025-10-10 18:08:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:08:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:08:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:08:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2088MB allocated=2053MB
2025-10-10 18:08:25 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.700000, curr=0.600000
2025-10-10 18:08:33 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 18:08:33 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 18:08:33 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 18:08:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-10-10 18:08:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:08:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:08:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-10 18:08:35 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=147, loss_sum=97.866867, avg_loss=0.665761, seen=147, correct=85, accuracy=0.578231
2025-10-10 18:08:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:08:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:08:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:08:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2088MB allocated=2053MB
2025-10-10 18:08:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:08:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:08:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:08:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:08:40 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.660109, avg_loss=0.616503, seen=40, correct=26, accuracy=0.650000
2025-10-10 18:08:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:08:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:08:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:08:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2088MB allocated=2053MB
2025-10-10 18:08:42 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.700000, curr=0.650000
2025-10-10 18:08:51 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 18:08:51 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 18:08:51 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 18:08:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-10-10 18:08:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:08:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:08:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-10 18:08:53 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=147, loss_sum=97.621765, avg_loss=0.664094, seen=147, correct=86, accuracy=0.585034
2025-10-10 18:08:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:08:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:08:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:08:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2088MB allocated=2053MB
2025-10-10 18:08:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:08:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:08:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:08:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:08:57 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.556686, avg_loss=0.613917, seen=40, correct=25, accuracy=0.625000
2025-10-10 18:08:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:08:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:08:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:08:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2088MB allocated=2053MB
2025-10-10 18:08:59 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/25), best=0.700000, curr=0.625000
2025-10-10 18:09:07 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 18:09:07 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 18:09:07 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 18:09:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-10-10 18:09:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:09:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:09:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-10 18:09:10 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=147, loss_sum=98.361778, avg_loss=0.669128, seen=147, correct=85, accuracy=0.578231
2025-10-10 18:09:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:09:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:09:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:09:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2088MB allocated=2053MB
2025-10-10 18:09:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:09:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:09:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:09:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:09:13 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.197227, avg_loss=0.629931, seen=40, correct=24, accuracy=0.600000
2025-10-10 18:09:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:09:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:09:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:09:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2088MB allocated=2053MB
2025-10-10 18:09:16 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/25), best=0.700000, curr=0.600000
2025-10-10 18:09:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 18:09:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 18:09:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:09:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:09:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2088MB allocated=2053MB
2025-10-10 18:09:17 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #47', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 18:09:17 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #47', 'Round': 0, 'Results_raw': {}}
2025-10-10 18:09:18 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 18:09:18 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 18:09:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:09:19 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 18:09:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:09:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:09:20 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 18:09:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-10 18:09:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:09:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:09:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-10 18:09:24 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=124.390404, avg_loss=0.661651, seen=188, correct=108, accuracy=0.574468
2025-10-10 18:09:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:09:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:09:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:09:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2082MB allocated=2044MB
2025-10-10 18:09:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:09:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:09:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:09:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:09:28 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.715218, avg_loss=0.642880, seen=40, correct=29, accuracy=0.725000
2025-10-10 18:09:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:09:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:09:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:09:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2082MB allocated=2044MB
2025-10-10 18:09:31 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.725000
2025-10-10 18:09:31 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 18:09:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=893, total=3572)
2025-10-10 18:09:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:09:31 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 18:09:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:09:31 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 18:09:31 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=447, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 18:09:40 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 18:09:40 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 18:09:40 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 18:09:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-10 18:09:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:09:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:09:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-10 18:09:42 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=123.849091, avg_loss=0.658772, seen=188, correct=113, accuracy=0.601064
2025-10-10 18:09:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:09:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:09:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:09:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2112MB allocated=2061MB
2025-10-10 18:09:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:09:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:09:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:09:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:09:47 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.052921, avg_loss=0.676323, seen=40, correct=28, accuracy=0.700000
2025-10-10 18:09:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:09:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:09:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:09:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2112MB allocated=2061MB
2025-10-10 18:09:50 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.725000, curr=0.700000
2025-10-10 18:09:58 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 18:09:58 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 18:09:58 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 18:09:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-10 18:09:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:09:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:10:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-10 18:10:01 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=123.475800, avg_loss=0.656786, seen=188, correct=112, accuracy=0.595745
2025-10-10 18:10:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:10:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:10:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:10:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2112MB allocated=2061MB
2025-10-10 18:10:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:10:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:10:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:10:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:10:04 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.568846, avg_loss=0.664221, seen=40, correct=28, accuracy=0.700000
2025-10-10 18:10:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:10:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:10:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:10:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2112MB allocated=2061MB
2025-10-10 18:10:07 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.725000, curr=0.700000
2025-10-10 18:10:14 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 18:10:14 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 18:10:14 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 18:10:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-10 18:10:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:10:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:10:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-10 18:10:17 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=125.674187, avg_loss=0.668480, seen=188, correct=108, accuracy=0.574468
2025-10-10 18:10:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:10:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:10:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:10:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2112MB allocated=2061MB
2025-10-10 18:10:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:10:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:10:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:10:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:10:21 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.573963, avg_loss=0.639349, seen=40, correct=29, accuracy=0.725000
2025-10-10 18:10:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:10:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:10:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:10:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2112MB allocated=2061MB
2025-10-10 18:10:24 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.725000
2025-10-10 18:10:32 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 18:10:32 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 18:10:32 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 18:10:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-10 18:10:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:10:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:10:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-10 18:10:35 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=124.497406, avg_loss=0.662220, seen=188, correct=108, accuracy=0.574468
2025-10-10 18:10:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:10:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:10:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:10:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2112MB allocated=2061MB
2025-10-10 18:10:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:10:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:10:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:10:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:10:38 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.288977, avg_loss=0.632224, seen=40, correct=29, accuracy=0.725000
2025-10-10 18:10:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:10:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:10:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:10:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2112MB allocated=2061MB
2025-10-10 18:10:41 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.725000
2025-10-10 18:10:49 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 18:10:49 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 18:10:49 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 18:10:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-10 18:10:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:10:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:10:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-10 18:10:51 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=123.672501, avg_loss=0.657832, seen=188, correct=112, accuracy=0.595745
2025-10-10 18:10:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:10:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:10:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:10:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2112MB allocated=2061MB
2025-10-10 18:10:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:10:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:10:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:10:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:10:56 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.390322, avg_loss=0.634758, seen=40, correct=29, accuracy=0.725000
2025-10-10 18:10:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:10:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:10:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:10:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2112MB allocated=2061MB
2025-10-10 18:10:58 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.725000
2025-10-10 18:11:07 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 18:11:07 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 18:11:07 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 18:11:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-10 18:11:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:11:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:11:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-10 18:11:10 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=124.152733, avg_loss=0.660387, seen=188, correct=113, accuracy=0.601064
2025-10-10 18:11:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:11:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:11:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:11:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2112MB allocated=2061MB
2025-10-10 18:11:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:11:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:11:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:11:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:11:13 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.275047, avg_loss=0.631876, seen=40, correct=29, accuracy=0.725000
2025-10-10 18:11:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:11:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:11:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:11:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2112MB allocated=2061MB
2025-10-10 18:11:15 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.725000
2025-10-10 18:11:24 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 18:11:24 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 18:11:24 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 18:11:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-10 18:11:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:11:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:11:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-10 18:11:26 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=122.937737, avg_loss=0.653924, seen=188, correct=114, accuracy=0.606383
2025-10-10 18:11:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:11:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:11:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:11:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2112MB allocated=2061MB
2025-10-10 18:11:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:11:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:11:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:11:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:11:30 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.624866, avg_loss=0.665622, seen=40, correct=28, accuracy=0.700000
2025-10-10 18:11:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:11:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:11:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:11:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2112MB allocated=2061MB
2025-10-10 18:11:32 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.725000, curr=0.700000
2025-10-10 18:11:40 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 18:11:40 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 18:11:40 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 18:11:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-10 18:11:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:11:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:11:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-10 18:11:43 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=122.530739, avg_loss=0.651759, seen=188, correct=116, accuracy=0.617021
2025-10-10 18:11:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:11:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:11:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:11:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2112MB allocated=2061MB
2025-10-10 18:11:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:11:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:11:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:11:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:11:47 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.758181, avg_loss=0.643955, seen=40, correct=28, accuracy=0.700000
2025-10-10 18:11:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:11:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:11:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:11:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2112MB allocated=2061MB
2025-10-10 18:11:49 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.725000, curr=0.700000
2025-10-10 18:11:58 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 18:11:58 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 18:11:58 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 18:11:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-10 18:11:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:11:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:12:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-10 18:12:01 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=123.555321, avg_loss=0.657209, seen=188, correct=112, accuracy=0.595745
2025-10-10 18:12:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:12:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:12:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:12:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2112MB allocated=2061MB
2025-10-10 18:12:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:12:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:12:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:12:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:12:06 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.319323, avg_loss=0.632983, seen=40, correct=29, accuracy=0.725000
2025-10-10 18:12:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:12:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:12:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:12:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2112MB allocated=2061MB
2025-10-10 18:12:07 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.725000
2025-10-10 18:12:15 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 18:12:15 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 18:12:15 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 18:12:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-10 18:12:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:12:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:12:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-10 18:12:18 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=124.701714, avg_loss=0.663307, seen=188, correct=113, accuracy=0.601064
2025-10-10 18:12:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:12:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:12:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:12:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2112MB allocated=2061MB
2025-10-10 18:12:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:12:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:12:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:12:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:12:21 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.316956, avg_loss=0.632924, seen=40, correct=28, accuracy=0.700000
2025-10-10 18:12:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:12:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:12:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:12:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2112MB allocated=2061MB
2025-10-10 18:12:23 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.725000, curr=0.700000
2025-10-10 18:12:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 18:12:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 18:12:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:12:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:12:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2112MB allocated=2061MB
2025-10-10 18:12:25 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #9', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 18:12:25 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #9', 'Round': 0, 'Results_raw': {}}
2025-10-10 18:12:25 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 18:12:26 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 18:12:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:12:27 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 18:12:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:12:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:12:28 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 18:12:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-10-10 18:12:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:12:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:12:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-10-10 18:12:31 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=160, loss_sum=103.560844, avg_loss=0.647255, seen=160, correct=91, accuracy=0.568750
2025-10-10 18:12:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:12:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:12:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:12:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2082MB allocated=2053MB
2025-10-10 18:12:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:12:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:12:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:12:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:12:34 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.747103, avg_loss=0.593678, seen=40, correct=29, accuracy=0.725000
2025-10-10 18:12:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:12:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:12:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:12:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2082MB allocated=2053MB
2025-10-10 18:12:36 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.725000
2025-10-10 18:12:36 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 18:12:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=764, total=3055)
2025-10-10 18:12:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:12:36 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 18:12:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:12:36 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 18:12:36 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=382, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 18:12:45 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 18:12:45 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 18:12:45 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 18:12:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-10-10 18:12:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:12:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:12:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-10-10 18:12:48 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=160, loss_sum=103.706673, avg_loss=0.648167, seen=160, correct=101, accuracy=0.631250
2025-10-10 18:12:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:12:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:12:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:12:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2104MB allocated=2070MB
2025-10-10 18:12:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:12:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:12:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:12:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:12:51 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.230198, avg_loss=0.655755, seen=40, correct=25, accuracy=0.625000
2025-10-10 18:12:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:12:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:12:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:12:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2104MB allocated=2070MB
2025-10-10 18:12:54 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.725000, curr=0.625000
2025-10-10 18:13:03 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 18:13:03 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 18:13:03 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 18:13:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-10-10 18:13:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:13:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:13:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-10-10 18:13:06 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=160, loss_sum=109.886597, avg_loss=0.686791, seen=160, correct=100, accuracy=0.625000
2025-10-10 18:13:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:13:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:13:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:13:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2104MB allocated=2070MB
2025-10-10 18:13:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:13:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:13:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:13:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:13:09 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.987776, avg_loss=0.599694, seen=40, correct=26, accuracy=0.650000
2025-10-10 18:13:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:13:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:13:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:13:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2104MB allocated=2070MB
2025-10-10 18:13:10 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.725000, curr=0.650000
2025-10-10 18:13:18 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 18:13:18 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 18:13:18 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 18:13:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-10-10 18:13:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:13:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:13:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-10-10 18:13:21 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=160, loss_sum=117.617958, avg_loss=0.735112, seen=160, correct=93, accuracy=0.581250
2025-10-10 18:13:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:13:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:13:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:13:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2104MB allocated=2070MB
2025-10-10 18:13:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:13:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:13:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:13:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:13:24 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.737934, avg_loss=0.618448, seen=40, correct=24, accuracy=0.600000
2025-10-10 18:13:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:13:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:13:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:13:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2104MB allocated=2070MB
2025-10-10 18:13:27 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.725000, curr=0.600000
2025-10-10 18:13:37 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 18:13:37 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 18:13:37 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 18:13:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-10-10 18:13:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:13:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:13:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-10-10 18:13:39 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=160, loss_sum=102.034973, avg_loss=0.637719, seen=160, correct=90, accuracy=0.562500
2025-10-10 18:13:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:13:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:13:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:13:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2104MB allocated=2070MB
2025-10-10 18:13:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:13:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:13:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:13:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:13:43 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.929522, avg_loss=0.598238, seen=40, correct=29, accuracy=0.725000
2025-10-10 18:13:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:13:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:13:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:13:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2104MB allocated=2070MB
2025-10-10 18:13:46 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.725000
2025-10-10 18:13:55 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 18:13:55 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 18:13:55 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 18:13:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-10-10 18:13:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:13:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:13:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-10-10 18:13:58 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=160, loss_sum=102.947556, avg_loss=0.643422, seen=160, correct=96, accuracy=0.600000
2025-10-10 18:13:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:13:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:14:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:14:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2104MB allocated=2070MB
2025-10-10 18:14:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:14:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:14:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:14:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:14:02 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.543194, avg_loss=0.638580, seen=40, correct=25, accuracy=0.625000
2025-10-10 18:14:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:14:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:14:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:14:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2104MB allocated=2070MB
2025-10-10 18:14:05 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.725000, curr=0.625000
2025-10-10 18:14:15 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 18:14:15 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 18:14:15 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 18:14:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-10-10 18:14:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:14:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:14:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-10-10 18:14:17 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=160, loss_sum=101.332565, avg_loss=0.633329, seen=160, correct=94, accuracy=0.587500
2025-10-10 18:14:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:14:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:14:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:14:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2104MB allocated=2070MB
2025-10-10 18:14:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:14:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:14:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:14:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:14:22 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.741470, avg_loss=0.593537, seen=40, correct=28, accuracy=0.700000
2025-10-10 18:14:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:14:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:14:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:14:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2104MB allocated=2070MB
2025-10-10 18:14:24 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.725000, curr=0.700000
2025-10-10 18:14:32 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 18:14:32 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 18:14:32 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 18:14:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-10-10 18:14:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:14:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:14:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-10-10 18:14:35 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=160, loss_sum=101.912125, avg_loss=0.636951, seen=160, correct=92, accuracy=0.575000
2025-10-10 18:14:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:14:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:14:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:14:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2104MB allocated=2070MB
2025-10-10 18:14:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:14:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:14:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:14:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:14:39 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.628746, avg_loss=0.590719, seen=40, correct=29, accuracy=0.725000
2025-10-10 18:14:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:14:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:14:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:14:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2104MB allocated=2070MB
2025-10-10 18:14:42 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.725000
2025-10-10 18:14:52 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 18:14:52 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 18:14:52 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 18:14:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-10-10 18:14:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:14:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:14:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-10-10 18:14:54 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=160, loss_sum=101.231415, avg_loss=0.632696, seen=160, correct=91, accuracy=0.568750
2025-10-10 18:14:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:14:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:14:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:14:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2104MB allocated=2070MB
2025-10-10 18:14:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:14:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:14:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:14:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:14:59 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.613903, avg_loss=0.590348, seen=40, correct=28, accuracy=0.700000
2025-10-10 18:14:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:14:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:15:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:15:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2104MB allocated=2070MB
2025-10-10 18:15:02 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.725000, curr=0.700000
2025-10-10 18:15:12 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 18:15:12 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 18:15:12 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 18:15:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-10-10 18:15:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:15:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:15:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-10-10 18:15:14 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=160, loss_sum=100.778854, avg_loss=0.629868, seen=160, correct=96, accuracy=0.600000
2025-10-10 18:15:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:15:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:15:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:15:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2104MB allocated=2070MB
2025-10-10 18:15:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:15:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:15:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:15:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:15:18 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.544731, avg_loss=0.588618, seen=40, correct=29, accuracy=0.725000
2025-10-10 18:15:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:15:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:15:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:15:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2104MB allocated=2070MB
2025-10-10 18:15:21 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.725000
2025-10-10 18:15:30 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 18:15:30 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 18:15:30 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 18:15:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-10-10 18:15:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:15:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:15:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-10-10 18:15:33 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=160, loss_sum=100.788284, avg_loss=0.629927, seen=160, correct=95, accuracy=0.593750
2025-10-10 18:15:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:15:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:15:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:15:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2104MB allocated=2070MB
2025-10-10 18:15:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:15:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:15:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:15:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:15:38 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.128153, avg_loss=0.603204, seen=40, correct=28, accuracy=0.700000
2025-10-10 18:15:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:15:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:15:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:15:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2104MB allocated=2070MB
2025-10-10 18:15:40 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.725000, curr=0.700000
2025-10-10 18:15:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 18:15:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 18:15:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:15:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:15:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2104MB allocated=2070MB
2025-10-10 18:15:41 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #14', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 18:15:41 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #14', 'Round': 0, 'Results_raw': {}}
2025-10-10 18:15:42 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 18:15:42 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 18:15:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:15:43 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 18:15:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:15:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:15:44 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 18:15:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-10-10 18:15:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:15:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:15:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-10-10 18:15:47 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=161, loss_sum=98.891998, avg_loss=0.614236, seen=161, correct=102, accuracy=0.633540
2025-10-10 18:15:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:15:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:15:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:15:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2102MB allocated=2061MB
2025-10-10 18:15:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:15:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:15:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:15:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:15:50 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.088808, avg_loss=0.652220, seen=40, correct=24, accuracy=0.600000
2025-10-10 18:15:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:15:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:15:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:15:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2102MB allocated=2061MB
2025-10-10 18:15:53 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-10 18:15:53 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 18:15:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=766, total=3063)
2025-10-10 18:15:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:15:53 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 18:15:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:15:53 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 18:15:53 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=383, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 18:16:03 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 18:16:03 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 18:16:03 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 18:16:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-10-10 18:16:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:16:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:16:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-10-10 18:16:05 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=161, loss_sum=102.211334, avg_loss=0.634853, seen=161, correct=103, accuracy=0.639752
2025-10-10 18:16:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:16:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:16:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:16:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2124MB allocated=2078MB
2025-10-10 18:16:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:16:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:16:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:16:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:16:09 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.292728, avg_loss=0.682318, seen=40, correct=26, accuracy=0.650000
2025-10-10 18:16:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:16:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:16:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:16:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2124MB allocated=2078MB
2025-10-10 18:16:12 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.650000
2025-10-10 18:16:20 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 18:16:20 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 18:16:20 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 18:16:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-10-10 18:16:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:16:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:16:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-10-10 18:16:23 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=161, loss_sum=104.044777, avg_loss=0.646241, seen=161, correct=102, accuracy=0.633540
2025-10-10 18:16:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:16:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:16:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:16:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2124MB allocated=2078MB
2025-10-10 18:16:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:16:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:16:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:16:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:16:26 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.135675, avg_loss=0.703392, seen=40, correct=27, accuracy=0.675000
2025-10-10 18:16:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:16:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:16:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:16:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2124MB allocated=2078MB
2025-10-10 18:16:29 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.675000
2025-10-10 18:16:37 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 18:16:37 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 18:16:37 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 18:16:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-10-10 18:16:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:16:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:16:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-10-10 18:16:40 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=161, loss_sum=99.339203, avg_loss=0.617014, seen=161, correct=99, accuracy=0.614907
2025-10-10 18:16:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:16:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:16:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:16:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2124MB allocated=2078MB
2025-10-10 18:16:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:16:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:16:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:16:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:16:43 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.999626, avg_loss=0.649991, seen=40, correct=23, accuracy=0.575000
2025-10-10 18:16:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:16:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:16:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:16:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2124MB allocated=2078MB
2025-10-10 18:16:45 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.675000, curr=0.575000
2025-10-10 18:16:53 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 18:16:53 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 18:16:53 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 18:16:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-10-10 18:16:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:16:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:16:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-10-10 18:16:55 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=161, loss_sum=100.781456, avg_loss=0.625972, seen=161, correct=102, accuracy=0.633540
2025-10-10 18:16:55 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:16:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:16:56 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:16:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2124MB allocated=2078MB
2025-10-10 18:16:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:16:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:16:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:17:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:17:00 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.293434, avg_loss=0.657336, seen=40, correct=22, accuracy=0.550000
2025-10-10 18:17:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:17:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:17:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:17:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2124MB allocated=2078MB
2025-10-10 18:17:01 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.675000, curr=0.550000
2025-10-10 18:17:11 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 18:17:11 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 18:17:11 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 18:17:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-10-10 18:17:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:17:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:17:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-10-10 18:17:13 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=161, loss_sum=99.470802, avg_loss=0.617831, seen=161, correct=103, accuracy=0.639752
2025-10-10 18:17:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:17:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:17:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:17:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2124MB allocated=2078MB
2025-10-10 18:17:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:17:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:17:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:17:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:17:17 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.615894, avg_loss=0.665397, seen=40, correct=26, accuracy=0.650000
2025-10-10 18:17:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:17:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:17:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:17:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2124MB allocated=2078MB
2025-10-10 18:17:19 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.675000, curr=0.650000
2025-10-10 18:17:28 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 18:17:28 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 18:17:28 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 18:17:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-10-10 18:17:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:17:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:17:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-10-10 18:17:30 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=161, loss_sum=99.365555, avg_loss=0.617177, seen=161, correct=103, accuracy=0.639752
2025-10-10 18:17:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:17:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:17:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:17:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2124MB allocated=2078MB
2025-10-10 18:17:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:17:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:17:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:17:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:17:34 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.223713, avg_loss=0.655593, seen=40, correct=26, accuracy=0.650000
2025-10-10 18:17:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:17:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:17:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:17:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2124MB allocated=2078MB
2025-10-10 18:17:36 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/25), best=0.675000, curr=0.650000
2025-10-10 18:17:45 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 18:17:45 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 18:17:45 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 18:17:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-10-10 18:17:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:17:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:17:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-10-10 18:17:48 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=161, loss_sum=98.763817, avg_loss=0.613440, seen=161, correct=102, accuracy=0.633540
2025-10-10 18:17:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:17:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:17:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:17:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2124MB allocated=2078MB
2025-10-10 18:17:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:17:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:17:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:17:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:17:51 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.691975, avg_loss=0.642299, seen=40, correct=25, accuracy=0.625000
2025-10-10 18:17:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:17:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:17:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:17:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2124MB allocated=2078MB
2025-10-10 18:17:53 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/25), best=0.675000, curr=0.625000
2025-10-10 18:18:02 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 18:18:02 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 18:18:02 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 18:18:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-10-10 18:18:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:18:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:18:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-10-10 18:18:04 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=161, loss_sum=99.336044, avg_loss=0.616994, seen=161, correct=103, accuracy=0.639752
2025-10-10 18:18:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:18:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:18:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:18:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2124MB allocated=2078MB
2025-10-10 18:18:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:18:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:18:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:18:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:18:08 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.627369, avg_loss=0.640684, seen=40, correct=21, accuracy=0.525000
2025-10-10 18:18:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:18:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:18:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:18:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2124MB allocated=2078MB
2025-10-10 18:18:10 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/25), best=0.675000, curr=0.525000
2025-10-10 18:18:19 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 18:18:19 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 18:18:19 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 18:18:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-10-10 18:18:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:18:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:18:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-10-10 18:18:21 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=161, loss_sum=98.855347, avg_loss=0.614008, seen=161, correct=104, accuracy=0.645963
2025-10-10 18:18:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:18:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:18:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:18:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2124MB allocated=2078MB
2025-10-10 18:18:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:18:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:18:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:18:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:18:25 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.530903, avg_loss=0.638273, seen=40, correct=23, accuracy=0.575000
2025-10-10 18:18:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:18:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:18:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:18:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2124MB allocated=2078MB
2025-10-10 18:18:27 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/25), best=0.675000, curr=0.575000
2025-10-10 18:18:36 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 18:18:36 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 18:18:36 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 18:18:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-10-10 18:18:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:18:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:18:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-10-10 18:18:39 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=161, loss_sum=97.684364, avg_loss=0.606735, seen=161, correct=102, accuracy=0.633540
2025-10-10 18:18:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:18:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:18:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:18:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2124MB allocated=2078MB
2025-10-10 18:18:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:18:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:18:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:18:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:18:44 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.770439, avg_loss=0.644261, seen=40, correct=24, accuracy=0.600000
2025-10-10 18:18:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:18:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:18:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:18:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2124MB allocated=2078MB
2025-10-10 18:18:45 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/25), best=0.675000, curr=0.600000
2025-10-10 18:18:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 18:18:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 18:18:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:18:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:18:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2124MB allocated=2078MB
2025-10-10 18:18:47 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #26', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 18:18:47 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #26', 'Round': 0, 'Results_raw': {}}
2025-10-10 18:18:47 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 18:18:48 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 18:18:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:18:49 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 18:18:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:18:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:18:50 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 18:18:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-10-10 18:18:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:18:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:18:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 18:18:53 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=135, loss_sum=87.960434, avg_loss=0.651559, seen=135, correct=83, accuracy=0.614815
2025-10-10 18:18:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:18:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:18:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:18:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2102MB allocated=2070MB
2025-10-10 18:18:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:18:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:18:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:18:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:18:57 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.334204, avg_loss=0.633355, seen=40, correct=23, accuracy=0.575000
2025-10-10 18:18:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:18:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:18:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:18:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2102MB allocated=2070MB
2025-10-10 18:18:59 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-10 18:18:59 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 18:18:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=644, total=2576)
2025-10-10 18:18:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:18:59 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 18:18:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:18:59 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 18:18:59 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=322, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 18:19:08 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 18:19:08 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 18:19:08 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 18:19:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-10-10 18:19:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:19:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:19:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 18:19:11 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=135, loss_sum=88.331932, avg_loss=0.654311, seen=135, correct=87, accuracy=0.644444
2025-10-10 18:19:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:19:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:19:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:19:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2146MB allocated=2086MB
2025-10-10 18:19:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:19:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:19:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:19:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:19:16 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.201317, avg_loss=0.630033, seen=40, correct=22, accuracy=0.550000
2025-10-10 18:19:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:19:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:19:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:19:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2146MB allocated=2086MB
2025-10-10 18:19:18 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.575000, curr=0.550000
2025-10-10 18:19:26 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 18:19:26 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 18:19:26 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 18:19:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-10-10 18:19:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:19:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:19:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 18:19:28 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=135, loss_sum=92.409882, avg_loss=0.684518, seen=135, correct=74, accuracy=0.548148
2025-10-10 18:19:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:19:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:19:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:19:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2146MB allocated=2086MB
2025-10-10 18:19:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:19:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:19:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:19:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:19:33 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.260113, avg_loss=0.731503, seen=40, correct=17, accuracy=0.425000
2025-10-10 18:19:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:19:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:19:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:19:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2146MB allocated=2086MB
2025-10-10 18:19:34 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.575000, curr=0.425000
2025-10-10 18:19:43 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 18:19:43 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 18:19:43 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 18:19:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-10-10 18:19:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:19:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:19:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 18:19:45 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=135, loss_sum=91.987953, avg_loss=0.681392, seen=135, correct=77, accuracy=0.570370
2025-10-10 18:19:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:19:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:19:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:19:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2146MB allocated=2086MB
2025-10-10 18:19:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:19:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:19:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:19:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:19:48 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.768560, avg_loss=0.719214, seen=40, correct=17, accuracy=0.425000
2025-10-10 18:19:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:19:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:19:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:19:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2146MB allocated=2086MB
2025-10-10 18:19:51 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.575000, curr=0.425000
2025-10-10 18:19:59 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 18:19:59 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 18:19:59 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 18:19:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-10-10 18:19:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:19:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:20:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 18:20:01 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=135, loss_sum=88.544357, avg_loss=0.655884, seen=135, correct=81, accuracy=0.600000
2025-10-10 18:20:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:20:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:20:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:20:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2146MB allocated=2086MB
2025-10-10 18:20:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:20:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:20:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:20:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:20:05 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.390463, avg_loss=0.659762, seen=40, correct=20, accuracy=0.500000
2025-10-10 18:20:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:20:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:20:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:20:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2146MB allocated=2086MB
2025-10-10 18:20:08 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/25), best=0.575000, curr=0.500000
2025-10-10 18:20:16 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 18:20:16 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 18:20:16 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 18:20:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-10-10 18:20:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:20:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:20:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 18:20:18 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=135, loss_sum=88.303101, avg_loss=0.654097, seen=135, correct=84, accuracy=0.622222
2025-10-10 18:20:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:20:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:20:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:20:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2146MB allocated=2086MB
2025-10-10 18:20:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:20:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:20:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:20:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:20:21 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.696299, avg_loss=0.642407, seen=40, correct=21, accuracy=0.525000
2025-10-10 18:20:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:20:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:20:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:20:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2146MB allocated=2086MB
2025-10-10 18:20:24 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/25), best=0.575000, curr=0.525000
2025-10-10 18:20:31 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 18:20:31 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 18:20:31 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 18:20:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-10-10 18:20:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:20:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:20:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 18:20:33 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=135, loss_sum=87.998932, avg_loss=0.651844, seen=135, correct=84, accuracy=0.622222
2025-10-10 18:20:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:20:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:20:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:20:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2146MB allocated=2086MB
2025-10-10 18:20:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:20:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:20:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:20:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:20:36 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.852913, avg_loss=0.621323, seen=40, correct=25, accuracy=0.625000
2025-10-10 18:20:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:20:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:20:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:20:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2146MB allocated=2086MB
2025-10-10 18:20:38 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-10 18:20:46 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 18:20:46 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 18:20:46 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 18:20:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-10-10 18:20:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:20:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:20:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 18:20:48 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=135, loss_sum=88.143822, avg_loss=0.652917, seen=135, correct=84, accuracy=0.622222
2025-10-10 18:20:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:20:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:20:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:20:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2146MB allocated=2086MB
2025-10-10 18:20:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:20:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:20:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:20:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:20:52 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.324389, avg_loss=0.608110, seen=40, correct=29, accuracy=0.725000
2025-10-10 18:20:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:20:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:20:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:20:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2146MB allocated=2086MB
2025-10-10 18:20:54 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.725000
2025-10-10 18:21:01 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 18:21:01 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 18:21:01 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 18:21:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-10-10 18:21:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:21:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:21:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 18:21:03 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=135, loss_sum=88.361557, avg_loss=0.654530, seen=135, correct=85, accuracy=0.629630
2025-10-10 18:21:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:21:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:21:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:21:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2146MB allocated=2086MB
2025-10-10 18:21:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:21:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:21:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:21:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:21:07 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.538347, avg_loss=0.638459, seen=40, correct=21, accuracy=0.525000
2025-10-10 18:21:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:21:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:21:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:21:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2146MB allocated=2086MB
2025-10-10 18:21:09 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.725000, curr=0.525000
2025-10-10 18:21:17 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 18:21:17 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 18:21:17 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 18:21:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-10-10 18:21:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:21:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:21:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 18:21:19 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=135, loss_sum=88.225792, avg_loss=0.653524, seen=135, correct=81, accuracy=0.600000
2025-10-10 18:21:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:21:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:21:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:21:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2146MB allocated=2086MB
2025-10-10 18:21:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:21:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:21:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:21:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:21:23 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.591297, avg_loss=0.639782, seen=40, correct=21, accuracy=0.525000
2025-10-10 18:21:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:21:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:21:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:21:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2146MB allocated=2086MB
2025-10-10 18:21:26 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.725000, curr=0.525000
2025-10-10 18:21:34 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 18:21:34 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 18:21:34 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 18:21:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-10-10 18:21:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:21:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:21:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 18:21:37 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=135, loss_sum=88.314445, avg_loss=0.654181, seen=135, correct=82, accuracy=0.607407
2025-10-10 18:21:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:21:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:21:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:21:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2146MB allocated=2086MB
2025-10-10 18:21:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:21:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:21:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:21:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:21:40 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.582052, avg_loss=0.614551, seen=40, correct=27, accuracy=0.675000
2025-10-10 18:21:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:21:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:21:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:21:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2146MB allocated=2086MB
2025-10-10 18:21:43 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.725000, curr=0.675000
2025-10-10 18:21:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 18:21:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 18:21:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:21:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:21:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2146MB allocated=2086MB
2025-10-10 18:21:44 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #18', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 18:21:44 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #18', 'Round': 0, 'Results_raw': {}}
2025-10-10 18:21:44 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 18:21:45 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 18:21:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:21:46 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 18:21:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:21:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:21:47 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 18:21:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-10 18:21:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:21:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:21:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-10 18:21:51 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=123.167404, avg_loss=0.655146, seen=188, correct=111, accuracy=0.590426
2025-10-10 18:21:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:21:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:21:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:21:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2102MB allocated=2078MB
2025-10-10 18:21:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:21:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:21:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:21:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:21:55 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.326498, avg_loss=0.633162, seen=40, correct=25, accuracy=0.625000
2025-10-10 18:21:55 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:21:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:21:56 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:21:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2102MB allocated=2078MB
2025-10-10 18:21:57 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-10 18:21:57 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 18:21:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=898, total=3589)
2025-10-10 18:21:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:21:58 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 18:21:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:21:58 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 18:21:58 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=449, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 18:22:07 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 18:22:07 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 18:22:07 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 18:22:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-10 18:22:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:22:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:22:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-10 18:22:10 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=123.246986, avg_loss=0.655569, seen=188, correct=111, accuracy=0.590426
2025-10-10 18:22:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:22:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:22:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:22:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2134MB allocated=2095MB
2025-10-10 18:22:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:22:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:22:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:22:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:22:14 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.207371, avg_loss=0.630184, seen=40, correct=26, accuracy=0.650000
2025-10-10 18:22:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:22:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:22:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:22:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2134MB allocated=2095MB
2025-10-10 18:22:17 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.650000
2025-10-10 18:22:25 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 18:22:25 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 18:22:25 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 18:22:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-10 18:22:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:22:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:22:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-10 18:22:28 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=122.808136, avg_loss=0.653235, seen=188, correct=114, accuracy=0.606383
2025-10-10 18:22:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:22:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:22:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:22:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2134MB allocated=2095MB
2025-10-10 18:22:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:22:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:22:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:22:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:22:33 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.259745, avg_loss=0.631494, seen=40, correct=25, accuracy=0.625000
2025-10-10 18:22:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:22:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:22:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:22:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2134MB allocated=2095MB
2025-10-10 18:22:36 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.650000, curr=0.625000
2025-10-10 18:22:45 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 18:22:45 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 18:22:45 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 18:22:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-10 18:22:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:22:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:22:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-10 18:22:48 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=122.395447, avg_loss=0.651040, seen=188, correct=111, accuracy=0.590426
2025-10-10 18:22:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:22:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:22:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:22:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2134MB allocated=2095MB
2025-10-10 18:22:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:22:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:22:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:22:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:22:52 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.340754, avg_loss=0.633519, seen=40, correct=24, accuracy=0.600000
2025-10-10 18:22:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:22:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:22:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:22:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2134MB allocated=2095MB
2025-10-10 18:22:55 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.650000, curr=0.600000
2025-10-10 18:23:03 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 18:23:03 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 18:23:03 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 18:23:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-10 18:23:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:23:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:23:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-10 18:23:06 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=122.833145, avg_loss=0.653368, seen=188, correct=112, accuracy=0.595745
2025-10-10 18:23:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:23:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:23:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:23:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2134MB allocated=2095MB
2025-10-10 18:23:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:23:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:23:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:23:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:23:09 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.092846, avg_loss=0.627321, seen=40, correct=26, accuracy=0.650000
2025-10-10 18:23:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:23:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:23:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:23:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2134MB allocated=2095MB
2025-10-10 18:23:12 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.650000
2025-10-10 18:23:19 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 18:23:19 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 18:23:19 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 18:23:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-10 18:23:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:23:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:23:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-10 18:23:22 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=121.448135, avg_loss=0.646001, seen=188, correct=112, accuracy=0.595745
2025-10-10 18:23:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:23:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:23:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:23:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2134MB allocated=2095MB
2025-10-10 18:23:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:23:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:23:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:23:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:23:26 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.110840, avg_loss=0.627771, seen=40, correct=25, accuracy=0.625000
2025-10-10 18:23:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:23:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:23:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:23:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2134MB allocated=2095MB
2025-10-10 18:23:28 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.650000, curr=0.625000
2025-10-10 18:23:37 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 18:23:37 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 18:23:37 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 18:23:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-10 18:23:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:23:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:23:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-10 18:23:40 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=122.193962, avg_loss=0.649968, seen=188, correct=111, accuracy=0.590426
2025-10-10 18:23:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:23:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:23:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:23:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2134MB allocated=2095MB
2025-10-10 18:23:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:23:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:23:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:23:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:23:43 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.027637, avg_loss=0.625691, seen=40, correct=24, accuracy=0.600000
2025-10-10 18:23:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:23:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:23:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:23:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2134MB allocated=2095MB
2025-10-10 18:23:45 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.650000, curr=0.600000
2025-10-10 18:23:54 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 18:23:54 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 18:23:54 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 18:23:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-10 18:23:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:23:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:23:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-10 18:23:57 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=123.140427, avg_loss=0.655002, seen=188, correct=112, accuracy=0.595745
2025-10-10 18:23:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:23:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:23:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:23:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2134MB allocated=2095MB
2025-10-10 18:23:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:23:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:23:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:24:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:24:00 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.611708, avg_loss=0.640293, seen=40, correct=27, accuracy=0.675000
2025-10-10 18:24:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:24:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:24:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:24:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2134MB allocated=2095MB
2025-10-10 18:24:02 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.675000
2025-10-10 18:24:10 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 18:24:10 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 18:24:10 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 18:24:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-10 18:24:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:24:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:24:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-10 18:24:13 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=123.266579, avg_loss=0.655673, seen=188, correct=111, accuracy=0.590426
2025-10-10 18:24:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:24:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:24:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:24:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2134MB allocated=2095MB
2025-10-10 18:24:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:24:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:24:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:24:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:24:17 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.182764, avg_loss=0.629569, seen=40, correct=27, accuracy=0.675000
2025-10-10 18:24:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:24:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:24:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:24:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2134MB allocated=2095MB
2025-10-10 18:24:19 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.675000
2025-10-10 18:24:28 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 18:24:28 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 18:24:28 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 18:24:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-10 18:24:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:24:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:24:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-10 18:24:31 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=122.898384, avg_loss=0.653715, seen=188, correct=121, accuracy=0.643617
2025-10-10 18:24:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:24:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:24:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:24:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2134MB allocated=2095MB
2025-10-10 18:24:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:24:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:24:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:24:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:24:36 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.414509, avg_loss=0.635363, seen=40, correct=25, accuracy=0.625000
2025-10-10 18:24:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:24:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:24:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:24:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2134MB allocated=2095MB
2025-10-10 18:24:38 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.675000, curr=0.625000
2025-10-10 18:24:46 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 18:24:46 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 18:24:46 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 18:24:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-10 18:24:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:24:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:24:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-10 18:24:49 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=122.097839, avg_loss=0.649457, seen=188, correct=112, accuracy=0.595745
2025-10-10 18:24:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:24:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:24:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:24:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2134MB allocated=2095MB
2025-10-10 18:24:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:24:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:24:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:24:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:24:53 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.946293, avg_loss=0.623657, seen=40, correct=26, accuracy=0.650000
2025-10-10 18:24:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:24:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:24:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:24:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2134MB allocated=2095MB
2025-10-10 18:24:55 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.675000, curr=0.650000
2025-10-10 18:24:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 18:24:55 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 18:24:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:24:56 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:24:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2134MB allocated=2095MB
2025-10-10 18:24:57 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #52', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 18:24:57 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #52', 'Round': 0, 'Results_raw': {}}
2025-10-10 18:24:57 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 18:24:58 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 18:24:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:24:59 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 18:24:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:24:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:25:00 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 18:25:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-10-10 18:25:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:25:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:25:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-10-10 18:25:02 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=89, loss_sum=62.383293, avg_loss=0.700936, seen=89, correct=52, accuracy=0.584270
2025-10-10 18:25:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:25:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:25:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:25:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2122MB allocated=2086MB
2025-10-10 18:25:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:25:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:25:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:25:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:25:06 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.074999, avg_loss=0.701875, seen=40, correct=23, accuracy=0.575000
2025-10-10 18:25:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:25:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:25:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:25:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2122MB allocated=2086MB
2025-10-10 18:25:08 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-10 18:25:08 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 18:25:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=424, total=1694)
2025-10-10 18:25:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:25:08 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 18:25:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:25:08 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 18:25:08 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=212, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 18:25:16 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 18:25:16 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 18:25:16 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 18:25:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-10-10 18:25:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:25:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:25:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-10-10 18:25:18 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=89, loss_sum=65.266319, avg_loss=0.733329, seen=89, correct=54, accuracy=0.606742
2025-10-10 18:25:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:25:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:25:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:25:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2156MB allocated=2103MB
2025-10-10 18:25:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:25:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:25:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:25:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:25:21 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.661293, avg_loss=0.716532, seen=40, correct=24, accuracy=0.600000
2025-10-10 18:25:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:25:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:25:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:25:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2156MB allocated=2103MB
2025-10-10 18:25:23 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-10 18:25:30 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 18:25:30 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 18:25:30 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 18:25:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-10-10 18:25:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:25:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:25:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-10-10 18:25:32 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=89, loss_sum=68.905113, avg_loss=0.774215, seen=89, correct=47, accuracy=0.528090
2025-10-10 18:25:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:25:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:25:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:25:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2156MB allocated=2103MB
2025-10-10 18:25:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:25:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:25:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:25:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:25:35 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.717224, avg_loss=0.767931, seen=40, correct=21, accuracy=0.525000
2025-10-10 18:25:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:25:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:25:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:25:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2156MB allocated=2103MB
2025-10-10 18:25:38 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.600000, curr=0.525000
2025-10-10 18:25:46 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 18:25:46 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 18:25:46 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 18:25:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-10-10 18:25:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:25:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:25:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-10-10 18:25:48 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=89, loss_sum=67.271225, avg_loss=0.755856, seen=89, correct=52, accuracy=0.584270
2025-10-10 18:25:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:25:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:25:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:25:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2156MB allocated=2103MB
2025-10-10 18:25:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:25:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:25:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:25:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:25:52 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.532845, avg_loss=0.713321, seen=40, correct=23, accuracy=0.575000
2025-10-10 18:25:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:25:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:25:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:25:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2156MB allocated=2103MB
2025-10-10 18:25:55 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.600000, curr=0.575000
2025-10-10 18:26:04 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 18:26:04 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 18:26:04 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 18:26:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-10-10 18:26:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:26:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:26:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-10-10 18:26:06 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=89, loss_sum=64.930031, avg_loss=0.729551, seen=89, correct=52, accuracy=0.584270
2025-10-10 18:26:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:26:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:26:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:26:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2156MB allocated=2103MB
2025-10-10 18:26:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:26:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:26:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:26:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:26:09 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.123089, avg_loss=0.703077, seen=40, correct=24, accuracy=0.600000
2025-10-10 18:26:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:26:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:26:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:26:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2156MB allocated=2103MB
2025-10-10 18:26:12 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-10 18:26:22 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 18:26:22 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 18:26:22 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 18:26:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-10-10 18:26:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:26:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:26:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-10-10 18:26:23 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=89, loss_sum=63.961342, avg_loss=0.718667, seen=89, correct=54, accuracy=0.606742
2025-10-10 18:26:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:26:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:26:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:26:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2156MB allocated=2103MB
2025-10-10 18:26:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:26:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:26:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:26:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:26:28 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.548035, avg_loss=0.688701, seen=40, correct=24, accuracy=0.600000
2025-10-10 18:26:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:26:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:26:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:26:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2156MB allocated=2103MB
2025-10-10 18:26:31 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-10 18:26:39 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 18:26:39 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 18:26:39 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 18:26:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-10-10 18:26:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:26:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:26:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-10-10 18:26:41 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=89, loss_sum=63.331619, avg_loss=0.711591, seen=89, correct=54, accuracy=0.606742
2025-10-10 18:26:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:26:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:26:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:26:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2156MB allocated=2103MB
2025-10-10 18:26:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:26:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:26:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:26:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:26:44 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.751492, avg_loss=0.693787, seen=40, correct=23, accuracy=0.575000
2025-10-10 18:26:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:26:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:26:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:26:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2156MB allocated=2103MB
2025-10-10 18:26:46 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.600000, curr=0.575000
2025-10-10 18:26:55 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 18:26:55 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 18:26:55 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 18:26:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-10-10 18:26:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:26:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:26:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-10-10 18:26:57 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=89, loss_sum=63.099846, avg_loss=0.708987, seen=89, correct=53, accuracy=0.595506
2025-10-10 18:26:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:26:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:26:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:27:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2156MB allocated=2103MB
2025-10-10 18:27:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:27:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:27:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:27:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:27:01 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.234932, avg_loss=0.680873, seen=40, correct=24, accuracy=0.600000
2025-10-10 18:27:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:27:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:27:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:27:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2156MB allocated=2103MB
2025-10-10 18:27:03 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-10 18:27:12 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 18:27:12 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 18:27:12 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 18:27:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-10-10 18:27:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:27:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:27:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-10-10 18:27:13 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=89, loss_sum=62.242195, avg_loss=0.699351, seen=89, correct=55, accuracy=0.617978
2025-10-10 18:27:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:27:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:27:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:27:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2156MB allocated=2103MB
2025-10-10 18:27:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:27:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:27:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:27:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:27:18 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.588408, avg_loss=0.689710, seen=40, correct=23, accuracy=0.575000
2025-10-10 18:27:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:27:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:27:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:27:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2156MB allocated=2103MB
2025-10-10 18:27:20 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.600000, curr=0.575000
2025-10-10 18:27:30 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 18:27:30 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 18:27:30 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 18:27:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-10-10 18:27:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:27:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:27:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-10-10 18:27:32 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=89, loss_sum=61.986732, avg_loss=0.696480, seen=89, correct=54, accuracy=0.606742
2025-10-10 18:27:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:27:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:27:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:27:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2156MB allocated=2103MB
2025-10-10 18:27:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:27:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:27:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:27:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:27:35 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.355824, avg_loss=0.683896, seen=40, correct=24, accuracy=0.600000
2025-10-10 18:27:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:27:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:27:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:27:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2156MB allocated=2103MB
2025-10-10 18:27:37 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-10 18:27:46 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 18:27:46 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 18:27:46 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 18:27:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-10-10 18:27:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:27:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:27:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-10-10 18:27:48 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=89, loss_sum=62.574455, avg_loss=0.703084, seen=89, correct=52, accuracy=0.584270
2025-10-10 18:27:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:27:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:27:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:27:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2156MB allocated=2103MB
2025-10-10 18:27:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:27:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:27:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:27:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:27:52 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.275829, avg_loss=0.681896, seen=40, correct=25, accuracy=0.625000
2025-10-10 18:27:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:27:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:27:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:27:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2156MB allocated=2103MB
2025-10-10 18:27:54 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-10 18:27:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 18:27:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 18:27:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:27:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:27:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2156MB allocated=2103MB
2025-10-10 18:27:56 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #43', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 18:27:56 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #43', 'Round': 0, 'Results_raw': {}}
2025-10-10 18:27:56 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 18:27:57 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 18:27:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:27:58 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 18:27:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:27:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:27:59 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 18:27:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-10 18:27:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:27:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:27:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-10 18:27:59 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=9.713543, avg_loss=0.883049, seen=11, correct=6, accuracy=0.545455
2025-10-10 18:27:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:27:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:28:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:28:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2122MB allocated=2095MB
2025-10-10 18:28:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:28:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:28:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:28:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:28:02 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=31.482134, avg_loss=0.787053, seen=40, correct=23, accuracy=0.575000
2025-10-10 18:28:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:28:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:28:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:28:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2122MB allocated=2095MB
2025-10-10 18:28:04 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-10 18:28:04 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 18:28:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=54, total=214)
2025-10-10 18:28:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:28:04 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 18:28:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:28:04 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 18:28:04 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=27, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 18:28:13 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 18:28:13 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 18:28:13 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 18:28:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-10 18:28:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:28:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:28:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-10 18:28:14 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=10.168329, avg_loss=0.924394, seen=11, correct=6, accuracy=0.545455
2025-10-10 18:28:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:28:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:28:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:28:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2156MB allocated=2112MB
2025-10-10 18:28:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:28:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:28:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:28:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:28:18 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=32.958374, avg_loss=0.823959, seen=40, correct=23, accuracy=0.575000
2025-10-10 18:28:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:28:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:28:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:28:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2156MB allocated=2112MB
2025-10-10 18:28:20 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-10 18:28:30 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 18:28:30 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 18:28:30 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 18:28:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-10 18:28:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:28:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:28:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-10 18:28:30 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=10.316586, avg_loss=0.937871, seen=11, correct=6, accuracy=0.545455
2025-10-10 18:28:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:28:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:28:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:28:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2156MB allocated=2112MB
2025-10-10 18:28:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:28:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:28:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:28:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:28:34 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=32.516434, avg_loss=0.812911, seen=40, correct=23, accuracy=0.575000
2025-10-10 18:28:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:28:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:28:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:28:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2156MB allocated=2112MB
2025-10-10 18:28:36 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-10 18:28:44 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 18:28:44 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 18:28:44 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 18:28:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-10 18:28:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:28:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:28:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-10 18:28:45 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=10.672482, avg_loss=0.970226, seen=11, correct=7, accuracy=0.636364
2025-10-10 18:28:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:28:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:28:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:28:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2156MB allocated=2112MB
2025-10-10 18:28:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:28:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:28:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:28:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:28:49 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=33.317726, avg_loss=0.832943, seen=40, correct=23, accuracy=0.575000
2025-10-10 18:28:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:28:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:28:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:28:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2156MB allocated=2112MB
2025-10-10 18:28:51 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-10 18:28:59 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 18:28:59 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 18:28:59 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 18:28:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-10 18:28:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:28:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:29:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-10 18:29:00 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=11.480984, avg_loss=1.043726, seen=11, correct=7, accuracy=0.636364
2025-10-10 18:29:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:29:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:29:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:29:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2156MB allocated=2112MB
2025-10-10 18:29:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:29:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:29:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:29:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:29:03 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=35.488045, avg_loss=0.887201, seen=40, correct=21, accuracy=0.525000
2025-10-10 18:29:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:29:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:29:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:29:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2156MB allocated=2112MB
2025-10-10 18:29:04 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.575000, curr=0.525000
2025-10-10 18:29:13 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 18:29:13 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 18:29:13 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 18:29:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-10 18:29:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:29:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:29:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-10 18:29:13 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=12.005400, avg_loss=1.091400, seen=11, correct=7, accuracy=0.636364
2025-10-10 18:29:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:29:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:29:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:29:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2156MB allocated=2112MB
2025-10-10 18:29:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:29:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:29:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:29:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:29:17 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=36.147816, avg_loss=0.903695, seen=40, correct=23, accuracy=0.575000
2025-10-10 18:29:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:29:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:29:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:29:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2156MB allocated=2112MB
2025-10-10 18:29:19 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-10 18:29:28 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 18:29:28 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 18:29:28 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 18:29:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-10 18:29:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:29:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:29:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-10 18:29:29 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=12.449075, avg_loss=1.131734, seen=11, correct=6, accuracy=0.545455
2025-10-10 18:29:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:29:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:29:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:29:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2156MB allocated=2112MB
2025-10-10 18:29:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:29:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:29:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:29:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:29:32 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=36.961876, avg_loss=0.924047, seen=40, correct=22, accuracy=0.550000
2025-10-10 18:29:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:29:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:29:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:29:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2156MB allocated=2112MB
2025-10-10 18:29:34 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.575000, curr=0.550000
2025-10-10 18:29:42 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 18:29:42 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 18:29:42 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 18:29:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-10 18:29:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:29:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:29:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-10 18:29:42 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=11.991007, avg_loss=1.090092, seen=11, correct=7, accuracy=0.636364
2025-10-10 18:29:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:29:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:29:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:29:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2156MB allocated=2112MB
2025-10-10 18:29:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:29:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:29:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:29:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:29:46 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=37.219212, avg_loss=0.930480, seen=40, correct=23, accuracy=0.575000
2025-10-10 18:29:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:29:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:29:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:29:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2156MB allocated=2112MB
2025-10-10 18:29:49 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-10 18:29:58 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 18:29:58 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 18:29:58 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 18:29:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-10 18:29:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:29:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:29:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-10 18:29:58 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=12.965174, avg_loss=1.178652, seen=11, correct=6, accuracy=0.545455
2025-10-10 18:29:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:29:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:29:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:30:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2156MB allocated=2112MB
2025-10-10 18:30:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:30:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:30:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:30:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:30:01 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=38.475147, avg_loss=0.961879, seen=40, correct=23, accuracy=0.575000
2025-10-10 18:30:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:30:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:30:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:30:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2156MB allocated=2112MB
2025-10-10 18:30:04 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-10 18:30:13 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 18:30:13 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 18:30:13 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 18:30:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-10 18:30:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:30:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:30:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-10 18:30:13 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=14.500826, avg_loss=1.318257, seen=11, correct=6, accuracy=0.545455
2025-10-10 18:30:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:30:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:30:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:30:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2156MB allocated=2112MB
2025-10-10 18:30:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:30:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:30:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:30:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:30:16 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=40.747990, avg_loss=1.018700, seen=40, correct=22, accuracy=0.550000
2025-10-10 18:30:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:30:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:30:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:30:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2156MB allocated=2112MB
2025-10-10 18:30:18 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.575000, curr=0.550000
2025-10-10 18:30:26 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 18:30:26 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 18:30:26 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 18:30:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-10 18:30:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:30:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:30:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-10 18:30:26 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=14.175169, avg_loss=1.288652, seen=11, correct=6, accuracy=0.545455
2025-10-10 18:30:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:30:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:30:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:30:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2156MB allocated=2112MB
2025-10-10 18:30:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:30:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:30:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:30:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:30:29 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=44.481453, avg_loss=1.112036, seen=40, correct=22, accuracy=0.550000
2025-10-10 18:30:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:30:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:30:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:30:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2156MB allocated=2112MB
2025-10-10 18:30:31 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.575000, curr=0.550000
2025-10-10 18:30:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 18:30:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 18:30:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:30:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:30:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2156MB allocated=2112MB
2025-10-10 18:30:33 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #2', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 18:30:33 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #2', 'Round': 0, 'Results_raw': {}}
2025-10-10 18:30:33 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 18:30:34 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 18:30:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:30:35 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 18:30:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:30:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:30:36 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 18:30:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-10-10 18:30:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:30:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:30:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-10 18:30:38 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=72, loss_sum=49.009960, avg_loss=0.680694, seen=72, correct=43, accuracy=0.597222
2025-10-10 18:30:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:30:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:30:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:30:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2142MB allocated=2103MB
2025-10-10 18:30:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:30:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:30:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:30:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:30:42 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.507202, avg_loss=0.712680, seen=40, correct=23, accuracy=0.575000
2025-10-10 18:30:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:30:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:30:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:30:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2142MB allocated=2103MB
2025-10-10 18:30:44 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-10 18:30:44 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 18:30:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=343, total=1372)
2025-10-10 18:30:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:30:44 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 18:30:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:30:44 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 18:30:44 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=172, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 18:30:53 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 18:30:53 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 18:30:53 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 18:30:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-10-10 18:30:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:30:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:30:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-10 18:30:54 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=72, loss_sum=48.245056, avg_loss=0.670070, seen=72, correct=43, accuracy=0.597222
2025-10-10 18:30:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:30:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:30:56 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:30:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2174MB allocated=2120MB
2025-10-10 18:30:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:30:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:30:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:30:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:30:58 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.610207, avg_loss=0.765255, seen=40, correct=19, accuracy=0.475000
2025-10-10 18:30:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:30:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:30:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:31:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2174MB allocated=2120MB
2025-10-10 18:31:01 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.575000, curr=0.475000
2025-10-10 18:31:10 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 18:31:10 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 18:31:10 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 18:31:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-10-10 18:31:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:31:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:31:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-10 18:31:11 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=72, loss_sum=48.367821, avg_loss=0.671775, seen=72, correct=41, accuracy=0.569444
2025-10-10 18:31:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:31:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:31:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:31:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2174MB allocated=2120MB
2025-10-10 18:31:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:31:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:31:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:31:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:31:15 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.084053, avg_loss=0.752101, seen=40, correct=19, accuracy=0.475000
2025-10-10 18:31:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:31:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:31:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:31:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2174MB allocated=2120MB
2025-10-10 18:31:17 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.575000, curr=0.475000
2025-10-10 18:31:26 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 18:31:26 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 18:31:26 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 18:31:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-10-10 18:31:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:31:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:31:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-10 18:31:27 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=72, loss_sum=50.001579, avg_loss=0.694466, seen=72, correct=43, accuracy=0.597222
2025-10-10 18:31:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:31:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:31:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:31:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2174MB allocated=2120MB
2025-10-10 18:31:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:31:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:31:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:31:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:31:30 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.415676, avg_loss=0.735392, seen=40, correct=24, accuracy=0.600000
2025-10-10 18:31:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:31:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:31:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:31:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2174MB allocated=2120MB
2025-10-10 18:31:33 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-10 18:31:41 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 18:31:41 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 18:31:41 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 18:31:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-10-10 18:31:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:31:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:31:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-10 18:31:43 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=72, loss_sum=47.897995, avg_loss=0.665250, seen=72, correct=45, accuracy=0.625000
2025-10-10 18:31:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:31:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:31:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:31:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2174MB allocated=2120MB
2025-10-10 18:31:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:31:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:31:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:31:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:31:47 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.690563, avg_loss=0.767264, seen=40, correct=20, accuracy=0.500000
2025-10-10 18:31:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:31:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:31:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:31:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2174MB allocated=2120MB
2025-10-10 18:31:50 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.600000, curr=0.500000
2025-10-10 18:32:00 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 18:32:00 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 18:32:00 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 18:32:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-10-10 18:32:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:32:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:32:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-10 18:32:01 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=72, loss_sum=48.283810, avg_loss=0.670608, seen=72, correct=46, accuracy=0.638889
2025-10-10 18:32:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:32:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:32:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:32:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2174MB allocated=2120MB
2025-10-10 18:32:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:32:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:32:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:32:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:32:04 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.247730, avg_loss=0.756193, seen=40, correct=22, accuracy=0.550000
2025-10-10 18:32:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:32:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:32:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:32:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2174MB allocated=2120MB
2025-10-10 18:32:07 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.600000, curr=0.550000
2025-10-10 18:32:15 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 18:32:15 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 18:32:15 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 18:32:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-10-10 18:32:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:32:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:32:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-10 18:32:16 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=72, loss_sum=47.400879, avg_loss=0.658346, seen=72, correct=44, accuracy=0.611111
2025-10-10 18:32:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:32:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:32:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:32:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2174MB allocated=2120MB
2025-10-10 18:32:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:32:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:32:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:32:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:32:21 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.175810, avg_loss=0.754395, seen=40, correct=22, accuracy=0.550000
2025-10-10 18:32:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:32:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:32:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:32:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2174MB allocated=2120MB
2025-10-10 18:32:23 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.600000, curr=0.550000
2025-10-10 18:32:30 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 18:32:30 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 18:32:30 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 18:32:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-10-10 18:32:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:32:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:32:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-10 18:32:31 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=72, loss_sum=48.231850, avg_loss=0.669887, seen=72, correct=42, accuracy=0.583333
2025-10-10 18:32:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:32:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:32:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:32:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2174MB allocated=2120MB
2025-10-10 18:32:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:32:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:32:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:32:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:32:34 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.856848, avg_loss=0.721421, seen=40, correct=23, accuracy=0.575000
2025-10-10 18:32:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:32:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:32:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:32:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2174MB allocated=2120MB
2025-10-10 18:32:36 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/25), best=0.600000, curr=0.575000
2025-10-10 18:32:43 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 18:32:43 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 18:32:43 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 18:32:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-10-10 18:32:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:32:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:32:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-10 18:32:44 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=72, loss_sum=48.578152, avg_loss=0.674697, seen=72, correct=43, accuracy=0.597222
2025-10-10 18:32:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:32:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:32:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:32:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2174MB allocated=2120MB
2025-10-10 18:32:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:32:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:32:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:32:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:32:47 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.222666, avg_loss=0.705567, seen=40, correct=24, accuracy=0.600000
2025-10-10 18:32:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:32:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:32:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:32:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2174MB allocated=2120MB
2025-10-10 18:32:50 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-10 18:32:59 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 18:32:59 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 18:32:59 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 18:32:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-10-10 18:32:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:32:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:33:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-10 18:33:00 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=72, loss_sum=46.827404, avg_loss=0.650381, seen=72, correct=44, accuracy=0.611111
2025-10-10 18:33:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:33:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:33:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:33:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2174MB allocated=2120MB
2025-10-10 18:33:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:33:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:33:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:33:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:33:04 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.139301, avg_loss=0.703483, seen=40, correct=23, accuracy=0.575000
2025-10-10 18:33:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:33:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:33:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:33:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2174MB allocated=2120MB
2025-10-10 18:33:07 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.600000, curr=0.575000
2025-10-10 18:33:14 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 18:33:14 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 18:33:14 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 18:33:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-10-10 18:33:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:33:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:33:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-10 18:33:16 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=72, loss_sum=46.718166, avg_loss=0.648863, seen=72, correct=45, accuracy=0.625000
2025-10-10 18:33:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:33:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:33:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:33:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2174MB allocated=2120MB
2025-10-10 18:33:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:33:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:33:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:33:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:33:20 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.855696, avg_loss=0.746392, seen=40, correct=19, accuracy=0.475000
2025-10-10 18:33:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:33:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:33:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:33:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2174MB allocated=2120MB
2025-10-10 18:33:22 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.600000, curr=0.475000
2025-10-10 18:33:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 18:33:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 18:33:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:33:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:33:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2174MB allocated=2120MB
2025-10-10 18:33:24 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #13', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 18:33:24 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #13', 'Round': 0, 'Results_raw': {}}
2025-10-10 18:33:24 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 18:33:25 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 18:33:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:33:26 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 18:33:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:33:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:33:27 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 18:33:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-10-10 18:33:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:33:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:33:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-10-10 18:33:29 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=119, loss_sum=83.720245, avg_loss=0.703531, seen=119, correct=65, accuracy=0.546218
2025-10-10 18:33:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:33:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:33:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:33:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2142MB allocated=2112MB
2025-10-10 18:33:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:33:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:33:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:33:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:33:33 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.376116, avg_loss=0.709403, seen=40, correct=16, accuracy=0.400000
2025-10-10 18:33:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:33:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:33:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:33:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2142MB allocated=2112MB
2025-10-10 18:33:35 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.400000
2025-10-10 18:33:35 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 18:33:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=569, total=2275)
2025-10-10 18:33:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:33:35 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 18:33:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:33:35 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 18:33:35 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=285, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 18:33:45 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 18:33:45 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 18:33:45 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 18:33:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-10-10 18:33:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:33:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:33:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-10-10 18:33:47 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=119, loss_sum=84.659149, avg_loss=0.711421, seen=119, correct=63, accuracy=0.529412
2025-10-10 18:33:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:33:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:33:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:33:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2164MB allocated=2128MB
2025-10-10 18:33:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:33:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:33:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:33:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:33:52 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.338152, avg_loss=0.708454, seen=40, correct=18, accuracy=0.450000
2025-10-10 18:33:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:33:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:33:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:33:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2164MB allocated=2128MB
2025-10-10 18:33:54 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.450000
2025-10-10 18:34:03 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 18:34:03 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 18:34:03 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 18:34:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-10-10 18:34:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:34:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:34:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-10-10 18:34:05 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=119, loss_sum=86.479378, avg_loss=0.726717, seen=119, correct=56, accuracy=0.470588
2025-10-10 18:34:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:34:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:34:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:34:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2164MB allocated=2128MB
2025-10-10 18:34:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:34:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:34:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:34:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:34:09 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.186146, avg_loss=0.679654, seen=40, correct=23, accuracy=0.575000
2025-10-10 18:34:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:34:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:34:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:34:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2164MB allocated=2128MB
2025-10-10 18:34:11 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-10 18:34:20 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 18:34:20 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 18:34:20 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 18:34:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-10-10 18:34:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:34:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:34:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-10-10 18:34:22 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=119, loss_sum=86.299240, avg_loss=0.725204, seen=119, correct=58, accuracy=0.487395
2025-10-10 18:34:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:34:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:34:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:34:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2164MB allocated=2128MB
2025-10-10 18:34:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:34:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:34:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:34:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:34:25 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.120584, avg_loss=0.678015, seen=40, correct=23, accuracy=0.575000
2025-10-10 18:34:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:34:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:34:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:34:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2164MB allocated=2128MB
2025-10-10 18:34:26 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-10 18:34:34 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 18:34:34 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 18:34:34 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 18:34:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-10-10 18:34:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:34:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:34:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-10-10 18:34:36 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=119, loss_sum=84.267456, avg_loss=0.708130, seen=119, correct=63, accuracy=0.529412
2025-10-10 18:34:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:34:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:34:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:34:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2164MB allocated=2128MB
2025-10-10 18:34:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:34:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:34:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:34:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:34:39 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.486797, avg_loss=0.712170, seen=40, correct=19, accuracy=0.475000
2025-10-10 18:34:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:34:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:34:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:34:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2164MB allocated=2128MB
2025-10-10 18:34:41 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.575000, curr=0.475000
2025-10-10 18:34:50 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 18:34:50 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 18:34:50 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 18:34:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-10-10 18:34:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:34:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:34:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-10-10 18:34:52 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=119, loss_sum=83.864174, avg_loss=0.704741, seen=119, correct=60, accuracy=0.504202
2025-10-10 18:34:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:34:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:34:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:34:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2164MB allocated=2128MB
2025-10-10 18:34:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:34:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:34:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:34:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:34:56 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.612423, avg_loss=0.690311, seen=40, correct=21, accuracy=0.525000
2025-10-10 18:34:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:34:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:34:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:34:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2164MB allocated=2128MB
2025-10-10 18:34:59 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.575000, curr=0.525000
2025-10-10 18:35:06 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 18:35:06 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 18:35:06 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 18:35:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-10-10 18:35:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:35:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:35:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-10-10 18:35:09 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=119, loss_sum=85.364471, avg_loss=0.717348, seen=119, correct=57, accuracy=0.478992
2025-10-10 18:35:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:35:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:35:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:35:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2164MB allocated=2128MB
2025-10-10 18:35:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:35:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:35:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:35:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:35:13 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.871189, avg_loss=0.671780, seen=40, correct=25, accuracy=0.625000
2025-10-10 18:35:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:35:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:35:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:35:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2164MB allocated=2128MB
2025-10-10 18:35:16 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-10 18:35:23 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 18:35:23 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 18:35:23 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 18:35:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-10-10 18:35:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:35:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:35:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-10-10 18:35:25 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=119, loss_sum=86.499207, avg_loss=0.726884, seen=119, correct=55, accuracy=0.462185
2025-10-10 18:35:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:35:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:35:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:35:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2164MB allocated=2128MB
2025-10-10 18:35:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:35:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:35:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:35:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:35:28 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.703941, avg_loss=0.667599, seen=40, correct=25, accuracy=0.625000
2025-10-10 18:35:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:35:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:35:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:35:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2164MB allocated=2128MB
2025-10-10 18:35:30 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-10 18:35:41 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 18:35:41 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 18:35:41 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 18:35:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-10-10 18:35:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:35:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:35:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-10-10 18:35:42 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=119, loss_sum=83.365677, avg_loss=0.700552, seen=119, correct=66, accuracy=0.554622
2025-10-10 18:35:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:35:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:35:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:35:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2164MB allocated=2128MB
2025-10-10 18:35:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:35:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:35:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:35:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:35:47 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.969093, avg_loss=0.699227, seen=40, correct=19, accuracy=0.475000
2025-10-10 18:35:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:35:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:35:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:35:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2164MB allocated=2128MB
2025-10-10 18:35:49 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.625000, curr=0.475000
2025-10-10 18:35:59 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 18:35:59 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 18:35:59 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 18:35:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-10-10 18:35:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:35:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:36:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-10-10 18:36:01 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=119, loss_sum=85.603081, avg_loss=0.719354, seen=119, correct=61, accuracy=0.512605
2025-10-10 18:36:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:36:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:36:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:36:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2164MB allocated=2128MB
2025-10-10 18:36:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:36:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:36:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:36:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:36:07 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.918228, avg_loss=0.747956, seen=40, correct=17, accuracy=0.425000
2025-10-10 18:36:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:36:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:36:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:36:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2164MB allocated=2128MB
2025-10-10 18:36:09 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.625000, curr=0.425000
2025-10-10 18:36:18 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 18:36:18 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 18:36:18 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 18:36:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-10-10 18:36:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:36:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:36:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-10-10 18:36:20 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=119, loss_sum=83.238556, avg_loss=0.699484, seen=119, correct=65, accuracy=0.546218
2025-10-10 18:36:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:36:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:36:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:36:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2164MB allocated=2128MB
2025-10-10 18:36:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:36:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:36:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:36:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:36:24 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.967220, avg_loss=0.699181, seen=40, correct=20, accuracy=0.500000
2025-10-10 18:36:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:36:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:36:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:36:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2164MB allocated=2128MB
2025-10-10 18:36:26 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.625000, curr=0.500000
2025-10-10 18:36:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 18:36:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 18:36:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:36:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:36:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2164MB allocated=2128MB
2025-10-10 18:36:28 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #41', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 18:36:28 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #41', 'Round': 0, 'Results_raw': {}}
2025-10-10 18:36:28 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 18:36:29 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 18:36:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:36:30 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 18:36:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:36:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:36:31 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 18:36:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 18:36:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:36:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:36:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 18:36:34 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=140.752167, avg_loss=0.703761, seen=200, correct=103, accuracy=0.515000
2025-10-10 18:36:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:36:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:36:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:36:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2162MB allocated=2120MB
2025-10-10 18:36:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:36:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:36:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:36:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:36:39 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=32.228783, avg_loss=0.805720, seen=40, correct=19, accuracy=0.475000
2025-10-10 18:36:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:36:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:36:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:36:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2162MB allocated=2120MB
2025-10-10 18:36:41 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.475000
2025-10-10 18:36:41 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 18:36:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1162, total=4647)
2025-10-10 18:36:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:36:41 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 18:36:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:36:41 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 18:36:41 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=581, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 18:36:52 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 18:36:52 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 18:36:52 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 18:36:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 18:36:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:36:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:36:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 18:36:55 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=140.880035, avg_loss=0.704400, seen=200, correct=102, accuracy=0.510000
2025-10-10 18:36:55 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:36:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:36:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:36:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2184MB allocated=2137MB
2025-10-10 18:36:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:36:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:36:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:37:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:37:00 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=32.719551, avg_loss=0.817989, seen=40, correct=16, accuracy=0.400000
2025-10-10 18:37:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:37:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:37:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:37:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2184MB allocated=2137MB
2025-10-10 18:37:02 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.475000, curr=0.400000
2025-10-10 18:37:11 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 18:37:11 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 18:37:11 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 18:37:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 18:37:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:37:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:37:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 18:37:14 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=149.742828, avg_loss=0.748714, seen=200, correct=92, accuracy=0.460000
2025-10-10 18:37:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:37:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:37:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:37:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2184MB allocated=2137MB
2025-10-10 18:37:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:37:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:37:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:37:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:37:17 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.196732, avg_loss=0.729918, seen=40, correct=21, accuracy=0.525000
2025-10-10 18:37:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:37:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:37:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:37:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2184MB allocated=2137MB
2025-10-10 18:37:19 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.525000
2025-10-10 18:37:30 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 18:37:30 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 18:37:30 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 18:37:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 18:37:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:37:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:37:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 18:37:33 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=141.577484, avg_loss=0.707887, seen=200, correct=105, accuracy=0.525000
2025-10-10 18:37:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:37:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:37:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:37:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2184MB allocated=2137MB
2025-10-10 18:37:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:37:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:37:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:37:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:37:37 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=32.148441, avg_loss=0.803711, seen=40, correct=14, accuracy=0.350000
2025-10-10 18:37:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:37:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:37:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:37:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2184MB allocated=2137MB
2025-10-10 18:37:38 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.525000, curr=0.350000
2025-10-10 18:37:46 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 18:37:46 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 18:37:46 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 18:37:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 18:37:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:37:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:37:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 18:37:49 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=141.085754, avg_loss=0.705429, seen=200, correct=102, accuracy=0.510000
2025-10-10 18:37:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:37:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:37:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:37:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2184MB allocated=2137MB
2025-10-10 18:37:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:37:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:37:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:37:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:37:54 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=32.560944, avg_loss=0.814024, seen=40, correct=11, accuracy=0.275000
2025-10-10 18:37:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:37:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:37:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:37:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2184MB allocated=2137MB
2025-10-10 18:37:55 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.525000, curr=0.275000
2025-10-10 18:38:03 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 18:38:03 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 18:38:03 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 18:38:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 18:38:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:38:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:38:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 18:38:06 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=144.454575, avg_loss=0.722273, seen=200, correct=107, accuracy=0.535000
2025-10-10 18:38:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:38:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:38:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:38:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2184MB allocated=2137MB
2025-10-10 18:38:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:38:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:38:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:38:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:38:10 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.221504, avg_loss=0.755538, seen=40, correct=17, accuracy=0.425000
2025-10-10 18:38:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:38:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:38:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:38:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2184MB allocated=2137MB
2025-10-10 18:38:12 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.525000, curr=0.425000
2025-10-10 18:38:20 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 18:38:20 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 18:38:20 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 18:38:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 18:38:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:38:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:38:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 18:38:24 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=144.259949, avg_loss=0.721300, seen=200, correct=108, accuracy=0.540000
2025-10-10 18:38:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:38:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:38:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:38:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2184MB allocated=2137MB
2025-10-10 18:38:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:38:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:38:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:38:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:38:26 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.028067, avg_loss=0.750702, seen=40, correct=16, accuracy=0.400000
2025-10-10 18:38:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:38:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:38:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:38:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2184MB allocated=2137MB
2025-10-10 18:38:29 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/25), best=0.525000, curr=0.400000
2025-10-10 18:38:37 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 18:38:37 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 18:38:37 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 18:38:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 18:38:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:38:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:38:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 18:38:40 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=141.721268, avg_loss=0.708606, seen=200, correct=104, accuracy=0.520000
2025-10-10 18:38:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:38:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:38:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:38:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2184MB allocated=2137MB
2025-10-10 18:38:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:38:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:38:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:38:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:38:44 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.861980, avg_loss=0.771550, seen=40, correct=14, accuracy=0.350000
2025-10-10 18:38:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:38:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:38:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:38:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2184MB allocated=2137MB
2025-10-10 18:38:46 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/25), best=0.525000, curr=0.350000
2025-10-10 18:38:56 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 18:38:56 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 18:38:56 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 18:38:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 18:38:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:38:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:38:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 18:38:59 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=139.954315, avg_loss=0.699772, seen=200, correct=106, accuracy=0.530000
2025-10-10 18:38:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:38:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:39:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:39:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2184MB allocated=2137MB
2025-10-10 18:39:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:39:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:39:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:39:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:39:02 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=32.432259, avg_loss=0.810806, seen=40, correct=16, accuracy=0.400000
2025-10-10 18:39:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:39:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:39:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:39:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2184MB allocated=2137MB
2025-10-10 18:39:05 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/25), best=0.525000, curr=0.400000
2025-10-10 18:39:15 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 18:39:15 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 18:39:15 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 18:39:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 18:39:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:39:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:39:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 18:39:18 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=139.981445, avg_loss=0.699907, seen=200, correct=106, accuracy=0.530000
2025-10-10 18:39:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:39:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:39:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:39:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2184MB allocated=2137MB
2025-10-10 18:39:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:39:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:39:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:39:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:39:22 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=32.525070, avg_loss=0.813127, seen=40, correct=19, accuracy=0.475000
2025-10-10 18:39:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:39:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:39:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:39:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2184MB allocated=2137MB
2025-10-10 18:39:24 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/25), best=0.525000, curr=0.475000
2025-10-10 18:39:32 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 18:39:32 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 18:39:32 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 18:39:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 18:39:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:39:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:39:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 18:39:35 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=140.691483, avg_loss=0.703457, seen=200, correct=102, accuracy=0.510000
2025-10-10 18:39:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:39:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:39:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:39:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2184MB allocated=2137MB
2025-10-10 18:39:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:39:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:39:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:39:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:39:38 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=31.004679, avg_loss=0.775117, seen=40, correct=14, accuracy=0.350000
2025-10-10 18:39:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:39:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:39:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:39:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2184MB allocated=2137MB
2025-10-10 18:39:41 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/25), best=0.525000, curr=0.350000
2025-10-10 18:39:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 18:39:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 18:39:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:39:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:39:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2184MB allocated=2137MB
2025-10-10 18:39:42 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #25', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 18:39:42 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #25', 'Round': 0, 'Results_raw': {}}
2025-10-10 18:39:43 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 18:39:43 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 18:39:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:39:44 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 18:39:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:39:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:39:45 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 18:39:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-10-10 18:39:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:39:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:39:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-10-10 18:39:47 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=57, loss_sum=36.257492, avg_loss=0.636096, seen=57, correct=39, accuracy=0.684211
2025-10-10 18:39:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:39:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:39:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:39:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2162MB allocated=2128MB
2025-10-10 18:39:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:39:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:39:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:39:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:39:51 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.345387, avg_loss=0.608635, seen=40, correct=27, accuracy=0.675000
2025-10-10 18:39:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:39:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:39:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:39:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2162MB allocated=2128MB
2025-10-10 18:39:53 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.675000
2025-10-10 18:39:53 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 18:39:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=272, total=1088)
2025-10-10 18:39:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:39:54 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 18:39:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:39:54 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 18:39:54 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=136, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 18:40:03 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 18:40:03 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 18:40:03 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 18:40:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-10-10 18:40:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:40:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:40:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-10-10 18:40:05 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=57, loss_sum=37.344517, avg_loss=0.655167, seen=57, correct=36, accuracy=0.631579
2025-10-10 18:40:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:40:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:40:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:40:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2208MB allocated=2145MB
2025-10-10 18:40:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:40:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:40:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:40:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:40:08 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.003475, avg_loss=0.575087, seen=40, correct=28, accuracy=0.700000
2025-10-10 18:40:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:40:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:40:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:40:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2208MB allocated=2145MB
2025-10-10 18:40:10 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.700000
2025-10-10 18:40:19 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 18:40:19 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 18:40:19 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 18:40:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-10-10 18:40:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:40:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:40:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-10-10 18:40:20 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=57, loss_sum=37.000687, avg_loss=0.649135, seen=57, correct=38, accuracy=0.666667
2025-10-10 18:40:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:40:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:40:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:40:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2208MB allocated=2145MB
2025-10-10 18:40:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:40:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:40:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:40:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:40:25 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=22.712526, avg_loss=0.567813, seen=40, correct=27, accuracy=0.675000
2025-10-10 18:40:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:40:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:40:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:40:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2208MB allocated=2145MB
2025-10-10 18:40:27 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.700000, curr=0.675000
2025-10-10 18:40:36 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 18:40:36 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 18:40:36 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 18:40:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-10-10 18:40:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:40:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:40:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-10-10 18:40:37 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=57, loss_sum=37.804947, avg_loss=0.663245, seen=57, correct=37, accuracy=0.649123
2025-10-10 18:40:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:40:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:40:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:40:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2208MB allocated=2145MB
2025-10-10 18:40:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:40:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:40:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:40:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:40:42 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=22.982143, avg_loss=0.574554, seen=40, correct=27, accuracy=0.675000
2025-10-10 18:40:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:40:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:40:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:40:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2208MB allocated=2145MB
2025-10-10 18:40:44 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.700000, curr=0.675000
2025-10-10 18:40:54 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 18:40:54 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 18:40:54 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 18:40:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-10-10 18:40:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:40:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:40:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-10-10 18:40:55 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=57, loss_sum=36.771206, avg_loss=0.645109, seen=57, correct=36, accuracy=0.631579
2025-10-10 18:40:55 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:40:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:40:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:40:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2208MB allocated=2145MB
2025-10-10 18:40:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:40:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:40:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:40:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:40:59 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=22.745777, avg_loss=0.568644, seen=40, correct=29, accuracy=0.725000
2025-10-10 18:40:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:40:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:41:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:41:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2208MB allocated=2145MB
2025-10-10 18:41:01 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.725000
2025-10-10 18:41:09 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 18:41:09 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 18:41:09 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 18:41:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-10-10 18:41:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:41:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:41:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-10-10 18:41:11 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=57, loss_sum=37.042511, avg_loss=0.649869, seen=57, correct=35, accuracy=0.614035
2025-10-10 18:41:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:41:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:41:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:41:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2208MB allocated=2145MB
2025-10-10 18:41:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:41:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:41:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:41:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:41:15 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=22.697926, avg_loss=0.567448, seen=40, correct=29, accuracy=0.725000
2025-10-10 18:41:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:41:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:41:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:41:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2208MB allocated=2145MB
2025-10-10 18:41:17 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.725000
2025-10-10 18:41:24 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 18:41:24 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 18:41:24 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 18:41:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-10-10 18:41:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:41:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:41:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-10-10 18:41:26 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=57, loss_sum=37.003525, avg_loss=0.649185, seen=57, correct=40, accuracy=0.701754
2025-10-10 18:41:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:41:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:41:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:41:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2208MB allocated=2145MB
2025-10-10 18:41:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:41:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:41:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:41:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:41:28 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=22.887554, avg_loss=0.572189, seen=40, correct=27, accuracy=0.675000
2025-10-10 18:41:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:41:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:41:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:41:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2208MB allocated=2145MB
2025-10-10 18:41:31 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.725000, curr=0.675000
2025-10-10 18:41:39 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 18:41:39 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 18:41:39 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 18:41:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-10-10 18:41:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:41:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:41:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-10-10 18:41:40 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=57, loss_sum=37.266800, avg_loss=0.653804, seen=57, correct=34, accuracy=0.596491
2025-10-10 18:41:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:41:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:41:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:41:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2208MB allocated=2145MB
2025-10-10 18:41:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:41:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:41:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:41:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:41:43 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.997164, avg_loss=0.599929, seen=40, correct=25, accuracy=0.625000
2025-10-10 18:41:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:41:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:41:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:41:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2208MB allocated=2145MB
2025-10-10 18:41:45 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.725000, curr=0.625000
2025-10-10 18:41:56 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 18:41:56 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 18:41:56 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 18:41:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-10-10 18:41:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:41:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:41:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-10-10 18:41:57 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=57, loss_sum=37.009697, avg_loss=0.649293, seen=57, correct=36, accuracy=0.631579
2025-10-10 18:41:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:41:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:41:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:41:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2208MB allocated=2145MB
2025-10-10 18:41:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:41:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:41:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:42:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:42:00 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.023308, avg_loss=0.575583, seen=40, correct=29, accuracy=0.725000
2025-10-10 18:42:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:42:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:42:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:42:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2208MB allocated=2145MB
2025-10-10 18:42:02 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.725000
2025-10-10 18:42:09 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 18:42:09 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 18:42:09 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 18:42:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-10-10 18:42:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:42:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:42:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-10-10 18:42:10 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=57, loss_sum=36.854080, avg_loss=0.646563, seen=57, correct=39, accuracy=0.684211
2025-10-10 18:42:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:42:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:42:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:42:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2208MB allocated=2145MB
2025-10-10 18:42:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:42:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:42:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:42:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:42:13 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=22.462353, avg_loss=0.561559, seen=40, correct=28, accuracy=0.700000
2025-10-10 18:42:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:42:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:42:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:42:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2208MB allocated=2145MB
2025-10-10 18:42:16 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.725000, curr=0.700000
2025-10-10 18:42:24 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 18:42:24 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 18:42:24 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 18:42:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-10-10 18:42:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:42:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:42:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-10-10 18:42:25 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=57, loss_sum=36.595436, avg_loss=0.642025, seen=57, correct=39, accuracy=0.684211
2025-10-10 18:42:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:42:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:42:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:42:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2208MB allocated=2145MB
2025-10-10 18:42:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:42:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:42:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:42:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:42:28 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=22.520866, avg_loss=0.563022, seen=40, correct=28, accuracy=0.700000
2025-10-10 18:42:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:42:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:42:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:42:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2208MB allocated=2145MB
2025-10-10 18:42:31 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.725000, curr=0.700000
2025-10-10 18:42:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 18:42:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 18:42:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:42:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:42:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2208MB allocated=2145MB
2025-10-10 18:42:32 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #7', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 18:42:32 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #7', 'Round': 0, 'Results_raw': {}}
2025-10-10 18:42:32 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 18:42:33 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 18:42:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:42:34 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 18:42:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:42:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:42:35 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 18:42:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 18:42:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:42:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:42:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 18:42:38 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=133.262878, avg_loss=0.666314, seen=200, correct=109, accuracy=0.545000
2025-10-10 18:42:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:42:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:42:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:42:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2162MB allocated=2137MB
2025-10-10 18:42:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:42:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:42:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:42:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:42:43 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.075104, avg_loss=0.726878, seen=40, correct=17, accuracy=0.425000
2025-10-10 18:42:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:42:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:42:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:42:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2162MB allocated=2137MB
2025-10-10 18:42:45 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.425000
2025-10-10 18:42:45 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 18:42:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1236, total=4944)
2025-10-10 18:42:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:42:46 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 18:42:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:42:46 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 18:42:46 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=618, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 18:42:55 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 18:42:55 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 18:42:55 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 18:42:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 18:42:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:42:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:42:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 18:42:58 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=132.979858, avg_loss=0.664899, seen=200, correct=118, accuracy=0.590000
2025-10-10 18:42:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:42:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:43:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:43:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2154MB
2025-10-10 18:43:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:43:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:43:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:43:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:43:02 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.414478, avg_loss=0.710362, seen=40, correct=23, accuracy=0.575000
2025-10-10 18:43:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:43:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:43:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:43:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2154MB
2025-10-10 18:43:04 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-10 18:43:14 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 18:43:14 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 18:43:14 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 18:43:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 18:43:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:43:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:43:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 18:43:17 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=133.960556, avg_loss=0.669803, seen=200, correct=112, accuracy=0.560000
2025-10-10 18:43:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:43:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:43:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:43:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2154MB
2025-10-10 18:43:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:43:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:43:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:43:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:43:21 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.133850, avg_loss=0.703346, seen=40, correct=23, accuracy=0.575000
2025-10-10 18:43:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:43:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:43:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:43:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2154MB
2025-10-10 18:43:24 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-10 18:43:32 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 18:43:32 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 18:43:32 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 18:43:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 18:43:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:43:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:43:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 18:43:36 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=132.516083, avg_loss=0.662580, seen=200, correct=115, accuracy=0.575000
2025-10-10 18:43:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:43:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:43:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:43:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2154MB
2025-10-10 18:43:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:43:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:43:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:43:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:43:40 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.038319, avg_loss=0.725958, seen=40, correct=16, accuracy=0.400000
2025-10-10 18:43:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:43:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:43:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:43:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2154MB
2025-10-10 18:43:43 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.575000, curr=0.400000
2025-10-10 18:43:51 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 18:43:51 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 18:43:51 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 18:43:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 18:43:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:43:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:43:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 18:43:54 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=133.306686, avg_loss=0.666533, seen=200, correct=112, accuracy=0.560000
2025-10-10 18:43:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:43:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:43:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:43:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2154MB
2025-10-10 18:43:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:43:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:43:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:43:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:43:57 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.768608, avg_loss=0.719215, seen=40, correct=17, accuracy=0.425000
2025-10-10 18:43:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:43:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:43:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:44:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2154MB
2025-10-10 18:44:00 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.575000, curr=0.425000
2025-10-10 18:44:08 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 18:44:08 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 18:44:08 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 18:44:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 18:44:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:44:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:44:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 18:44:11 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=134.229034, avg_loss=0.671145, seen=200, correct=111, accuracy=0.555000
2025-10-10 18:44:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:44:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:44:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:44:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2154MB
2025-10-10 18:44:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:44:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:44:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:44:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:44:16 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.415714, avg_loss=0.710393, seen=40, correct=22, accuracy=0.550000
2025-10-10 18:44:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:44:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:44:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:44:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2154MB
2025-10-10 18:44:18 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.575000, curr=0.550000
2025-10-10 18:44:27 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 18:44:27 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 18:44:27 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 18:44:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 18:44:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:44:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:44:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 18:44:30 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=134.341965, avg_loss=0.671710, seen=200, correct=117, accuracy=0.585000
2025-10-10 18:44:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:44:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:44:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:44:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2154MB
2025-10-10 18:44:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:44:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:44:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:44:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:44:33 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.222336, avg_loss=0.705558, seen=40, correct=22, accuracy=0.550000
2025-10-10 18:44:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:44:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:44:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:44:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2154MB
2025-10-10 18:44:35 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/25), best=0.575000, curr=0.550000
2025-10-10 18:44:44 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 18:44:44 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 18:44:44 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 18:44:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 18:44:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:44:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:44:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 18:44:47 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=134.910706, avg_loss=0.674554, seen=200, correct=114, accuracy=0.570000
2025-10-10 18:44:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:44:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:44:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:44:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2154MB
2025-10-10 18:44:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:44:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:44:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:44:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:44:50 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.696913, avg_loss=0.717423, seen=40, correct=18, accuracy=0.450000
2025-10-10 18:44:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:44:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:44:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:44:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2154MB
2025-10-10 18:44:52 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/25), best=0.575000, curr=0.450000
2025-10-10 18:45:01 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 18:45:01 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 18:45:01 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 18:45:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 18:45:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:45:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:45:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 18:45:04 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=135.114716, avg_loss=0.675574, seen=200, correct=113, accuracy=0.565000
2025-10-10 18:45:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:45:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:45:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:45:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2154MB
2025-10-10 18:45:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:45:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:45:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:45:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:45:07 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.122566, avg_loss=0.703064, seen=40, correct=22, accuracy=0.550000
2025-10-10 18:45:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:45:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:45:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:45:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2154MB
2025-10-10 18:45:09 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/25), best=0.575000, curr=0.550000
2025-10-10 18:45:18 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 18:45:18 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 18:45:18 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 18:45:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 18:45:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:45:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:45:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 18:45:21 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=134.359985, avg_loss=0.671800, seen=200, correct=114, accuracy=0.570000
2025-10-10 18:45:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:45:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:45:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:45:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2154MB
2025-10-10 18:45:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:45:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:45:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:45:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:45:26 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.206472, avg_loss=0.705162, seen=40, correct=20, accuracy=0.500000
2025-10-10 18:45:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:45:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:45:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:45:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2154MB
2025-10-10 18:45:29 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/25), best=0.575000, curr=0.500000
2025-10-10 18:45:36 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 18:45:36 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 18:45:36 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 18:45:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 18:45:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:45:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:45:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 18:45:40 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=135.124756, avg_loss=0.675624, seen=200, correct=112, accuracy=0.560000
2025-10-10 18:45:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:45:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:45:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:45:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2154MB
2025-10-10 18:45:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:45:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:45:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:45:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:45:43 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.110771, avg_loss=0.702769, seen=40, correct=22, accuracy=0.550000
2025-10-10 18:45:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:45:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:45:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:45:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2154MB
2025-10-10 18:45:45 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/25), best=0.575000, curr=0.550000
2025-10-10 18:45:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 18:45:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 18:45:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:45:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:45:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2154MB
2025-10-10 18:45:47 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #24', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 18:45:47 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #24', 'Round': 0, 'Results_raw': {}}
2025-10-10 18:45:47 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 18:45:48 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 18:45:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:45:49 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 18:45:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:45:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:45:50 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 18:45:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 18:45:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:45:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:45:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 18:45:53 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=141.326538, avg_loss=0.706633, seen=200, correct=107, accuracy=0.535000
2025-10-10 18:45:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:45:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:45:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:45:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2145MB
2025-10-10 18:45:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:45:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:45:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:45:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:45:56 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.521166, avg_loss=0.713029, seen=40, correct=19, accuracy=0.475000
2025-10-10 18:45:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:45:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:45:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:45:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2145MB
2025-10-10 18:45:58 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.475000
2025-10-10 18:45:58 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 18:45:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1069, total=4273)
2025-10-10 18:45:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:45:59 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 18:45:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:45:59 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 18:45:59 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=535, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 18:46:08 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 18:46:08 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 18:46:08 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 18:46:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 18:46:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:46:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:46:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 18:46:11 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=138.960785, avg_loss=0.694804, seen=200, correct=107, accuracy=0.535000
2025-10-10 18:46:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:46:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:46:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:46:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2208MB allocated=2162MB
2025-10-10 18:46:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:46:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:46:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:46:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:46:14 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.676416, avg_loss=0.691910, seen=40, correct=21, accuracy=0.525000
2025-10-10 18:46:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:46:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:46:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:46:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2208MB allocated=2162MB
2025-10-10 18:46:16 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.525000
2025-10-10 18:46:26 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 18:46:26 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 18:46:26 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 18:46:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 18:46:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:46:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:46:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 18:46:29 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=144.959045, avg_loss=0.724795, seen=200, correct=104, accuracy=0.520000
2025-10-10 18:46:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:46:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:46:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:46:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2208MB allocated=2162MB
2025-10-10 18:46:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:46:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:46:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:46:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:46:33 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.922932, avg_loss=0.748073, seen=40, correct=18, accuracy=0.450000
2025-10-10 18:46:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:46:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:46:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:46:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2208MB allocated=2162MB
2025-10-10 18:46:35 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.525000, curr=0.450000
2025-10-10 18:46:45 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 18:46:45 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 18:46:45 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 18:46:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 18:46:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:46:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:46:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 18:46:48 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=144.730133, avg_loss=0.723651, seen=200, correct=103, accuracy=0.515000
2025-10-10 18:46:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:46:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:46:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:46:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2208MB allocated=2162MB
2025-10-10 18:46:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:46:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:46:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:46:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:46:51 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.913309, avg_loss=0.747833, seen=40, correct=18, accuracy=0.450000
2025-10-10 18:46:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:46:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:46:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:46:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2208MB allocated=2162MB
2025-10-10 18:46:53 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.525000, curr=0.450000
2025-10-10 18:47:02 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 18:47:02 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 18:47:02 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 18:47:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 18:47:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:47:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:47:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 18:47:05 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=137.402298, avg_loss=0.687011, seen=200, correct=113, accuracy=0.565000
2025-10-10 18:47:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:47:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:47:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:47:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2208MB allocated=2162MB
2025-10-10 18:47:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:47:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:47:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:47:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:47:08 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.461847, avg_loss=0.686546, seen=40, correct=22, accuracy=0.550000
2025-10-10 18:47:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:47:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:47:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:47:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2208MB allocated=2162MB
2025-10-10 18:47:10 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.550000
2025-10-10 18:47:18 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 18:47:18 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 18:47:18 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 18:47:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 18:47:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:47:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:47:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 18:47:22 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=137.296112, avg_loss=0.686481, seen=200, correct=117, accuracy=0.585000
2025-10-10 18:47:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:47:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:47:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:47:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2208MB allocated=2162MB
2025-10-10 18:47:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:47:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:47:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:47:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:47:24 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.216312, avg_loss=0.680408, seen=40, correct=22, accuracy=0.550000
2025-10-10 18:47:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:47:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:47:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:47:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2208MB allocated=2162MB
2025-10-10 18:47:26 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.550000
2025-10-10 18:47:33 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 18:47:33 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 18:47:33 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 18:47:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 18:47:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:47:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:47:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 18:47:36 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=139.507553, avg_loss=0.697538, seen=200, correct=111, accuracy=0.555000
2025-10-10 18:47:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:47:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:47:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:47:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2208MB allocated=2162MB
2025-10-10 18:47:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:47:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:47:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:47:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:47:40 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.128239, avg_loss=0.703206, seen=40, correct=21, accuracy=0.525000
2025-10-10 18:47:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:47:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:47:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:47:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2208MB allocated=2162MB
2025-10-10 18:47:43 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.550000, curr=0.525000
2025-10-10 18:47:51 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 18:47:51 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 18:47:51 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 18:47:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 18:47:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:47:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:47:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 18:47:54 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=137.253922, avg_loss=0.686270, seen=200, correct=102, accuracy=0.510000
2025-10-10 18:47:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:47:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:47:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:47:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2208MB allocated=2162MB
2025-10-10 18:47:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:47:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:47:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:47:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:47:57 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.984951, avg_loss=0.674624, seen=40, correct=24, accuracy=0.600000
2025-10-10 18:47:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:47:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:47:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:48:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2208MB allocated=2162MB
2025-10-10 18:48:00 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-10 18:48:09 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 18:48:09 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 18:48:09 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 18:48:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 18:48:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:48:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:48:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 18:48:12 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=137.819321, avg_loss=0.689097, seen=200, correct=110, accuracy=0.550000
2025-10-10 18:48:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:48:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:48:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:48:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2208MB allocated=2162MB
2025-10-10 18:48:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:48:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:48:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:48:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:48:16 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.241512, avg_loss=0.681038, seen=40, correct=23, accuracy=0.575000
2025-10-10 18:48:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:48:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:48:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:48:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2208MB allocated=2162MB
2025-10-10 18:48:19 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.600000, curr=0.575000
2025-10-10 18:48:28 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 18:48:28 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 18:48:28 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 18:48:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 18:48:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:48:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:48:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 18:48:31 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=138.419479, avg_loss=0.692097, seen=200, correct=113, accuracy=0.565000
2025-10-10 18:48:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:48:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:48:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:48:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2208MB allocated=2162MB
2025-10-10 18:48:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:48:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:48:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:48:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:48:35 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.812346, avg_loss=0.695309, seen=40, correct=22, accuracy=0.550000
2025-10-10 18:48:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:48:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:48:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:48:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2208MB allocated=2162MB
2025-10-10 18:48:37 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.600000, curr=0.550000
2025-10-10 18:48:45 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 18:48:45 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 18:48:45 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 18:48:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 18:48:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:48:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:48:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 18:48:48 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=138.348785, avg_loss=0.691744, seen=200, correct=115, accuracy=0.575000
2025-10-10 18:48:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:48:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:48:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:48:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2208MB allocated=2162MB
2025-10-10 18:48:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:48:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:48:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:48:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:48:51 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.711637, avg_loss=0.692791, seen=40, correct=22, accuracy=0.550000
2025-10-10 18:48:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:48:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:48:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:48:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2208MB allocated=2162MB
2025-10-10 18:48:54 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.600000, curr=0.550000
2025-10-10 18:48:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 18:48:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 18:48:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:48:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:48:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2208MB allocated=2162MB
2025-10-10 18:48:55 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #37', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 18:48:55 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #37', 'Round': 0, 'Results_raw': {}}
2025-10-10 18:48:55 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 18:48:56 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 18:48:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:48:57 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 18:48:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:48:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:48:58 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 18:48:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-10 18:48:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:48:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:48:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-10 18:48:59 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=8.432915, avg_loss=0.766629, seen=11, correct=7, accuracy=0.636364
2025-10-10 18:48:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:48:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:49:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:49:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2154MB
2025-10-10 18:49:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:49:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:49:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:49:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:49:02 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.901249, avg_loss=0.647531, seen=40, correct=23, accuracy=0.575000
2025-10-10 18:49:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:49:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:49:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:49:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2154MB
2025-10-10 18:49:04 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-10 18:49:04 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 18:49:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=56, total=224)
2025-10-10 18:49:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:49:05 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 18:49:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:49:05 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 18:49:05 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=28, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 18:49:13 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 18:49:13 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 18:49:13 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 18:49:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-10 18:49:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:49:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:49:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-10 18:49:14 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=8.575123, avg_loss=0.779557, seen=11, correct=6, accuracy=0.545455
2025-10-10 18:49:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:49:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:49:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:49:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2216MB allocated=2170MB
2025-10-10 18:49:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:49:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:49:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:49:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:49:19 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.434334, avg_loss=0.635858, seen=40, correct=24, accuracy=0.600000
2025-10-10 18:49:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:49:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:49:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:49:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2216MB allocated=2170MB
2025-10-10 18:49:21 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-10 18:49:30 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 18:49:30 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 18:49:30 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 18:49:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-10 18:49:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:49:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:49:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-10 18:49:30 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=8.143064, avg_loss=0.740279, seen=11, correct=7, accuracy=0.636364
2025-10-10 18:49:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:49:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:49:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:49:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2216MB allocated=2170MB
2025-10-10 18:49:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:49:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:49:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:49:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:49:34 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.120464, avg_loss=0.653012, seen=40, correct=24, accuracy=0.600000
2025-10-10 18:49:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:49:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:49:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:49:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2216MB allocated=2170MB
2025-10-10 18:49:36 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-10 18:49:44 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 18:49:44 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 18:49:44 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 18:49:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-10 18:49:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:49:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:49:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-10 18:49:44 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=7.589833, avg_loss=0.689985, seen=11, correct=7, accuracy=0.636364
2025-10-10 18:49:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:49:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:49:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:49:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2216MB allocated=2170MB
2025-10-10 18:49:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:49:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:49:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:49:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:49:49 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.317410, avg_loss=0.657935, seen=40, correct=22, accuracy=0.550000
2025-10-10 18:49:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:49:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:49:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:49:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2216MB allocated=2170MB
2025-10-10 18:49:51 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.600000, curr=0.550000
2025-10-10 18:50:00 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 18:50:00 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 18:50:00 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 18:50:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-10 18:50:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:50:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:50:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-10 18:50:00 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=7.674985, avg_loss=0.697726, seen=11, correct=7, accuracy=0.636364
2025-10-10 18:50:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:50:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:50:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:50:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2216MB allocated=2170MB
2025-10-10 18:50:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:50:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:50:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:50:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:50:03 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.872215, avg_loss=0.646805, seen=40, correct=23, accuracy=0.575000
2025-10-10 18:50:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:50:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:50:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:50:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2216MB allocated=2170MB
2025-10-10 18:50:06 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.600000, curr=0.575000
2025-10-10 18:50:14 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 18:50:14 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 18:50:14 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 18:50:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-10 18:50:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:50:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:50:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-10 18:50:15 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=7.277590, avg_loss=0.661599, seen=11, correct=7, accuracy=0.636364
2025-10-10 18:50:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:50:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:50:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:50:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2216MB allocated=2170MB
2025-10-10 18:50:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:50:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:50:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:50:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:50:18 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.296797, avg_loss=0.657420, seen=40, correct=24, accuracy=0.600000
2025-10-10 18:50:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:50:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:50:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:50:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2216MB allocated=2170MB
2025-10-10 18:50:19 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-10 18:50:26 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 18:50:26 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 18:50:26 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 18:50:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-10 18:50:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:50:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:50:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-10 18:50:27 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=6.904081, avg_loss=0.627644, seen=11, correct=7, accuracy=0.636364
2025-10-10 18:50:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:50:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:50:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:50:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2216MB allocated=2170MB
2025-10-10 18:50:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:50:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:50:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:50:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:50:30 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.495907, avg_loss=0.662398, seen=40, correct=23, accuracy=0.575000
2025-10-10 18:50:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:50:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:50:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:50:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2216MB allocated=2170MB
2025-10-10 18:50:32 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.600000, curr=0.575000
2025-10-10 18:50:41 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 18:50:41 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 18:50:41 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 18:50:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-10 18:50:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:50:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:50:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-10 18:50:41 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=6.905230, avg_loss=0.627748, seen=11, correct=7, accuracy=0.636364
2025-10-10 18:50:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:50:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:50:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:50:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2216MB allocated=2170MB
2025-10-10 18:50:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:50:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:50:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:50:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:50:44 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.530449, avg_loss=0.663261, seen=40, correct=24, accuracy=0.600000
2025-10-10 18:50:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:50:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:50:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:50:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2216MB allocated=2170MB
2025-10-10 18:50:46 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-10 18:50:54 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 18:50:54 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 18:50:54 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 18:50:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-10 18:50:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:50:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:50:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-10 18:50:55 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=7.003226, avg_loss=0.636657, seen=11, correct=7, accuracy=0.636364
2025-10-10 18:50:55 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:50:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:50:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:50:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2216MB allocated=2170MB
2025-10-10 18:50:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:50:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:50:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:50:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:50:59 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.958965, avg_loss=0.673974, seen=40, correct=24, accuracy=0.600000
2025-10-10 18:50:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:50:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:51:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:51:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2216MB allocated=2170MB
2025-10-10 18:51:01 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-10 18:51:10 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 18:51:10 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 18:51:10 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 18:51:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-10 18:51:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:51:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:51:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-10 18:51:11 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=6.586785, avg_loss=0.598799, seen=11, correct=8, accuracy=0.727273
2025-10-10 18:51:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:51:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:51:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:51:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2216MB allocated=2170MB
2025-10-10 18:51:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:51:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:51:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:51:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:51:14 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.652588, avg_loss=0.666315, seen=40, correct=23, accuracy=0.575000
2025-10-10 18:51:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:51:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:51:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:51:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2216MB allocated=2170MB
2025-10-10 18:51:16 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.600000, curr=0.575000
2025-10-10 18:51:24 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 18:51:24 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 18:51:24 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 18:51:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-10 18:51:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:51:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:51:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-10 18:51:24 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=6.141185, avg_loss=0.558290, seen=11, correct=8, accuracy=0.727273
2025-10-10 18:51:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:51:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:51:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:51:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2216MB allocated=2170MB
2025-10-10 18:51:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:51:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:51:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:51:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:51:28 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.817240, avg_loss=0.695431, seen=40, correct=24, accuracy=0.600000
2025-10-10 18:51:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:51:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:51:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:51:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2216MB allocated=2170MB
2025-10-10 18:51:30 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-10 18:51:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 18:51:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 18:51:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:51:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:51:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2216MB allocated=2170MB
2025-10-10 18:51:31 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #22', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 18:51:31 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #22', 'Round': 0, 'Results_raw': {}}
2025-10-10 18:51:31 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 18:51:32 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 18:51:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:51:33 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 18:51:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:51:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:51:34 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 18:51:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-10-10 18:51:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:51:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:51:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-10-10 18:51:37 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=126, loss_sum=88.168076, avg_loss=0.699747, seen=126, correct=77, accuracy=0.611111
2025-10-10 18:51:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:51:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:51:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:51:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2202MB allocated=2162MB
2025-10-10 18:51:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:51:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:51:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:51:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:51:40 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.058575, avg_loss=0.601464, seen=40, correct=26, accuracy=0.650000
2025-10-10 18:51:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:51:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:51:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:51:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2202MB allocated=2162MB
2025-10-10 18:51:42 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.650000
2025-10-10 18:51:42 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 18:51:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=600, total=2399)
2025-10-10 18:51:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:51:43 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 18:51:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:51:43 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 18:51:43 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=300, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 18:51:52 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 18:51:52 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 18:51:52 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 18:51:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-10-10 18:51:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:51:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:51:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-10-10 18:51:54 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=126, loss_sum=93.753067, avg_loss=0.744072, seen=126, correct=79, accuracy=0.626984
2025-10-10 18:51:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:51:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:51:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:51:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2240MB allocated=2179MB
2025-10-10 18:51:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:51:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:51:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:51:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:51:57 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.573565, avg_loss=0.589339, seen=40, correct=29, accuracy=0.725000
2025-10-10 18:51:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:51:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:51:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:51:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2240MB allocated=2179MB
2025-10-10 18:51:59 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.725000
2025-10-10 18:52:08 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 18:52:08 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 18:52:08 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 18:52:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-10-10 18:52:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:52:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:52:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-10-10 18:52:10 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=126, loss_sum=95.047050, avg_loss=0.754342, seen=126, correct=78, accuracy=0.619048
2025-10-10 18:52:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:52:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:52:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:52:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2240MB allocated=2179MB
2025-10-10 18:52:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:52:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:52:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:52:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:52:13 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.075771, avg_loss=0.576894, seen=40, correct=30, accuracy=0.750000
2025-10-10 18:52:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:52:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:52:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:52:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2240MB allocated=2179MB
2025-10-10 18:52:15 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.750000
2025-10-10 18:52:24 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 18:52:24 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 18:52:24 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 18:52:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-10-10 18:52:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:52:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:52:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-10-10 18:52:26 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=126, loss_sum=94.300293, avg_loss=0.748415, seen=126, correct=78, accuracy=0.619048
2025-10-10 18:52:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:52:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:52:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:52:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2240MB allocated=2179MB
2025-10-10 18:52:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:52:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:52:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:52:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:52:29 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.110115, avg_loss=0.577753, seen=40, correct=27, accuracy=0.675000
2025-10-10 18:52:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:52:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:52:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:52:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2240MB allocated=2179MB
2025-10-10 18:52:32 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.750000, curr=0.675000
2025-10-10 18:52:40 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 18:52:40 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 18:52:40 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 18:52:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-10-10 18:52:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:52:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:52:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-10-10 18:52:42 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=126, loss_sum=93.818214, avg_loss=0.744589, seen=126, correct=78, accuracy=0.619048
2025-10-10 18:52:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:52:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:52:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:52:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2240MB allocated=2179MB
2025-10-10 18:52:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:52:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:52:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:52:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:52:46 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=22.829321, avg_loss=0.570733, seen=40, correct=30, accuracy=0.750000
2025-10-10 18:52:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:52:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:52:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:52:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2240MB allocated=2179MB
2025-10-10 18:52:49 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.750000
2025-10-10 18:52:58 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 18:52:58 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 18:52:58 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 18:52:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-10-10 18:52:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:52:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:53:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-10-10 18:53:00 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=126, loss_sum=91.924759, avg_loss=0.729562, seen=126, correct=73, accuracy=0.579365
2025-10-10 18:53:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:53:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:53:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:53:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2240MB allocated=2179MB
2025-10-10 18:53:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:53:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:53:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:53:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:53:03 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.199469, avg_loss=0.579987, seen=40, correct=30, accuracy=0.750000
2025-10-10 18:53:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:53:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:53:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:53:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2240MB allocated=2179MB
2025-10-10 18:53:06 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.750000
2025-10-10 18:53:14 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 18:53:14 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 18:53:14 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 18:53:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-10-10 18:53:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:53:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:53:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-10-10 18:53:16 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=126, loss_sum=89.632820, avg_loss=0.711372, seen=126, correct=78, accuracy=0.619048
2025-10-10 18:53:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:53:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:53:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:53:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2240MB allocated=2179MB
2025-10-10 18:53:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:53:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:53:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:53:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:53:21 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=22.779287, avg_loss=0.569482, seen=40, correct=31, accuracy=0.775000
2025-10-10 18:53:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:53:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:53:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:53:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2240MB allocated=2179MB
2025-10-10 18:53:23 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.775000
2025-10-10 18:53:32 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 18:53:32 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 18:53:32 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 18:53:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-10-10 18:53:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:53:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:53:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-10-10 18:53:34 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=126, loss_sum=90.202835, avg_loss=0.715896, seen=126, correct=79, accuracy=0.626984
2025-10-10 18:53:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:53:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:53:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:53:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2240MB allocated=2179MB
2025-10-10 18:53:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:53:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:53:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:53:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:53:38 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=22.859516, avg_loss=0.571488, seen=40, correct=30, accuracy=0.750000
2025-10-10 18:53:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:53:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:53:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:53:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2240MB allocated=2179MB
2025-10-10 18:53:40 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.775000, curr=0.750000
2025-10-10 18:53:48 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 18:53:48 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 18:53:48 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 18:53:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-10-10 18:53:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:53:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:53:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-10-10 18:53:50 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=126, loss_sum=89.056252, avg_loss=0.706796, seen=126, correct=79, accuracy=0.626984
2025-10-10 18:53:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:53:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:53:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:53:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2240MB allocated=2179MB
2025-10-10 18:53:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:53:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:53:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:53:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:53:54 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.051718, avg_loss=0.576293, seen=40, correct=30, accuracy=0.750000
2025-10-10 18:53:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:53:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:53:56 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:53:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2240MB allocated=2179MB
2025-10-10 18:53:57 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.775000, curr=0.750000
2025-10-10 18:54:06 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 18:54:06 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 18:54:06 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 18:54:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-10-10 18:54:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:54:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:54:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-10-10 18:54:08 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=126, loss_sum=89.016609, avg_loss=0.706481, seen=126, correct=76, accuracy=0.603175
2025-10-10 18:54:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:54:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:54:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:54:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2240MB allocated=2179MB
2025-10-10 18:54:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:54:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:54:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:54:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:54:11 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.277014, avg_loss=0.581925, seen=40, correct=30, accuracy=0.750000
2025-10-10 18:54:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:54:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:54:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:54:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2240MB allocated=2179MB
2025-10-10 18:54:14 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.775000, curr=0.750000
2025-10-10 18:54:23 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 18:54:23 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 18:54:23 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 18:54:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-10-10 18:54:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:54:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:54:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-10-10 18:54:25 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=126, loss_sum=89.173454, avg_loss=0.707726, seen=126, correct=80, accuracy=0.634921
2025-10-10 18:54:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:54:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:54:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:54:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2240MB allocated=2179MB
2025-10-10 18:54:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:54:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:54:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:54:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:54:28 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.242308, avg_loss=0.581058, seen=40, correct=28, accuracy=0.700000
2025-10-10 18:54:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:54:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:54:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:54:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2240MB allocated=2179MB
2025-10-10 18:54:31 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/25), best=0.775000, curr=0.700000
2025-10-10 18:54:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 18:54:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 18:54:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:54:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:54:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2240MB allocated=2179MB
2025-10-10 18:54:32 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #20', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 18:54:32 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #20', 'Round': 0, 'Results_raw': {}}
2025-10-10 18:54:32 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 18:54:33 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 18:54:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:54:34 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 18:54:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:54:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:54:35 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 18:54:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-10-10 18:54:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:54:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:54:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-10-10 18:54:36 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=63, loss_sum=42.035954, avg_loss=0.667237, seen=63, correct=42, accuracy=0.666667
2025-10-10 18:54:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:54:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:54:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:54:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2202MB allocated=2170MB
2025-10-10 18:54:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:54:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:54:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:54:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:54:41 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=22.601482, avg_loss=0.565037, seen=40, correct=27, accuracy=0.675000
2025-10-10 18:54:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:54:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:54:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:54:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2202MB allocated=2170MB
2025-10-10 18:54:43 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.675000
2025-10-10 18:54:43 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 18:54:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=303, total=1209)
2025-10-10 18:54:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:54:43 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 18:54:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:54:43 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 18:54:43 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=152, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 18:54:52 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 18:54:52 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 18:54:52 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 18:54:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-10-10 18:54:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:54:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:54:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-10-10 18:54:54 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=63, loss_sum=44.017647, avg_loss=0.698693, seen=63, correct=37, accuracy=0.587302
2025-10-10 18:54:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:54:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:54:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:54:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2242MB allocated=2187MB
2025-10-10 18:54:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:54:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:54:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:54:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:54:58 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=21.452801, avg_loss=0.536320, seen=40, correct=30, accuracy=0.750000
2025-10-10 18:54:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:54:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:54:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:55:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2242MB allocated=2187MB
2025-10-10 18:55:00 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.750000
2025-10-10 18:55:08 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 18:55:08 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 18:55:08 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 18:55:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-10-10 18:55:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:55:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:55:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-10-10 18:55:09 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=63, loss_sum=40.107914, avg_loss=0.636634, seen=63, correct=47, accuracy=0.746032
2025-10-10 18:55:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:55:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:55:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:55:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2242MB allocated=2187MB
2025-10-10 18:55:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:55:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:55:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:55:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:55:12 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=20.527958, avg_loss=0.513199, seen=40, correct=29, accuracy=0.725000
2025-10-10 18:55:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:55:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:55:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:55:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2242MB allocated=2187MB
2025-10-10 18:55:15 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.750000, curr=0.725000
2025-10-10 18:55:23 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 18:55:23 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 18:55:23 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 18:55:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-10-10 18:55:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:55:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:55:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-10-10 18:55:24 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=63, loss_sum=41.666565, avg_loss=0.661374, seen=63, correct=45, accuracy=0.714286
2025-10-10 18:55:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:55:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:55:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:55:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2242MB allocated=2187MB
2025-10-10 18:55:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:55:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:55:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:55:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:55:29 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=19.682682, avg_loss=0.492067, seen=40, correct=32, accuracy=0.800000
2025-10-10 18:55:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:55:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:55:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:55:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2242MB allocated=2187MB
2025-10-10 18:55:30 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.800000
2025-10-10 18:55:39 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 18:55:39 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 18:55:39 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 18:55:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-10-10 18:55:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:55:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:55:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-10-10 18:55:40 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=63, loss_sum=41.261993, avg_loss=0.654952, seen=63, correct=44, accuracy=0.698413
2025-10-10 18:55:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:55:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:55:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:55:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2242MB allocated=2187MB
2025-10-10 18:55:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:55:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:55:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:55:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:55:43 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=19.321484, avg_loss=0.483037, seen=40, correct=31, accuracy=0.775000
2025-10-10 18:55:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:55:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:55:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:55:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2242MB allocated=2187MB
2025-10-10 18:55:45 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.800000, curr=0.775000
2025-10-10 18:55:54 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 18:55:54 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 18:55:54 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 18:55:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-10-10 18:55:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:55:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:55:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-10-10 18:55:55 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=63, loss_sum=41.794846, avg_loss=0.663410, seen=63, correct=44, accuracy=0.698413
2025-10-10 18:55:55 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:55:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:55:56 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:55:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2242MB allocated=2187MB
2025-10-10 18:55:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:55:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:55:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:56:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:56:00 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=19.292274, avg_loss=0.482307, seen=40, correct=31, accuracy=0.775000
2025-10-10 18:56:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:56:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:56:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:56:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2242MB allocated=2187MB
2025-10-10 18:56:01 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.800000, curr=0.775000
2025-10-10 18:56:10 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 18:56:10 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 18:56:10 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 18:56:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-10-10 18:56:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:56:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:56:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-10-10 18:56:11 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=63, loss_sum=39.574993, avg_loss=0.628174, seen=63, correct=45, accuracy=0.714286
2025-10-10 18:56:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:56:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:56:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:56:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2242MB allocated=2187MB
2025-10-10 18:56:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:56:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:56:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:56:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:56:16 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=18.779997, avg_loss=0.469500, seen=40, correct=32, accuracy=0.800000
2025-10-10 18:56:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:56:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:56:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:56:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2242MB allocated=2187MB
2025-10-10 18:56:18 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.800000
2025-10-10 18:56:26 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 18:56:26 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 18:56:26 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 18:56:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-10-10 18:56:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:56:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:56:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-10-10 18:56:28 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=63, loss_sum=38.178501, avg_loss=0.606008, seen=63, correct=47, accuracy=0.746032
2025-10-10 18:56:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:56:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:56:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:56:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2242MB allocated=2187MB
2025-10-10 18:56:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:56:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:56:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:56:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:56:30 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=19.216616, avg_loss=0.480415, seen=40, correct=30, accuracy=0.750000
2025-10-10 18:56:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:56:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:56:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:56:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2242MB allocated=2187MB
2025-10-10 18:56:33 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.800000, curr=0.750000
2025-10-10 18:56:42 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 18:56:42 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 18:56:42 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 18:56:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-10-10 18:56:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:56:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:56:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-10-10 18:56:43 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=63, loss_sum=39.760559, avg_loss=0.631120, seen=63, correct=46, accuracy=0.730159
2025-10-10 18:56:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:56:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:56:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:56:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2242MB allocated=2187MB
2025-10-10 18:56:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:56:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:56:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:56:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:56:47 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=19.018970, avg_loss=0.475474, seen=40, correct=30, accuracy=0.750000
2025-10-10 18:56:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:56:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:56:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:56:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2242MB allocated=2187MB
2025-10-10 18:56:50 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.800000, curr=0.750000
2025-10-10 18:56:57 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 18:56:57 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 18:56:57 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 18:56:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-10-10 18:56:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:56:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:56:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-10-10 18:56:58 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=63, loss_sum=41.710846, avg_loss=0.662077, seen=63, correct=42, accuracy=0.666667
2025-10-10 18:56:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:56:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:56:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:57:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2242MB allocated=2187MB
2025-10-10 18:57:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:57:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:57:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:57:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:57:01 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=19.581676, avg_loss=0.489542, seen=40, correct=31, accuracy=0.775000
2025-10-10 18:57:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:57:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:57:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:57:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2242MB allocated=2187MB
2025-10-10 18:57:03 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.800000, curr=0.775000
2025-10-10 18:57:10 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 18:57:10 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 18:57:10 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 18:57:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-10-10 18:57:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:57:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:57:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-10-10 18:57:12 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=63, loss_sum=39.019691, avg_loss=0.619360, seen=63, correct=47, accuracy=0.746032
2025-10-10 18:57:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:57:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:57:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:57:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2242MB allocated=2187MB
2025-10-10 18:57:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:57:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:57:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:57:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:57:16 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=18.653616, avg_loss=0.466340, seen=40, correct=31, accuracy=0.775000
2025-10-10 18:57:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:57:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:57:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:57:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2242MB allocated=2187MB
2025-10-10 18:57:18 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/25), best=0.800000, curr=0.775000
2025-10-10 18:57:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 18:57:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 18:57:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:57:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:57:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2242MB allocated=2187MB
2025-10-10 18:57:19 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #10', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 18:57:19 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #10', 'Round': 0, 'Results_raw': {}}
2025-10-10 18:57:19 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 18:57:20 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 18:57:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:57:21 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 18:57:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:57:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:57:22 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 18:57:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 18:57:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:57:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:57:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 18:57:25 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=128.651474, avg_loss=0.643257, seen=200, correct=131, accuracy=0.655000
2025-10-10 18:57:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:57:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:57:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:57:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2202MB allocated=2179MB
2025-10-10 18:57:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:57:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:57:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:57:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:57:29 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.687325, avg_loss=0.642183, seen=40, correct=27, accuracy=0.675000
2025-10-10 18:57:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:57:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:57:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:57:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2202MB allocated=2179MB
2025-10-10 18:57:30 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.675000
2025-10-10 18:57:30 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 18:57:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1002, total=4005)
2025-10-10 18:57:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:57:31 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 18:57:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:57:31 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 18:57:31 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=501, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 18:57:39 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 18:57:39 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 18:57:39 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 18:57:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 18:57:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:57:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:57:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 18:57:42 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=132.365173, avg_loss=0.661826, seen=200, correct=113, accuracy=0.565000
2025-10-10 18:57:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:57:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:57:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:57:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2224MB allocated=2196MB
2025-10-10 18:57:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:57:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:57:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:57:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:57:46 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.541782, avg_loss=0.663545, seen=40, correct=26, accuracy=0.650000
2025-10-10 18:57:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:57:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:57:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:57:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2224MB allocated=2196MB
2025-10-10 18:57:49 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.675000, curr=0.650000
2025-10-10 18:57:57 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 18:57:57 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 18:57:57 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 18:57:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 18:57:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:57:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:58:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 18:58:00 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=126.587097, avg_loss=0.632935, seen=200, correct=126, accuracy=0.630000
2025-10-10 18:58:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:58:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:58:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:58:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2224MB allocated=2196MB
2025-10-10 18:58:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:58:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:58:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:58:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:58:03 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.040276, avg_loss=0.651007, seen=40, correct=25, accuracy=0.625000
2025-10-10 18:58:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:58:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:58:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:58:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2224MB allocated=2196MB
2025-10-10 18:58:05 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.675000, curr=0.625000
2025-10-10 18:58:13 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 18:58:13 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 18:58:13 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 18:58:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 18:58:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:58:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:58:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 18:58:16 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=127.145432, avg_loss=0.635727, seen=200, correct=128, accuracy=0.640000
2025-10-10 18:58:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:58:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:58:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:58:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2224MB allocated=2196MB
2025-10-10 18:58:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:58:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:58:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:58:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:58:19 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.824005, avg_loss=0.645600, seen=40, correct=27, accuracy=0.675000
2025-10-10 18:58:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:58:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:58:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:58:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2224MB allocated=2196MB
2025-10-10 18:58:21 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.675000
2025-10-10 18:58:30 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 18:58:30 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 18:58:30 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 18:58:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 18:58:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:58:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:58:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 18:58:33 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=134.878769, avg_loss=0.674394, seen=200, correct=105, accuracy=0.525000
2025-10-10 18:58:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:58:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:58:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:58:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2224MB allocated=2196MB
2025-10-10 18:58:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:58:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:58:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:58:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:58:36 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.689552, avg_loss=0.667239, seen=40, correct=22, accuracy=0.550000
2025-10-10 18:58:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:58:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:58:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:58:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2224MB allocated=2196MB
2025-10-10 18:58:38 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.675000, curr=0.550000
2025-10-10 18:58:46 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 18:58:46 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 18:58:46 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 18:58:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 18:58:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:58:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:58:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 18:58:49 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=127.053177, avg_loss=0.635266, seen=200, correct=122, accuracy=0.610000
2025-10-10 18:58:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:58:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:58:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:58:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2224MB allocated=2196MB
2025-10-10 18:58:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:58:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:58:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:58:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:58:53 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.112417, avg_loss=0.652810, seen=40, correct=25, accuracy=0.625000
2025-10-10 18:58:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:58:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:58:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:58:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2224MB allocated=2196MB
2025-10-10 18:58:56 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.675000, curr=0.625000
2025-10-10 18:59:03 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 18:59:03 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 18:59:04 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 18:59:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 18:59:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:59:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:59:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 18:59:07 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=127.684311, avg_loss=0.638422, seen=200, correct=126, accuracy=0.630000
2025-10-10 18:59:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:59:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:59:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:59:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2224MB allocated=2196MB
2025-10-10 18:59:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:59:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:59:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:59:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:59:10 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.931889, avg_loss=0.648297, seen=40, correct=23, accuracy=0.575000
2025-10-10 18:59:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:59:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:59:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:59:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2224MB allocated=2196MB
2025-10-10 18:59:13 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.675000, curr=0.575000
2025-10-10 18:59:19 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 18:59:19 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 18:59:19 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 18:59:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 18:59:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:59:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:59:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 18:59:22 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=127.663483, avg_loss=0.638317, seen=200, correct=124, accuracy=0.620000
2025-10-10 18:59:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:59:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:59:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:59:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2224MB allocated=2196MB
2025-10-10 18:59:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:59:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:59:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:59:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:59:26 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.269640, avg_loss=0.656741, seen=40, correct=25, accuracy=0.625000
2025-10-10 18:59:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:59:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:59:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:59:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2224MB allocated=2196MB
2025-10-10 18:59:28 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/25), best=0.675000, curr=0.625000
2025-10-10 18:59:36 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 18:59:36 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 18:59:36 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 18:59:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 18:59:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:59:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:59:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 18:59:40 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=130.585907, avg_loss=0.652930, seen=200, correct=118, accuracy=0.590000
2025-10-10 18:59:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:59:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:59:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:59:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2224MB allocated=2196MB
2025-10-10 18:59:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 18:59:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:59:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:59:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 18:59:43 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.410343, avg_loss=0.660259, seen=40, correct=24, accuracy=0.600000
2025-10-10 18:59:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:59:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:59:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 18:59:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2224MB allocated=2196MB
2025-10-10 18:59:45 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/25), best=0.675000, curr=0.600000
2025-10-10 18:59:54 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 18:59:54 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 18:59:54 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 18:59:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 18:59:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 18:59:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 18:59:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 18:59:58 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=130.242233, avg_loss=0.651211, seen=200, correct=124, accuracy=0.620000
2025-10-10 18:59:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 18:59:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:00:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:00:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2224MB allocated=2196MB
2025-10-10 19:00:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 19:00:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:00:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:00:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 19:00:02 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.610577, avg_loss=0.665264, seen=40, correct=24, accuracy=0.600000
2025-10-10 19:00:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:00:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:00:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:00:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2224MB allocated=2196MB
2025-10-10 19:00:04 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/25), best=0.675000, curr=0.600000
2025-10-10 19:00:12 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 19:00:12 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 19:00:12 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 19:00:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 19:00:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:00:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:00:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 19:00:15 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=134.459946, avg_loss=0.672300, seen=200, correct=112, accuracy=0.560000
2025-10-10 19:00:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:00:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:00:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:00:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2224MB allocated=2196MB
2025-10-10 19:00:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 19:00:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:00:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:00:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 19:00:19 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.846611, avg_loss=0.671165, seen=40, correct=22, accuracy=0.550000
2025-10-10 19:00:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:00:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:00:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:00:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2224MB allocated=2196MB
2025-10-10 19:00:21 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/25), best=0.675000, curr=0.550000
2025-10-10 19:00:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 19:00:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 19:00:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:00:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:00:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2224MB allocated=2196MB
2025-10-10 19:00:23 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #40', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 19:00:23 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #40', 'Round': 0, 'Results_raw': {}}
2025-10-10 19:00:23 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 19:00:24 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 19:00:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:00:25 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 19:00:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 19:00:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:00:26 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 19:00:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-10-10 19:00:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:00:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:00:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 19:00:28 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=133, loss_sum=91.110817, avg_loss=0.685044, seen=133, correct=81, accuracy=0.609023
2025-10-10 19:00:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:00:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:00:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:00:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2222MB allocated=2187MB
2025-10-10 19:00:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 19:00:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:00:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:00:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 19:00:31 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.533047, avg_loss=0.713326, seen=40, correct=20, accuracy=0.500000
2025-10-10 19:00:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:00:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:00:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:00:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2222MB allocated=2187MB
2025-10-10 19:00:34 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.500000
2025-10-10 19:00:34 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 19:00:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=632, total=2527)
2025-10-10 19:00:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:00:34 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 19:00:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:00:34 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 19:00:34 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=316, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 19:00:42 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 19:00:42 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 19:00:42 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 19:00:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-10-10 19:00:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:00:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:00:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 19:00:45 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=133, loss_sum=90.240501, avg_loss=0.678500, seen=133, correct=77, accuracy=0.578947
2025-10-10 19:00:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:00:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:00:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:00:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-10-10 19:00:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 19:00:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:00:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:00:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 19:00:48 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.341944, avg_loss=0.708549, seen=40, correct=19, accuracy=0.475000
2025-10-10 19:00:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:00:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:00:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:00:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-10-10 19:00:51 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.500000, curr=0.475000
2025-10-10 19:00:59 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 19:00:59 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 19:00:59 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 19:00:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-10-10 19:00:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:00:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:01:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 19:01:01 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=133, loss_sum=90.362839, avg_loss=0.679420, seen=133, correct=79, accuracy=0.593985
2025-10-10 19:01:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:01:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:01:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:01:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-10-10 19:01:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 19:01:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:01:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:01:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 19:01:04 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.133183, avg_loss=0.703330, seen=40, correct=21, accuracy=0.525000
2025-10-10 19:01:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:01:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:01:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:01:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-10-10 19:01:06 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.525000
2025-10-10 19:01:16 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 19:01:16 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 19:01:16 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 19:01:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-10-10 19:01:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:01:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:01:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 19:01:19 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=133, loss_sum=91.335777, avg_loss=0.686735, seen=133, correct=79, accuracy=0.593985
2025-10-10 19:01:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:01:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:01:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:01:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-10-10 19:01:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 19:01:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:01:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:01:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 19:01:23 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.058025, avg_loss=0.726451, seen=40, correct=20, accuracy=0.500000
2025-10-10 19:01:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:01:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:01:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:01:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-10-10 19:01:25 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.525000, curr=0.500000
2025-10-10 19:01:34 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 19:01:34 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 19:01:34 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 19:01:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-10-10 19:01:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:01:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:01:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 19:01:36 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=133, loss_sum=90.560715, avg_loss=0.680908, seen=133, correct=81, accuracy=0.609023
2025-10-10 19:01:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:01:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:01:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:01:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-10-10 19:01:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 19:01:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:01:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:01:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 19:01:40 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.334198, avg_loss=0.708355, seen=40, correct=21, accuracy=0.525000
2025-10-10 19:01:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:01:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:01:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:01:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-10-10 19:01:42 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.525000
2025-10-10 19:01:51 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 19:01:51 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 19:01:51 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 19:01:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-10-10 19:01:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:01:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:01:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 19:01:54 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=133, loss_sum=90.014908, avg_loss=0.676804, seen=133, correct=74, accuracy=0.556391
2025-10-10 19:01:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:01:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:01:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:01:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-10-10 19:01:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 19:01:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:01:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:01:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 19:01:57 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.088266, avg_loss=0.702207, seen=40, correct=20, accuracy=0.500000
2025-10-10 19:01:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:01:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:01:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:01:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-10-10 19:01:59 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.525000, curr=0.500000
2025-10-10 19:02:08 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 19:02:08 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 19:02:08 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 19:02:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-10-10 19:02:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:02:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:02:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 19:02:10 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=133, loss_sum=90.217728, avg_loss=0.678329, seen=133, correct=78, accuracy=0.586466
2025-10-10 19:02:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:02:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:02:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:02:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-10-10 19:02:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 19:02:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:02:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:02:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 19:02:14 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.266289, avg_loss=0.706657, seen=40, correct=21, accuracy=0.525000
2025-10-10 19:02:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:02:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:02:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:02:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-10-10 19:02:16 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.525000
2025-10-10 19:02:25 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 19:02:25 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 19:02:25 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 19:02:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-10-10 19:02:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:02:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:02:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 19:02:28 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=133, loss_sum=90.492462, avg_loss=0.680394, seen=133, correct=79, accuracy=0.593985
2025-10-10 19:02:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:02:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:02:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:02:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-10-10 19:02:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 19:02:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:02:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:02:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 19:02:31 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.584608, avg_loss=0.714615, seen=40, correct=18, accuracy=0.450000
2025-10-10 19:02:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:02:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:02:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:02:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-10-10 19:02:33 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.525000, curr=0.450000
2025-10-10 19:02:42 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 19:02:42 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 19:02:42 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 19:02:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-10-10 19:02:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:02:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:02:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 19:02:44 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=133, loss_sum=90.844116, avg_loss=0.683038, seen=133, correct=76, accuracy=0.571429
2025-10-10 19:02:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:02:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:02:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:02:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-10-10 19:02:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 19:02:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:02:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:02:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 19:02:47 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.752678, avg_loss=0.718817, seen=40, correct=19, accuracy=0.475000
2025-10-10 19:02:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:02:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:02:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:02:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-10-10 19:02:48 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.525000, curr=0.475000
2025-10-10 19:02:57 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 19:02:57 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 19:02:57 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 19:02:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-10-10 19:02:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:02:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:02:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 19:02:59 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=133, loss_sum=90.914070, avg_loss=0.683564, seen=133, correct=75, accuracy=0.563910
2025-10-10 19:02:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:02:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:03:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:03:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-10-10 19:03:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 19:03:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:03:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:03:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 19:03:03 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.030048, avg_loss=0.725751, seen=40, correct=20, accuracy=0.500000
2025-10-10 19:03:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:03:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:03:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:03:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-10-10 19:03:06 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.525000, curr=0.500000
2025-10-10 19:03:14 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 19:03:14 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 19:03:14 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 19:03:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-10-10 19:03:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:03:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:03:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 19:03:16 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=133, loss_sum=90.802414, avg_loss=0.682725, seen=133, correct=72, accuracy=0.541353
2025-10-10 19:03:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:03:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:03:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:03:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-10-10 19:03:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 19:03:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:03:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:03:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 19:03:20 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.046455, avg_loss=0.726161, seen=40, correct=20, accuracy=0.500000
2025-10-10 19:03:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:03:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:03:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:03:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-10-10 19:03:22 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/25), best=0.525000, curr=0.500000
2025-10-10 19:03:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 19:03:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 19:03:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:03:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:03:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-10-10 19:03:24 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #50', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 19:03:24 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #50', 'Round': 0, 'Results_raw': {}}
2025-10-10 19:03:24 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 19:03:25 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 19:03:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:03:25 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 19:03:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 19:03:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:03:26 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 19:03:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-10 19:03:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:03:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:03:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-10 19:03:27 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=7.244605, avg_loss=0.658600, seen=11, correct=8, accuracy=0.727273
2025-10-10 19:03:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:03:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:03:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:03:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2222MB allocated=2196MB
2025-10-10 19:03:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 19:03:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:03:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:03:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 19:03:31 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.983738, avg_loss=0.699593, seen=40, correct=22, accuracy=0.550000
2025-10-10 19:03:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:03:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:03:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:03:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2222MB allocated=2196MB
2025-10-10 19:03:33 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.550000
2025-10-10 19:03:33 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 19:03:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=54, total=213)
2025-10-10 19:03:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:03:33 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 19:03:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:03:33 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 19:03:33 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=27, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 19:03:42 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 19:03:42 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 19:03:42 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 19:03:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-10 19:03:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:03:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:03:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-10 19:03:43 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=8.155372, avg_loss=0.741397, seen=11, correct=5, accuracy=0.454545
2025-10-10 19:03:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:03:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:03:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:03:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2262MB allocated=2212MB
2025-10-10 19:03:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 19:03:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:03:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:03:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 19:03:45 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.173870, avg_loss=0.629347, seen=40, correct=26, accuracy=0.650000
2025-10-10 19:03:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:03:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:03:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:03:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2262MB allocated=2212MB
2025-10-10 19:03:48 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.650000
2025-10-10 19:03:57 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 19:03:57 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 19:03:57 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 19:03:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-10 19:03:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:03:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:03:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-10 19:03:58 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=7.359662, avg_loss=0.669060, seen=11, correct=6, accuracy=0.545455
2025-10-10 19:03:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:03:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:04:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:04:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2262MB allocated=2212MB
2025-10-10 19:04:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 19:04:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:04:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:04:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 19:04:02 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.630547, avg_loss=0.690764, seen=40, correct=21, accuracy=0.525000
2025-10-10 19:04:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:04:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:04:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:04:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2262MB allocated=2212MB
2025-10-10 19:04:04 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.650000, curr=0.525000
2025-10-10 19:04:11 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 19:04:11 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 19:04:11 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 19:04:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-10 19:04:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:04:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:04:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-10 19:04:12 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=7.582994, avg_loss=0.689363, seen=11, correct=6, accuracy=0.545455
2025-10-10 19:04:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:04:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:04:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:04:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2262MB allocated=2212MB
2025-10-10 19:04:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 19:04:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:04:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:04:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 19:04:15 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.550173, avg_loss=0.663754, seen=40, correct=25, accuracy=0.625000
2025-10-10 19:04:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:04:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:04:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:04:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2262MB allocated=2212MB
2025-10-10 19:04:17 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.650000, curr=0.625000
2025-10-10 19:04:25 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 19:04:25 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 19:04:25 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 19:04:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-10 19:04:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:04:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:04:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-10 19:04:26 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=7.801592, avg_loss=0.709236, seen=11, correct=6, accuracy=0.545455
2025-10-10 19:04:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:04:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:04:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:04:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2262MB allocated=2212MB
2025-10-10 19:04:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 19:04:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:04:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:04:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 19:04:30 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.958414, avg_loss=0.673960, seen=40, correct=25, accuracy=0.625000
2025-10-10 19:04:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:04:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:04:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:04:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2262MB allocated=2212MB
2025-10-10 19:04:32 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.650000, curr=0.625000
2025-10-10 19:04:41 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 19:04:41 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 19:04:41 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 19:04:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-10 19:04:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:04:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:04:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-10 19:04:41 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=7.817014, avg_loss=0.710638, seen=11, correct=6, accuracy=0.545455
2025-10-10 19:04:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:04:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:04:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:04:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2262MB allocated=2212MB
2025-10-10 19:04:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 19:04:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:04:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:04:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 19:04:45 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.411165, avg_loss=0.710279, seen=40, correct=25, accuracy=0.625000
2025-10-10 19:04:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:04:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:04:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:04:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2262MB allocated=2212MB
2025-10-10 19:04:47 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/25), best=0.650000, curr=0.625000
2025-10-10 19:04:56 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 19:04:56 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 19:04:56 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 19:04:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-10 19:04:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:04:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:04:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-10 19:04:56 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=8.028727, avg_loss=0.729884, seen=11, correct=5, accuracy=0.454545
2025-10-10 19:04:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:04:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:04:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:05:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2262MB allocated=2212MB
2025-10-10 19:05:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 19:05:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:05:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:05:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 19:05:01 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.782372, avg_loss=0.744559, seen=40, correct=24, accuracy=0.600000
2025-10-10 19:05:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:05:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:05:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:05:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2262MB allocated=2212MB
2025-10-10 19:05:03 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/25), best=0.650000, curr=0.600000
2025-10-10 19:05:12 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 19:05:12 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 19:05:12 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 19:05:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-10 19:05:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:05:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:05:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-10 19:05:13 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=8.095079, avg_loss=0.735916, seen=11, correct=5, accuracy=0.454545
2025-10-10 19:05:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:05:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:05:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:05:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2262MB allocated=2212MB
2025-10-10 19:05:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 19:05:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:05:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:05:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 19:05:16 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=31.296816, avg_loss=0.782420, seen=40, correct=23, accuracy=0.575000
2025-10-10 19:05:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:05:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:05:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:05:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2262MB allocated=2212MB
2025-10-10 19:05:19 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/25), best=0.650000, curr=0.575000
2025-10-10 19:05:28 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 19:05:28 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 19:05:28 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 19:05:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-10 19:05:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:05:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:05:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-10 19:05:28 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=8.632631, avg_loss=0.784785, seen=11, correct=6, accuracy=0.545455
2025-10-10 19:05:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:05:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:05:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:05:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2262MB allocated=2212MB
2025-10-10 19:05:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 19:05:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:05:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:05:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 19:05:31 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=33.200512, avg_loss=0.830013, seen=40, correct=24, accuracy=0.600000
2025-10-10 19:05:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:05:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:05:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:05:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2262MB allocated=2212MB
2025-10-10 19:05:33 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/25), best=0.650000, curr=0.600000
2025-10-10 19:05:41 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 19:05:41 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 19:05:41 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 19:05:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-10 19:05:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:05:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:05:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-10 19:05:42 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=8.750320, avg_loss=0.795484, seen=11, correct=5, accuracy=0.454545
2025-10-10 19:05:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:05:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:05:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:05:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2262MB allocated=2212MB
2025-10-10 19:05:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 19:05:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:05:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:05:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 19:05:45 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=33.391052, avg_loss=0.834776, seen=40, correct=24, accuracy=0.600000
2025-10-10 19:05:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:05:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:05:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:05:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2262MB allocated=2212MB
2025-10-10 19:05:47 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/25), best=0.650000, curr=0.600000
2025-10-10 19:05:56 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 19:05:56 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 19:05:56 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 19:05:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-10 19:05:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:05:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:05:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-10 19:05:56 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=8.609312, avg_loss=0.782665, seen=11, correct=7, accuracy=0.636364
2025-10-10 19:05:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:05:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:05:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:05:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2262MB allocated=2212MB
2025-10-10 19:05:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 19:05:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:05:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:05:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 19:05:59 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=35.837784, avg_loss=0.895945, seen=40, correct=24, accuracy=0.600000
2025-10-10 19:05:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:05:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:06:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:06:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2262MB allocated=2212MB
2025-10-10 19:06:02 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=9/25), best=0.650000, curr=0.600000
2025-10-10 19:06:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 19:06:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 19:06:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:06:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:06:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2262MB allocated=2212MB
2025-10-10 19:06:03 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #4', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 19:06:03 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #4', 'Round': 0, 'Results_raw': {}}
2025-10-10 19:06:03 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 19:06:04 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 19:06:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:06:05 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 19:06:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 19:06:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:06:06 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 19:06:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-10-10 19:06:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:06:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:06:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-10 19:06:09 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=146, loss_sum=95.702415, avg_loss=0.655496, seen=146, correct=94, accuracy=0.643836
2025-10-10 19:06:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:06:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:06:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:06:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-10-10 19:06:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 19:06:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:06:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:06:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 19:06:12 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=22.519281, avg_loss=0.562982, seen=40, correct=31, accuracy=0.775000
2025-10-10 19:06:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:06:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:06:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:06:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-10-10 19:06:14 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.775000
2025-10-10 19:06:14 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 19:06:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=699, total=2793)
2025-10-10 19:06:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:06:14 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 19:06:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:06:14 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 19:06:15 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=350, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 19:06:22 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 19:06:22 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 19:06:22 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 19:06:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-10-10 19:06:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:06:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:06:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-10 19:06:24 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=146, loss_sum=95.159203, avg_loss=0.651775, seen=146, correct=92, accuracy=0.630137
2025-10-10 19:06:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:06:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:06:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:06:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2264MB allocated=2221MB
2025-10-10 19:06:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 19:06:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:06:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:06:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 19:06:27 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.002920, avg_loss=0.575073, seen=40, correct=31, accuracy=0.775000
2025-10-10 19:06:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:06:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:06:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:06:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2264MB allocated=2221MB
2025-10-10 19:06:30 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.775000
2025-10-10 19:06:38 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 19:06:38 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 19:06:38 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 19:06:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-10-10 19:06:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:06:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:06:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-10 19:06:41 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=146, loss_sum=94.638283, avg_loss=0.648207, seen=146, correct=88, accuracy=0.602740
2025-10-10 19:06:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:06:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:06:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:06:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2264MB allocated=2221MB
2025-10-10 19:06:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 19:06:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:06:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:06:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 19:06:44 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.118477, avg_loss=0.577962, seen=40, correct=30, accuracy=0.750000
2025-10-10 19:06:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:06:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:06:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:06:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2264MB allocated=2221MB
2025-10-10 19:06:47 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.775000, curr=0.750000
2025-10-10 19:06:55 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 19:06:55 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 19:06:55 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 19:06:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-10-10 19:06:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:06:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:06:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-10 19:06:57 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=146, loss_sum=94.189041, avg_loss=0.645130, seen=146, correct=93, accuracy=0.636986
2025-10-10 19:06:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:06:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:06:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:06:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2264MB allocated=2221MB
2025-10-10 19:07:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 19:07:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:07:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:07:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 19:07:00 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=22.951916, avg_loss=0.573798, seen=40, correct=30, accuracy=0.750000
2025-10-10 19:07:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:07:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:07:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:07:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2264MB allocated=2221MB
2025-10-10 19:07:02 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.775000, curr=0.750000
2025-10-10 19:07:11 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 19:07:11 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 19:07:11 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 19:07:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-10-10 19:07:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:07:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:07:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-10 19:07:13 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=146, loss_sum=95.058693, avg_loss=0.651087, seen=146, correct=90, accuracy=0.616438
2025-10-10 19:07:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:07:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:07:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:07:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2264MB allocated=2221MB
2025-10-10 19:07:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 19:07:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:07:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:07:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 19:07:17 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=22.926395, avg_loss=0.573160, seen=40, correct=29, accuracy=0.725000
2025-10-10 19:07:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:07:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:07:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:07:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2264MB allocated=2221MB
2025-10-10 19:07:19 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.775000, curr=0.725000
2025-10-10 19:07:28 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 19:07:28 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 19:07:28 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 19:07:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-10-10 19:07:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:07:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:07:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-10 19:07:30 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=146, loss_sum=94.501717, avg_loss=0.647272, seen=146, correct=91, accuracy=0.623288
2025-10-10 19:07:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:07:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:07:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:07:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2264MB allocated=2221MB
2025-10-10 19:07:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 19:07:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:07:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:07:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 19:07:33 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.025541, avg_loss=0.575639, seen=40, correct=32, accuracy=0.800000
2025-10-10 19:07:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:07:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:07:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:07:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2264MB allocated=2221MB
2025-10-10 19:07:35 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.800000
2025-10-10 19:07:42 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 19:07:42 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 19:07:42 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 19:07:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-10-10 19:07:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:07:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:07:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-10 19:07:45 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=146, loss_sum=95.179634, avg_loss=0.651915, seen=146, correct=94, accuracy=0.643836
2025-10-10 19:07:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:07:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:07:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:07:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2264MB allocated=2221MB
2025-10-10 19:07:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 19:07:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:07:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:07:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 19:07:48 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.006207, avg_loss=0.575155, seen=40, correct=31, accuracy=0.775000
2025-10-10 19:07:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:07:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:07:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:07:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2264MB allocated=2221MB
2025-10-10 19:07:51 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.800000, curr=0.775000
2025-10-10 19:07:59 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 19:07:59 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 19:07:59 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 19:07:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-10-10 19:07:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:07:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:08:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-10 19:08:02 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=146, loss_sum=94.782951, avg_loss=0.649198, seen=146, correct=94, accuracy=0.643836
2025-10-10 19:08:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:08:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:08:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:08:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2264MB allocated=2221MB
2025-10-10 19:08:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 19:08:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:08:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:08:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 19:08:05 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=22.713142, avg_loss=0.567829, seen=40, correct=30, accuracy=0.750000
2025-10-10 19:08:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:08:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:08:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:08:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2264MB allocated=2221MB
2025-10-10 19:08:08 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.800000, curr=0.750000
2025-10-10 19:08:15 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 19:08:15 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 19:08:16 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 19:08:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-10-10 19:08:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:08:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:08:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-10 19:08:18 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=146, loss_sum=95.101059, avg_loss=0.651377, seen=146, correct=89, accuracy=0.609589
2025-10-10 19:08:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:08:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:08:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:08:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2264MB allocated=2221MB
2025-10-10 19:08:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 19:08:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:08:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:08:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 19:08:22 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=22.610992, avg_loss=0.565275, seen=40, correct=31, accuracy=0.775000
2025-10-10 19:08:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:08:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:08:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:08:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2264MB allocated=2221MB
2025-10-10 19:08:25 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.800000, curr=0.775000
2025-10-10 19:08:33 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 19:08:33 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 19:08:33 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 19:08:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-10-10 19:08:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:08:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:08:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-10 19:08:35 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=146, loss_sum=95.082153, avg_loss=0.651248, seen=146, correct=88, accuracy=0.602740
2025-10-10 19:08:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:08:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:08:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:08:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2264MB allocated=2221MB
2025-10-10 19:08:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 19:08:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:08:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:08:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 19:08:39 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=22.707371, avg_loss=0.567684, seen=40, correct=31, accuracy=0.775000
2025-10-10 19:08:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:08:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:08:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:08:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2264MB allocated=2221MB
2025-10-10 19:08:41 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/25), best=0.800000, curr=0.775000
2025-10-10 19:08:49 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 19:08:49 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 19:08:49 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 19:08:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-10-10 19:08:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:08:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:08:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-10 19:08:51 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=146, loss_sum=97.429123, avg_loss=0.667323, seen=146, correct=93, accuracy=0.636986
2025-10-10 19:08:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:08:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:08:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:08:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2264MB allocated=2221MB
2025-10-10 19:08:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 19:08:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:08:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:08:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 19:08:55 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.626476, avg_loss=0.590662, seen=40, correct=26, accuracy=0.650000
2025-10-10 19:08:55 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:08:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:08:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:08:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2264MB allocated=2221MB
2025-10-10 19:08:57 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/25), best=0.800000, curr=0.650000
2025-10-10 19:08:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 19:08:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 19:08:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:08:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:08:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2264MB allocated=2221MB
2025-10-10 19:08:59 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #1', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 19:08:59 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #1', 'Round': 0, 'Results_raw': {}}
2025-10-10 19:08:59 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 19:09:00 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 19:09:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:09:01 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 19:09:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 19:09:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:09:02 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 19:09:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-10-10 19:09:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:09:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:09:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-10-10 19:09:03 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=46, loss_sum=33.410976, avg_loss=0.726326, seen=46, correct=23, accuracy=0.500000
2025-10-10 19:09:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:09:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:09:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:09:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2242MB allocated=2212MB
2025-10-10 19:09:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 19:09:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:09:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:09:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 19:09:06 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.634106, avg_loss=0.665853, seen=40, correct=24, accuracy=0.600000
2025-10-10 19:09:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:09:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:09:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:09:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2242MB allocated=2212MB
2025-10-10 19:09:08 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-10 19:09:08 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 19:09:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=220, total=880)
2025-10-10 19:09:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:09:08 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 19:09:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:09:08 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 19:09:08 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=110, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 19:09:17 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 19:09:17 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 19:09:17 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 19:09:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-10-10 19:09:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:09:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:09:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-10-10 19:09:18 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=46, loss_sum=32.557911, avg_loss=0.707781, seen=46, correct=25, accuracy=0.543478
2025-10-10 19:09:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:09:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:09:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:09:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2274MB allocated=2229MB
2025-10-10 19:09:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 19:09:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:09:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:09:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 19:09:22 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.011246, avg_loss=0.675281, seen=40, correct=24, accuracy=0.600000
2025-10-10 19:09:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:09:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:09:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:09:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2274MB allocated=2229MB
2025-10-10 19:09:25 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-10 19:09:34 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 19:09:34 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 19:09:34 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 19:09:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-10-10 19:09:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:09:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:09:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-10-10 19:09:35 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=46, loss_sum=32.303337, avg_loss=0.702246, seen=46, correct=25, accuracy=0.543478
2025-10-10 19:09:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:09:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:09:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:09:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2274MB allocated=2229MB
2025-10-10 19:09:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 19:09:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:09:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:09:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 19:09:39 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.713892, avg_loss=0.692847, seen=40, correct=24, accuracy=0.600000
2025-10-10 19:09:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:09:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:09:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:09:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2274MB allocated=2229MB
2025-10-10 19:09:42 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-10 19:09:51 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 19:09:51 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 19:09:51 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 19:09:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-10-10 19:09:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:09:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:09:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-10-10 19:09:52 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=46, loss_sum=32.537945, avg_loss=0.707347, seen=46, correct=23, accuracy=0.500000
2025-10-10 19:09:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:09:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:09:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:09:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2274MB allocated=2229MB
2025-10-10 19:09:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 19:09:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:09:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:09:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 19:09:56 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.362217, avg_loss=0.659055, seen=40, correct=24, accuracy=0.600000
2025-10-10 19:09:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:09:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:09:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:09:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2274MB allocated=2229MB
2025-10-10 19:09:59 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-10 19:10:06 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 19:10:06 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 19:10:06 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 19:10:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-10-10 19:10:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:10:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:10:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-10-10 19:10:07 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=46, loss_sum=31.981361, avg_loss=0.695247, seen=46, correct=24, accuracy=0.521739
2025-10-10 19:10:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:10:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:10:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:10:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2274MB allocated=2229MB
2025-10-10 19:10:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 19:10:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:10:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:10:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 19:10:11 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.383331, avg_loss=0.684583, seen=40, correct=23, accuracy=0.575000
2025-10-10 19:10:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:10:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:10:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:10:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2274MB allocated=2229MB
2025-10-10 19:10:13 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.600000, curr=0.575000
2025-10-10 19:10:22 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 19:10:22 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 19:10:22 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 19:10:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-10-10 19:10:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:10:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:10:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-10-10 19:10:23 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=46, loss_sum=32.290710, avg_loss=0.701972, seen=46, correct=27, accuracy=0.586957
2025-10-10 19:10:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:10:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:10:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:10:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2274MB allocated=2229MB
2025-10-10 19:10:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 19:10:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:10:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:10:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 19:10:26 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.107111, avg_loss=0.727678, seen=40, correct=21, accuracy=0.525000
2025-10-10 19:10:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:10:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:10:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:10:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2274MB allocated=2229MB
2025-10-10 19:10:27 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.600000, curr=0.525000
2025-10-10 19:10:37 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 19:10:37 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 19:10:37 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 19:10:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-10-10 19:10:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:10:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:10:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-10-10 19:10:38 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=46, loss_sum=32.648758, avg_loss=0.709756, seen=46, correct=23, accuracy=0.500000
2025-10-10 19:10:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:10:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:10:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:10:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2274MB allocated=2229MB
2025-10-10 19:10:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 19:10:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:10:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:10:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 19:10:41 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.353811, avg_loss=0.658845, seen=40, correct=24, accuracy=0.600000
2025-10-10 19:10:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:10:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:10:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:10:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2274MB allocated=2229MB
2025-10-10 19:10:44 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-10 19:10:52 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 19:10:52 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 19:10:52 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 19:10:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-10-10 19:10:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:10:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:10:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-10-10 19:10:53 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=46, loss_sum=32.893379, avg_loss=0.715073, seen=46, correct=23, accuracy=0.500000
2025-10-10 19:10:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:10:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:10:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:10:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2274MB allocated=2229MB
2025-10-10 19:10:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 19:10:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:10:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:10:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 19:10:56 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.157986, avg_loss=0.653950, seen=40, correct=24, accuracy=0.600000
2025-10-10 19:10:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:10:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:10:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:10:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2274MB allocated=2229MB
2025-10-10 19:10:59 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-10 19:11:07 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 19:11:07 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 19:11:07 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 19:11:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-10-10 19:11:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:11:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:11:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-10-10 19:11:08 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=46, loss_sum=32.462601, avg_loss=0.705709, seen=46, correct=24, accuracy=0.521739
2025-10-10 19:11:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:11:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:11:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:11:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2274MB allocated=2229MB
2025-10-10 19:11:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 19:11:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:11:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:11:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 19:11:11 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.489916, avg_loss=0.662248, seen=40, correct=24, accuracy=0.600000
2025-10-10 19:11:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:11:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:11:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:11:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2274MB allocated=2229MB
2025-10-10 19:11:13 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-10 19:11:22 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 19:11:22 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 19:11:22 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 19:11:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-10-10 19:11:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:11:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:11:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-10-10 19:11:23 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=46, loss_sum=32.335171, avg_loss=0.702938, seen=46, correct=22, accuracy=0.478261
2025-10-10 19:11:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:11:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:11:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:11:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2274MB allocated=2229MB
2025-10-10 19:11:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 19:11:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:11:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:11:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 19:11:26 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.874081, avg_loss=0.671852, seen=40, correct=23, accuracy=0.575000
2025-10-10 19:11:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:11:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:11:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:11:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2274MB allocated=2229MB
2025-10-10 19:11:28 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.600000, curr=0.575000
2025-10-10 19:11:37 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 19:11:37 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 19:11:37 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 19:11:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-10-10 19:11:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:11:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:11:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-10-10 19:11:38 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=46, loss_sum=32.343018, avg_loss=0.703109, seen=46, correct=26, accuracy=0.565217
2025-10-10 19:11:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:11:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:11:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:11:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2274MB allocated=2229MB
2025-10-10 19:11:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 19:11:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:11:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:11:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 19:11:41 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.596609, avg_loss=0.739915, seen=40, correct=21, accuracy=0.525000
2025-10-10 19:11:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:11:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:11:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:11:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2274MB allocated=2229MB
2025-10-10 19:11:44 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.600000, curr=0.525000
2025-10-10 19:11:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 19:11:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 19:11:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:11:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:11:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2274MB allocated=2229MB
2025-10-10 19:11:45 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #48', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 19:11:45 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #48', 'Round': 0, 'Results_raw': {}}
2025-10-10 19:11:45 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 19:11:46 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 19:11:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:11:47 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 19:11:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 19:11:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:11:48 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 19:11:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-10-10 19:11:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:11:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:11:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-10-10 19:11:50 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=100, loss_sum=65.836281, avg_loss=0.658363, seen=100, correct=62, accuracy=0.620000
2025-10-10 19:11:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:11:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:11:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:11:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2262MB allocated=2221MB
2025-10-10 19:11:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 19:11:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:11:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:11:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 19:11:54 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.894367, avg_loss=0.722359, seen=40, correct=22, accuracy=0.550000
2025-10-10 19:11:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:11:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:11:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:11:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2262MB allocated=2221MB
2025-10-10 19:11:56 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.550000
2025-10-10 19:11:56 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 19:11:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=476, total=1901)
2025-10-10 19:11:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:11:57 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 19:11:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:11:57 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 19:11:57 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=238, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 19:12:06 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 19:12:06 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 19:12:06 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 19:12:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-10-10 19:12:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:12:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:12:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-10-10 19:12:07 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=100, loss_sum=75.583374, avg_loss=0.755834, seen=100, correct=56, accuracy=0.560000
2025-10-10 19:12:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:12:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:12:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:12:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2290MB allocated=2238MB
2025-10-10 19:12:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 19:12:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:12:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:12:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 19:12:11 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.275253, avg_loss=0.756881, seen=40, correct=24, accuracy=0.600000
2025-10-10 19:12:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:12:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:12:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:12:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2290MB allocated=2238MB
2025-10-10 19:12:12 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-10 19:12:20 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 19:12:20 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 19:12:20 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 19:12:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-10-10 19:12:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:12:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:12:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-10-10 19:12:22 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=100, loss_sum=65.830673, avg_loss=0.658307, seen=100, correct=60, accuracy=0.600000
2025-10-10 19:12:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:12:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:12:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:12:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2290MB allocated=2238MB
2025-10-10 19:12:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 19:12:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:12:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:12:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 19:12:26 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.177225, avg_loss=0.729431, seen=40, correct=23, accuracy=0.575000
2025-10-10 19:12:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:12:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:12:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:12:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2290MB allocated=2238MB
2025-10-10 19:12:29 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.600000, curr=0.575000
2025-10-10 19:12:38 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 19:12:38 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 19:12:38 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 19:12:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-10-10 19:12:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:12:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:12:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-10-10 19:12:40 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=100, loss_sum=69.353401, avg_loss=0.693534, seen=100, correct=59, accuracy=0.590000
2025-10-10 19:12:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:12:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:12:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:12:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2290MB allocated=2238MB
2025-10-10 19:12:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 19:12:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:12:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:12:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 19:12:43 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.683001, avg_loss=0.742075, seen=40, correct=23, accuracy=0.575000
2025-10-10 19:12:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:12:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:12:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:12:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2290MB allocated=2238MB
2025-10-10 19:12:46 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.600000, curr=0.575000
2025-10-10 19:12:54 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 19:12:54 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 19:12:54 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 19:12:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-10-10 19:12:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:12:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:12:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-10-10 19:12:56 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=100, loss_sum=63.121918, avg_loss=0.631219, seen=100, correct=62, accuracy=0.620000
2025-10-10 19:12:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:12:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:12:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:12:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2290MB allocated=2238MB
2025-10-10 19:12:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 19:12:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:12:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:13:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 19:13:00 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.246727, avg_loss=0.731168, seen=40, correct=21, accuracy=0.525000
2025-10-10 19:13:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:13:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:13:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:13:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2290MB allocated=2238MB
2025-10-10 19:13:03 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.600000, curr=0.525000
2025-10-10 19:13:11 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 19:13:11 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 19:13:11 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 19:13:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-10-10 19:13:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:13:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:13:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-10-10 19:13:13 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=100, loss_sum=61.833527, avg_loss=0.618335, seen=100, correct=66, accuracy=0.660000
2025-10-10 19:13:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:13:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:13:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:13:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2290MB allocated=2238MB
2025-10-10 19:13:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 19:13:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:13:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:13:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 19:13:17 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.764315, avg_loss=0.744108, seen=40, correct=21, accuracy=0.525000
2025-10-10 19:13:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:13:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:13:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:13:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2290MB allocated=2238MB
2025-10-10 19:13:19 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/25), best=0.600000, curr=0.525000
2025-10-10 19:13:28 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 19:13:28 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 19:13:28 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 19:13:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-10-10 19:13:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:13:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:13:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-10-10 19:13:30 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=100, loss_sum=65.526680, avg_loss=0.655267, seen=100, correct=64, accuracy=0.640000
2025-10-10 19:13:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:13:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:13:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:13:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2290MB allocated=2238MB
2025-10-10 19:13:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 19:13:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:13:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:13:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 19:13:34 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.816448, avg_loss=0.720411, seen=40, correct=22, accuracy=0.550000
2025-10-10 19:13:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:13:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:13:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:13:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2290MB allocated=2238MB
2025-10-10 19:13:36 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/25), best=0.600000, curr=0.550000
2025-10-10 19:13:45 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 19:13:45 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 19:13:45 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 19:13:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-10-10 19:13:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:13:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:13:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-10-10 19:13:47 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=100, loss_sum=68.209396, avg_loss=0.682094, seen=100, correct=57, accuracy=0.570000
2025-10-10 19:13:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:13:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:13:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:13:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2290MB allocated=2238MB
2025-10-10 19:13:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 19:13:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:13:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:13:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 19:13:51 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.717882, avg_loss=0.717947, seen=40, correct=25, accuracy=0.625000
2025-10-10 19:13:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:13:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:13:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:13:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2290MB allocated=2238MB
2025-10-10 19:13:53 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-10 19:14:00 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 19:14:00 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 19:14:00 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 19:14:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-10-10 19:14:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:14:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:14:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-10-10 19:14:02 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=100, loss_sum=69.485062, avg_loss=0.694851, seen=100, correct=57, accuracy=0.570000
2025-10-10 19:14:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:14:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:14:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:14:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2290MB allocated=2238MB
2025-10-10 19:14:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 19:14:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:14:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:14:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 19:14:07 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.101292, avg_loss=0.727532, seen=40, correct=24, accuracy=0.600000
2025-10-10 19:14:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:14:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:14:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:14:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2290MB allocated=2238MB
2025-10-10 19:14:09 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.625000, curr=0.600000
2025-10-10 19:14:19 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 19:14:19 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 19:14:19 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 19:14:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-10-10 19:14:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:14:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:14:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-10-10 19:14:21 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=100, loss_sum=63.318344, avg_loss=0.633183, seen=100, correct=63, accuracy=0.630000
2025-10-10 19:14:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:14:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:14:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:14:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2290MB allocated=2238MB
2025-10-10 19:14:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 19:14:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:14:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:14:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 19:14:26 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.785234, avg_loss=0.719631, seen=40, correct=25, accuracy=0.625000
2025-10-10 19:14:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:14:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:14:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:14:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2290MB allocated=2238MB
2025-10-10 19:14:28 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-10 19:14:36 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 19:14:36 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 19:14:36 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 19:14:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-10-10 19:14:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:14:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:14:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-10-10 19:14:37 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=100, loss_sum=60.934914, avg_loss=0.609349, seen=100, correct=66, accuracy=0.660000
2025-10-10 19:14:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:14:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:14:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:14:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2290MB allocated=2238MB
2025-10-10 19:14:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 19:14:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:14:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 19:14:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 19:14:41 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.469002, avg_loss=0.736725, seen=40, correct=21, accuracy=0.525000
2025-10-10 19:14:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 19:14:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:14:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:14:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2290MB allocated=2238MB
2025-10-10 19:14:43 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.625000, curr=0.525000
2025-10-10 19:14:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 19:14:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 19:14:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 19:14:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 19:14:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2290MB allocated=2238MB
2025-10-10 19:14:45 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #45', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 19:14:45 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #45', 'Round': 0, 'Results_raw': {}}
2025-10-10 19:14:46 (federatedscope.core.workers.server:493) INFO: Server: Training is finished! (skip final evaluation)
2025-10-10 19:14:46 (federatedscope.core.monitors.monitor:268) INFO: In worker #0, the system-related metrics are: {'id': 0, 'fl_end_time_minutes': 155.29471075, 'total_model_size': 0, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 8752, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 19:14:46 (federatedscope.core.workers.client:842) INFO: ================= client 1 received finish message =================
2025-10-10 19:14:46 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 19:14:46 (federatedscope.core.monitors.monitor:268) INFO: In worker #1, the system-related metrics are: {'id': 1, 'fl_end_time_minutes': 155.29654046666667, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 711096, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 19:14:46 (federatedscope.core.workers.client:842) INFO: ================= client 2 received finish message =================
2025-10-10 19:14:46 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 19:14:46 (federatedscope.core.monitors.monitor:268) INFO: In worker #2, the system-related metrics are: {'id': 2, 'fl_end_time_minutes': 155.23020300000002, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 711088, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 19:14:46 (federatedscope.core.workers.client:842) INFO: ================= client 3 received finish message =================
2025-10-10 19:14:46 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 19:14:46 (federatedscope.core.monitors.monitor:268) INFO: In worker #3, the system-related metrics are: {'id': 3, 'fl_end_time_minutes': 155.18967515, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 711096, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 19:14:46 (federatedscope.core.workers.client:842) INFO: ================= client 4 received finish message =================
2025-10-10 19:14:46 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 19:14:46 (federatedscope.core.monitors.monitor:268) INFO: In worker #4, the system-related metrics are: {'id': 4, 'fl_end_time_minutes': 155.14864203333335, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 711096, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 19:14:46 (federatedscope.core.workers.client:842) INFO: ================= client 5 received finish message =================
2025-10-10 19:14:46 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 19:14:46 (federatedscope.core.monitors.monitor:268) INFO: In worker #5, the system-related metrics are: {'id': 5, 'fl_end_time_minutes': 155.10656168333335, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 711096, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 19:14:46 (federatedscope.core.workers.client:842) INFO: ================= client 6 received finish message =================
2025-10-10 19:14:46 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 19:14:46 (federatedscope.core.monitors.monitor:268) INFO: In worker #6, the system-related metrics are: {'id': 6, 'fl_end_time_minutes': 155.06095241666665, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 711096, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 19:14:46 (federatedscope.core.workers.client:842) INFO: ================= client 7 received finish message =================
2025-10-10 19:14:46 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 19:14:46 (federatedscope.core.monitors.monitor:268) INFO: In worker #7, the system-related metrics are: {'id': 7, 'fl_end_time_minutes': 155.01724296666669, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 711096, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 19:14:46 (federatedscope.core.workers.client:842) INFO: ================= client 8 received finish message =================
2025-10-10 19:14:47 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 19:14:47 (federatedscope.core.monitors.monitor:268) INFO: In worker #8, the system-related metrics are: {'id': 8, 'fl_end_time_minutes': 154.97404493333335, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 711088, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 19:14:47 (federatedscope.core.workers.client:842) INFO: ================= client 9 received finish message =================
2025-10-10 19:14:47 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 19:14:47 (federatedscope.core.monitors.monitor:268) INFO: In worker #9, the system-related metrics are: {'id': 9, 'fl_end_time_minutes': 154.93057508333334, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 711096, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 19:14:47 (federatedscope.core.workers.client:842) INFO: ================= client 10 received finish message =================
2025-10-10 19:14:47 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 19:14:47 (federatedscope.core.monitors.monitor:268) INFO: In worker #10, the system-related metrics are: {'id': 10, 'fl_end_time_minutes': 154.88696735, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 711096, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 19:14:47 (federatedscope.core.workers.client:842) INFO: ================= client 11 received finish message =================
2025-10-10 19:14:47 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 19:14:47 (federatedscope.core.monitors.monitor:268) INFO: In worker #11, the system-related metrics are: {'id': 11, 'fl_end_time_minutes': 154.8442461, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 711088, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 19:14:47 (federatedscope.core.workers.client:842) INFO: ================= client 12 received finish message =================
2025-10-10 19:14:47 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 19:14:47 (federatedscope.core.monitors.monitor:268) INFO: In worker #12, the system-related metrics are: {'id': 12, 'fl_end_time_minutes': 154.80126803333332, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 711096, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 19:14:47 (federatedscope.core.workers.client:842) INFO: ================= client 13 received finish message =================
2025-10-10 19:14:47 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 19:14:47 (federatedscope.core.monitors.monitor:268) INFO: In worker #13, the system-related metrics are: {'id': 13, 'fl_end_time_minutes': 154.75837063333333, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 711096, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 19:14:47 (federatedscope.core.workers.client:842) INFO: ================= client 14 received finish message =================
2025-10-10 19:14:47 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 19:14:47 (federatedscope.core.monitors.monitor:268) INFO: In worker #14, the system-related metrics are: {'id': 14, 'fl_end_time_minutes': 154.69709446666667, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 711096, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 19:14:47 (federatedscope.core.workers.client:842) INFO: ================= client 15 received finish message =================
2025-10-10 19:14:47 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 19:14:47 (federatedscope.core.monitors.monitor:268) INFO: In worker #15, the system-related metrics are: {'id': 15, 'fl_end_time_minutes': 154.65317696666668, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 711096, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 19:14:47 (federatedscope.core.workers.client:842) INFO: ================= client 16 received finish message =================
2025-10-10 19:14:47 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 19:14:47 (federatedscope.core.monitors.monitor:268) INFO: In worker #16, the system-related metrics are: {'id': 16, 'fl_end_time_minutes': 154.60668423333334, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 711096, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 19:14:47 (federatedscope.core.workers.client:842) INFO: ================= client 17 received finish message =================
2025-10-10 19:14:47 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 19:14:47 (federatedscope.core.monitors.monitor:268) INFO: In worker #17, the system-related metrics are: {'id': 17, 'fl_end_time_minutes': 154.56341778333334, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 711096, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 19:14:47 (federatedscope.core.workers.client:842) INFO: ================= client 18 received finish message =================
2025-10-10 19:14:47 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 19:14:47 (federatedscope.core.monitors.monitor:268) INFO: In worker #18, the system-related metrics are: {'id': 18, 'fl_end_time_minutes': 154.52021053333334, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 711096, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 19:14:47 (federatedscope.core.workers.client:842) INFO: ================= client 19 received finish message =================
2025-10-10 19:14:47 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 19:14:47 (federatedscope.core.monitors.monitor:268) INFO: In worker #19, the system-related metrics are: {'id': 19, 'fl_end_time_minutes': 154.47670038333334, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 711096, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 19:14:47 (federatedscope.core.workers.client:842) INFO: ================= client 20 received finish message =================
2025-10-10 19:14:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 19:14:48 (federatedscope.core.monitors.monitor:268) INFO: In worker #20, the system-related metrics are: {'id': 20, 'fl_end_time_minutes': 154.4337461, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 711096, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 19:14:48 (federatedscope.core.workers.client:842) INFO: ================= client 21 received finish message =================
2025-10-10 19:14:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 19:14:48 (federatedscope.core.monitors.monitor:268) INFO: In worker #21, the system-related metrics are: {'id': 21, 'fl_end_time_minutes': 154.39074530000002, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 711096, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 19:14:48 (federatedscope.core.workers.client:842) INFO: ================= client 22 received finish message =================
2025-10-10 19:14:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 19:14:48 (federatedscope.core.monitors.monitor:268) INFO: In worker #22, the system-related metrics are: {'id': 22, 'fl_end_time_minutes': 154.34815138333335, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 711096, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 19:14:48 (federatedscope.core.workers.client:842) INFO: ================= client 23 received finish message =================
2025-10-10 19:14:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 19:14:48 (federatedscope.core.monitors.monitor:268) INFO: In worker #23, the system-related metrics are: {'id': 23, 'fl_end_time_minutes': 154.30567966666666, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 711096, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 19:14:48 (federatedscope.core.workers.client:842) INFO: ================= client 24 received finish message =================
2025-10-10 19:14:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 19:14:48 (federatedscope.core.monitors.monitor:268) INFO: In worker #24, the system-related metrics are: {'id': 24, 'fl_end_time_minutes': 154.26239771666667, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 711096, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 19:14:48 (federatedscope.core.workers.client:842) INFO: ================= client 25 received finish message =================
2025-10-10 19:14:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 19:14:48 (federatedscope.core.monitors.monitor:268) INFO: In worker #25, the system-related metrics are: {'id': 25, 'fl_end_time_minutes': 154.21468948333333, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 711096, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 19:14:48 (federatedscope.core.workers.client:842) INFO: ================= client 26 received finish message =================
2025-10-10 19:14:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 19:14:48 (federatedscope.core.monitors.monitor:268) INFO: In worker #26, the system-related metrics are: {'id': 26, 'fl_end_time_minutes': 154.17109805, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 711096, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 19:14:48 (federatedscope.core.workers.client:842) INFO: ================= client 27 received finish message =================
2025-10-10 19:14:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 19:14:48 (federatedscope.core.monitors.monitor:268) INFO: In worker #27, the system-related metrics are: {'id': 27, 'fl_end_time_minutes': 154.1108758, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 711096, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 19:14:48 (federatedscope.core.workers.client:842) INFO: ================= client 28 received finish message =================
2025-10-10 19:14:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 19:14:48 (federatedscope.core.monitors.monitor:268) INFO: In worker #28, the system-related metrics are: {'id': 28, 'fl_end_time_minutes': 154.06725728333333, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 711096, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 19:14:48 (federatedscope.core.workers.client:842) INFO: ================= client 29 received finish message =================
2025-10-10 19:14:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 19:14:48 (federatedscope.core.monitors.monitor:268) INFO: In worker #29, the system-related metrics are: {'id': 29, 'fl_end_time_minutes': 154.0242937, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 711096, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 19:14:48 (federatedscope.core.workers.client:842) INFO: ================= client 30 received finish message =================
2025-10-10 19:14:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 19:14:48 (federatedscope.core.monitors.monitor:268) INFO: In worker #30, the system-related metrics are: {'id': 30, 'fl_end_time_minutes': 153.9812922, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 711096, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 19:14:48 (federatedscope.core.workers.client:842) INFO: ================= client 31 received finish message =================
2025-10-10 19:14:49 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 19:14:49 (federatedscope.core.monitors.monitor:268) INFO: In worker #31, the system-related metrics are: {'id': 31, 'fl_end_time_minutes': 153.93815848333332, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 711096, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 19:14:49 (federatedscope.core.workers.client:842) INFO: ================= client 32 received finish message =================
2025-10-10 19:14:49 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 19:14:49 (federatedscope.core.monitors.monitor:268) INFO: In worker #32, the system-related metrics are: {'id': 32, 'fl_end_time_minutes': 153.89492045, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 711096, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 19:14:49 (federatedscope.core.workers.client:842) INFO: ================= client 33 received finish message =================
2025-10-10 19:14:49 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 19:14:49 (federatedscope.core.monitors.monitor:268) INFO: In worker #33, the system-related metrics are: {'id': 33, 'fl_end_time_minutes': 153.85142151666665, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 711096, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 19:14:49 (federatedscope.core.workers.client:842) INFO: ================= client 34 received finish message =================
2025-10-10 19:14:49 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 19:14:49 (federatedscope.core.monitors.monitor:268) INFO: In worker #34, the system-related metrics are: {'id': 34, 'fl_end_time_minutes': 153.80643653333334, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 711096, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 19:14:49 (federatedscope.core.workers.client:842) INFO: ================= client 35 received finish message =================
2025-10-10 19:14:49 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 19:14:49 (federatedscope.core.monitors.monitor:268) INFO: In worker #35, the system-related metrics are: {'id': 35, 'fl_end_time_minutes': 153.75895836666666, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 711096, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 19:14:49 (federatedscope.core.workers.client:842) INFO: ================= client 36 received finish message =================
2025-10-10 19:14:49 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 19:14:49 (federatedscope.core.monitors.monitor:268) INFO: In worker #36, the system-related metrics are: {'id': 36, 'fl_end_time_minutes': 153.71513915, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 711096, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 19:14:49 (federatedscope.core.workers.client:842) INFO: ================= client 37 received finish message =================
2025-10-10 19:14:49 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 19:14:49 (federatedscope.core.monitors.monitor:268) INFO: In worker #37, the system-related metrics are: {'id': 37, 'fl_end_time_minutes': 153.67132083333334, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 711096, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 19:14:49 (federatedscope.core.workers.client:842) INFO: ================= client 38 received finish message =================
2025-10-10 19:14:49 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 19:14:49 (federatedscope.core.monitors.monitor:268) INFO: In worker #38, the system-related metrics are: {'id': 38, 'fl_end_time_minutes': 153.62742661666667, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 711096, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 19:14:49 (federatedscope.core.workers.client:842) INFO: ================= client 39 received finish message =================
2025-10-10 19:14:49 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 19:14:49 (federatedscope.core.monitors.monitor:268) INFO: In worker #39, the system-related metrics are: {'id': 39, 'fl_end_time_minutes': 153.58365236666666, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 711096, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 19:14:49 (federatedscope.core.workers.client:842) INFO: ================= client 40 received finish message =================
2025-10-10 19:14:49 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 19:14:49 (federatedscope.core.monitors.monitor:268) INFO: In worker #40, the system-related metrics are: {'id': 40, 'fl_end_time_minutes': 153.52219920000002, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 711096, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 19:14:49 (federatedscope.core.workers.client:842) INFO: ================= client 41 received finish message =================
2025-10-10 19:14:49 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 19:14:49 (federatedscope.core.monitors.monitor:268) INFO: In worker #41, the system-related metrics are: {'id': 41, 'fl_end_time_minutes': 153.47871636666665, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 711096, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 19:14:49 (federatedscope.core.workers.client:842) INFO: ================= client 42 received finish message =================
2025-10-10 19:14:49 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 19:14:49 (federatedscope.core.monitors.monitor:268) INFO: In worker #42, the system-related metrics are: {'id': 42, 'fl_end_time_minutes': 153.43641001666668, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 711096, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 19:14:49 (federatedscope.core.workers.client:842) INFO: ================= client 43 received finish message =================
2025-10-10 19:14:50 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 19:14:50 (federatedscope.core.monitors.monitor:268) INFO: In worker #43, the system-related metrics are: {'id': 43, 'fl_end_time_minutes': 153.39272628333336, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 711096, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 19:14:50 (federatedscope.core.workers.client:842) INFO: ================= client 44 received finish message =================
2025-10-10 19:14:50 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 19:14:50 (federatedscope.core.monitors.monitor:268) INFO: In worker #44, the system-related metrics are: {'id': 44, 'fl_end_time_minutes': 153.34673853333334, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 711096, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 19:14:50 (federatedscope.core.workers.client:842) INFO: ================= client 45 received finish message =================
2025-10-10 19:14:50 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 19:14:50 (federatedscope.core.monitors.monitor:268) INFO: In worker #45, the system-related metrics are: {'id': 45, 'fl_end_time_minutes': 153.30154215, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 711096, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 19:14:50 (federatedscope.core.workers.client:842) INFO: ================= client 46 received finish message =================
2025-10-10 19:14:50 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 19:14:50 (federatedscope.core.monitors.monitor:268) INFO: In worker #46, the system-related metrics are: {'id': 46, 'fl_end_time_minutes': 153.25851728333333, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 711096, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 19:14:50 (federatedscope.core.workers.client:842) INFO: ================= client 47 received finish message =================
2025-10-10 19:14:50 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 19:14:50 (federatedscope.core.monitors.monitor:268) INFO: In worker #47, the system-related metrics are: {'id': 47, 'fl_end_time_minutes': 153.21512828333334, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 711096, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 19:14:50 (federatedscope.core.workers.client:842) INFO: ================= client 48 received finish message =================
2025-10-10 19:14:50 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 19:14:50 (federatedscope.core.monitors.monitor:268) INFO: In worker #48, the system-related metrics are: {'id': 48, 'fl_end_time_minutes': 153.17136566666667, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 711096, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 19:14:50 (federatedscope.core.workers.client:842) INFO: ================= client 49 received finish message =================
2025-10-10 19:14:50 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 19:14:50 (federatedscope.core.monitors.monitor:268) INFO: In worker #49, the system-related metrics are: {'id': 49, 'fl_end_time_minutes': 153.12796616666668, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 711096, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 19:14:50 (federatedscope.core.workers.client:842) INFO: ================= client 50 received finish message =================
2025-10-10 19:14:50 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 19:14:50 (federatedscope.core.monitors.monitor:268) INFO: In worker #50, the system-related metrics are: {'id': 50, 'fl_end_time_minutes': 153.08473053333333, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 711096, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 19:14:50 (federatedscope.core.workers.client:842) INFO: ================= client 51 received finish message =================
2025-10-10 19:14:50 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 19:14:50 (federatedscope.core.monitors.monitor:268) INFO: In worker #51, the system-related metrics are: {'id': 51, 'fl_end_time_minutes': 153.04190425, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 711096, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 19:14:50 (federatedscope.core.workers.client:842) INFO: ================= client 52 received finish message =================
2025-10-10 19:14:50 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 19:14:50 (federatedscope.core.monitors.monitor:268) INFO: In worker #52, the system-related metrics are: {'id': 52, 'fl_end_time_minutes': 152.99924475, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 711096, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 19:14:50 (federatedscope.core.workers.client:842) INFO: ================= client 53 received finish message =================
2025-10-10 19:14:50 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-10-10 19:14:50 (federatedscope.core.monitors.monitor:268) INFO: In worker #53, the system-related metrics are: {'id': 53, 'fl_end_time_minutes': 152.95658575000002, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 711096, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 19:14:50 (federatedscope.core.monitors.monitor:359) INFO: After merging the system metrics from all works, we got avg: defaultdict(None, {'id': 'sys_avg', 'sys_avg/fl_end_time_minutes': 154.13979113333335, 'sys_avg/total_model_size': '486.88M', 'sys_avg/total_flops': '0.0', 'sys_avg/total_upload_bytes': '0.0', 'sys_avg/total_download_bytes': '681.73K', 'sys_avg/global_convergence_round': 0.0, 'sys_avg/local_convergence_round': 0.0, 'sys_avg/global_convergence_time_minutes': 0.0, 'sys_avg/local_convergence_time_minutes': 0.0})
2025-10-10 19:14:50 (federatedscope.core.monitors.monitor:360) INFO: After merging the system metrics from all works, we got std: defaultdict(None, {'id': 'sys_std', 'sys_std/fl_end_time_minutes': 0.7001778667367891, 'sys_std/total_model_size': '66.88M', 'sys_std/total_flops': '0.0', 'sys_std/total_upload_bytes': '0.0', 'sys_std/total_download_bytes': '92.47K', 'sys_std/global_convergence_round': 0.0, 'sys_std/local_convergence_round': 0.0, 'sys_std/global_convergence_time_minutes': 0.0, 'sys_std/local_convergence_time_minutes': 0.0})
