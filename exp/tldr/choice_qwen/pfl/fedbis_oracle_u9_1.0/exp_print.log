2025-10-10 12:10:50 (root:426) INFO: [logger] file handler -> exp/tldr/choice_qwen/pfl/fedbis_oracle_u9_1.0/exp_print.log
2025-10-10 12:10:50 (root:51) INFO: [main] outdir=exp/tldr/choice_qwen/pfl/fedbis_oracle_u9_1.0/
2025-10-10 12:11:14 (federatedscope.core.data.base_translator:234) INFO: Main process: Completion file found. Skipping generation.
2025-10-10 12:11:56 (federatedscope.core.data.base_translator:264) INFO: [Final Split Summary][loaded][server=0][rank=0/4] Train=92858, Val=33082, Test=50715, Total=176655
2025-10-10 12:11:56 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=1][rank=0/4] Train=2793, Val=146, Test=40, Total=2979
2025-10-10 12:11:56 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=2][rank=0/4] Train=214, Val=11, Test=40, Total=265
2025-10-10 12:11:56 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=3][rank=0/4] Train=691, Val=36, Test=40, Total=767
2025-10-10 12:11:56 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=4][rank=0/4] Train=213, Val=11, Test=40, Total=264
2025-10-10 12:11:56 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=5][rank=0/4] Train=285, Val=14, Test=40, Total=339
2025-10-10 12:11:56 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=6][rank=0/4] Train=2547, Val=134, Test=40, Total=2721
2025-10-10 12:11:56 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=7][rank=0/4] Train=1088, Val=57, Test=40, Total=1185
2025-10-10 12:11:56 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=8][rank=0/4] Train=1316, Val=69, Test=40, Total=1425
2025-10-10 12:11:56 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=9][rank=0/4] Train=3572, Val=188, Test=40, Total=3800
2025-10-10 12:11:56 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=10][rank=0/4] Train=1209, Val=63, Test=40, Total=1312
2025-10-10 12:11:56 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=11][rank=0/4] Train=621, Val=32, Test=40, Total=693
2025-10-10 12:11:56 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=12][rank=0/4] Train=2605, Val=137, Test=40, Total=2782
2025-10-10 12:11:56 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=13][rank=0/4] Train=1372, Val=72, Test=40, Total=1484
2025-10-10 12:11:56 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=14][rank=0/4] Train=3055, Val=160, Test=40, Total=3255
2025-10-10 12:11:56 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=15][rank=0/4] Train=14550, Val=200, Test=40, Total=14790
2025-10-10 12:11:56 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=16][rank=0/4] Train=2589, Val=136, Test=40, Total=2765
2025-10-10 12:11:56 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=17][rank=0/4] Train=5883, Val=200, Test=40, Total=6123
2025-10-10 12:11:56 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=18][rank=0/4] Train=2576, Val=135, Test=40, Total=2751
2025-10-10 12:11:56 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=19][rank=0/4] Train=2102, Val=110, Test=40, Total=2252
2025-10-10 12:11:56 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=20][rank=0/4] Train=2399, Val=126, Test=40, Total=2565
2025-10-10 12:11:56 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=21][rank=0/4] Train=2915, Val=153, Test=40, Total=3108
2025-10-10 12:11:56 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=22][rank=0/4] Train=224, Val=11, Test=40, Total=275
2025-10-10 12:11:56 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=23][rank=0/4] Train=583, Val=30, Test=40, Total=653
2025-10-10 12:11:56 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=24][rank=0/4] Train=4944, Val=200, Test=40, Total=5184
2025-10-10 12:11:56 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=25][rank=0/4] Train=4647, Val=200, Test=40, Total=4887
2025-10-10 12:11:56 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=26][rank=0/4] Train=3063, Val=161, Test=40, Total=3264
2025-10-10 12:11:56 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=27][rank=0/4] Train=2342, Val=123, Test=40, Total=2505
2025-10-10 12:11:56 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=28][rank=0/4] Train=1434, Val=75, Test=40, Total=1549
2025-10-10 12:11:56 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=29][rank=0/4] Train=6191, Val=200, Test=40, Total=6431
2025-10-10 12:11:56 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=30][rank=0/4] Train=3247, Val=170, Test=40, Total=3457
2025-10-10 12:11:56 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=31][rank=0/4] Train=3679, Val=193, Test=40, Total=3912
2025-10-10 12:11:56 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=32][rank=0/4] Train=2144, Val=112, Test=40, Total=2296
2025-10-10 12:11:56 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=33][rank=0/4] Train=1409, Val=74, Test=40, Total=1523
2025-10-10 12:11:56 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=34][rank=0/4] Train=4486, Val=200, Test=40, Total=4726
2025-10-10 12:11:56 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=35][rank=0/4] Train=4736, Val=200, Test=40, Total=4976
2025-10-10 12:11:56 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=36][rank=0/4] Train=1030, Val=54, Test=40, Total=1124
2025-10-10 12:11:56 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=37][rank=0/4] Train=4273, Val=200, Test=40, Total=4513
2025-10-10 12:11:56 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=38][rank=0/4] Train=6171, Val=200, Test=40, Total=6411
2025-10-10 12:11:56 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=39][rank=0/4] Train=1594, Val=83, Test=40, Total=1717
2025-10-10 12:11:56 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=40][rank=0/4] Train=4005, Val=200, Test=40, Total=4245
2025-10-10 12:11:56 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=41][rank=0/4] Train=2275, Val=119, Test=40, Total=2434
2025-10-10 12:11:56 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=42][rank=0/4] Train=5772, Val=200, Test=40, Total=6012
2025-10-10 12:11:56 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=43][rank=0/4] Train=1694, Val=89, Test=40, Total=1823
2025-10-10 12:11:56 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=44][rank=0/4] Train=7916, Val=200, Test=40, Total=8156
2025-10-10 12:11:56 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=45][rank=0/4] Train=1901, Val=100, Test=40, Total=2041
2025-10-10 12:11:56 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=46][rank=0/4] Train=2100, Val=110, Test=40, Total=2250
2025-10-10 12:11:56 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=47][rank=0/4] Train=2812, Val=147, Test=40, Total=2999
2025-10-10 12:11:56 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=48][rank=0/4] Train=880, Val=46, Test=40, Total=966
2025-10-10 12:11:56 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=49][rank=0/4] Train=2521, Val=132, Test=40, Total=2693
2025-10-10 12:11:56 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=50][rank=0/4] Train=2527, Val=133, Test=40, Total=2700
2025-10-10 12:11:56 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=51][rank=0/4] Train=1580, Val=83, Test=40, Total=1703
2025-10-10 12:11:56 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=52][rank=0/4] Train=3589, Val=188, Test=40, Total=3817
2025-10-10 12:11:56 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=53][rank=0/4] Train=6791, Val=200, Test=40, Total=7031
2025-10-10 12:11:58 (federatedscope.core.auxiliaries.utils:175) INFO: The device information file is not provided
2025-10-10 12:11:58 (federatedscope.core.auxiliaries.model_builder:139) WARNING: The input shape is None. Please specify the `data.input_shape`(a tuple) or give the representative data to `get_model` if necessary
2025-10-10 12:12:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-build][rank=0] tok_len=151643 | base=Qwen2ForCausalLM | in_emb=(Embedding) num=151646 ptr=140599838957632 | out_emb=(Linear) num=151646 ptr=140599838957632 | lora_ptr=None
2025-10-10 12:12:14 (federatedscope.llm.model.model_builder:188) INFO: [Warmup-Init] loaded from checkpoints_1.0_oracle/final_tldr_choice_qwen_fedbis_oracle_u9_round_241.ckpt (round=241) | missing=291 unexpected=0
2025-10-10 12:12:14 (federatedscope.core.fed_runner:211) INFO: Server has been set up ... 
2025-10-10 12:12:16 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 12:12:19 (federatedscope.core.fed_runner:275) INFO: Client 1 has been set up ... 
2025-10-10 12:12:19 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 12:12:22 (federatedscope.core.fed_runner:275) INFO: Client 2 has been set up ... 
2025-10-10 12:12:22 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 12:12:24 (federatedscope.core.fed_runner:275) INFO: Client 3 has been set up ... 
2025-10-10 12:12:25 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 12:12:27 (federatedscope.core.fed_runner:275) INFO: Client 4 has been set up ... 
2025-10-10 12:12:27 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 12:12:30 (federatedscope.core.fed_runner:275) INFO: Client 5 has been set up ... 
2025-10-10 12:12:30 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 12:12:33 (federatedscope.core.fed_runner:275) INFO: Client 6 has been set up ... 
2025-10-10 12:12:33 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 12:12:35 (federatedscope.core.fed_runner:275) INFO: Client 7 has been set up ... 
2025-10-10 12:12:36 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 12:12:39 (federatedscope.core.fed_runner:275) INFO: Client 8 has been set up ... 
2025-10-10 12:12:39 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 12:12:42 (federatedscope.core.fed_runner:275) INFO: Client 9 has been set up ... 
2025-10-10 12:12:42 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 12:12:45 (federatedscope.core.fed_runner:275) INFO: Client 10 has been set up ... 
2025-10-10 12:12:45 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 12:12:47 (federatedscope.core.fed_runner:275) INFO: Client 11 has been set up ... 
2025-10-10 12:12:48 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 12:12:50 (federatedscope.core.fed_runner:275) INFO: Client 12 has been set up ... 
2025-10-10 12:12:51 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 12:12:53 (federatedscope.core.fed_runner:275) INFO: Client 13 has been set up ... 
2025-10-10 12:12:53 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 12:12:56 (federatedscope.core.fed_runner:275) INFO: Client 14 has been set up ... 
2025-10-10 12:12:56 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 12:12:58 (federatedscope.core.fed_runner:275) INFO: Client 15 has been set up ... 
2025-10-10 12:12:59 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 12:13:01 (federatedscope.core.fed_runner:275) INFO: Client 16 has been set up ... 
2025-10-10 12:13:01 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 12:13:05 (federatedscope.core.fed_runner:275) INFO: Client 17 has been set up ... 
2025-10-10 12:13:05 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 12:13:08 (federatedscope.core.fed_runner:275) INFO: Client 18 has been set up ... 
2025-10-10 12:13:08 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 12:13:10 (federatedscope.core.fed_runner:275) INFO: Client 19 has been set up ... 
2025-10-10 12:13:11 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 12:13:13 (federatedscope.core.fed_runner:275) INFO: Client 20 has been set up ... 
2025-10-10 12:13:14 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 12:13:16 (federatedscope.core.fed_runner:275) INFO: Client 21 has been set up ... 
2025-10-10 12:13:16 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 12:13:19 (federatedscope.core.fed_runner:275) INFO: Client 22 has been set up ... 
2025-10-10 12:13:19 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 12:13:22 (federatedscope.core.fed_runner:275) INFO: Client 23 has been set up ... 
2025-10-10 12:13:22 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 12:13:25 (federatedscope.core.fed_runner:275) INFO: Client 24 has been set up ... 
2025-10-10 12:13:25 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 12:13:29 (federatedscope.core.fed_runner:275) INFO: Client 25 has been set up ... 
2025-10-10 12:13:29 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 12:13:32 (federatedscope.core.fed_runner:275) INFO: Client 26 has been set up ... 
2025-10-10 12:13:32 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 12:13:35 (federatedscope.core.fed_runner:275) INFO: Client 27 has been set up ... 
2025-10-10 12:13:35 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 12:13:37 (federatedscope.core.fed_runner:275) INFO: Client 28 has been set up ... 
2025-10-10 12:13:38 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 12:13:40 (federatedscope.core.fed_runner:275) INFO: Client 29 has been set up ... 
2025-10-10 12:13:40 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 12:13:43 (federatedscope.core.fed_runner:275) INFO: Client 30 has been set up ... 
2025-10-10 12:13:43 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 12:13:46 (federatedscope.core.fed_runner:275) INFO: Client 31 has been set up ... 
2025-10-10 12:13:46 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 12:13:49 (federatedscope.core.fed_runner:275) INFO: Client 32 has been set up ... 
2025-10-10 12:13:49 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 12:13:52 (federatedscope.core.fed_runner:275) INFO: Client 33 has been set up ... 
2025-10-10 12:13:52 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 12:13:55 (federatedscope.core.fed_runner:275) INFO: Client 34 has been set up ... 
2025-10-10 12:13:55 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 12:13:58 (federatedscope.core.fed_runner:275) INFO: Client 35 has been set up ... 
2025-10-10 12:13:58 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 12:14:01 (federatedscope.core.fed_runner:275) INFO: Client 36 has been set up ... 
2025-10-10 12:14:01 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 12:14:03 (federatedscope.core.fed_runner:275) INFO: Client 37 has been set up ... 
2025-10-10 12:14:04 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 12:14:06 (federatedscope.core.fed_runner:275) INFO: Client 38 has been set up ... 
2025-10-10 12:14:06 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 12:14:09 (federatedscope.core.fed_runner:275) INFO: Client 39 has been set up ... 
2025-10-10 12:14:09 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 12:14:12 (federatedscope.core.fed_runner:275) INFO: Client 40 has been set up ... 
2025-10-10 12:14:12 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 12:14:14 (federatedscope.core.fed_runner:275) INFO: Client 41 has been set up ... 
2025-10-10 12:14:15 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 12:14:18 (federatedscope.core.fed_runner:275) INFO: Client 42 has been set up ... 
2025-10-10 12:14:18 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 12:14:21 (federatedscope.core.fed_runner:275) INFO: Client 43 has been set up ... 
2025-10-10 12:14:21 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 12:14:23 (federatedscope.core.fed_runner:275) INFO: Client 44 has been set up ... 
2025-10-10 12:14:24 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 12:14:26 (federatedscope.core.fed_runner:275) INFO: Client 45 has been set up ... 
2025-10-10 12:14:27 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 12:14:29 (federatedscope.core.fed_runner:275) INFO: Client 46 has been set up ... 
2025-10-10 12:14:29 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 12:14:32 (federatedscope.core.fed_runner:275) INFO: Client 47 has been set up ... 
2025-10-10 12:14:32 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 12:14:35 (federatedscope.core.fed_runner:275) INFO: Client 48 has been set up ... 
2025-10-10 12:14:35 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 12:14:37 (federatedscope.core.fed_runner:275) INFO: Client 49 has been set up ... 
2025-10-10 12:14:38 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 12:14:40 (federatedscope.core.fed_runner:275) INFO: Client 50 has been set up ... 
2025-10-10 12:14:40 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 12:14:44 (federatedscope.core.fed_runner:275) INFO: Client 51 has been set up ... 
2025-10-10 12:14:44 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 12:14:46 (federatedscope.core.fed_runner:275) INFO: Client 52 has been set up ... 
2025-10-10 12:14:47 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-10 12:14:49 (federatedscope.core.fed_runner:275) INFO: Client 53 has been set up ... 
2025-10-10 12:14:49 (federatedscope.core.trainers.trainer:569) INFO: Model meta-info: <class 'federatedscope.llm.model.adapter_builder.AdapterModel'>.
2025-10-10 12:14:49 (federatedscope.core.trainers.trainer:584) INFO: Num of original para names: 3360.
2025-10-10 12:14:49 (federatedscope.core.trainers.trainer:585) INFO: Num of original trainable para names: 3650.
2025-10-10 12:14:49 (federatedscope.core.trainers.trainer:587) INFO: Num of preserved para names in local update: 3360. 
Preserved para names in local update: {'base_model.model.model.layers.20.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.Adapter_7.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.Adapter_8.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.Adapter_7.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.Adapter_8.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.Adapter_0.weight'}.
2025-10-10 12:14:49 (federatedscope.core.trainers.trainer:591) INFO: Num of filtered para names in local update: 0. 
Filtered para names in local update: set().
2025-10-10 12:14:49 (federatedscope.core.trainers.trainer:599) INFO: After register default hooks,
	the hooks_in_train is:
	{
	  "on_fit_start": [
	    "_hook_on_fit_start_numerical_precision",
	    "_hook_on_data_parallel_init",
	    "_hook_on_fit_start_init",
	    "_hook_on_fit_start_calculate_model_size"
	  ],
	  "on_batch_start": [
	    "_hook_on_batch_start_init"
	  ],
	  "on_batch_forward": [
	    "_hook_on_batch_forward",
	    "_hook_on_batch_forward_regularizer",
	    "_hook_on_batch_forward_flop_count"
	  ],
	  "on_batch_backward": [
	    "_hook_on_batch_backward"
	  ],
	  "on_batch_end": [
	    "_hook_on_batch_end"
	  ],
	  "on_fit_end": [
	    "_hook_on_fit_end",
	    "_hook_on_fit_end_free_space"
	  ]
	};
	the hooks_in_eval is:
            t{
	  "on_fit_start": [
	    "_hook_on_fit_start_numerical_precision",
	    "_hook_on_data_parallel_init",
	    "_hook_on_fit_start_init"
	  ],
	  "on_batch_start": [
	    "_hook_on_batch_start_init"
	  ],
	  "on_batch_forward": [
	    "_hook_on_batch_forward"
	  ],
	  "on_batch_end": [
	    "_hook_on_batch_end"
	  ],
	  "on_fit_end": [
	    "_hook_on_fit_end",
	    "_hook_on_fit_end_free_space"
	  ]
	}
2025-10-10 12:14:49 (federatedscope.llm.llm_local.server:200) INFO: Waited all clients join, start now...
2025-10-10 12:14:50 (federatedscope.llm.llm_local.server:217) INFO: ----------- Starting training (Round #0) -------------
2025-10-10 12:15:01 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 12:15:02 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 12:15:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:15:05 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 12:15:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:15:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:15:09 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 12:15:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 12:15:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:15:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:15:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 12:15:13 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=142.722702, avg_loss=0.713614, seen=200, correct=95, accuracy=0.475000
2025-10-10 12:15:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:15:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:15:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:15:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1836MB allocated=1810MB
2025-10-10 12:15:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:15:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:15:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:15:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:15:17 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.921844, avg_loss=0.723046, seen=40, correct=18, accuracy=0.450000
2025-10-10 12:15:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:15:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:15:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:15:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1836MB allocated=1810MB
2025-10-10 12:15:19 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.450000
2025-10-10 12:15:19 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 12:15:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1979, total=7916)
2025-10-10 12:15:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:15:19 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 12:15:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:15:19 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 12:15:19 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=990, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 12:15:28 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 12:15:28 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 12:15:28 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 12:15:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 12:15:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:15:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:15:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 12:15:31 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=143.225189, avg_loss=0.716126, seen=200, correct=89, accuracy=0.445000
2025-10-10 12:15:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:15:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:15:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:15:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1902MB allocated=1835MB
2025-10-10 12:15:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:15:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:15:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:15:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:15:36 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.975170, avg_loss=0.699379, seen=40, correct=23, accuracy=0.575000
2025-10-10 12:15:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:15:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:15:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:15:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1902MB allocated=1835MB
2025-10-10 12:15:37 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-10 12:15:46 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 12:15:46 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 12:15:46 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 12:15:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 12:15:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:15:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:15:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 12:15:49 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=144.440170, avg_loss=0.722201, seen=200, correct=95, accuracy=0.475000
2025-10-10 12:15:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:15:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:15:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:15:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1902MB allocated=1835MB
2025-10-10 12:15:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:15:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:15:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:15:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:15:53 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.586330, avg_loss=0.739658, seen=40, correct=18, accuracy=0.450000
2025-10-10 12:15:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:15:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:15:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:15:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1902MB allocated=1835MB
2025-10-10 12:15:54 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.575000, curr=0.450000
2025-10-10 12:16:04 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 12:16:04 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 12:16:04 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 12:16:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 12:16:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:16:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:16:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 12:16:07 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=144.307343, avg_loss=0.721537, seen=200, correct=97, accuracy=0.485000
2025-10-10 12:16:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:16:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:16:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:16:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1902MB allocated=1835MB
2025-10-10 12:16:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:16:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:16:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:16:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:16:12 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.902979, avg_loss=0.722574, seen=40, correct=20, accuracy=0.500000
2025-10-10 12:16:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:16:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:16:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:16:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1902MB allocated=1835MB
2025-10-10 12:16:14 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.575000, curr=0.500000
2025-10-10 12:16:24 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 12:16:24 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 12:16:24 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 12:16:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 12:16:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:16:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:16:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 12:16:27 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=145.138336, avg_loss=0.725692, seen=200, correct=103, accuracy=0.515000
2025-10-10 12:16:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:16:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:16:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:16:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1902MB allocated=1835MB
2025-10-10 12:16:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:16:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:16:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:16:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:16:31 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.724064, avg_loss=0.668102, seen=40, correct=24, accuracy=0.600000
2025-10-10 12:16:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:16:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:16:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:16:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1902MB allocated=1835MB
2025-10-10 12:16:33 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-10 12:16:42 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 12:16:42 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 12:16:42 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 12:16:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 12:16:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:16:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:16:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 12:16:45 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=144.942581, avg_loss=0.724713, seen=200, correct=106, accuracy=0.530000
2025-10-10 12:16:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:16:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:16:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:16:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1902MB allocated=1835MB
2025-10-10 12:16:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:16:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:16:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:16:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:16:50 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.340599, avg_loss=0.658515, seen=40, correct=25, accuracy=0.625000
2025-10-10 12:16:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:16:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:16:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:16:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1902MB allocated=1835MB
2025-10-10 12:16:51 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-10 12:17:02 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 12:17:02 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 12:17:02 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 12:17:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 12:17:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:17:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:17:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 12:17:05 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=141.904297, avg_loss=0.709521, seen=200, correct=98, accuracy=0.490000
2025-10-10 12:17:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:17:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:17:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:17:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1902MB allocated=1835MB
2025-10-10 12:17:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:17:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:17:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:17:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:17:09 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.011997, avg_loss=0.675300, seen=40, correct=23, accuracy=0.575000
2025-10-10 12:17:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:17:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:17:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:17:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1902MB allocated=1835MB
2025-10-10 12:17:11 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.625000, curr=0.575000
2025-10-10 12:17:19 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 12:17:19 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 12:17:19 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 12:17:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 12:17:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:17:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:17:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 12:17:22 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=141.329544, avg_loss=0.706648, seen=200, correct=98, accuracy=0.490000
2025-10-10 12:17:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:17:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:17:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:17:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1902MB allocated=1835MB
2025-10-10 12:17:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:17:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:17:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:17:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:17:27 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.075127, avg_loss=0.676878, seen=40, correct=23, accuracy=0.575000
2025-10-10 12:17:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:17:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:17:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:17:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1902MB allocated=1835MB
2025-10-10 12:17:29 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.625000, curr=0.575000
2025-10-10 12:17:38 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 12:17:38 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 12:17:38 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 12:17:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 12:17:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:17:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:17:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 12:17:41 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=141.391479, avg_loss=0.706957, seen=200, correct=95, accuracy=0.475000
2025-10-10 12:17:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:17:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:17:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:17:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1902MB allocated=1835MB
2025-10-10 12:17:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:17:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:17:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:17:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:17:44 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.763960, avg_loss=0.694099, seen=40, correct=21, accuracy=0.525000
2025-10-10 12:17:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:17:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:17:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:17:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1902MB allocated=1835MB
2025-10-10 12:17:46 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.625000, curr=0.525000
2025-10-10 12:17:55 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 12:17:55 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 12:17:55 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 12:17:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 12:17:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:17:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:17:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 12:17:58 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=141.159119, avg_loss=0.705796, seen=200, correct=100, accuracy=0.500000
2025-10-10 12:17:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:17:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:17:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:18:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1902MB allocated=1835MB
2025-10-10 12:18:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:18:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:18:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:18:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:18:01 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.316942, avg_loss=0.707924, seen=40, correct=20, accuracy=0.500000
2025-10-10 12:18:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:18:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:18:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:18:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1902MB allocated=1835MB
2025-10-10 12:18:02 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/25), best=0.625000, curr=0.500000
2025-10-10 12:18:11 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 12:18:11 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 12:18:11 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 12:18:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 12:18:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:18:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:18:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 12:18:14 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=141.550400, avg_loss=0.707752, seen=200, correct=100, accuracy=0.500000
2025-10-10 12:18:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:18:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:18:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:18:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1902MB allocated=1835MB
2025-10-10 12:18:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:18:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:18:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:18:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:18:20 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.932714, avg_loss=0.673318, seen=40, correct=23, accuracy=0.575000
2025-10-10 12:18:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:18:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:18:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:18:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1902MB allocated=1835MB
2025-10-10 12:18:21 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/25), best=0.625000, curr=0.575000
2025-10-10 12:18:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 12:18:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 12:18:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:18:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:18:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1902MB allocated=1835MB
2025-10-10 12:18:22 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #44', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 12:18:22 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #44', 'Round': 0, 'Results_raw': {}}
2025-10-10 12:18:23 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 12:18:24 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 12:18:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:18:24 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 12:18:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:18:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:18:26 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 12:18:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-10-10 12:18:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:18:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:18:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-10 12:18:27 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=74, loss_sum=58.449047, avg_loss=0.789852, seen=74, correct=35, accuracy=0.472973
2025-10-10 12:18:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:18:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:18:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:18:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1876MB allocated=1826MB
2025-10-10 12:18:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:18:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:18:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:18:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:18:32 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.786354, avg_loss=0.669659, seen=40, correct=22, accuracy=0.550000
2025-10-10 12:18:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:18:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:18:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:18:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1876MB allocated=1826MB
2025-10-10 12:18:34 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.550000
2025-10-10 12:18:34 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 12:18:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=353, total=1409)
2025-10-10 12:18:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:18:34 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 12:18:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:18:34 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 12:18:34 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=177, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 12:18:43 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 12:18:43 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 12:18:43 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 12:18:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-10-10 12:18:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:18:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:18:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-10 12:18:44 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=74, loss_sum=57.143635, avg_loss=0.772211, seen=74, correct=40, accuracy=0.540541
2025-10-10 12:18:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:18:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:18:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:18:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1906MB allocated=1843MB
2025-10-10 12:18:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:18:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:18:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:18:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:18:48 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.705173, avg_loss=0.692629, seen=40, correct=23, accuracy=0.575000
2025-10-10 12:18:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:18:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:18:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:18:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1906MB allocated=1843MB
2025-10-10 12:18:49 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-10 12:18:59 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 12:18:59 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 12:18:59 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 12:18:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-10-10 12:18:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:18:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:19:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-10 12:19:00 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=74, loss_sum=58.251415, avg_loss=0.787181, seen=74, correct=34, accuracy=0.459459
2025-10-10 12:19:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:19:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:19:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:19:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1906MB allocated=1843MB
2025-10-10 12:19:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:19:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:19:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:19:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:19:05 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.333498, avg_loss=0.658337, seen=40, correct=26, accuracy=0.650000
2025-10-10 12:19:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:19:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:19:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:19:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1906MB allocated=1843MB
2025-10-10 12:19:06 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.650000
2025-10-10 12:19:17 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 12:19:17 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 12:19:17 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 12:19:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-10-10 12:19:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:19:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:19:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-10 12:19:18 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=74, loss_sum=57.124092, avg_loss=0.771947, seen=74, correct=40, accuracy=0.540541
2025-10-10 12:19:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:19:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:19:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:19:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1906MB allocated=1843MB
2025-10-10 12:19:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:19:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:19:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:19:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:19:21 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.025465, avg_loss=0.700637, seen=40, correct=21, accuracy=0.525000
2025-10-10 12:19:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:19:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:19:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:19:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1906MB allocated=1843MB
2025-10-10 12:19:23 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.650000, curr=0.525000
2025-10-10 12:19:32 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 12:19:32 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 12:19:32 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 12:19:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-10-10 12:19:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:19:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:19:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-10 12:19:33 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=74, loss_sum=56.624786, avg_loss=0.765200, seen=74, correct=37, accuracy=0.500000
2025-10-10 12:19:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:19:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:19:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:19:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1906MB allocated=1843MB
2025-10-10 12:19:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:19:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:19:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:19:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:19:38 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.447357, avg_loss=0.686184, seen=40, correct=22, accuracy=0.550000
2025-10-10 12:19:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:19:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:19:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:19:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1906MB allocated=1843MB
2025-10-10 12:19:40 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.650000, curr=0.550000
2025-10-10 12:19:51 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 12:19:51 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 12:19:51 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 12:19:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-10-10 12:19:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:19:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:19:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-10 12:19:53 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=74, loss_sum=58.184036, avg_loss=0.786271, seen=74, correct=34, accuracy=0.459459
2025-10-10 12:19:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:19:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:19:56 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:19:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1906MB allocated=1843MB
2025-10-10 12:19:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:19:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:19:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:19:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:19:57 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.369364, avg_loss=0.659234, seen=40, correct=23, accuracy=0.575000
2025-10-10 12:19:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:19:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:19:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:20:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1906MB allocated=1843MB
2025-10-10 12:20:00 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.650000, curr=0.575000
2025-10-10 12:20:07 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 12:20:07 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 12:20:07 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 12:20:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-10-10 12:20:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:20:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:20:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-10 12:20:09 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=74, loss_sum=55.792187, avg_loss=0.753948, seen=74, correct=37, accuracy=0.500000
2025-10-10 12:20:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:20:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:20:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:20:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1906MB allocated=1843MB
2025-10-10 12:20:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:20:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:20:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:20:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:20:13 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.204115, avg_loss=0.655103, seen=40, correct=26, accuracy=0.650000
2025-10-10 12:20:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:20:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:20:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:20:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1906MB allocated=1843MB
2025-10-10 12:20:15 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.650000
2025-10-10 12:20:26 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 12:20:26 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 12:20:26 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 12:20:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-10-10 12:20:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:20:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:20:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-10 12:20:27 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=74, loss_sum=55.693378, avg_loss=0.752613, seen=74, correct=38, accuracy=0.513514
2025-10-10 12:20:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:20:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:20:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:20:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1906MB allocated=1843MB
2025-10-10 12:20:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:20:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:20:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:20:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:20:33 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.786205, avg_loss=0.694655, seen=40, correct=21, accuracy=0.525000
2025-10-10 12:20:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:20:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:20:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:20:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1906MB allocated=1843MB
2025-10-10 12:20:34 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.650000, curr=0.525000
2025-10-10 12:20:44 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 12:20:44 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 12:20:45 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 12:20:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-10-10 12:20:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:20:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:20:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-10 12:20:46 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=74, loss_sum=54.716103, avg_loss=0.739407, seen=74, correct=39, accuracy=0.527027
2025-10-10 12:20:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:20:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:20:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:20:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1906MB allocated=1843MB
2025-10-10 12:20:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:20:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:20:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:20:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:20:51 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.686455, avg_loss=0.667161, seen=40, correct=23, accuracy=0.575000
2025-10-10 12:20:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:20:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:20:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:20:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1906MB allocated=1843MB
2025-10-10 12:20:54 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.650000, curr=0.575000
2025-10-10 12:21:02 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 12:21:02 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 12:21:02 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 12:21:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-10-10 12:21:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:21:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:21:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-10 12:21:03 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=74, loss_sum=54.571899, avg_loss=0.737458, seen=74, correct=38, accuracy=0.513514
2025-10-10 12:21:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:21:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:21:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:21:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1906MB allocated=1843MB
2025-10-10 12:21:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:21:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:21:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:21:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:21:09 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.307293, avg_loss=0.657682, seen=40, correct=23, accuracy=0.575000
2025-10-10 12:21:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:21:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:21:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:21:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1906MB allocated=1843MB
2025-10-10 12:21:11 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.650000, curr=0.575000
2025-10-10 12:21:21 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 12:21:21 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 12:21:21 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 12:21:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-10-10 12:21:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:21:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:21:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-10 12:21:22 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=74, loss_sum=54.990414, avg_loss=0.743114, seen=74, correct=38, accuracy=0.513514
2025-10-10 12:21:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:21:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:21:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:21:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1906MB allocated=1843MB
2025-10-10 12:21:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:21:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:21:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:21:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:21:27 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.224991, avg_loss=0.655625, seen=40, correct=24, accuracy=0.600000
2025-10-10 12:21:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:21:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:21:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:21:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1906MB allocated=1843MB
2025-10-10 12:21:29 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/25), best=0.650000, curr=0.600000
2025-10-10 12:21:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 12:21:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 12:21:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:21:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:21:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1906MB allocated=1843MB
2025-10-10 12:21:30 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #33', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 12:21:30 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #33', 'Round': 0, 'Results_raw': {}}
2025-10-10 12:21:30 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 12:21:31 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 12:21:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:21:32 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 12:21:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:21:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:21:33 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 12:21:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-10-10 12:21:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:21:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:21:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-10-10 12:21:35 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=83, loss_sum=60.317551, avg_loss=0.726717, seen=83, correct=43, accuracy=0.518072
2025-10-10 12:21:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:21:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:21:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:21:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1876MB allocated=1835MB
2025-10-10 12:21:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:21:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:21:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:21:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:21:39 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.657494, avg_loss=0.741437, seen=40, correct=21, accuracy=0.525000
2025-10-10 12:21:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:21:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:21:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:21:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1876MB allocated=1835MB
2025-10-10 12:21:40 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.525000
2025-10-10 12:21:40 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 12:21:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=399, total=1594)
2025-10-10 12:21:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:21:41 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 12:21:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:21:41 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 12:21:41 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=200, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 12:21:49 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 12:21:49 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 12:21:49 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 12:21:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-10-10 12:21:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:21:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:21:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-10-10 12:21:50 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=83, loss_sum=59.529999, avg_loss=0.717229, seen=83, correct=45, accuracy=0.542169
2025-10-10 12:21:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:21:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:21:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:21:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1900MB allocated=1851MB
2025-10-10 12:21:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:21:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:21:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:21:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:21:54 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.959503, avg_loss=0.748988, seen=40, correct=24, accuracy=0.600000
2025-10-10 12:21:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:21:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:21:56 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:21:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1900MB allocated=1851MB
2025-10-10 12:21:56 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-10 12:22:05 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 12:22:05 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 12:22:05 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 12:22:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-10-10 12:22:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:22:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:22:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-10-10 12:22:06 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=83, loss_sum=58.793797, avg_loss=0.708359, seen=83, correct=47, accuracy=0.566265
2025-10-10 12:22:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:22:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:22:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:22:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1900MB allocated=1851MB
2025-10-10 12:22:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:22:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:22:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:22:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:22:10 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.552240, avg_loss=0.763806, seen=40, correct=24, accuracy=0.600000
2025-10-10 12:22:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:22:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:22:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:22:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1900MB allocated=1851MB
2025-10-10 12:22:12 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-10 12:22:22 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 12:22:22 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 12:22:22 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 12:22:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-10-10 12:22:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:22:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:22:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-10-10 12:22:23 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=83, loss_sum=60.239540, avg_loss=0.725778, seen=83, correct=45, accuracy=0.542169
2025-10-10 12:22:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:22:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:22:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:22:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1900MB allocated=1851MB
2025-10-10 12:22:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:22:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:22:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:22:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:22:28 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.943949, avg_loss=0.748599, seen=40, correct=21, accuracy=0.525000
2025-10-10 12:22:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:22:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:22:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:22:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1900MB allocated=1851MB
2025-10-10 12:22:30 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.600000, curr=0.525000
2025-10-10 12:22:41 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 12:22:41 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 12:22:41 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 12:22:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-10-10 12:22:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:22:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:22:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-10-10 12:22:42 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=83, loss_sum=59.534092, avg_loss=0.717278, seen=83, correct=47, accuracy=0.566265
2025-10-10 12:22:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:22:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:22:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:22:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1900MB allocated=1851MB
2025-10-10 12:22:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:22:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:22:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:22:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:22:47 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.494072, avg_loss=0.737352, seen=40, correct=24, accuracy=0.600000
2025-10-10 12:22:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:22:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:22:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:22:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1900MB allocated=1851MB
2025-10-10 12:22:48 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-10 12:22:58 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 12:22:58 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 12:22:58 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 12:22:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-10-10 12:22:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:22:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:23:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-10-10 12:23:00 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=83, loss_sum=59.070538, avg_loss=0.711693, seen=83, correct=46, accuracy=0.554217
2025-10-10 12:23:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:23:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:23:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:23:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1900MB allocated=1851MB
2025-10-10 12:23:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:23:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:23:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:23:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:23:04 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.668812, avg_loss=0.741720, seen=40, correct=22, accuracy=0.550000
2025-10-10 12:23:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:23:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:23:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:23:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1900MB allocated=1851MB
2025-10-10 12:23:07 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.600000, curr=0.550000
2025-10-10 12:23:18 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 12:23:18 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 12:23:18 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 12:23:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-10-10 12:23:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:23:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:23:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-10-10 12:23:19 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=83, loss_sum=58.534805, avg_loss=0.705239, seen=83, correct=47, accuracy=0.566265
2025-10-10 12:23:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:23:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:23:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:23:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1900MB allocated=1851MB
2025-10-10 12:23:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:23:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:23:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:23:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:23:23 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.550095, avg_loss=0.738752, seen=40, correct=22, accuracy=0.550000
2025-10-10 12:23:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:23:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:23:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:23:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1900MB allocated=1851MB
2025-10-10 12:23:25 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.600000, curr=0.550000
2025-10-10 12:23:34 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 12:23:34 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 12:23:34 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 12:23:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-10-10 12:23:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:23:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:23:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-10-10 12:23:35 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=83, loss_sum=58.156002, avg_loss=0.700675, seen=83, correct=46, accuracy=0.554217
2025-10-10 12:23:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:23:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:23:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:23:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1900MB allocated=1851MB
2025-10-10 12:23:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:23:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:23:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:23:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:23:40 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.426853, avg_loss=0.735671, seen=40, correct=23, accuracy=0.575000
2025-10-10 12:23:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:23:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:23:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:23:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1900MB allocated=1851MB
2025-10-10 12:23:41 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.600000, curr=0.575000
2025-10-10 12:23:50 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 12:23:50 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 12:23:50 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 12:23:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-10-10 12:23:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:23:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:23:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-10-10 12:23:52 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=83, loss_sum=58.525742, avg_loss=0.705129, seen=83, correct=46, accuracy=0.554217
2025-10-10 12:23:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:23:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:23:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:23:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1900MB allocated=1851MB
2025-10-10 12:23:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:23:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:23:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:23:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:23:56 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.314903, avg_loss=0.732873, seen=40, correct=23, accuracy=0.575000
2025-10-10 12:23:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:23:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:23:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:23:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1900MB allocated=1851MB
2025-10-10 12:23:58 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/25), best=0.600000, curr=0.575000
2025-10-10 12:24:08 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 12:24:08 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 12:24:08 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 12:24:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-10-10 12:24:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:24:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:24:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-10-10 12:24:09 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=83, loss_sum=58.472862, avg_loss=0.704492, seen=83, correct=46, accuracy=0.554217
2025-10-10 12:24:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:24:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:24:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:24:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1900MB allocated=1851MB
2025-10-10 12:24:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:24:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:24:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:24:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:24:15 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.990719, avg_loss=0.724768, seen=40, correct=22, accuracy=0.550000
2025-10-10 12:24:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:24:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:24:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:24:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1900MB allocated=1851MB
2025-10-10 12:24:16 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/25), best=0.600000, curr=0.550000
2025-10-10 12:24:27 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 12:24:27 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 12:24:27 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 12:24:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-10-10 12:24:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:24:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:24:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-10-10 12:24:29 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=83, loss_sum=59.032417, avg_loss=0.711234, seen=83, correct=42, accuracy=0.506024
2025-10-10 12:24:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:24:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:24:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:24:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1900MB allocated=1851MB
2025-10-10 12:24:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:24:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:24:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:24:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:24:34 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.905682, avg_loss=0.722642, seen=40, correct=21, accuracy=0.525000
2025-10-10 12:24:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:24:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:24:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:24:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1900MB allocated=1851MB
2025-10-10 12:24:35 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/25), best=0.600000, curr=0.525000
2025-10-10 12:24:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 12:24:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 12:24:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:24:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:24:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1900MB allocated=1851MB
2025-10-10 12:24:36 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #39', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 12:24:36 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #39', 'Round': 0, 'Results_raw': {}}
2025-10-10 12:24:36 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 12:24:37 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 12:24:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:24:38 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 12:24:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:24:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:24:39 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 12:24:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 12:24:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:24:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:24:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 12:24:42 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=133.274261, avg_loss=0.666371, seen=200, correct=123, accuracy=0.615000
2025-10-10 12:24:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:24:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:24:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:24:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1876MB allocated=1843MB
2025-10-10 12:24:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:24:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:24:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:24:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:24:46 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.973837, avg_loss=0.674346, seen=40, correct=21, accuracy=0.525000
2025-10-10 12:24:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:24:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:24:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:24:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1876MB allocated=1843MB
2025-10-10 12:24:48 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.525000
2025-10-10 12:24:48 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 12:24:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1122, total=4486)
2025-10-10 12:24:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:24:48 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 12:24:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:24:48 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 12:24:48 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=561, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 12:24:57 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 12:24:57 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 12:24:57 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 12:24:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 12:24:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:24:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:25:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 12:25:00 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=136.602631, avg_loss=0.683013, seen=200, correct=110, accuracy=0.550000
2025-10-10 12:25:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:25:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:25:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:25:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1906MB allocated=1860MB
2025-10-10 12:25:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:25:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:25:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:25:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:25:03 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.064270, avg_loss=0.651607, seen=40, correct=24, accuracy=0.600000
2025-10-10 12:25:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:25:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:25:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:25:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1906MB allocated=1860MB
2025-10-10 12:25:04 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-10 12:25:15 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 12:25:15 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 12:25:15 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 12:25:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 12:25:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:25:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:25:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 12:25:18 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=136.724564, avg_loss=0.683623, seen=200, correct=117, accuracy=0.585000
2025-10-10 12:25:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:25:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:25:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:25:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1906MB allocated=1860MB
2025-10-10 12:25:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:25:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:25:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:25:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:25:22 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.237034, avg_loss=0.705926, seen=40, correct=20, accuracy=0.500000
2025-10-10 12:25:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:25:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:25:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:25:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1906MB allocated=1860MB
2025-10-10 12:25:23 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.600000, curr=0.500000
2025-10-10 12:25:34 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 12:25:34 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 12:25:34 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 12:25:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 12:25:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:25:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:25:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 12:25:37 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=137.768280, avg_loss=0.688841, seen=200, correct=107, accuracy=0.535000
2025-10-10 12:25:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:25:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:25:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:25:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1906MB allocated=1860MB
2025-10-10 12:25:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:25:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:25:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:25:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:25:41 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.510153, avg_loss=0.712754, seen=40, correct=22, accuracy=0.550000
2025-10-10 12:25:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:25:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:25:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:25:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1906MB allocated=1860MB
2025-10-10 12:25:43 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.600000, curr=0.550000
2025-10-10 12:25:52 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 12:25:52 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 12:25:52 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 12:25:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 12:25:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:25:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:25:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 12:25:55 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=134.176407, avg_loss=0.670882, seen=200, correct=119, accuracy=0.595000
2025-10-10 12:25:55 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:25:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:25:56 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:25:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1906MB allocated=1860MB
2025-10-10 12:25:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:25:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:25:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:26:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:26:01 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.877743, avg_loss=0.671944, seen=40, correct=22, accuracy=0.550000
2025-10-10 12:26:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:26:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:26:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:26:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1906MB allocated=1860MB
2025-10-10 12:26:02 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.600000, curr=0.550000
2025-10-10 12:26:10 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 12:26:10 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 12:26:10 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 12:26:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 12:26:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:26:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:26:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 12:26:13 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=134.891724, avg_loss=0.674459, seen=200, correct=116, accuracy=0.580000
2025-10-10 12:26:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:26:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:26:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:26:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1906MB allocated=1860MB
2025-10-10 12:26:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:26:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:26:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:26:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:26:17 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.757856, avg_loss=0.668946, seen=40, correct=22, accuracy=0.550000
2025-10-10 12:26:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:26:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:26:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:26:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1906MB allocated=1860MB
2025-10-10 12:26:18 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/25), best=0.600000, curr=0.550000
2025-10-10 12:26:27 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 12:26:27 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 12:26:27 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 12:26:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 12:26:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:26:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:26:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 12:26:30 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=135.993347, avg_loss=0.679967, seen=200, correct=108, accuracy=0.540000
2025-10-10 12:26:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:26:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:26:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:26:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1906MB allocated=1860MB
2025-10-10 12:26:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:26:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:26:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:26:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:26:36 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.475676, avg_loss=0.686892, seen=40, correct=22, accuracy=0.550000
2025-10-10 12:26:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:26:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:26:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:26:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1906MB allocated=1860MB
2025-10-10 12:26:37 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/25), best=0.600000, curr=0.550000
2025-10-10 12:26:47 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 12:26:47 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 12:26:47 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 12:26:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 12:26:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:26:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:26:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 12:26:50 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=134.196533, avg_loss=0.670983, seen=200, correct=117, accuracy=0.585000
2025-10-10 12:26:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:26:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:26:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:26:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1906MB allocated=1860MB
2025-10-10 12:26:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:26:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:26:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:26:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:26:54 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.801380, avg_loss=0.670035, seen=40, correct=26, accuracy=0.650000
2025-10-10 12:26:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:26:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:26:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:26:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1906MB allocated=1860MB
2025-10-10 12:26:57 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.650000
2025-10-10 12:27:06 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 12:27:06 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 12:27:06 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 12:27:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 12:27:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:27:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:27:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 12:27:09 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=134.605377, avg_loss=0.673027, seen=200, correct=115, accuracy=0.575000
2025-10-10 12:27:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:27:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:27:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:27:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1906MB allocated=1860MB
2025-10-10 12:27:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:27:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:27:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:27:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:27:13 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.987898, avg_loss=0.674697, seen=40, correct=24, accuracy=0.600000
2025-10-10 12:27:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:27:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:27:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:27:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1906MB allocated=1860MB
2025-10-10 12:27:14 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.650000, curr=0.600000
2025-10-10 12:27:24 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 12:27:24 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 12:27:24 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 12:27:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 12:27:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:27:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:27:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 12:27:27 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=134.963715, avg_loss=0.674819, seen=200, correct=115, accuracy=0.575000
2025-10-10 12:27:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:27:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:27:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:27:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1906MB allocated=1860MB
2025-10-10 12:27:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:27:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:27:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:27:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:27:31 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.292187, avg_loss=0.682305, seen=40, correct=20, accuracy=0.500000
2025-10-10 12:27:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:27:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:27:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:27:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1906MB allocated=1860MB
2025-10-10 12:27:32 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.650000, curr=0.500000
2025-10-10 12:27:42 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 12:27:42 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 12:27:42 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 12:27:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 12:27:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:27:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:27:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 12:27:45 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=132.850143, avg_loss=0.664251, seen=200, correct=117, accuracy=0.585000
2025-10-10 12:27:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:27:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:27:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:27:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1906MB allocated=1860MB
2025-10-10 12:27:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:27:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:27:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:27:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:27:48 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.974762, avg_loss=0.649369, seen=40, correct=26, accuracy=0.650000
2025-10-10 12:27:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:27:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:27:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:27:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1906MB allocated=1860MB
2025-10-10 12:27:51 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.650000
2025-10-10 12:27:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 12:27:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 12:27:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:27:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:27:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1906MB allocated=1860MB
2025-10-10 12:27:52 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #34', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 12:27:52 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #34', 'Round': 0, 'Results_raw': {}}
2025-10-10 12:27:52 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 12:27:53 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 12:27:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:27:54 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 12:27:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:27:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:27:56 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 12:27:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-10-10 12:27:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:27:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:27:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-10-10 12:27:59 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=137, loss_sum=88.450005, avg_loss=0.645620, seen=137, correct=90, accuracy=0.656934
2025-10-10 12:27:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:27:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:28:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:28:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1896MB allocated=1851MB
2025-10-10 12:28:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:28:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:28:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:28:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:28:03 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.246494, avg_loss=0.631162, seen=40, correct=30, accuracy=0.750000
2025-10-10 12:28:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:28:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:28:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:28:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1896MB allocated=1851MB
2025-10-10 12:28:06 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.750000
2025-10-10 12:28:06 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 12:28:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=652, total=2605)
2025-10-10 12:28:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:28:06 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 12:28:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:28:06 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 12:28:06 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=326, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 12:28:14 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 12:28:14 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 12:28:14 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 12:28:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-10-10 12:28:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:28:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:28:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-10-10 12:28:17 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=137, loss_sum=88.206917, avg_loss=0.643846, seen=137, correct=90, accuracy=0.656934
2025-10-10 12:28:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:28:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:28:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:28:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1930MB allocated=1868MB
2025-10-10 12:28:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:28:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:28:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:28:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:28:22 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.946241, avg_loss=0.673656, seen=40, correct=23, accuracy=0.575000
2025-10-10 12:28:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:28:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:28:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:28:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1930MB allocated=1868MB
2025-10-10 12:28:23 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.750000, curr=0.575000
2025-10-10 12:28:32 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 12:28:32 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 12:28:32 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 12:28:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-10-10 12:28:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:28:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:28:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-10-10 12:28:34 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=137, loss_sum=87.976357, avg_loss=0.642163, seen=137, correct=89, accuracy=0.649635
2025-10-10 12:28:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:28:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:28:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:28:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1930MB allocated=1868MB
2025-10-10 12:28:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:28:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:28:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:28:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:28:39 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.842709, avg_loss=0.621068, seen=40, correct=32, accuracy=0.800000
2025-10-10 12:28:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:28:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:28:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:28:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1930MB allocated=1868MB
2025-10-10 12:28:40 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.800000
2025-10-10 12:28:52 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 12:28:52 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 12:28:52 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 12:28:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-10-10 12:28:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:28:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:28:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-10-10 12:28:54 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=137, loss_sum=86.859909, avg_loss=0.634014, seen=137, correct=94, accuracy=0.686131
2025-10-10 12:28:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:28:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:28:56 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:28:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1930MB allocated=1868MB
2025-10-10 12:28:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:28:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:28:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:28:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:28:58 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.116871, avg_loss=0.627922, seen=40, correct=29, accuracy=0.725000
2025-10-10 12:28:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:28:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:28:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:28:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1930MB allocated=1868MB
2025-10-10 12:28:59 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.800000, curr=0.725000
2025-10-10 12:29:09 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 12:29:09 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 12:29:09 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 12:29:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-10-10 12:29:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:29:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:29:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-10-10 12:29:11 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=137, loss_sum=87.430153, avg_loss=0.638176, seen=137, correct=90, accuracy=0.656934
2025-10-10 12:29:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:29:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:29:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:29:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1930MB allocated=1868MB
2025-10-10 12:29:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:29:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:29:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:29:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:29:15 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.326252, avg_loss=0.633156, seen=40, correct=29, accuracy=0.725000
2025-10-10 12:29:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:29:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:29:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:29:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1930MB allocated=1868MB
2025-10-10 12:29:18 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.800000, curr=0.725000
2025-10-10 12:29:28 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 12:29:28 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 12:29:28 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 12:29:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-10-10 12:29:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:29:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:29:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-10-10 12:29:30 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=137, loss_sum=89.015610, avg_loss=0.649749, seen=137, correct=84, accuracy=0.613139
2025-10-10 12:29:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:29:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:29:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:29:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1930MB allocated=1868MB
2025-10-10 12:29:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:29:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:29:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:29:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:29:35 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.851782, avg_loss=0.621295, seen=40, correct=30, accuracy=0.750000
2025-10-10 12:29:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:29:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:29:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:29:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1930MB allocated=1868MB
2025-10-10 12:29:37 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.800000, curr=0.750000
2025-10-10 12:29:44 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 12:29:44 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 12:29:44 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 12:29:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-10-10 12:29:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:29:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:29:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-10-10 12:29:47 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=137, loss_sum=91.979736, avg_loss=0.671385, seen=137, correct=77, accuracy=0.562044
2025-10-10 12:29:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:29:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:29:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:29:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1930MB allocated=1868MB
2025-10-10 12:29:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:29:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:29:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:29:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:29:51 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.937725, avg_loss=0.623443, seen=40, correct=28, accuracy=0.700000
2025-10-10 12:29:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:29:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:29:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:29:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1930MB allocated=1868MB
2025-10-10 12:29:52 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/25), best=0.800000, curr=0.700000
2025-10-10 12:30:03 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 12:30:03 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 12:30:03 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 12:30:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-10-10 12:30:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:30:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:30:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-10-10 12:30:05 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=137, loss_sum=87.867447, avg_loss=0.641368, seen=137, correct=96, accuracy=0.700730
2025-10-10 12:30:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:30:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:30:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:30:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1930MB allocated=1868MB
2025-10-10 12:30:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:30:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:30:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:30:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:30:11 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.307589, avg_loss=0.632690, seen=40, correct=29, accuracy=0.725000
2025-10-10 12:30:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:30:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:30:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:30:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1930MB allocated=1868MB
2025-10-10 12:30:13 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/25), best=0.800000, curr=0.725000
2025-10-10 12:30:21 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 12:30:21 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 12:30:21 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 12:30:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-10-10 12:30:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:30:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:30:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-10-10 12:30:23 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=137, loss_sum=88.811348, avg_loss=0.648258, seen=137, correct=81, accuracy=0.591241
2025-10-10 12:30:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:30:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:30:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:30:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1930MB allocated=1868MB
2025-10-10 12:30:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:30:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:30:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:30:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:30:27 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.496418, avg_loss=0.662410, seen=40, correct=17, accuracy=0.425000
2025-10-10 12:30:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:30:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:30:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:30:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1930MB allocated=1868MB
2025-10-10 12:30:29 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/25), best=0.800000, curr=0.425000
2025-10-10 12:30:37 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 12:30:37 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 12:30:37 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 12:30:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-10-10 12:30:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:30:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:30:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-10-10 12:30:40 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=137, loss_sum=89.843567, avg_loss=0.655792, seen=137, correct=83, accuracy=0.605839
2025-10-10 12:30:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:30:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:30:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:30:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1930MB allocated=1868MB
2025-10-10 12:30:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:30:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:30:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:30:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:30:43 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.253336, avg_loss=0.681333, seen=40, correct=19, accuracy=0.475000
2025-10-10 12:30:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:30:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:30:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:30:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1930MB allocated=1868MB
2025-10-10 12:30:46 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/25), best=0.800000, curr=0.475000
2025-10-10 12:30:55 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 12:30:56 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 12:30:56 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 12:30:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-10-10 12:30:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:30:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:30:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-10-10 12:30:58 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=137, loss_sum=89.201736, avg_loss=0.651108, seen=137, correct=88, accuracy=0.642336
2025-10-10 12:30:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:30:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:30:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:30:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1930MB allocated=1868MB
2025-10-10 12:31:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:31:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:31:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:31:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:31:01 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.770945, avg_loss=0.619274, seen=40, correct=31, accuracy=0.775000
2025-10-10 12:31:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:31:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:31:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:31:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1930MB allocated=1868MB
2025-10-10 12:31:02 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/25), best=0.800000, curr=0.775000
2025-10-10 12:31:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 12:31:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 12:31:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:31:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:31:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1930MB allocated=1868MB
2025-10-10 12:31:03 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #12', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 12:31:03 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #12', 'Round': 0, 'Results_raw': {}}
2025-10-10 12:31:03 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 12:31:05 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 12:31:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:31:05 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 12:31:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:31:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:31:07 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 12:31:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-10-10 12:31:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:31:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:31:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-10-10 12:31:08 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=36, loss_sum=27.064941, avg_loss=0.751804, seen=36, correct=13, accuracy=0.361111
2025-10-10 12:31:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:31:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:31:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:31:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1898MB allocated=1860MB
2025-10-10 12:31:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:31:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:31:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:31:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:31:13 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.215166, avg_loss=0.730379, seen=40, correct=20, accuracy=0.500000
2025-10-10 12:31:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:31:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:31:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:31:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1898MB allocated=1860MB
2025-10-10 12:31:16 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.500000
2025-10-10 12:31:16 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 12:31:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=173, total=691)
2025-10-10 12:31:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:31:16 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 12:31:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:31:16 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 12:31:16 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=87, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 12:31:26 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 12:31:26 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 12:31:26 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 12:31:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-10-10 12:31:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:31:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:31:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-10-10 12:31:26 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=36, loss_sum=25.190971, avg_loss=0.699749, seen=36, correct=23, accuracy=0.638889
2025-10-10 12:31:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:31:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:31:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:31:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1928MB allocated=1877MB
2025-10-10 12:31:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:31:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:31:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:31:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:31:31 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.795446, avg_loss=0.694886, seen=40, correct=20, accuracy=0.500000
2025-10-10 12:31:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:31:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:31:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:31:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1928MB allocated=1877MB
2025-10-10 12:31:33 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.500000
2025-10-10 12:31:42 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 12:31:42 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 12:31:42 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 12:31:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-10-10 12:31:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:31:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:31:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-10-10 12:31:42 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=36, loss_sum=25.218655, avg_loss=0.700518, seen=36, correct=20, accuracy=0.555556
2025-10-10 12:31:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:31:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:31:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:31:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1928MB allocated=1877MB
2025-10-10 12:31:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:31:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:31:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:31:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:31:46 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.794571, avg_loss=0.694864, seen=40, correct=22, accuracy=0.550000
2025-10-10 12:31:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:31:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:31:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:31:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1928MB allocated=1877MB
2025-10-10 12:31:47 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.550000
2025-10-10 12:31:56 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 12:31:56 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 12:31:56 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 12:31:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-10-10 12:31:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:31:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:31:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-10-10 12:31:57 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=36, loss_sum=25.496754, avg_loss=0.708243, seen=36, correct=24, accuracy=0.666667
2025-10-10 12:31:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:31:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:31:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:31:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1928MB allocated=1877MB
2025-10-10 12:31:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:31:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:31:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:32:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:32:00 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.889423, avg_loss=0.697236, seen=40, correct=18, accuracy=0.450000
2025-10-10 12:32:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:32:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:32:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:32:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1928MB allocated=1877MB
2025-10-10 12:32:02 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.550000, curr=0.450000
2025-10-10 12:32:10 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 12:32:10 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 12:32:10 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 12:32:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-10-10 12:32:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:32:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:32:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-10-10 12:32:11 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=36, loss_sum=25.934256, avg_loss=0.720396, seen=36, correct=21, accuracy=0.583333
2025-10-10 12:32:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:32:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:32:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:32:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1928MB allocated=1877MB
2025-10-10 12:32:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:32:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:32:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:32:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:32:15 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.262218, avg_loss=0.706555, seen=40, correct=19, accuracy=0.475000
2025-10-10 12:32:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:32:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:32:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:32:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1928MB allocated=1877MB
2025-10-10 12:32:16 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.550000, curr=0.475000
2025-10-10 12:32:25 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 12:32:25 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 12:32:25 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 12:32:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-10-10 12:32:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:32:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:32:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-10-10 12:32:26 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=36, loss_sum=26.059267, avg_loss=0.723869, seen=36, correct=18, accuracy=0.500000
2025-10-10 12:32:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:32:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:32:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:32:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1928MB allocated=1877MB
2025-10-10 12:32:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:32:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:32:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:32:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:32:31 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.264818, avg_loss=0.706620, seen=40, correct=17, accuracy=0.425000
2025-10-10 12:32:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:32:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:32:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:32:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1928MB allocated=1877MB
2025-10-10 12:32:33 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.550000, curr=0.425000
2025-10-10 12:32:43 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 12:32:43 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 12:32:43 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 12:32:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-10-10 12:32:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:32:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:32:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-10-10 12:32:43 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=36, loss_sum=25.529938, avg_loss=0.709165, seen=36, correct=26, accuracy=0.722222
2025-10-10 12:32:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:32:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:32:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:32:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1928MB allocated=1877MB
2025-10-10 12:32:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:32:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:32:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:32:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:32:48 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.812017, avg_loss=0.695300, seen=40, correct=18, accuracy=0.450000
2025-10-10 12:32:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:32:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:32:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:32:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1928MB allocated=1877MB
2025-10-10 12:32:49 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/25), best=0.550000, curr=0.450000
2025-10-10 12:32:59 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 12:32:59 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 12:32:59 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 12:33:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-10-10 12:33:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:33:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:33:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-10-10 12:33:00 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=36, loss_sum=25.110886, avg_loss=0.697525, seen=36, correct=21, accuracy=0.583333
2025-10-10 12:33:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:33:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:33:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:33:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1928MB allocated=1877MB
2025-10-10 12:33:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:33:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:33:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:33:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:33:04 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.601982, avg_loss=0.690050, seen=40, correct=21, accuracy=0.525000
2025-10-10 12:33:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:33:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:33:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:33:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1928MB allocated=1877MB
2025-10-10 12:33:06 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/25), best=0.550000, curr=0.525000
2025-10-10 12:33:14 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 12:33:14 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 12:33:14 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 12:33:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-10-10 12:33:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:33:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:33:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-10-10 12:33:14 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=36, loss_sum=25.770742, avg_loss=0.715854, seen=36, correct=23, accuracy=0.638889
2025-10-10 12:33:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:33:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:33:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:33:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1928MB allocated=1877MB
2025-10-10 12:33:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:33:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:33:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:33:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:33:18 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.934397, avg_loss=0.698360, seen=40, correct=18, accuracy=0.450000
2025-10-10 12:33:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:33:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:33:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:33:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1928MB allocated=1877MB
2025-10-10 12:33:20 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/25), best=0.550000, curr=0.450000
2025-10-10 12:33:30 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 12:33:30 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 12:33:30 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 12:33:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-10-10 12:33:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:33:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:33:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-10-10 12:33:31 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=36, loss_sum=26.146664, avg_loss=0.726296, seen=36, correct=18, accuracy=0.500000
2025-10-10 12:33:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:33:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:33:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:33:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1928MB allocated=1877MB
2025-10-10 12:33:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:33:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:33:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:33:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:33:34 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.161165, avg_loss=0.704029, seen=40, correct=21, accuracy=0.525000
2025-10-10 12:33:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:33:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:33:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:33:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1928MB allocated=1877MB
2025-10-10 12:33:35 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/25), best=0.550000, curr=0.525000
2025-10-10 12:33:44 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 12:33:44 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 12:33:44 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 12:33:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-10-10 12:33:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:33:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:33:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-10-10 12:33:45 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=36, loss_sum=26.174683, avg_loss=0.727075, seen=36, correct=22, accuracy=0.611111
2025-10-10 12:33:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:33:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:33:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:33:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1928MB allocated=1877MB
2025-10-10 12:33:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:33:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:33:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:33:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:33:48 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.960732, avg_loss=0.699018, seen=40, correct=18, accuracy=0.450000
2025-10-10 12:33:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:33:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:33:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:33:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1928MB allocated=1877MB
2025-10-10 12:33:50 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/25), best=0.550000, curr=0.450000
2025-10-10 12:33:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 12:33:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 12:33:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:33:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:33:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1928MB allocated=1877MB
2025-10-10 12:33:51 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #3', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 12:33:51 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #3', 'Round': 0, 'Results_raw': {}}
2025-10-10 12:33:51 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 12:33:52 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 12:33:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:33:53 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 12:33:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:33:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:33:54 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 12:33:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-10-10 12:33:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:33:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:33:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-10 12:33:56 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=112, loss_sum=79.245598, avg_loss=0.707550, seen=112, correct=57, accuracy=0.508929
2025-10-10 12:33:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:33:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:33:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:33:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1898MB allocated=1868MB
2025-10-10 12:33:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:33:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:33:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:33:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:33:59 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.755421, avg_loss=0.768886, seen=40, correct=17, accuracy=0.425000
2025-10-10 12:33:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:33:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:34:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:34:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1898MB allocated=1868MB
2025-10-10 12:34:01 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.425000
2025-10-10 12:34:01 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 12:34:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=536, total=2144)
2025-10-10 12:34:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:34:01 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 12:34:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:34:01 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 12:34:01 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=268, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 12:34:09 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 12:34:09 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 12:34:09 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 12:34:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-10-10 12:34:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:34:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:34:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-10 12:34:11 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=112, loss_sum=75.776863, avg_loss=0.676579, seen=112, correct=68, accuracy=0.607143
2025-10-10 12:34:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:34:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:34:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:34:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1885MB
2025-10-10 12:34:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:34:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:34:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:34:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:34:14 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.844049, avg_loss=0.696101, seen=40, correct=23, accuracy=0.575000
2025-10-10 12:34:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:34:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:34:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:34:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1885MB
2025-10-10 12:34:16 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-10 12:34:25 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 12:34:25 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 12:34:25 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 12:34:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-10-10 12:34:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:34:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:34:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-10 12:34:26 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=112, loss_sum=75.679871, avg_loss=0.675713, seen=112, correct=58, accuracy=0.517857
2025-10-10 12:34:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:34:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:34:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:34:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1885MB
2025-10-10 12:34:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:34:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:34:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:34:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:34:30 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.562782, avg_loss=0.714070, seen=40, correct=19, accuracy=0.475000
2025-10-10 12:34:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:34:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:34:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:34:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1885MB
2025-10-10 12:34:31 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.575000, curr=0.475000
2025-10-10 12:34:40 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 12:34:40 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 12:34:40 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 12:34:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-10-10 12:34:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:34:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:34:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-10 12:34:42 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=112, loss_sum=75.913658, avg_loss=0.677801, seen=112, correct=58, accuracy=0.517857
2025-10-10 12:34:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:34:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:34:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:34:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1885MB
2025-10-10 12:34:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:34:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:34:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:34:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:34:45 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.067612, avg_loss=0.726690, seen=40, correct=18, accuracy=0.450000
2025-10-10 12:34:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:34:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:34:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:34:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1885MB
2025-10-10 12:34:47 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.575000, curr=0.450000
2025-10-10 12:34:56 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 12:34:56 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 12:34:56 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 12:34:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-10-10 12:34:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:34:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:34:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-10 12:34:58 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=112, loss_sum=74.529411, avg_loss=0.665441, seen=112, correct=60, accuracy=0.535714
2025-10-10 12:34:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:34:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:34:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:35:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1885MB
2025-10-10 12:35:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:35:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:35:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:35:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:35:02 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.968994, avg_loss=0.699225, seen=40, correct=20, accuracy=0.500000
2025-10-10 12:35:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:35:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:35:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:35:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1885MB
2025-10-10 12:35:03 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.575000, curr=0.500000
2025-10-10 12:35:10 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 12:35:10 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 12:35:10 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 12:35:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-10-10 12:35:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:35:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:35:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-10 12:35:12 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=112, loss_sum=73.706314, avg_loss=0.658092, seen=112, correct=71, accuracy=0.633929
2025-10-10 12:35:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:35:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:35:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:35:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1885MB
2025-10-10 12:35:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:35:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:35:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:35:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:35:16 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.477814, avg_loss=0.661945, seen=40, correct=25, accuracy=0.625000
2025-10-10 12:35:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:35:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:35:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:35:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1885MB
2025-10-10 12:35:19 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-10 12:35:31 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 12:35:31 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 12:35:31 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 12:35:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-10-10 12:35:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:35:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:35:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-10 12:35:32 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=112, loss_sum=76.114227, avg_loss=0.679591, seen=112, correct=66, accuracy=0.589286
2025-10-10 12:35:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:35:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:35:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:35:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1885MB
2025-10-10 12:35:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:35:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:35:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:35:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:35:39 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.670166, avg_loss=0.666754, seen=40, correct=24, accuracy=0.600000
2025-10-10 12:35:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:35:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:35:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:35:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1885MB
2025-10-10 12:35:40 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.625000, curr=0.600000
2025-10-10 12:35:50 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 12:35:50 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 12:35:50 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 12:35:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-10-10 12:35:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:35:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:35:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-10 12:35:52 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=112, loss_sum=73.097900, avg_loss=0.652660, seen=112, correct=73, accuracy=0.651786
2025-10-10 12:35:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:35:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:35:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:35:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1885MB
2025-10-10 12:35:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:35:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:35:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:35:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:35:54 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.894684, avg_loss=0.672367, seen=40, correct=23, accuracy=0.575000
2025-10-10 12:35:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:35:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:35:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:35:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1885MB
2025-10-10 12:35:57 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.625000, curr=0.575000
2025-10-10 12:36:06 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 12:36:06 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 12:36:06 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 12:36:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-10-10 12:36:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:36:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:36:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-10 12:36:07 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=112, loss_sum=74.177704, avg_loss=0.662301, seen=112, correct=67, accuracy=0.598214
2025-10-10 12:36:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:36:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:36:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:36:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1885MB
2025-10-10 12:36:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:36:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:36:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:36:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:36:11 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.109035, avg_loss=0.702726, seen=40, correct=21, accuracy=0.525000
2025-10-10 12:36:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:36:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:36:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:36:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1885MB
2025-10-10 12:36:12 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.625000, curr=0.525000
2025-10-10 12:36:23 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 12:36:23 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 12:36:23 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 12:36:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-10-10 12:36:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:36:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:36:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-10 12:36:24 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=112, loss_sum=74.855499, avg_loss=0.668353, seen=112, correct=64, accuracy=0.571429
2025-10-10 12:36:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:36:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:36:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:36:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1885MB
2025-10-10 12:36:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:36:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:36:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:36:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:36:28 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.697536, avg_loss=0.717438, seen=40, correct=18, accuracy=0.450000
2025-10-10 12:36:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:36:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:36:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:36:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1885MB
2025-10-10 12:36:31 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/25), best=0.625000, curr=0.450000
2025-10-10 12:36:39 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 12:36:39 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 12:36:39 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 12:36:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-10-10 12:36:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:36:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:36:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-10 12:36:41 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=112, loss_sum=75.419838, avg_loss=0.673391, seen=112, correct=62, accuracy=0.553571
2025-10-10 12:36:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:36:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:36:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:36:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1885MB
2025-10-10 12:36:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:36:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:36:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:36:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:36:47 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.324604, avg_loss=0.733115, seen=40, correct=20, accuracy=0.500000
2025-10-10 12:36:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:36:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:36:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:36:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1885MB
2025-10-10 12:36:48 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/25), best=0.625000, curr=0.500000
2025-10-10 12:36:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 12:36:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 12:36:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:36:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:36:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1885MB
2025-10-10 12:36:49 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #32', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 12:36:49 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #32', 'Round': 0, 'Results_raw': {}}
2025-10-10 12:36:50 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 12:36:50 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 12:36:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:36:51 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 12:36:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:36:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:36:52 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 12:36:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 12:36:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:36:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:36:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 12:36:56 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=130.100983, avg_loss=0.650505, seen=200, correct=120, accuracy=0.600000
2025-10-10 12:36:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:36:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:36:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:36:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1918MB allocated=1877MB
2025-10-10 12:36:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:36:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:36:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:37:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:37:00 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.952229, avg_loss=0.623806, seen=40, correct=28, accuracy=0.700000
2025-10-10 12:37:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:37:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:37:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:37:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1918MB allocated=1877MB
2025-10-10 12:37:02 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.700000
2025-10-10 12:37:02 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 12:37:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1443, total=5772)
2025-10-10 12:37:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:37:02 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 12:37:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:37:02 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 12:37:02 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=722, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 12:37:11 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 12:37:11 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 12:37:11 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 12:37:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 12:37:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:37:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:37:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 12:37:14 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=129.111816, avg_loss=0.645559, seen=200, correct=124, accuracy=0.620000
2025-10-10 12:37:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:37:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:37:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:37:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1952MB allocated=1893MB
2025-10-10 12:37:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:37:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:37:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:37:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:37:17 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.186878, avg_loss=0.604672, seen=40, correct=26, accuracy=0.650000
2025-10-10 12:37:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:37:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:37:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:37:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1952MB allocated=1893MB
2025-10-10 12:37:19 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.700000, curr=0.650000
2025-10-10 12:37:28 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 12:37:28 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 12:37:28 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 12:37:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 12:37:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:37:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:37:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 12:37:31 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=128.617874, avg_loss=0.643089, seen=200, correct=122, accuracy=0.610000
2025-10-10 12:37:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:37:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:37:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:37:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1952MB allocated=1893MB
2025-10-10 12:37:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:37:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:37:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:37:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:37:35 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.520849, avg_loss=0.613021, seen=40, correct=28, accuracy=0.700000
2025-10-10 12:37:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:37:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:37:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:37:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1952MB allocated=1893MB
2025-10-10 12:37:37 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.700000
2025-10-10 12:37:47 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 12:37:47 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 12:37:47 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 12:37:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 12:37:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:37:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:37:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 12:37:50 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=130.209915, avg_loss=0.651050, seen=200, correct=116, accuracy=0.580000
2025-10-10 12:37:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:37:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:37:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:37:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1952MB allocated=1893MB
2025-10-10 12:37:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:37:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:37:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:37:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:37:54 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.545792, avg_loss=0.613645, seen=40, correct=28, accuracy=0.700000
2025-10-10 12:37:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:37:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:37:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:37:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1952MB allocated=1893MB
2025-10-10 12:37:56 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.700000
2025-10-10 12:38:05 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 12:38:05 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 12:38:05 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 12:38:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 12:38:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:38:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:38:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 12:38:08 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=127.897949, avg_loss=0.639490, seen=200, correct=124, accuracy=0.620000
2025-10-10 12:38:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:38:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:38:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:38:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1952MB allocated=1893MB
2025-10-10 12:38:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:38:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:38:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:38:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:38:13 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.850155, avg_loss=0.621254, seen=40, correct=26, accuracy=0.650000
2025-10-10 12:38:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:38:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:38:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:38:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1952MB allocated=1893MB
2025-10-10 12:38:14 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.700000, curr=0.650000
2025-10-10 12:38:24 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 12:38:24 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 12:38:24 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 12:38:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 12:38:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:38:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:38:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 12:38:27 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=127.705826, avg_loss=0.638529, seen=200, correct=119, accuracy=0.595000
2025-10-10 12:38:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:38:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:38:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:38:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1952MB allocated=1893MB
2025-10-10 12:38:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:38:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:38:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:38:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:38:30 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.788599, avg_loss=0.619715, seen=40, correct=28, accuracy=0.700000
2025-10-10 12:38:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:38:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:38:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:38:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1952MB allocated=1893MB
2025-10-10 12:38:33 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.700000
2025-10-10 12:38:40 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 12:38:40 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 12:38:40 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 12:38:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 12:38:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:38:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:38:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 12:38:43 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=127.176865, avg_loss=0.635884, seen=200, correct=125, accuracy=0.625000
2025-10-10 12:38:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:38:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:38:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:38:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1952MB allocated=1893MB
2025-10-10 12:38:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:38:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:38:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:38:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:38:47 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.697680, avg_loss=0.617442, seen=40, correct=28, accuracy=0.700000
2025-10-10 12:38:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:38:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:38:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:38:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1952MB allocated=1893MB
2025-10-10 12:38:48 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.700000
2025-10-10 12:38:59 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 12:38:59 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 12:38:59 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 12:38:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 12:38:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:38:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:39:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 12:39:02 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=130.015045, avg_loss=0.650075, seen=200, correct=113, accuracy=0.565000
2025-10-10 12:39:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:39:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:39:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:39:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1952MB allocated=1893MB
2025-10-10 12:39:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:39:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:39:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:39:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:39:07 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.076696, avg_loss=0.626917, seen=40, correct=29, accuracy=0.725000
2025-10-10 12:39:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:39:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:39:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:39:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1952MB allocated=1893MB
2025-10-10 12:39:08 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.725000
2025-10-10 12:39:17 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 12:39:17 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 12:39:17 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 12:39:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 12:39:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:39:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:39:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 12:39:19 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=127.440720, avg_loss=0.637204, seen=200, correct=121, accuracy=0.605000
2025-10-10 12:39:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:39:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:39:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:39:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1952MB allocated=1893MB
2025-10-10 12:39:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:39:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:39:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:39:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:39:25 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.682379, avg_loss=0.617059, seen=40, correct=27, accuracy=0.675000
2025-10-10 12:39:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:39:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:39:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:39:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1952MB allocated=1893MB
2025-10-10 12:39:27 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.725000, curr=0.675000
2025-10-10 12:39:37 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 12:39:37 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 12:39:37 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 12:39:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 12:39:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:39:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:39:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 12:39:40 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=126.359406, avg_loss=0.631797, seen=200, correct=132, accuracy=0.660000
2025-10-10 12:39:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:39:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:39:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:39:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1952MB allocated=1893MB
2025-10-10 12:39:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:39:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:39:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:39:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:39:44 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.524038, avg_loss=0.613101, seen=40, correct=28, accuracy=0.700000
2025-10-10 12:39:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:39:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:39:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:39:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1952MB allocated=1893MB
2025-10-10 12:39:47 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.725000, curr=0.700000
2025-10-10 12:39:55 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 12:39:55 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 12:39:55 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 12:39:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 12:39:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:39:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:39:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 12:39:58 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=127.272171, avg_loss=0.636361, seen=200, correct=126, accuracy=0.630000
2025-10-10 12:39:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:39:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:40:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:40:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1952MB allocated=1893MB
2025-10-10 12:40:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:40:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:40:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:40:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:40:03 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.986654, avg_loss=0.624666, seen=40, correct=28, accuracy=0.700000
2025-10-10 12:40:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:40:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:40:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:40:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1952MB allocated=1893MB
2025-10-10 12:40:05 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.725000, curr=0.700000
2025-10-10 12:40:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 12:40:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 12:40:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:40:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:40:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1952MB allocated=1893MB
2025-10-10 12:40:07 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #42', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 12:40:07 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #42', 'Round': 0, 'Results_raw': {}}
2025-10-10 12:40:07 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 12:40:08 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 12:40:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:40:08 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 12:40:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:40:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:40:10 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 12:40:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-10-10 12:40:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:40:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:40:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-10-10 12:40:13 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=170, loss_sum=114.258209, avg_loss=0.672107, seen=170, correct=99, accuracy=0.582353
2025-10-10 12:40:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:40:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:40:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:40:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1916MB allocated=1885MB
2025-10-10 12:40:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:40:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:40:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:40:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:40:17 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.206333, avg_loss=0.705158, seen=40, correct=22, accuracy=0.550000
2025-10-10 12:40:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:40:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:40:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:40:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1916MB allocated=1885MB
2025-10-10 12:40:19 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.550000
2025-10-10 12:40:19 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 12:40:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=812, total=3247)
2025-10-10 12:40:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:40:19 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 12:40:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:40:19 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 12:40:19 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=406, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 12:40:27 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 12:40:27 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 12:40:27 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 12:40:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-10-10 12:40:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:40:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:40:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-10-10 12:40:29 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=170, loss_sum=115.009613, avg_loss=0.676527, seen=170, correct=98, accuracy=0.576471
2025-10-10 12:40:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:40:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:40:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:40:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1940MB allocated=1902MB
2025-10-10 12:40:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:40:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:40:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:40:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:40:34 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.515747, avg_loss=0.687894, seen=40, correct=19, accuracy=0.475000
2025-10-10 12:40:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:40:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:40:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:40:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1940MB allocated=1902MB
2025-10-10 12:40:36 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.550000, curr=0.475000
2025-10-10 12:40:45 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 12:40:45 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 12:40:45 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 12:40:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-10-10 12:40:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:40:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:40:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-10-10 12:40:47 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=170, loss_sum=114.139397, avg_loss=0.671408, seen=170, correct=103, accuracy=0.605882
2025-10-10 12:40:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:40:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:40:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:40:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1940MB allocated=1902MB
2025-10-10 12:40:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:40:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:40:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:40:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:40:53 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.003183, avg_loss=0.700080, seen=40, correct=22, accuracy=0.550000
2025-10-10 12:40:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:40:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:40:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:40:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1940MB allocated=1902MB
2025-10-10 12:40:54 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.550000
2025-10-10 12:41:04 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 12:41:04 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 12:41:04 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 12:41:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-10-10 12:41:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:41:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:41:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-10-10 12:41:07 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=170, loss_sum=114.101715, avg_loss=0.671187, seen=170, correct=97, accuracy=0.570588
2025-10-10 12:41:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:41:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:41:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:41:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1940MB allocated=1902MB
2025-10-10 12:41:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:41:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:41:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:41:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:41:13 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.873943, avg_loss=0.696849, seen=40, correct=22, accuracy=0.550000
2025-10-10 12:41:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:41:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:41:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:41:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1940MB allocated=1902MB
2025-10-10 12:41:14 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.550000
2025-10-10 12:41:22 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 12:41:22 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 12:41:22 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 12:41:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-10-10 12:41:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:41:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:41:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-10-10 12:41:25 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=170, loss_sum=119.165276, avg_loss=0.700972, seen=170, correct=95, accuracy=0.558824
2025-10-10 12:41:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:41:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:41:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:41:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1940MB allocated=1902MB
2025-10-10 12:41:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:41:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:41:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:41:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:41:28 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.382645, avg_loss=0.684566, seen=40, correct=22, accuracy=0.550000
2025-10-10 12:41:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:41:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:41:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:41:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1940MB allocated=1902MB
2025-10-10 12:41:30 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.550000
2025-10-10 12:41:39 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 12:41:39 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 12:41:39 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 12:41:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-10-10 12:41:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:41:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:41:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-10-10 12:41:41 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=170, loss_sum=113.947586, avg_loss=0.670280, seen=170, correct=101, accuracy=0.594118
2025-10-10 12:41:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:41:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:41:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:41:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1940MB allocated=1902MB
2025-10-10 12:41:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:41:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:41:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:41:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:41:47 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.715872, avg_loss=0.692897, seen=40, correct=21, accuracy=0.525000
2025-10-10 12:41:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:41:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:41:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:41:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1940MB allocated=1902MB
2025-10-10 12:41:48 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.550000, curr=0.525000
2025-10-10 12:41:59 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 12:41:59 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 12:41:59 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 12:41:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-10-10 12:41:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:41:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:42:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-10-10 12:42:02 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=170, loss_sum=114.140137, avg_loss=0.671413, seen=170, correct=104, accuracy=0.611765
2025-10-10 12:42:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:42:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:42:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:42:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1940MB allocated=1902MB
2025-10-10 12:42:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:42:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:42:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:42:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:42:05 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.532627, avg_loss=0.713316, seen=40, correct=24, accuracy=0.600000
2025-10-10 12:42:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:42:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:42:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:42:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1940MB allocated=1902MB
2025-10-10 12:42:07 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-10 12:42:16 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 12:42:16 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 12:42:16 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 12:42:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-10-10 12:42:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:42:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:42:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-10-10 12:42:19 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=170, loss_sum=113.953987, avg_loss=0.670318, seen=170, correct=103, accuracy=0.605882
2025-10-10 12:42:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:42:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:42:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:42:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1940MB allocated=1902MB
2025-10-10 12:42:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:42:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:42:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:42:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:42:22 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.418409, avg_loss=0.710460, seen=40, correct=23, accuracy=0.575000
2025-10-10 12:42:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:42:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:42:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:42:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1940MB allocated=1902MB
2025-10-10 12:42:24 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.600000, curr=0.575000
2025-10-10 12:42:34 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 12:42:34 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 12:42:34 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 12:42:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-10-10 12:42:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:42:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:42:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-10-10 12:42:37 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=170, loss_sum=115.412880, avg_loss=0.678899, seen=170, correct=94, accuracy=0.552941
2025-10-10 12:42:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:42:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:42:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:42:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1940MB allocated=1902MB
2025-10-10 12:42:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:42:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:42:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:42:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:42:41 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.430464, avg_loss=0.685762, seen=40, correct=20, accuracy=0.500000
2025-10-10 12:42:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:42:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:42:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:42:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1940MB allocated=1902MB
2025-10-10 12:42:42 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.600000, curr=0.500000
2025-10-10 12:42:50 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 12:42:50 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 12:42:50 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 12:42:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-10-10 12:42:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:42:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:42:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-10-10 12:42:52 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=170, loss_sum=114.518578, avg_loss=0.673639, seen=170, correct=101, accuracy=0.594118
2025-10-10 12:42:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:42:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:42:56 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:42:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1940MB allocated=1902MB
2025-10-10 12:42:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:42:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:42:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:42:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:42:58 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.894016, avg_loss=0.697350, seen=40, correct=24, accuracy=0.600000
2025-10-10 12:42:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:42:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:43:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:43:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1940MB allocated=1902MB
2025-10-10 12:43:00 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-10 12:43:09 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 12:43:09 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 12:43:09 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 12:43:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-10-10 12:43:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:43:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:43:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-10-10 12:43:12 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=170, loss_sum=115.250038, avg_loss=0.677941, seen=170, correct=90, accuracy=0.529412
2025-10-10 12:43:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:43:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:43:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:43:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1940MB allocated=1902MB
2025-10-10 12:43:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:43:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:43:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:43:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:43:16 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.795446, avg_loss=0.719886, seen=40, correct=24, accuracy=0.600000
2025-10-10 12:43:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:43:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:43:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:43:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1940MB allocated=1902MB
2025-10-10 12:43:18 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-10 12:43:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 12:43:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 12:43:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:43:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:43:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1940MB allocated=1902MB
2025-10-10 12:43:20 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #30', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 12:43:20 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #30', 'Round': 0, 'Results_raw': {}}
2025-10-10 12:43:20 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 12:43:21 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 12:43:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:43:22 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 12:43:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:43:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:43:23 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 12:43:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-10-10 12:43:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:43:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:43:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-10-10 12:43:25 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=123, loss_sum=83.791626, avg_loss=0.681233, seen=123, correct=66, accuracy=0.536585
2025-10-10 12:43:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:43:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:43:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:43:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1916MB allocated=1893MB
2025-10-10 12:43:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:43:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:43:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:43:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:43:29 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.117613, avg_loss=0.727940, seen=40, correct=19, accuracy=0.475000
2025-10-10 12:43:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:43:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:43:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:43:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1916MB allocated=1893MB
2025-10-10 12:43:31 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.475000
2025-10-10 12:43:31 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 12:43:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=586, total=2342)
2025-10-10 12:43:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:43:32 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 12:43:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:43:32 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 12:43:32 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=293, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 12:43:41 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 12:43:41 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 12:43:41 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 12:43:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-10-10 12:43:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:43:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:43:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-10-10 12:43:43 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=123, loss_sum=84.367325, avg_loss=0.685913, seen=123, correct=62, accuracy=0.504065
2025-10-10 12:43:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:43:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:43:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:43:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1952MB allocated=1910MB
2025-10-10 12:43:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:43:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:43:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:43:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:43:48 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.672768, avg_loss=0.741819, seen=40, correct=16, accuracy=0.400000
2025-10-10 12:43:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:43:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:43:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:43:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1952MB allocated=1910MB
2025-10-10 12:43:49 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.475000, curr=0.400000
2025-10-10 12:43:58 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 12:43:58 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 12:43:59 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 12:43:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-10-10 12:43:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:43:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:44:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-10-10 12:44:01 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=123, loss_sum=83.816505, avg_loss=0.681435, seen=123, correct=68, accuracy=0.552846
2025-10-10 12:44:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:44:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:44:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:44:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1952MB allocated=1910MB
2025-10-10 12:44:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:44:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:44:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:44:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:44:04 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.580164, avg_loss=0.714504, seen=40, correct=18, accuracy=0.450000
2025-10-10 12:44:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:44:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:44:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:44:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1952MB allocated=1910MB
2025-10-10 12:44:07 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.475000, curr=0.450000
2025-10-10 12:44:16 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 12:44:16 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 12:44:16 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 12:44:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-10-10 12:44:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:44:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:44:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-10-10 12:44:18 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=123, loss_sum=83.692009, avg_loss=0.680423, seen=123, correct=65, accuracy=0.528455
2025-10-10 12:44:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:44:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:44:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:44:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1952MB allocated=1910MB
2025-10-10 12:44:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:44:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:44:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:44:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:44:21 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.358227, avg_loss=0.708956, seen=40, correct=19, accuracy=0.475000
2025-10-10 12:44:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:44:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:44:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:44:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1952MB allocated=1910MB
2025-10-10 12:44:23 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.475000
2025-10-10 12:44:32 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 12:44:32 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 12:44:32 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 12:44:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-10-10 12:44:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:44:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:44:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-10-10 12:44:35 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=123, loss_sum=83.714569, avg_loss=0.680606, seen=123, correct=76, accuracy=0.617886
2025-10-10 12:44:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:44:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:44:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:44:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1952MB allocated=1910MB
2025-10-10 12:44:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:44:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:44:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:44:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:44:38 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.445444, avg_loss=0.686136, seen=40, correct=20, accuracy=0.500000
2025-10-10 12:44:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:44:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:44:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:44:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1952MB allocated=1910MB
2025-10-10 12:44:40 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.500000
2025-10-10 12:44:47 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 12:44:47 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 12:44:47 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 12:44:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-10-10 12:44:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:44:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:44:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-10-10 12:44:49 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=123, loss_sum=83.380379, avg_loss=0.677889, seen=123, correct=68, accuracy=0.552846
2025-10-10 12:44:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:44:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:44:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:44:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1952MB allocated=1910MB
2025-10-10 12:44:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:44:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:44:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:44:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:44:54 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.328959, avg_loss=0.708224, seen=40, correct=19, accuracy=0.475000
2025-10-10 12:44:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:44:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:44:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:44:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1952MB allocated=1910MB
2025-10-10 12:44:56 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.500000, curr=0.475000
2025-10-10 12:45:06 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 12:45:06 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 12:45:06 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 12:45:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-10-10 12:45:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:45:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:45:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-10-10 12:45:08 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=123, loss_sum=83.624054, avg_loss=0.679870, seen=123, correct=64, accuracy=0.520325
2025-10-10 12:45:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:45:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:45:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:45:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1952MB allocated=1910MB
2025-10-10 12:45:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:45:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:45:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:45:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:45:12 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.251152, avg_loss=0.706279, seen=40, correct=21, accuracy=0.525000
2025-10-10 12:45:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:45:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:45:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:45:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1952MB allocated=1910MB
2025-10-10 12:45:15 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.525000
2025-10-10 12:45:25 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 12:45:25 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 12:45:25 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 12:45:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-10-10 12:45:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:45:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:45:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-10-10 12:45:27 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=123, loss_sum=83.123428, avg_loss=0.675800, seen=123, correct=71, accuracy=0.577236
2025-10-10 12:45:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:45:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:45:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:45:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1952MB allocated=1910MB
2025-10-10 12:45:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:45:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:45:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:45:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:45:31 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.651241, avg_loss=0.691281, seen=40, correct=22, accuracy=0.550000
2025-10-10 12:45:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:45:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:45:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:45:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1952MB allocated=1910MB
2025-10-10 12:45:32 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.550000
2025-10-10 12:45:43 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 12:45:43 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 12:45:43 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 12:45:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-10-10 12:45:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:45:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:45:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-10-10 12:45:45 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=123, loss_sum=83.466484, avg_loss=0.678589, seen=123, correct=81, accuracy=0.658537
2025-10-10 12:45:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:45:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:45:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:45:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1952MB allocated=1910MB
2025-10-10 12:45:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:45:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:45:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:45:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:45:48 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.758730, avg_loss=0.668968, seen=40, correct=24, accuracy=0.600000
2025-10-10 12:45:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:45:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:45:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:45:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1952MB allocated=1910MB
2025-10-10 12:45:49 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-10 12:45:58 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 12:45:58 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 12:45:58 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 12:45:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-10-10 12:45:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:45:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:46:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-10-10 12:46:00 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=123, loss_sum=82.861649, avg_loss=0.673672, seen=123, correct=75, accuracy=0.609756
2025-10-10 12:46:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:46:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:46:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:46:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1952MB allocated=1910MB
2025-10-10 12:46:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:46:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:46:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:46:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:46:04 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.280151, avg_loss=0.682004, seen=40, correct=21, accuracy=0.525000
2025-10-10 12:46:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:46:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:46:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:46:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1952MB allocated=1910MB
2025-10-10 12:46:07 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.600000, curr=0.525000
2025-10-10 12:46:14 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 12:46:14 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 12:46:14 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 12:46:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-10-10 12:46:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:46:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:46:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-10-10 12:46:16 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=123, loss_sum=83.650215, avg_loss=0.680083, seen=123, correct=76, accuracy=0.617886
2025-10-10 12:46:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:46:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:46:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:46:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1952MB allocated=1910MB
2025-10-10 12:46:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:46:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:46:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:46:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:46:20 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.843761, avg_loss=0.671094, seen=40, correct=24, accuracy=0.600000
2025-10-10 12:46:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:46:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:46:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:46:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1952MB allocated=1910MB
2025-10-10 12:46:22 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-10 12:46:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 12:46:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 12:46:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:46:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:46:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1952MB allocated=1910MB
2025-10-10 12:46:23 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #27', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 12:46:23 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #27', 'Round': 0, 'Results_raw': {}}
2025-10-10 12:46:23 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 12:46:24 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 12:46:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:46:25 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 12:46:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:46:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:46:26 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 12:46:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-10-10 12:46:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:46:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:46:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-10 12:46:27 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=14, loss_sum=13.411657, avg_loss=0.957976, seen=14, correct=7, accuracy=0.500000
2025-10-10 12:46:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:46:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:46:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:46:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1936MB allocated=1902MB
2025-10-10 12:46:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:46:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:46:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:46:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:46:31 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=32.425945, avg_loss=0.810649, seen=40, correct=25, accuracy=0.625000
2025-10-10 12:46:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:46:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:46:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:46:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1936MB allocated=1902MB
2025-10-10 12:46:32 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-10 12:46:32 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 12:46:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=72, total=285)
2025-10-10 12:46:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:46:33 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 12:46:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:46:33 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 12:46:33 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=36, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 12:46:43 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 12:46:43 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 12:46:43 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 12:46:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-10-10 12:46:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:46:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:46:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-10 12:46:44 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=14, loss_sum=13.625088, avg_loss=0.973221, seen=14, correct=7, accuracy=0.500000
2025-10-10 12:46:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:46:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:46:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:46:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1986MB allocated=1919MB
2025-10-10 12:46:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:46:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:46:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:46:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:46:48 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=31.468155, avg_loss=0.786704, seen=40, correct=27, accuracy=0.675000
2025-10-10 12:46:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:46:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:46:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:46:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1986MB allocated=1919MB
2025-10-10 12:46:50 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.675000
2025-10-10 12:46:57 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 12:46:57 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 12:46:57 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 12:46:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-10-10 12:46:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:46:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:46:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-10 12:46:58 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=14, loss_sum=13.527875, avg_loss=0.966277, seen=14, correct=8, accuracy=0.571429
2025-10-10 12:46:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:46:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:46:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:47:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1986MB allocated=1919MB
2025-10-10 12:47:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:47:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:47:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:47:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:47:02 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.565388, avg_loss=0.739135, seen=40, correct=30, accuracy=0.750000
2025-10-10 12:47:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:47:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:47:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:47:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1986MB allocated=1919MB
2025-10-10 12:47:04 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.750000
2025-10-10 12:47:13 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 12:47:13 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 12:47:13 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 12:47:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-10-10 12:47:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:47:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:47:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-10 12:47:14 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=14, loss_sum=13.602969, avg_loss=0.971641, seen=14, correct=7, accuracy=0.500000
2025-10-10 12:47:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:47:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:47:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:47:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1986MB allocated=1919MB
2025-10-10 12:47:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:47:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:47:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:47:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:47:18 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=33.111366, avg_loss=0.827784, seen=40, correct=24, accuracy=0.600000
2025-10-10 12:47:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:47:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:47:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:47:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1986MB allocated=1919MB
2025-10-10 12:47:20 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.750000, curr=0.600000
2025-10-10 12:47:28 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 12:47:28 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 12:47:28 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 12:47:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-10-10 12:47:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:47:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:47:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-10 12:47:29 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=14, loss_sum=13.019535, avg_loss=0.929967, seen=14, correct=8, accuracy=0.571429
2025-10-10 12:47:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:47:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:47:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:47:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1986MB allocated=1919MB
2025-10-10 12:47:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:47:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:47:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:47:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:47:32 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.763758, avg_loss=0.719094, seen=40, correct=30, accuracy=0.750000
2025-10-10 12:47:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:47:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:47:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:47:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1986MB allocated=1919MB
2025-10-10 12:47:34 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.750000
2025-10-10 12:47:43 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 12:47:43 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 12:47:43 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 12:47:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-10-10 12:47:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:47:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:47:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-10 12:47:44 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=14, loss_sum=13.018221, avg_loss=0.929873, seen=14, correct=7, accuracy=0.500000
2025-10-10 12:47:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:47:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:47:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:47:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1986MB allocated=1919MB
2025-10-10 12:47:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:47:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:47:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:47:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:47:47 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=31.463341, avg_loss=0.786584, seen=40, correct=29, accuracy=0.725000
2025-10-10 12:47:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:47:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:47:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:47:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1986MB allocated=1919MB
2025-10-10 12:47:50 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.750000, curr=0.725000
2025-10-10 12:47:58 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 12:47:58 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 12:47:58 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 12:47:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-10-10 12:47:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:47:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:47:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-10 12:47:59 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=14, loss_sum=12.610425, avg_loss=0.900745, seen=14, correct=9, accuracy=0.642857
2025-10-10 12:47:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:47:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:48:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:48:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1986MB allocated=1919MB
2025-10-10 12:48:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:48:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:48:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:48:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:48:02 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.568077, avg_loss=0.739202, seen=40, correct=28, accuracy=0.700000
2025-10-10 12:48:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:48:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:48:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:48:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1986MB allocated=1919MB
2025-10-10 12:48:04 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.750000, curr=0.700000
2025-10-10 12:48:12 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 12:48:12 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 12:48:12 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 12:48:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-10-10 12:48:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:48:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:48:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-10 12:48:13 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=14, loss_sum=12.784166, avg_loss=0.913155, seen=14, correct=8, accuracy=0.571429
2025-10-10 12:48:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:48:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:48:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:48:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1986MB allocated=1919MB
2025-10-10 12:48:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:48:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:48:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:48:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:48:18 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.276012, avg_loss=0.756900, seen=40, correct=28, accuracy=0.700000
2025-10-10 12:48:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:48:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:48:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:48:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1986MB allocated=1919MB
2025-10-10 12:48:19 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.750000, curr=0.700000
2025-10-10 12:48:28 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 12:48:28 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 12:48:28 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 12:48:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-10-10 12:48:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:48:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:48:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-10 12:48:29 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=14, loss_sum=12.443321, avg_loss=0.888809, seen=14, correct=8, accuracy=0.571429
2025-10-10 12:48:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:48:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:48:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:48:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1986MB allocated=1919MB
2025-10-10 12:48:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:48:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:48:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:48:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:48:33 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=31.328819, avg_loss=0.783220, seen=40, correct=29, accuracy=0.725000
2025-10-10 12:48:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:48:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:48:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:48:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1986MB allocated=1919MB
2025-10-10 12:48:34 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/25), best=0.750000, curr=0.725000
2025-10-10 12:48:42 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 12:48:42 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 12:48:42 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 12:48:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-10-10 12:48:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:48:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:48:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-10 12:48:43 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=14, loss_sum=12.557358, avg_loss=0.896954, seen=14, correct=10, accuracy=0.714286
2025-10-10 12:48:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:48:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:48:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:48:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1986MB allocated=1919MB
2025-10-10 12:48:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:48:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:48:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:48:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:48:47 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.507614, avg_loss=0.737690, seen=40, correct=28, accuracy=0.700000
2025-10-10 12:48:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:48:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:48:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:48:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1986MB allocated=1919MB
2025-10-10 12:48:48 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/25), best=0.750000, curr=0.700000
2025-10-10 12:48:57 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 12:48:57 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 12:48:57 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 12:48:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-10-10 12:48:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:48:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:48:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-10 12:48:58 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=14, loss_sum=13.194372, avg_loss=0.942455, seen=14, correct=8, accuracy=0.571429
2025-10-10 12:48:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:48:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:49:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:49:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1986MB allocated=1919MB
2025-10-10 12:49:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:49:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:49:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:49:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:49:03 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=32.077980, avg_loss=0.801950, seen=40, correct=28, accuracy=0.700000
2025-10-10 12:49:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:49:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:49:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:49:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1986MB allocated=1919MB
2025-10-10 12:49:04 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/25), best=0.750000, curr=0.700000
2025-10-10 12:49:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 12:49:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 12:49:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:49:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:49:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1986MB allocated=1919MB
2025-10-10 12:49:05 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #5', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 12:49:05 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #5', 'Round': 0, 'Results_raw': {}}
2025-10-10 12:49:05 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 12:49:06 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 12:49:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:49:08 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 12:49:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:49:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:49:09 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 12:49:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-10-10 12:49:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:49:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:49:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-10 12:49:10 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=32, loss_sum=21.704006, avg_loss=0.678250, seen=32, correct=18, accuracy=0.562500
2025-10-10 12:49:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:49:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:49:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:49:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1936MB allocated=1910MB
2025-10-10 12:49:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:49:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:49:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:49:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:49:13 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.896837, avg_loss=0.772421, seen=40, correct=19, accuracy=0.475000
2025-10-10 12:49:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:49:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:49:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:49:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1936MB allocated=1910MB
2025-10-10 12:49:14 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.475000
2025-10-10 12:49:15 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 12:49:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=156, total=621)
2025-10-10 12:49:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:49:15 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 12:49:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:49:15 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 12:49:15 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=78, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 12:49:24 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 12:49:24 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 12:49:24 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 12:49:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-10-10 12:49:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:49:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:49:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-10 12:49:24 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=32, loss_sum=21.676331, avg_loss=0.677385, seen=32, correct=20, accuracy=0.625000
2025-10-10 12:49:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:49:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:49:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:49:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1976MB allocated=1927MB
2025-10-10 12:49:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:49:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:49:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:49:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:49:27 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=31.217094, avg_loss=0.780427, seen=40, correct=20, accuracy=0.500000
2025-10-10 12:49:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:49:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:49:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:49:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1976MB allocated=1927MB
2025-10-10 12:49:29 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.500000
2025-10-10 12:49:38 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 12:49:38 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 12:49:38 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 12:49:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-10-10 12:49:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:49:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:49:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-10 12:49:38 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=32, loss_sum=21.911079, avg_loss=0.684721, seen=32, correct=20, accuracy=0.625000
2025-10-10 12:49:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:49:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:49:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:49:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1976MB allocated=1927MB
2025-10-10 12:49:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:49:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:49:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:49:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:49:43 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.467854, avg_loss=0.711696, seen=40, correct=20, accuracy=0.500000
2025-10-10 12:49:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:49:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:49:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:49:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1976MB allocated=1927MB
2025-10-10 12:49:44 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.500000
2025-10-10 12:49:53 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 12:49:53 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 12:49:54 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 12:49:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-10-10 12:49:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:49:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:49:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-10 12:49:54 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=32, loss_sum=22.415382, avg_loss=0.700481, seen=32, correct=18, accuracy=0.562500
2025-10-10 12:49:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:49:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:49:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:49:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1976MB allocated=1927MB
2025-10-10 12:49:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:49:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:49:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:49:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:49:58 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=32.938362, avg_loss=0.823459, seen=40, correct=16, accuracy=0.400000
2025-10-10 12:49:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:49:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:50:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:50:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1976MB allocated=1927MB
2025-10-10 12:50:00 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.500000, curr=0.400000
2025-10-10 12:50:08 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 12:50:08 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 12:50:08 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 12:50:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-10-10 12:50:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:50:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:50:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-10 12:50:09 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=32, loss_sum=21.542099, avg_loss=0.673191, seen=32, correct=21, accuracy=0.656250
2025-10-10 12:50:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:50:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:50:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:50:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1976MB allocated=1927MB
2025-10-10 12:50:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:50:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:50:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:50:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:50:13 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.283211, avg_loss=0.707080, seen=40, correct=19, accuracy=0.475000
2025-10-10 12:50:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:50:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:50:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:50:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1976MB allocated=1927MB
2025-10-10 12:50:15 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.500000, curr=0.475000
2025-10-10 12:50:25 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 12:50:25 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 12:50:25 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 12:50:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-10-10 12:50:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:50:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:50:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-10 12:50:26 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=32, loss_sum=21.447392, avg_loss=0.670231, seen=32, correct=22, accuracy=0.687500
2025-10-10 12:50:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:50:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:50:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:50:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1976MB allocated=1927MB
2025-10-10 12:50:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:50:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:50:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:50:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:50:31 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.993059, avg_loss=0.699826, seen=40, correct=19, accuracy=0.475000
2025-10-10 12:50:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:50:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:50:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:50:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1976MB allocated=1927MB
2025-10-10 12:50:32 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.500000, curr=0.475000
2025-10-10 12:50:42 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 12:50:42 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 12:50:42 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 12:50:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-10-10 12:50:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:50:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:50:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-10 12:50:42 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=32, loss_sum=21.446505, avg_loss=0.670203, seen=32, correct=21, accuracy=0.656250
2025-10-10 12:50:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:50:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:50:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:50:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1976MB allocated=1927MB
2025-10-10 12:50:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:50:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:50:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:50:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:50:47 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.901993, avg_loss=0.697550, seen=40, correct=19, accuracy=0.475000
2025-10-10 12:50:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:50:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:50:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:50:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1976MB allocated=1927MB
2025-10-10 12:50:49 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/25), best=0.500000, curr=0.475000
2025-10-10 12:50:56 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 12:50:56 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 12:50:56 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 12:50:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-10-10 12:50:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:50:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:50:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-10 12:50:57 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=32, loss_sum=22.487785, avg_loss=0.702743, seen=32, correct=17, accuracy=0.531250
2025-10-10 12:50:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:50:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:50:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:50:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1976MB allocated=1927MB
2025-10-10 12:51:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:51:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:51:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:51:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:51:01 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=31.002827, avg_loss=0.775071, seen=40, correct=18, accuracy=0.450000
2025-10-10 12:51:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:51:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:51:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:51:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1976MB allocated=1927MB
2025-10-10 12:51:03 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/25), best=0.500000, curr=0.450000
2025-10-10 12:51:11 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 12:51:11 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 12:51:11 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 12:51:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-10-10 12:51:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:51:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:51:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-10 12:51:12 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=32, loss_sum=21.561243, avg_loss=0.673789, seen=32, correct=20, accuracy=0.625000
2025-10-10 12:51:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:51:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:51:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:51:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1976MB allocated=1927MB
2025-10-10 12:51:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:51:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:51:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:51:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:51:17 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.410603, avg_loss=0.685265, seen=40, correct=19, accuracy=0.475000
2025-10-10 12:51:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:51:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:51:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:51:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1976MB allocated=1927MB
2025-10-10 12:51:19 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/25), best=0.500000, curr=0.475000
2025-10-10 12:51:29 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 12:51:29 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 12:51:29 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 12:51:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-10-10 12:51:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:51:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:51:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-10 12:51:29 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=32, loss_sum=21.711491, avg_loss=0.678484, seen=32, correct=20, accuracy=0.625000
2025-10-10 12:51:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:51:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:51:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:51:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1976MB allocated=1927MB
2025-10-10 12:51:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:51:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:51:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:51:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:51:32 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.406761, avg_loss=0.710169, seen=40, correct=20, accuracy=0.500000
2025-10-10 12:51:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:51:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:51:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:51:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1976MB allocated=1927MB
2025-10-10 12:51:34 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.500000
2025-10-10 12:51:42 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 12:51:42 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 12:51:42 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 12:51:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-10-10 12:51:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:51:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:51:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-10 12:51:42 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=32, loss_sum=21.453547, avg_loss=0.670423, seen=32, correct=19, accuracy=0.593750
2025-10-10 12:51:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:51:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:51:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:51:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1976MB allocated=1927MB
2025-10-10 12:51:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:51:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:51:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:51:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:51:46 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.132650, avg_loss=0.703316, seen=40, correct=19, accuracy=0.475000
2025-10-10 12:51:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:51:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:51:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:51:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1976MB allocated=1927MB
2025-10-10 12:51:48 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.500000, curr=0.475000
2025-10-10 12:51:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 12:51:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 12:51:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:51:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:51:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1976MB allocated=1927MB
2025-10-10 12:51:49 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #11', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 12:51:49 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #11', 'Round': 0, 'Results_raw': {}}
2025-10-10 12:51:49 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 12:51:50 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 12:51:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:51:51 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 12:51:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:51:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:51:53 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 12:51:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-10-10 12:51:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:51:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:51:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-10 12:51:55 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=75, loss_sum=48.729263, avg_loss=0.649724, seen=75, correct=47, accuracy=0.626667
2025-10-10 12:51:55 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:51:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:51:56 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:51:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1956MB allocated=1919MB
2025-10-10 12:51:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:51:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:51:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:51:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:51:59 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.122265, avg_loss=0.628057, seen=40, correct=21, accuracy=0.525000
2025-10-10 12:51:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:51:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:51:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:52:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1956MB allocated=1919MB
2025-10-10 12:52:01 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.525000
2025-10-10 12:52:01 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 12:52:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=359, total=1434)
2025-10-10 12:52:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:52:02 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 12:52:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:52:02 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 12:52:02 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=180, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 12:52:12 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 12:52:12 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 12:52:12 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 12:52:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-10-10 12:52:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:52:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:52:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-10 12:52:13 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=75, loss_sum=49.921074, avg_loss=0.665614, seen=75, correct=47, accuracy=0.626667
2025-10-10 12:52:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:52:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:52:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:52:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1978MB allocated=1935MB
2025-10-10 12:52:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:52:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:52:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:52:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:52:18 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.016359, avg_loss=0.625409, seen=40, correct=23, accuracy=0.575000
2025-10-10 12:52:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:52:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:52:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:52:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1978MB allocated=1935MB
2025-10-10 12:52:19 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-10 12:52:29 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 12:52:29 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 12:52:29 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 12:52:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-10-10 12:52:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:52:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:52:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-10 12:52:30 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=75, loss_sum=49.837299, avg_loss=0.664497, seen=75, correct=46, accuracy=0.613333
2025-10-10 12:52:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:52:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:52:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:52:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1978MB allocated=1935MB
2025-10-10 12:52:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:52:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:52:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:52:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:52:35 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.375895, avg_loss=0.609397, seen=40, correct=23, accuracy=0.575000
2025-10-10 12:52:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:52:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:52:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:52:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1978MB allocated=1935MB
2025-10-10 12:52:36 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-10 12:52:46 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 12:52:46 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 12:52:46 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 12:52:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-10-10 12:52:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:52:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:52:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-10 12:52:47 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=75, loss_sum=49.761196, avg_loss=0.663483, seen=75, correct=47, accuracy=0.626667
2025-10-10 12:52:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:52:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:52:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:52:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1978MB allocated=1935MB
2025-10-10 12:52:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:52:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:52:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:52:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:52:52 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.192232, avg_loss=0.629806, seen=40, correct=24, accuracy=0.600000
2025-10-10 12:52:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:52:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:52:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:52:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1978MB allocated=1935MB
2025-10-10 12:52:54 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-10 12:53:02 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 12:53:02 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 12:53:02 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 12:53:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-10-10 12:53:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:53:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:53:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-10 12:53:04 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=75, loss_sum=48.686897, avg_loss=0.649159, seen=75, correct=45, accuracy=0.600000
2025-10-10 12:53:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:53:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:53:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:53:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1978MB allocated=1935MB
2025-10-10 12:53:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:53:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:53:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:53:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:53:07 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.884054, avg_loss=0.647101, seen=40, correct=21, accuracy=0.525000
2025-10-10 12:53:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:53:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:53:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:53:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1978MB allocated=1935MB
2025-10-10 12:53:09 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.600000, curr=0.525000
2025-10-10 12:53:20 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 12:53:20 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 12:53:20 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 12:53:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-10-10 12:53:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:53:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:53:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-10 12:53:21 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=75, loss_sum=48.008610, avg_loss=0.640115, seen=75, correct=44, accuracy=0.586667
2025-10-10 12:53:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:53:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:53:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:53:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1978MB allocated=1935MB
2025-10-10 12:53:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:53:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:53:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:53:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:53:25 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.319733, avg_loss=0.632993, seen=40, correct=23, accuracy=0.575000
2025-10-10 12:53:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:53:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:53:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:53:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1978MB allocated=1935MB
2025-10-10 12:53:27 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.600000, curr=0.575000
2025-10-10 12:53:37 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 12:53:37 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 12:53:37 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 12:53:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-10-10 12:53:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:53:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:53:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-10 12:53:38 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=75, loss_sum=49.249847, avg_loss=0.656665, seen=75, correct=46, accuracy=0.613333
2025-10-10 12:53:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:53:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:53:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:53:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1978MB allocated=1935MB
2025-10-10 12:53:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:53:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:53:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:53:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:53:43 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.739414, avg_loss=0.618485, seen=40, correct=23, accuracy=0.575000
2025-10-10 12:53:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:53:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:53:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:53:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1978MB allocated=1935MB
2025-10-10 12:53:45 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.600000, curr=0.575000
2025-10-10 12:53:56 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 12:53:56 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 12:53:56 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 12:53:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-10-10 12:53:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:53:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:53:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-10 12:53:57 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=75, loss_sum=48.729259, avg_loss=0.649723, seen=75, correct=46, accuracy=0.613333
2025-10-10 12:53:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:53:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:53:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:53:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1978MB allocated=1935MB
2025-10-10 12:53:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:53:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:54:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:54:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:54:00 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.670889, avg_loss=0.616772, seen=40, correct=23, accuracy=0.575000
2025-10-10 12:54:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:54:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:54:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:54:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1978MB allocated=1935MB
2025-10-10 12:54:02 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/25), best=0.600000, curr=0.575000
2025-10-10 12:54:11 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 12:54:11 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 12:54:11 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 12:54:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-10-10 12:54:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:54:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:54:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-10 12:54:13 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=75, loss_sum=47.837296, avg_loss=0.637831, seen=75, correct=46, accuracy=0.613333
2025-10-10 12:54:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:54:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:54:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:54:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1978MB allocated=1935MB
2025-10-10 12:54:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:54:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:54:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:54:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:54:17 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.961899, avg_loss=0.649047, seen=40, correct=23, accuracy=0.575000
2025-10-10 12:54:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:54:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:54:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:54:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1978MB allocated=1935MB
2025-10-10 12:54:20 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/25), best=0.600000, curr=0.575000
2025-10-10 12:54:30 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 12:54:30 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 12:54:30 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 12:54:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-10-10 12:54:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:54:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:54:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-10 12:54:31 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=75, loss_sum=47.903503, avg_loss=0.638713, seen=75, correct=48, accuracy=0.640000
2025-10-10 12:54:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:54:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:54:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:54:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1978MB allocated=1935MB
2025-10-10 12:54:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:54:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:54:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:54:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:54:34 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.861046, avg_loss=0.621526, seen=40, correct=23, accuracy=0.575000
2025-10-10 12:54:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:54:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:54:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:54:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1978MB allocated=1935MB
2025-10-10 12:54:35 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/25), best=0.600000, curr=0.575000
2025-10-10 12:54:45 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 12:54:45 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 12:54:45 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 12:54:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-10-10 12:54:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:54:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:54:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-10 12:54:47 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=75, loss_sum=47.992828, avg_loss=0.639904, seen=75, correct=47, accuracy=0.626667
2025-10-10 12:54:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:54:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:54:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:54:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1978MB allocated=1935MB
2025-10-10 12:54:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:54:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:54:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:54:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:54:52 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.793468, avg_loss=0.619837, seen=40, correct=24, accuracy=0.600000
2025-10-10 12:54:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:54:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:54:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:54:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1978MB allocated=1935MB
2025-10-10 12:54:54 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-10 12:54:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 12:54:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 12:54:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:54:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:54:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1978MB allocated=1935MB
2025-10-10 12:54:55 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #28', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 12:54:55 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #28', 'Round': 0, 'Results_raw': {}}
2025-10-10 12:54:55 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 12:54:56 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 12:54:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:54:56 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 12:54:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:54:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:54:58 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 12:54:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 12:54:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:54:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:55:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 12:55:02 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=131.247162, avg_loss=0.656236, seen=200, correct=125, accuracy=0.625000
2025-10-10 12:55:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:55:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:55:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:55:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1956MB allocated=1927MB
2025-10-10 12:55:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:55:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:55:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:55:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:55:07 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.538971, avg_loss=0.688474, seen=40, correct=26, accuracy=0.650000
2025-10-10 12:55:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:55:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:55:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:55:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1956MB allocated=1927MB
2025-10-10 12:55:08 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.650000
2025-10-10 12:55:08 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 12:55:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1698, total=6791)
2025-10-10 12:55:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:55:09 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 12:55:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:55:09 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 12:55:09 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=849, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 12:55:19 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 12:55:19 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 12:55:19 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 12:55:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 12:55:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:55:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:55:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 12:55:22 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=130.259888, avg_loss=0.651299, seen=200, correct=123, accuracy=0.615000
2025-10-10 12:55:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:55:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:55:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:55:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1944MB
2025-10-10 12:55:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:55:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:55:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:55:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:55:27 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.607990, avg_loss=0.690200, seen=40, correct=25, accuracy=0.625000
2025-10-10 12:55:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:55:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:55:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:55:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1944MB
2025-10-10 12:55:28 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.650000, curr=0.625000
2025-10-10 12:55:38 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 12:55:38 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 12:55:38 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 12:55:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 12:55:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:55:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:55:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 12:55:41 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=132.191788, avg_loss=0.660959, seen=200, correct=117, accuracy=0.585000
2025-10-10 12:55:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:55:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:55:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:55:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1944MB
2025-10-10 12:55:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:55:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:55:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:55:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:55:46 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.915657, avg_loss=0.697891, seen=40, correct=26, accuracy=0.650000
2025-10-10 12:55:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:55:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:55:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:55:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1944MB
2025-10-10 12:55:47 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.650000
2025-10-10 12:55:57 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 12:55:57 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 12:55:57 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 12:55:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 12:55:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:55:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:56:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 12:56:00 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=130.192261, avg_loss=0.650961, seen=200, correct=120, accuracy=0.600000
2025-10-10 12:56:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:56:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:56:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:56:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1944MB
2025-10-10 12:56:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:56:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:56:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:56:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:56:05 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.822659, avg_loss=0.695566, seen=40, correct=23, accuracy=0.575000
2025-10-10 12:56:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:56:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:56:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:56:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1944MB
2025-10-10 12:56:08 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.650000, curr=0.575000
2025-10-10 12:56:17 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 12:56:17 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 12:56:17 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 12:56:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 12:56:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:56:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:56:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 12:56:20 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=130.368683, avg_loss=0.651843, seen=200, correct=122, accuracy=0.610000
2025-10-10 12:56:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:56:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:56:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:56:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1944MB
2025-10-10 12:56:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:56:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:56:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:56:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:56:23 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.991116, avg_loss=0.699778, seen=40, correct=26, accuracy=0.650000
2025-10-10 12:56:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:56:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:56:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:56:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1944MB
2025-10-10 12:56:24 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.650000
2025-10-10 12:56:33 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 12:56:33 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 12:56:33 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 12:56:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 12:56:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:56:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:56:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 12:56:36 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=130.436264, avg_loss=0.652181, seen=200, correct=124, accuracy=0.620000
2025-10-10 12:56:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:56:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:56:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:56:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1944MB
2025-10-10 12:56:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:56:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:56:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:56:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:56:41 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.656101, avg_loss=0.691403, seen=40, correct=26, accuracy=0.650000
2025-10-10 12:56:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:56:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:56:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:56:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1944MB
2025-10-10 12:56:42 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.650000
2025-10-10 12:56:52 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 12:56:52 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 12:56:52 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 12:56:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 12:56:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:56:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:56:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 12:56:55 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=130.671158, avg_loss=0.653356, seen=200, correct=118, accuracy=0.590000
2025-10-10 12:56:55 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:56:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:56:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:56:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1944MB
2025-10-10 12:56:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:56:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:56:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:56:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:56:59 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.698635, avg_loss=0.692466, seen=40, correct=23, accuracy=0.575000
2025-10-10 12:56:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:56:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:57:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:57:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1944MB
2025-10-10 12:57:01 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.650000, curr=0.575000
2025-10-10 12:57:13 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 12:57:13 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 12:57:13 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 12:57:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 12:57:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:57:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:57:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 12:57:16 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=130.192322, avg_loss=0.650962, seen=200, correct=121, accuracy=0.605000
2025-10-10 12:57:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:57:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:57:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:57:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1944MB
2025-10-10 12:57:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:57:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:57:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:57:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:57:22 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.342293, avg_loss=0.683557, seen=40, correct=27, accuracy=0.675000
2025-10-10 12:57:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:57:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:57:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:57:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1944MB
2025-10-10 12:57:24 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.675000
2025-10-10 12:57:33 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 12:57:33 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 12:57:33 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 12:57:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 12:57:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:57:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:57:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 12:57:36 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=131.926773, avg_loss=0.659634, seen=200, correct=125, accuracy=0.625000
2025-10-10 12:57:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:57:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:57:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:57:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1944MB
2025-10-10 12:57:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:57:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:57:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:57:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:57:40 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.023922, avg_loss=0.700598, seen=40, correct=22, accuracy=0.550000
2025-10-10 12:57:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:57:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:57:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:57:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1944MB
2025-10-10 12:57:43 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.675000, curr=0.550000
2025-10-10 12:57:52 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 12:57:52 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 12:57:52 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 12:57:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 12:57:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:57:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:57:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 12:57:55 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=131.804718, avg_loss=0.659024, seen=200, correct=126, accuracy=0.630000
2025-10-10 12:57:55 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:57:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:57:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:57:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1944MB
2025-10-10 12:57:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:57:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:57:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:58:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:58:00 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.042202, avg_loss=0.701055, seen=40, correct=21, accuracy=0.525000
2025-10-10 12:58:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:58:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:58:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:58:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1944MB
2025-10-10 12:58:01 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.675000, curr=0.525000
2025-10-10 12:58:12 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 12:58:12 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 12:58:12 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 12:58:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 12:58:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:58:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:58:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 12:58:15 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=131.693863, avg_loss=0.658469, seen=200, correct=124, accuracy=0.620000
2025-10-10 12:58:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:58:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:58:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:58:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1944MB
2025-10-10 12:58:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:58:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:58:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:58:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:58:19 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.780207, avg_loss=0.694505, seen=40, correct=20, accuracy=0.500000
2025-10-10 12:58:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:58:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:58:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:58:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1944MB
2025-10-10 12:58:21 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.675000, curr=0.500000
2025-10-10 12:58:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 12:58:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 12:58:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:58:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:58:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1944MB
2025-10-10 12:58:22 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #53', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 12:58:22 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #53', 'Round': 0, 'Results_raw': {}}
2025-10-10 12:58:22 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 12:58:23 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 12:58:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:58:23 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 12:58:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:58:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:58:26 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 12:58:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-10-10 12:58:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:58:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:58:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-10-10 12:58:29 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=193, loss_sum=143.880493, avg_loss=0.745495, seen=193, correct=83, accuracy=0.430052
2025-10-10 12:58:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:58:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:58:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:58:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1976MB allocated=1935MB
2025-10-10 12:58:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:58:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:58:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:58:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:58:33 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.184937, avg_loss=0.704623, seen=40, correct=23, accuracy=0.575000
2025-10-10 12:58:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:58:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:58:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:58:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1976MB allocated=1935MB
2025-10-10 12:58:35 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-10 12:58:35 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 12:58:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=920, total=3679)
2025-10-10 12:58:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:58:36 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 12:58:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:58:36 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 12:58:36 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=460, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 12:58:45 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 12:58:45 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 12:58:46 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 12:58:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-10-10 12:58:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:58:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:58:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-10-10 12:58:48 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=193, loss_sum=135.703537, avg_loss=0.703127, seen=193, correct=88, accuracy=0.455959
2025-10-10 12:58:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:58:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:58:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:58:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2008MB allocated=1952MB
2025-10-10 12:58:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:58:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:58:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:58:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:58:53 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.643021, avg_loss=0.691076, seen=40, correct=21, accuracy=0.525000
2025-10-10 12:58:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:58:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:58:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:58:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2008MB allocated=1952MB
2025-10-10 12:58:55 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.575000, curr=0.525000
2025-10-10 12:59:03 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 12:59:03 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 12:59:03 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 12:59:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-10-10 12:59:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:59:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:59:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-10-10 12:59:06 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=193, loss_sum=132.411835, avg_loss=0.686072, seen=193, correct=109, accuracy=0.564767
2025-10-10 12:59:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:59:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:59:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:59:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2008MB allocated=1952MB
2025-10-10 12:59:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:59:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:59:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:59:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:59:11 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.289267, avg_loss=0.682232, seen=40, correct=24, accuracy=0.600000
2025-10-10 12:59:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:59:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:59:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:59:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2008MB allocated=1952MB
2025-10-10 12:59:13 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-10 12:59:21 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 12:59:21 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 12:59:21 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 12:59:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-10-10 12:59:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:59:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:59:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-10-10 12:59:24 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=193, loss_sum=136.746826, avg_loss=0.708533, seen=193, correct=86, accuracy=0.445596
2025-10-10 12:59:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:59:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:59:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:59:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2008MB allocated=1952MB
2025-10-10 12:59:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:59:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:59:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:59:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:59:28 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.338989, avg_loss=0.683475, seen=40, correct=26, accuracy=0.650000
2025-10-10 12:59:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:59:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:59:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:59:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2008MB allocated=1952MB
2025-10-10 12:59:30 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.650000
2025-10-10 12:59:39 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 12:59:39 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 12:59:39 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 12:59:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-10-10 12:59:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:59:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:59:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-10-10 12:59:42 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=193, loss_sum=134.713440, avg_loss=0.697997, seen=193, correct=95, accuracy=0.492228
2025-10-10 12:59:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:59:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:59:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:59:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2008MB allocated=1952MB
2025-10-10 12:59:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 12:59:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:59:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:59:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 12:59:46 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.209396, avg_loss=0.680235, seen=40, correct=23, accuracy=0.575000
2025-10-10 12:59:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:59:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:59:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 12:59:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2008MB allocated=1952MB
2025-10-10 12:59:47 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.650000, curr=0.575000
2025-10-10 12:59:56 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 12:59:56 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 12:59:56 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 12:59:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-10-10 12:59:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 12:59:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 12:59:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-10-10 12:59:59 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=193, loss_sum=131.463104, avg_loss=0.681156, seen=193, correct=113, accuracy=0.585492
2025-10-10 12:59:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 12:59:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:00:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:00:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2008MB allocated=1952MB
2025-10-10 13:00:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:00:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:00:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:00:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:00:03 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.388357, avg_loss=0.684709, seen=40, correct=20, accuracy=0.500000
2025-10-10 13:00:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:00:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:00:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:00:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2008MB allocated=1952MB
2025-10-10 13:00:06 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.650000, curr=0.500000
2025-10-10 13:00:15 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 13:00:15 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 13:00:15 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 13:00:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-10-10 13:00:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:00:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:00:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-10-10 13:00:18 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=193, loss_sum=132.530167, avg_loss=0.686685, seen=193, correct=112, accuracy=0.580311
2025-10-10 13:00:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:00:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:00:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:00:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2008MB allocated=1952MB
2025-10-10 13:00:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:00:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:00:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:00:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:00:22 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.993099, avg_loss=0.674827, seen=40, correct=22, accuracy=0.550000
2025-10-10 13:00:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:00:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:00:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:00:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2008MB allocated=1952MB
2025-10-10 13:00:23 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.650000, curr=0.550000
2025-10-10 13:00:31 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 13:00:31 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 13:00:31 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 13:00:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-10-10 13:00:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:00:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:00:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-10-10 13:00:34 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=193, loss_sum=138.260941, avg_loss=0.716378, seen=193, correct=81, accuracy=0.419689
2025-10-10 13:00:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:00:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:00:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:00:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2008MB allocated=1952MB
2025-10-10 13:00:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:00:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:00:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:00:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:00:38 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.049032, avg_loss=0.676226, seen=40, correct=22, accuracy=0.550000
2025-10-10 13:00:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:00:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:00:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:00:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2008MB allocated=1952MB
2025-10-10 13:00:40 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/25), best=0.650000, curr=0.550000
2025-10-10 13:00:49 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 13:00:49 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 13:00:49 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 13:00:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-10-10 13:00:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:00:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:00:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-10-10 13:00:52 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=193, loss_sum=136.429291, avg_loss=0.706888, seen=193, correct=98, accuracy=0.507772
2025-10-10 13:00:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:00:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:00:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:00:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2008MB allocated=1952MB
2025-10-10 13:00:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:00:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:00:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:00:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:00:56 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.615875, avg_loss=0.665397, seen=40, correct=24, accuracy=0.600000
2025-10-10 13:00:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:00:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:00:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:00:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2008MB allocated=1952MB
2025-10-10 13:00:57 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/25), best=0.650000, curr=0.600000
2025-10-10 13:01:06 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 13:01:06 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 13:01:06 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 13:01:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-10-10 13:01:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:01:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:01:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-10-10 13:01:09 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=193, loss_sum=132.218903, avg_loss=0.685072, seen=193, correct=110, accuracy=0.569948
2025-10-10 13:01:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:01:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:01:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:01:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2008MB allocated=1952MB
2025-10-10 13:01:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:01:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:01:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:01:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:01:15 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.918262, avg_loss=0.672957, seen=40, correct=22, accuracy=0.550000
2025-10-10 13:01:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:01:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:01:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:01:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2008MB allocated=1952MB
2025-10-10 13:01:16 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/25), best=0.650000, curr=0.550000
2025-10-10 13:01:25 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 13:01:25 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 13:01:25 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 13:01:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-10-10 13:01:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:01:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:01:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-10-10 13:01:28 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=193, loss_sum=132.800690, avg_loss=0.688086, seen=193, correct=108, accuracy=0.559585
2025-10-10 13:01:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:01:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:01:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:01:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2008MB allocated=1952MB
2025-10-10 13:01:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:01:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:01:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:01:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:01:31 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.967731, avg_loss=0.674193, seen=40, correct=22, accuracy=0.550000
2025-10-10 13:01:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:01:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:01:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:01:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2008MB allocated=1952MB
2025-10-10 13:01:34 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/25), best=0.650000, curr=0.550000
2025-10-10 13:01:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 13:01:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 13:01:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:01:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:01:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2008MB allocated=1952MB
2025-10-10 13:01:35 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #31', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 13:01:35 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #31', 'Round': 0, 'Results_raw': {}}
2025-10-10 13:01:35 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 13:01:36 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 13:01:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:01:37 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 13:01:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:01:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:01:38 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 13:01:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 13:01:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:01:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:01:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 13:01:42 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=132.753113, avg_loss=0.663766, seen=200, correct=119, accuracy=0.595000
2025-10-10 13:01:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:01:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:01:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:01:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1976MB allocated=1944MB
2025-10-10 13:01:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:01:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:01:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:01:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:01:45 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.740526, avg_loss=0.618513, seen=40, correct=26, accuracy=0.650000
2025-10-10 13:01:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:01:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:01:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:01:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1976MB allocated=1944MB
2025-10-10 13:01:46 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.650000
2025-10-10 13:01:46 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 13:01:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1543, total=6171)
2025-10-10 13:01:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:01:47 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 13:01:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:01:47 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 13:01:47 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=772, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 13:01:55 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 13:01:55 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 13:01:55 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 13:01:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 13:01:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:01:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:01:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 13:01:58 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=130.020233, avg_loss=0.650101, seen=200, correct=124, accuracy=0.620000
2025-10-10 13:01:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:01:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:02:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:02:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2004MB allocated=1961MB
2025-10-10 13:02:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:02:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:02:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:02:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:02:02 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.753199, avg_loss=0.593830, seen=40, correct=26, accuracy=0.650000
2025-10-10 13:02:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:02:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:02:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:02:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2004MB allocated=1961MB
2025-10-10 13:02:03 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.650000
2025-10-10 13:02:14 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 13:02:14 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 13:02:14 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 13:02:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 13:02:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:02:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:02:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 13:02:17 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=130.472382, avg_loss=0.652362, seen=200, correct=125, accuracy=0.625000
2025-10-10 13:02:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:02:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:02:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:02:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2004MB allocated=1961MB
2025-10-10 13:02:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:02:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:02:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:02:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:02:21 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.694456, avg_loss=0.592361, seen=40, correct=27, accuracy=0.675000
2025-10-10 13:02:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:02:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:02:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:02:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2004MB allocated=1961MB
2025-10-10 13:02:23 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.675000
2025-10-10 13:02:33 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 13:02:33 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 13:02:33 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 13:02:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 13:02:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:02:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:02:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 13:02:36 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=133.818085, avg_loss=0.669090, seen=200, correct=121, accuracy=0.605000
2025-10-10 13:02:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:02:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:02:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:02:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2004MB allocated=1961MB
2025-10-10 13:02:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:02:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:02:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:02:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:02:40 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.642443, avg_loss=0.616061, seen=40, correct=28, accuracy=0.700000
2025-10-10 13:02:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:02:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:02:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:02:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2004MB allocated=1961MB
2025-10-10 13:02:41 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.700000
2025-10-10 13:02:50 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 13:02:50 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 13:02:50 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 13:02:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 13:02:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:02:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:02:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 13:02:53 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=130.807098, avg_loss=0.654035, seen=200, correct=128, accuracy=0.640000
2025-10-10 13:02:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:02:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:02:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:02:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2004MB allocated=1961MB
2025-10-10 13:02:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:02:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:02:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:02:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:02:57 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.044849, avg_loss=0.601121, seen=40, correct=26, accuracy=0.650000
2025-10-10 13:02:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:02:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:02:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:02:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2004MB allocated=1961MB
2025-10-10 13:02:59 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.700000, curr=0.650000
2025-10-10 13:03:07 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 13:03:07 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 13:03:07 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 13:03:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 13:03:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:03:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:03:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 13:03:10 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=132.083420, avg_loss=0.660417, seen=200, correct=124, accuracy=0.620000
2025-10-10 13:03:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:03:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:03:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:03:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2004MB allocated=1961MB
2025-10-10 13:03:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:03:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:03:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:03:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:03:15 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.289797, avg_loss=0.607245, seen=40, correct=27, accuracy=0.675000
2025-10-10 13:03:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:03:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:03:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:03:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2004MB allocated=1961MB
2025-10-10 13:03:18 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.700000, curr=0.675000
2025-10-10 13:03:29 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 13:03:29 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 13:03:29 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 13:03:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 13:03:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:03:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:03:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 13:03:32 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=131.298462, avg_loss=0.656492, seen=200, correct=128, accuracy=0.640000
2025-10-10 13:03:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:03:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:03:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:03:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2004MB allocated=1961MB
2025-10-10 13:03:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:03:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:03:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:03:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:03:37 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.116631, avg_loss=0.602916, seen=40, correct=27, accuracy=0.675000
2025-10-10 13:03:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:03:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:03:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:03:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2004MB allocated=1961MB
2025-10-10 13:03:40 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.700000, curr=0.675000
2025-10-10 13:03:49 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 13:03:49 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 13:03:49 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 13:03:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 13:03:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:03:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:03:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 13:03:53 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=130.362335, avg_loss=0.651812, seen=200, correct=123, accuracy=0.615000
2025-10-10 13:03:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:03:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:03:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:03:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2004MB allocated=1961MB
2025-10-10 13:03:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:03:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:03:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:03:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:03:58 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.880898, avg_loss=0.597022, seen=40, correct=26, accuracy=0.650000
2025-10-10 13:03:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:03:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:03:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:04:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2004MB allocated=1961MB
2025-10-10 13:04:01 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/25), best=0.700000, curr=0.650000
2025-10-10 13:04:11 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 13:04:11 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 13:04:11 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 13:04:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 13:04:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:04:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:04:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 13:04:14 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=131.451965, avg_loss=0.657260, seen=200, correct=126, accuracy=0.630000
2025-10-10 13:04:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:04:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:04:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:04:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2004MB allocated=1961MB
2025-10-10 13:04:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:04:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:04:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:04:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:04:18 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.847973, avg_loss=0.596199, seen=40, correct=24, accuracy=0.600000
2025-10-10 13:04:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:04:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:04:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:04:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2004MB allocated=1961MB
2025-10-10 13:04:19 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/25), best=0.700000, curr=0.600000
2025-10-10 13:04:29 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 13:04:29 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 13:04:29 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 13:04:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 13:04:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:04:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:04:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 13:04:32 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=131.268097, avg_loss=0.656340, seen=200, correct=125, accuracy=0.625000
2025-10-10 13:04:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:04:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:04:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:04:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2004MB allocated=1961MB
2025-10-10 13:04:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:04:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:04:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:04:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:04:35 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.184515, avg_loss=0.604613, seen=40, correct=27, accuracy=0.675000
2025-10-10 13:04:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:04:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:04:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:04:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2004MB allocated=1961MB
2025-10-10 13:04:38 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/25), best=0.700000, curr=0.675000
2025-10-10 13:04:46 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 13:04:46 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 13:04:46 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 13:04:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 13:04:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:04:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:04:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 13:04:49 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=132.999451, avg_loss=0.664997, seen=200, correct=124, accuracy=0.620000
2025-10-10 13:04:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:04:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:04:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:04:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2004MB allocated=1961MB
2025-10-10 13:04:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:04:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:04:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:04:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:04:53 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.946127, avg_loss=0.623653, seen=40, correct=26, accuracy=0.650000
2025-10-10 13:04:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:04:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:04:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:04:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2004MB allocated=1961MB
2025-10-10 13:04:55 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/25), best=0.700000, curr=0.650000
2025-10-10 13:04:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 13:04:55 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 13:04:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:04:56 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:04:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2004MB allocated=1961MB
2025-10-10 13:04:57 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #38', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 13:04:57 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #38', 'Round': 0, 'Results_raw': {}}
2025-10-10 13:04:57 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 13:04:58 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 13:04:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:04:58 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 13:04:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:04:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:05:01 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 13:05:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-10-10 13:05:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:05:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:05:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-10 13:05:02 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=30, loss_sum=18.888012, avg_loss=0.629600, seen=30, correct=16, accuracy=0.533333
2025-10-10 13:05:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:05:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:05:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:05:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1976MB allocated=1952MB
2025-10-10 13:05:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:05:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:05:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:05:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:05:05 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.417639, avg_loss=0.610441, seen=40, correct=26, accuracy=0.650000
2025-10-10 13:05:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:05:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:05:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:05:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1976MB allocated=1952MB
2025-10-10 13:05:07 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.650000
2025-10-10 13:05:07 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 13:05:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=146, total=583)
2025-10-10 13:05:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:05:07 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 13:05:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:05:07 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 13:05:07 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=73, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 13:05:14 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 13:05:14 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 13:05:14 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 13:05:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-10-10 13:05:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:05:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:05:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-10 13:05:15 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=30, loss_sum=19.185017, avg_loss=0.639501, seen=30, correct=17, accuracy=0.566667
2025-10-10 13:05:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:05:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:05:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:05:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2000MB allocated=1969MB
2025-10-10 13:05:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:05:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:05:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:05:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:05:19 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.709160, avg_loss=0.617729, seen=40, correct=23, accuracy=0.575000
2025-10-10 13:05:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:05:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:05:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:05:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2000MB allocated=1969MB
2025-10-10 13:05:20 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.650000, curr=0.575000
2025-10-10 13:05:31 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 13:05:31 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 13:05:31 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 13:05:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-10-10 13:05:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:05:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:05:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-10 13:05:32 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=30, loss_sum=18.409666, avg_loss=0.613656, seen=30, correct=17, accuracy=0.566667
2025-10-10 13:05:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:05:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:05:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:05:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2000MB allocated=1969MB
2025-10-10 13:05:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:05:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:05:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:05:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:05:36 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.530197, avg_loss=0.588255, seen=40, correct=29, accuracy=0.725000
2025-10-10 13:05:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:05:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:05:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:05:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2000MB allocated=1969MB
2025-10-10 13:05:38 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.725000
2025-10-10 13:05:46 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 13:05:46 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 13:05:46 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 13:05:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-10-10 13:05:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:05:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:05:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-10 13:05:46 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=30, loss_sum=18.504122, avg_loss=0.616804, seen=30, correct=18, accuracy=0.600000
2025-10-10 13:05:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:05:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:05:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:05:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2000MB allocated=1969MB
2025-10-10 13:05:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:05:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:05:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:05:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:05:50 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.709225, avg_loss=0.592731, seen=40, correct=28, accuracy=0.700000
2025-10-10 13:05:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:05:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:05:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:05:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2000MB allocated=1969MB
2025-10-10 13:05:51 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.725000, curr=0.700000
2025-10-10 13:06:01 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 13:06:01 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 13:06:01 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 13:06:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-10-10 13:06:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:06:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:06:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-10 13:06:01 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=30, loss_sum=18.629110, avg_loss=0.620970, seen=30, correct=17, accuracy=0.566667
2025-10-10 13:06:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:06:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:06:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:06:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2000MB allocated=1969MB
2025-10-10 13:06:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:06:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:06:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:06:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:06:07 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.501835, avg_loss=0.587546, seen=40, correct=28, accuracy=0.700000
2025-10-10 13:06:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:06:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:06:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:06:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2000MB allocated=1969MB
2025-10-10 13:06:08 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.725000, curr=0.700000
2025-10-10 13:06:18 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 13:06:18 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 13:06:18 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 13:06:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-10-10 13:06:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:06:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:06:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-10 13:06:19 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=30, loss_sum=18.428850, avg_loss=0.614295, seen=30, correct=18, accuracy=0.600000
2025-10-10 13:06:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:06:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:06:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:06:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2000MB allocated=1969MB
2025-10-10 13:06:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:06:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:06:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:06:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:06:24 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.406160, avg_loss=0.610154, seen=40, correct=25, accuracy=0.625000
2025-10-10 13:06:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:06:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:06:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:06:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2000MB allocated=1969MB
2025-10-10 13:06:26 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.725000, curr=0.625000
2025-10-10 13:06:34 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 13:06:34 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 13:06:34 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 13:06:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-10-10 13:06:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:06:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:06:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-10 13:06:35 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=30, loss_sum=17.861586, avg_loss=0.595386, seen=30, correct=20, accuracy=0.666667
2025-10-10 13:06:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:06:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:06:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:06:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2000MB allocated=1969MB
2025-10-10 13:06:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:06:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:06:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:06:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:06:39 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.642994, avg_loss=0.591075, seen=40, correct=29, accuracy=0.725000
2025-10-10 13:06:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:06:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:06:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:06:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2000MB allocated=1969MB
2025-10-10 13:06:40 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.725000
2025-10-10 13:06:49 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 13:06:49 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 13:06:49 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 13:06:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-10-10 13:06:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:06:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:06:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-10 13:06:50 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=30, loss_sum=18.054089, avg_loss=0.601803, seen=30, correct=17, accuracy=0.566667
2025-10-10 13:06:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:06:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:06:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:06:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2000MB allocated=1969MB
2025-10-10 13:06:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:06:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:06:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:06:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:06:54 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.535112, avg_loss=0.588378, seen=40, correct=29, accuracy=0.725000
2025-10-10 13:06:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:06:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:06:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:06:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2000MB allocated=1969MB
2025-10-10 13:06:56 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.725000
2025-10-10 13:07:05 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 13:07:05 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 13:07:05 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 13:07:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-10-10 13:07:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:07:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:07:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-10 13:07:06 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=30, loss_sum=17.986382, avg_loss=0.599546, seen=30, correct=21, accuracy=0.700000
2025-10-10 13:07:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:07:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:07:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:07:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2000MB allocated=1969MB
2025-10-10 13:07:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:07:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:07:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:07:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:07:08 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.832985, avg_loss=0.595825, seen=40, correct=25, accuracy=0.625000
2025-10-10 13:07:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:07:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:07:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:07:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2000MB allocated=1969MB
2025-10-10 13:07:10 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.725000, curr=0.625000
2025-10-10 13:07:20 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 13:07:20 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 13:07:20 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 13:07:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-10-10 13:07:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:07:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:07:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-10 13:07:21 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=30, loss_sum=17.883162, avg_loss=0.596105, seen=30, correct=21, accuracy=0.700000
2025-10-10 13:07:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:07:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:07:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:07:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2000MB allocated=1969MB
2025-10-10 13:07:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:07:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:07:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:07:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:07:25 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.734735, avg_loss=0.593368, seen=40, correct=26, accuracy=0.650000
2025-10-10 13:07:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:07:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:07:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:07:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2000MB allocated=1969MB
2025-10-10 13:07:27 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.725000, curr=0.650000
2025-10-10 13:07:37 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 13:07:37 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 13:07:37 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 13:07:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-10-10 13:07:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:07:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:07:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-10 13:07:38 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=30, loss_sum=18.181232, avg_loss=0.606041, seen=30, correct=20, accuracy=0.666667
2025-10-10 13:07:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:07:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:07:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:07:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2000MB allocated=1969MB
2025-10-10 13:07:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:07:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:07:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:07:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:07:42 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.282782, avg_loss=0.582070, seen=40, correct=28, accuracy=0.700000
2025-10-10 13:07:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:07:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:07:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:07:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2000MB allocated=1969MB
2025-10-10 13:07:43 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.725000, curr=0.700000
2025-10-10 13:07:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 13:07:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 13:07:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:07:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:07:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2000MB allocated=1969MB
2025-10-10 13:07:44 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #23', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 13:07:44 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #23', 'Round': 0, 'Results_raw': {}}
2025-10-10 13:07:44 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 13:07:45 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 13:07:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:07:46 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 13:07:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:07:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:07:47 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 13:07:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-10-10 13:07:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:07:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:07:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-10 13:07:49 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=69, loss_sum=44.820103, avg_loss=0.649567, seen=69, correct=45, accuracy=0.652174
2025-10-10 13:07:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:07:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:07:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:07:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1996MB allocated=1961MB
2025-10-10 13:07:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:07:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:07:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:07:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:07:53 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.608967, avg_loss=0.715224, seen=40, correct=20, accuracy=0.500000
2025-10-10 13:07:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:07:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:07:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:07:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1996MB allocated=1961MB
2025-10-10 13:07:54 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.500000
2025-10-10 13:07:54 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 13:07:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=329, total=1316)
2025-10-10 13:07:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:07:55 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 13:07:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:07:55 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 13:07:55 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=165, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 13:08:04 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 13:08:04 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 13:08:04 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 13:08:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-10-10 13:08:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:08:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:08:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-10 13:08:05 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=69, loss_sum=43.931896, avg_loss=0.636694, seen=69, correct=40, accuracy=0.579710
2025-10-10 13:08:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:08:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:08:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:08:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2026MB allocated=1977MB
2025-10-10 13:08:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:08:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:08:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:08:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:08:10 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.150780, avg_loss=0.728769, seen=40, correct=25, accuracy=0.625000
2025-10-10 13:08:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:08:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:08:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:08:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2026MB allocated=1977MB
2025-10-10 13:08:11 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-10 13:08:20 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 13:08:20 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 13:08:20 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 13:08:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-10-10 13:08:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:08:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:08:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-10 13:08:21 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=69, loss_sum=42.503143, avg_loss=0.615988, seen=69, correct=40, accuracy=0.579710
2025-10-10 13:08:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:08:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:08:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:08:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2026MB allocated=1977MB
2025-10-10 13:08:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:08:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:08:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:08:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:08:26 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.131006, avg_loss=0.728275, seen=40, correct=24, accuracy=0.600000
2025-10-10 13:08:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:08:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:08:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:08:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2026MB allocated=1977MB
2025-10-10 13:08:28 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.625000, curr=0.600000
2025-10-10 13:08:36 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 13:08:36 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 13:08:36 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 13:08:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-10-10 13:08:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:08:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:08:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-10 13:08:37 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=69, loss_sum=44.039574, avg_loss=0.638255, seen=69, correct=43, accuracy=0.623188
2025-10-10 13:08:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:08:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:08:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:08:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2026MB allocated=1977MB
2025-10-10 13:08:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:08:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:08:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:08:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:08:42 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.632603, avg_loss=0.740815, seen=40, correct=20, accuracy=0.500000
2025-10-10 13:08:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:08:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:08:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:08:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2026MB allocated=1977MB
2025-10-10 13:08:44 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.625000, curr=0.500000
2025-10-10 13:08:55 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 13:08:55 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 13:08:55 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 13:08:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-10-10 13:08:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:08:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:08:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-10 13:08:56 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=69, loss_sum=44.499279, avg_loss=0.644917, seen=69, correct=39, accuracy=0.565217
2025-10-10 13:08:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:08:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:08:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:08:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2026MB allocated=1977MB
2025-10-10 13:08:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:08:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:09:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:09:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:09:01 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.223694, avg_loss=0.755592, seen=40, correct=20, accuracy=0.500000
2025-10-10 13:09:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:09:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:09:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:09:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2026MB allocated=1977MB
2025-10-10 13:09:02 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.625000, curr=0.500000
2025-10-10 13:09:11 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 13:09:11 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 13:09:11 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 13:09:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-10-10 13:09:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:09:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:09:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-10 13:09:12 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=69, loss_sum=45.078621, avg_loss=0.653313, seen=69, correct=38, accuracy=0.550725
2025-10-10 13:09:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:09:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:09:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:09:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2026MB allocated=1977MB
2025-10-10 13:09:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:09:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:09:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:09:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:09:17 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.518627, avg_loss=0.762966, seen=40, correct=19, accuracy=0.475000
2025-10-10 13:09:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:09:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:09:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:09:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2026MB allocated=1977MB
2025-10-10 13:09:18 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/25), best=0.625000, curr=0.475000
2025-10-10 13:09:26 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 13:09:26 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 13:09:26 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 13:09:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-10-10 13:09:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:09:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:09:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-10 13:09:28 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=69, loss_sum=45.822090, avg_loss=0.664088, seen=69, correct=35, accuracy=0.507246
2025-10-10 13:09:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:09:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:09:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:09:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2026MB allocated=1977MB
2025-10-10 13:09:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:09:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:09:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:09:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:09:31 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=31.762079, avg_loss=0.794052, seen=40, correct=20, accuracy=0.500000
2025-10-10 13:09:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:09:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:09:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:09:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2026MB allocated=1977MB
2025-10-10 13:09:33 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/25), best=0.625000, curr=0.500000
2025-10-10 13:09:42 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 13:09:42 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 13:09:42 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 13:09:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-10-10 13:09:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:09:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:09:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-10 13:09:43 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=69, loss_sum=45.567711, avg_loss=0.660402, seen=69, correct=45, accuracy=0.652174
2025-10-10 13:09:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:09:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:09:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:09:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2026MB allocated=1977MB
2025-10-10 13:09:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:09:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:09:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:09:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:09:46 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.789129, avg_loss=0.719728, seen=40, correct=20, accuracy=0.500000
2025-10-10 13:09:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:09:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:09:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:09:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2026MB allocated=1977MB
2025-10-10 13:09:49 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/25), best=0.625000, curr=0.500000
2025-10-10 13:09:58 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 13:09:58 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 13:09:58 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 13:09:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-10-10 13:09:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:09:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:09:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-10 13:09:59 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=69, loss_sum=45.002037, avg_loss=0.652203, seen=69, correct=36, accuracy=0.521739
2025-10-10 13:09:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:09:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:10:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:10:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2026MB allocated=1977MB
2025-10-10 13:10:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:10:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:10:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:10:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:10:04 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.616148, avg_loss=0.740404, seen=40, correct=21, accuracy=0.525000
2025-10-10 13:10:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:10:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:10:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:10:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2026MB allocated=1977MB
2025-10-10 13:10:05 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/25), best=0.625000, curr=0.525000
2025-10-10 13:10:14 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 13:10:14 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 13:10:14 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 13:10:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-10-10 13:10:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:10:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:10:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-10 13:10:16 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=69, loss_sum=45.322624, avg_loss=0.656850, seen=69, correct=36, accuracy=0.521739
2025-10-10 13:10:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:10:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:10:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:10:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2026MB allocated=1977MB
2025-10-10 13:10:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:10:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:10:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:10:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:10:19 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.206800, avg_loss=0.755170, seen=40, correct=18, accuracy=0.450000
2025-10-10 13:10:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:10:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:10:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:10:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2026MB allocated=1977MB
2025-10-10 13:10:21 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/25), best=0.625000, curr=0.450000
2025-10-10 13:10:30 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 13:10:30 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 13:10:30 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 13:10:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-10-10 13:10:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:10:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:10:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-10 13:10:31 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=69, loss_sum=45.233643, avg_loss=0.655560, seen=69, correct=43, accuracy=0.623188
2025-10-10 13:10:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:10:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:10:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:10:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2026MB allocated=1977MB
2025-10-10 13:10:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:10:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:10:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:10:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:10:36 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.930550, avg_loss=0.723264, seen=40, correct=21, accuracy=0.525000
2025-10-10 13:10:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:10:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:10:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:10:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2026MB allocated=1977MB
2025-10-10 13:10:39 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=9/25), best=0.625000, curr=0.525000
2025-10-10 13:10:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 13:10:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 13:10:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:10:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:10:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2026MB allocated=1977MB
2025-10-10 13:10:40 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #8', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 13:10:40 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #8', 'Round': 0, 'Results_raw': {}}
2025-10-10 13:10:40 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 13:10:41 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 13:10:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:10:41 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 13:10:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:10:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:10:43 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 13:10:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 13:10:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:10:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:10:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 13:10:46 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=129.031525, avg_loss=0.645158, seen=200, correct=124, accuracy=0.620000
2025-10-10 13:10:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:10:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:10:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:10:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1996MB allocated=1969MB
2025-10-10 13:10:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:10:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:10:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:10:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:10:50 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.145416, avg_loss=0.678635, seen=40, correct=27, accuracy=0.675000
2025-10-10 13:10:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:10:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:10:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:10:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1996MB allocated=1969MB
2025-10-10 13:10:51 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.675000
2025-10-10 13:10:51 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 13:10:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3638, total=14550)
2025-10-10 13:10:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:10:51 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 13:10:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:10:51 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 13:10:51 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=1819, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 13:11:02 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 13:11:02 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 13:11:02 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 13:11:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 13:11:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:11:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:11:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 13:11:05 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=129.703125, avg_loss=0.648516, seen=200, correct=125, accuracy=0.625000
2025-10-10 13:11:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:11:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:11:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:11:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2018MB allocated=1986MB
2025-10-10 13:11:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:11:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:11:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:11:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:11:09 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.275530, avg_loss=0.681888, seen=40, correct=27, accuracy=0.675000
2025-10-10 13:11:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:11:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:11:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:11:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2018MB allocated=1986MB
2025-10-10 13:11:11 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.675000
2025-10-10 13:11:22 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 13:11:22 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 13:11:22 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 13:11:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 13:11:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:11:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:11:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 13:11:25 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=131.884888, avg_loss=0.659424, seen=200, correct=117, accuracy=0.585000
2025-10-10 13:11:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:11:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:11:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:11:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2018MB allocated=1986MB
2025-10-10 13:11:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:11:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:11:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:11:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:11:28 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.327053, avg_loss=0.658176, seen=40, correct=26, accuracy=0.650000
2025-10-10 13:11:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:11:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:11:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:11:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2018MB allocated=1986MB
2025-10-10 13:11:29 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.675000, curr=0.650000
2025-10-10 13:11:40 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 13:11:40 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 13:11:40 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 13:11:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 13:11:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:11:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:11:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 13:11:43 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=128.968628, avg_loss=0.644843, seen=200, correct=119, accuracy=0.595000
2025-10-10 13:11:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:11:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:11:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:11:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2018MB allocated=1986MB
2025-10-10 13:11:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:11:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:11:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:11:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:11:47 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.274418, avg_loss=0.656860, seen=40, correct=29, accuracy=0.725000
2025-10-10 13:11:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:11:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:11:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:11:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2018MB allocated=1986MB
2025-10-10 13:11:48 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.725000
2025-10-10 13:11:58 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 13:11:58 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 13:11:58 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 13:11:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 13:11:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:11:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:12:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 13:12:01 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=129.697754, avg_loss=0.648489, seen=200, correct=121, accuracy=0.605000
2025-10-10 13:12:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:12:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:12:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:12:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2018MB allocated=1986MB
2025-10-10 13:12:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:12:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:12:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:12:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:12:04 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.105597, avg_loss=0.652640, seen=40, correct=29, accuracy=0.725000
2025-10-10 13:12:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:12:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:12:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:12:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2018MB allocated=1986MB
2025-10-10 13:12:06 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.725000
2025-10-10 13:12:13 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 13:12:13 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 13:12:13 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 13:12:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 13:12:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:12:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:12:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 13:12:16 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=131.114700, avg_loss=0.655574, seen=200, correct=121, accuracy=0.605000
2025-10-10 13:12:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:12:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:12:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:12:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2018MB allocated=1986MB
2025-10-10 13:12:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:12:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:12:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:12:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:12:20 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.891800, avg_loss=0.647295, seen=40, correct=25, accuracy=0.625000
2025-10-10 13:12:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:12:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:12:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:12:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2018MB allocated=1986MB
2025-10-10 13:12:22 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.725000, curr=0.625000
2025-10-10 13:12:33 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 13:12:33 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 13:12:33 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 13:12:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 13:12:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:12:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:12:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 13:12:36 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=130.928375, avg_loss=0.654642, seen=200, correct=118, accuracy=0.590000
2025-10-10 13:12:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:12:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:12:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:12:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2018MB allocated=1986MB
2025-10-10 13:12:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:12:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:12:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:12:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:12:40 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.932507, avg_loss=0.648313, seen=40, correct=28, accuracy=0.700000
2025-10-10 13:12:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:12:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:12:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:12:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2018MB allocated=1986MB
2025-10-10 13:12:42 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.725000, curr=0.700000
2025-10-10 13:12:50 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 13:12:50 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 13:12:50 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 13:12:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 13:12:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:12:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:12:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 13:12:53 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=127.388504, avg_loss=0.636943, seen=200, correct=128, accuracy=0.640000
2025-10-10 13:12:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:12:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:12:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:12:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2018MB allocated=1986MB
2025-10-10 13:12:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:12:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:12:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:12:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:12:59 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.173153, avg_loss=0.679329, seen=40, correct=23, accuracy=0.575000
2025-10-10 13:12:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:12:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:13:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:13:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2018MB allocated=1986MB
2025-10-10 13:13:00 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.725000, curr=0.575000
2025-10-10 13:13:09 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 13:13:09 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 13:13:09 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 13:13:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 13:13:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:13:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:13:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 13:13:12 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=127.947937, avg_loss=0.639740, seen=200, correct=125, accuracy=0.625000
2025-10-10 13:13:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:13:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:13:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:13:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2018MB allocated=1986MB
2025-10-10 13:13:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:13:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:13:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:13:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:13:17 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.378363, avg_loss=0.659459, seen=40, correct=28, accuracy=0.700000
2025-10-10 13:13:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:13:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:13:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:13:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2018MB allocated=1986MB
2025-10-10 13:13:18 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/25), best=0.725000, curr=0.700000
2025-10-10 13:13:28 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 13:13:28 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 13:13:28 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 13:13:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 13:13:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:13:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:13:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 13:13:31 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=133.650177, avg_loss=0.668251, seen=200, correct=111, accuracy=0.555000
2025-10-10 13:13:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:13:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:13:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:13:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2018MB allocated=1986MB
2025-10-10 13:13:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:13:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:13:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:13:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:13:35 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.011852, avg_loss=0.650296, seen=40, correct=22, accuracy=0.550000
2025-10-10 13:13:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:13:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:13:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:13:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2018MB allocated=1986MB
2025-10-10 13:13:36 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/25), best=0.725000, curr=0.550000
2025-10-10 13:13:46 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 13:13:46 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 13:13:46 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 13:13:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 13:13:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:13:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:13:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 13:13:49 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=126.809143, avg_loss=0.634046, seen=200, correct=122, accuracy=0.610000
2025-10-10 13:13:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:13:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:13:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:13:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2018MB allocated=1986MB
2025-10-10 13:13:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:13:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:13:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:13:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:13:54 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.160892, avg_loss=0.679022, seen=40, correct=24, accuracy=0.600000
2025-10-10 13:13:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:13:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:13:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:13:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2018MB allocated=1986MB
2025-10-10 13:13:55 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/25), best=0.725000, curr=0.600000
2025-10-10 13:13:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 13:13:55 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 13:13:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:13:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:13:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2018MB allocated=1986MB
2025-10-10 13:13:56 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #15', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 13:13:56 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #15', 'Round': 0, 'Results_raw': {}}
2025-10-10 13:13:56 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 13:13:57 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 13:13:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:13:58 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 13:13:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:13:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:14:01 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 13:14:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 13:14:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:14:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:14:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 13:14:04 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=131.468658, avg_loss=0.657343, seen=200, correct=119, accuracy=0.595000
2025-10-10 13:14:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:14:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:14:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:14:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2016MB allocated=1977MB
2025-10-10 13:14:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:14:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:14:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:14:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:14:07 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.494892, avg_loss=0.612372, seen=40, correct=26, accuracy=0.650000
2025-10-10 13:14:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:14:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:14:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:14:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2016MB allocated=1977MB
2025-10-10 13:14:08 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.650000
2025-10-10 13:14:08 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 13:14:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1184, total=4736)
2025-10-10 13:14:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:14:08 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 13:14:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:14:08 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 13:14:08 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=592, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 13:14:16 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 13:14:16 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 13:14:16 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 13:14:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 13:14:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:14:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:14:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 13:14:19 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=131.131866, avg_loss=0.655659, seen=200, correct=118, accuracy=0.590000
2025-10-10 13:14:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:14:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:14:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:14:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2050MB allocated=1994MB
2025-10-10 13:14:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:14:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:14:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:14:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:14:23 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.172369, avg_loss=0.604309, seen=40, correct=27, accuracy=0.675000
2025-10-10 13:14:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:14:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:14:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:14:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2050MB allocated=1994MB
2025-10-10 13:14:25 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.675000
2025-10-10 13:14:35 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 13:14:35 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 13:14:35 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 13:14:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 13:14:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:14:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:14:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 13:14:38 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=134.312973, avg_loss=0.671565, seen=200, correct=119, accuracy=0.595000
2025-10-10 13:14:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:14:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:14:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:14:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2050MB allocated=1994MB
2025-10-10 13:14:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:14:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:14:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:14:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:14:42 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.067009, avg_loss=0.626675, seen=40, correct=28, accuracy=0.700000
2025-10-10 13:14:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:14:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:14:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:14:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2050MB allocated=1994MB
2025-10-10 13:14:44 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.700000
2025-10-10 13:14:54 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 13:14:54 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 13:14:55 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 13:14:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 13:14:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:14:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:14:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 13:14:58 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=131.391937, avg_loss=0.656960, seen=200, correct=115, accuracy=0.575000
2025-10-10 13:14:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:14:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:14:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:15:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2050MB allocated=1994MB
2025-10-10 13:15:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:15:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:15:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:15:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:15:02 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.352146, avg_loss=0.608804, seen=40, correct=30, accuracy=0.750000
2025-10-10 13:15:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:15:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:15:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:15:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2050MB allocated=1994MB
2025-10-10 13:15:03 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.750000
2025-10-10 13:15:13 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 13:15:13 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 13:15:13 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 13:15:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 13:15:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:15:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:15:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 13:15:16 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=130.754395, avg_loss=0.653772, seen=200, correct=115, accuracy=0.575000
2025-10-10 13:15:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:15:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:15:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:15:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2050MB allocated=1994MB
2025-10-10 13:15:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:15:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:15:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:15:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:15:20 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.463533, avg_loss=0.611588, seen=40, correct=28, accuracy=0.700000
2025-10-10 13:15:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:15:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:15:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:15:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2050MB allocated=1994MB
2025-10-10 13:15:21 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.750000, curr=0.700000
2025-10-10 13:15:32 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 13:15:32 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 13:15:32 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 13:15:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 13:15:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:15:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:15:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 13:15:35 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=132.804657, avg_loss=0.664023, seen=200, correct=116, accuracy=0.580000
2025-10-10 13:15:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:15:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:15:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:15:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2050MB allocated=1994MB
2025-10-10 13:15:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:15:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:15:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:15:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:15:39 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.629383, avg_loss=0.615735, seen=40, correct=31, accuracy=0.775000
2025-10-10 13:15:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:15:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:15:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:15:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2050MB allocated=1994MB
2025-10-10 13:15:40 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.775000
2025-10-10 13:15:50 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 13:15:50 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 13:15:50 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 13:15:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 13:15:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:15:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:15:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 13:15:53 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=136.753708, avg_loss=0.683769, seen=200, correct=115, accuracy=0.575000
2025-10-10 13:15:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:15:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:15:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:15:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2050MB allocated=1994MB
2025-10-10 13:15:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:15:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:15:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:15:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:15:58 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.671307, avg_loss=0.641783, seen=40, correct=25, accuracy=0.625000
2025-10-10 13:15:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:15:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:15:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:15:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2050MB allocated=1994MB
2025-10-10 13:15:59 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.775000, curr=0.625000
2025-10-10 13:16:10 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 13:16:10 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 13:16:10 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 13:16:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 13:16:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:16:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:16:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 13:16:13 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=134.663025, avg_loss=0.673315, seen=200, correct=114, accuracy=0.570000
2025-10-10 13:16:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:16:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:16:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:16:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2050MB allocated=1994MB
2025-10-10 13:16:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:16:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:16:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:16:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:16:17 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.374868, avg_loss=0.634372, seen=40, correct=28, accuracy=0.700000
2025-10-10 13:16:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:16:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:16:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:16:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2050MB allocated=1994MB
2025-10-10 13:16:19 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.775000, curr=0.700000
2025-10-10 13:16:27 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 13:16:27 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 13:16:27 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 13:16:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 13:16:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:16:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:16:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 13:16:30 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=131.506699, avg_loss=0.657533, seen=200, correct=119, accuracy=0.595000
2025-10-10 13:16:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:16:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:16:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:16:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2050MB allocated=1994MB
2025-10-10 13:16:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:16:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:16:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:16:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:16:34 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.604567, avg_loss=0.615114, seen=40, correct=30, accuracy=0.750000
2025-10-10 13:16:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:16:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:16:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:16:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2050MB allocated=1994MB
2025-10-10 13:16:35 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.775000, curr=0.750000
2025-10-10 13:16:43 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 13:16:43 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 13:16:43 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 13:16:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 13:16:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:16:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:16:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 13:16:46 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=131.315536, avg_loss=0.656578, seen=200, correct=122, accuracy=0.610000
2025-10-10 13:16:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:16:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:16:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:16:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2050MB allocated=1994MB
2025-10-10 13:16:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:16:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:16:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:16:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:16:51 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.934212, avg_loss=0.623355, seen=40, correct=30, accuracy=0.750000
2025-10-10 13:16:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:16:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:16:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:16:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2050MB allocated=1994MB
2025-10-10 13:16:53 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/25), best=0.775000, curr=0.750000
2025-10-10 13:17:01 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 13:17:01 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 13:17:01 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 13:17:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 13:17:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:17:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:17:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 13:17:04 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=133.649292, avg_loss=0.668246, seen=200, correct=116, accuracy=0.580000
2025-10-10 13:17:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:17:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:17:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:17:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2050MB allocated=1994MB
2025-10-10 13:17:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:17:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:17:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:17:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:17:10 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.693474, avg_loss=0.642337, seen=40, correct=26, accuracy=0.650000
2025-10-10 13:17:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:17:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:17:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:17:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2050MB allocated=1994MB
2025-10-10 13:17:12 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/25), best=0.775000, curr=0.650000
2025-10-10 13:17:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 13:17:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 13:17:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:17:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:17:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2050MB allocated=1994MB
2025-10-10 13:17:13 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #35', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 13:17:13 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #35', 'Round': 0, 'Results_raw': {}}
2025-10-10 13:17:13 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 13:17:14 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 13:17:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:17:15 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 13:17:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:17:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:17:17 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 13:17:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-10-10 13:17:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:17:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:17:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-10-10 13:17:20 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=132, loss_sum=85.724907, avg_loss=0.649431, seen=132, correct=86, accuracy=0.651515
2025-10-10 13:17:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:17:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:17:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:17:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2016MB allocated=1986MB
2025-10-10 13:17:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:17:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:17:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:17:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:17:25 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.359329, avg_loss=0.758983, seen=40, correct=21, accuracy=0.525000
2025-10-10 13:17:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:17:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:17:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:17:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2016MB allocated=1986MB
2025-10-10 13:17:26 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.525000
2025-10-10 13:17:26 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 13:17:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=631, total=2521)
2025-10-10 13:17:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:17:26 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 13:17:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:17:26 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 13:17:26 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=316, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 13:17:37 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 13:17:37 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 13:17:37 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 13:17:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-10-10 13:17:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:17:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:17:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-10-10 13:17:39 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=132, loss_sum=86.106178, avg_loss=0.652320, seen=132, correct=88, accuracy=0.666667
2025-10-10 13:17:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:17:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:17:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:17:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2054MB allocated=2003MB
2025-10-10 13:17:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:17:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:17:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:17:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:17:42 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.640957, avg_loss=0.766024, seen=40, correct=23, accuracy=0.575000
2025-10-10 13:17:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:17:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:17:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:17:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2054MB allocated=2003MB
2025-10-10 13:17:43 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-10 13:17:54 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 13:17:54 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 13:17:54 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 13:17:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-10-10 13:17:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:17:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:17:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-10-10 13:17:56 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=132, loss_sum=90.730934, avg_loss=0.687356, seen=132, correct=76, accuracy=0.575758
2025-10-10 13:17:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:17:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:17:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:18:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2054MB allocated=2003MB
2025-10-10 13:18:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:18:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:18:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:18:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:18:01 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=32.436634, avg_loss=0.810916, seen=40, correct=18, accuracy=0.450000
2025-10-10 13:18:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:18:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:18:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:18:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2054MB allocated=2003MB
2025-10-10 13:18:04 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.575000, curr=0.450000
2025-10-10 13:18:13 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 13:18:13 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 13:18:13 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 13:18:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-10-10 13:18:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:18:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:18:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-10-10 13:18:15 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=132, loss_sum=86.094246, avg_loss=0.652229, seen=132, correct=85, accuracy=0.643939
2025-10-10 13:18:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:18:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:18:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:18:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2054MB allocated=2003MB
2025-10-10 13:18:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:18:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:18:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:18:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:18:18 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.457008, avg_loss=0.761425, seen=40, correct=22, accuracy=0.550000
2025-10-10 13:18:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:18:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:18:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:18:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2054MB allocated=2003MB
2025-10-10 13:18:19 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.575000, curr=0.550000
2025-10-10 13:18:28 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 13:18:28 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 13:18:28 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 13:18:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-10-10 13:18:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:18:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:18:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-10-10 13:18:30 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=132, loss_sum=86.078522, avg_loss=0.652110, seen=132, correct=87, accuracy=0.659091
2025-10-10 13:18:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:18:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:18:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:18:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2054MB allocated=2003MB
2025-10-10 13:18:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:18:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:18:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:18:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:18:35 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.030834, avg_loss=0.750771, seen=40, correct=22, accuracy=0.550000
2025-10-10 13:18:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:18:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:18:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:18:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2054MB allocated=2003MB
2025-10-10 13:18:38 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.575000, curr=0.550000
2025-10-10 13:18:46 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 13:18:46 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 13:18:46 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 13:18:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-10-10 13:18:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:18:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:18:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-10-10 13:18:49 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=132, loss_sum=86.058640, avg_loss=0.651959, seen=132, correct=87, accuracy=0.659091
2025-10-10 13:18:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:18:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:18:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:18:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2054MB allocated=2003MB
2025-10-10 13:18:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:18:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:18:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:18:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:18:52 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.989510, avg_loss=0.749738, seen=40, correct=24, accuracy=0.600000
2025-10-10 13:18:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:18:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:18:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:18:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2054MB allocated=2003MB
2025-10-10 13:18:54 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-10 13:19:03 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 13:19:03 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 13:19:03 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 13:19:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-10-10 13:19:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:19:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:19:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-10-10 13:19:05 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=132, loss_sum=86.488686, avg_loss=0.655217, seen=132, correct=90, accuracy=0.681818
2025-10-10 13:19:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:19:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:19:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:19:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2054MB allocated=2003MB
2025-10-10 13:19:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:19:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:19:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:19:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:19:10 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.417789, avg_loss=0.760445, seen=40, correct=23, accuracy=0.575000
2025-10-10 13:19:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:19:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:19:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:19:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2054MB allocated=2003MB
2025-10-10 13:19:12 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.600000, curr=0.575000
2025-10-10 13:19:22 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 13:19:22 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 13:19:22 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 13:19:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-10-10 13:19:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:19:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:19:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-10-10 13:19:24 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=132, loss_sum=87.059547, avg_loss=0.659542, seen=132, correct=78, accuracy=0.590909
2025-10-10 13:19:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:19:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:19:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:19:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2054MB allocated=2003MB
2025-10-10 13:19:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:19:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:19:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:19:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:19:28 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.074919, avg_loss=0.751873, seen=40, correct=22, accuracy=0.550000
2025-10-10 13:19:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:19:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:19:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:19:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2054MB allocated=2003MB
2025-10-10 13:19:30 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.600000, curr=0.550000
2025-10-10 13:19:39 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 13:19:39 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 13:19:39 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 13:19:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-10-10 13:19:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:19:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:19:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-10-10 13:19:41 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=132, loss_sum=87.801048, avg_loss=0.665159, seen=132, correct=75, accuracy=0.568182
2025-10-10 13:19:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:19:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:19:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:19:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2054MB allocated=2003MB
2025-10-10 13:19:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:19:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:19:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:19:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:19:47 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.108427, avg_loss=0.752711, seen=40, correct=22, accuracy=0.550000
2025-10-10 13:19:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:19:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:19:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:19:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2054MB allocated=2003MB
2025-10-10 13:19:48 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.600000, curr=0.550000
2025-10-10 13:19:58 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 13:19:58 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 13:19:58 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 13:19:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-10-10 13:19:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:19:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:20:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-10-10 13:20:00 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=132, loss_sum=85.682022, avg_loss=0.649106, seen=132, correct=88, accuracy=0.666667
2025-10-10 13:20:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:20:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:20:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:20:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2054MB allocated=2003MB
2025-10-10 13:20:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:20:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:20:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:20:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:20:03 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.625845, avg_loss=0.740646, seen=40, correct=24, accuracy=0.600000
2025-10-10 13:20:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:20:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:20:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:20:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2054MB allocated=2003MB
2025-10-10 13:20:05 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-10 13:20:15 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 13:20:15 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 13:20:15 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 13:20:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-10-10 13:20:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:20:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:20:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-10-10 13:20:17 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=132, loss_sum=85.749374, avg_loss=0.649616, seen=132, correct=89, accuracy=0.674242
2025-10-10 13:20:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:20:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:20:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:20:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2054MB allocated=2003MB
2025-10-10 13:20:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:20:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:20:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:20:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:20:22 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.637854, avg_loss=0.740946, seen=40, correct=23, accuracy=0.575000
2025-10-10 13:20:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:20:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:20:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:20:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2054MB allocated=2003MB
2025-10-10 13:20:24 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.600000, curr=0.575000
2025-10-10 13:20:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 13:20:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 13:20:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:20:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:20:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2054MB allocated=2003MB
2025-10-10 13:20:25 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #49', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 13:20:25 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #49', 'Round': 0, 'Results_raw': {}}
2025-10-10 13:20:25 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 13:20:26 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 13:20:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:20:27 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 13:20:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:20:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:20:29 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 13:20:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-10 13:20:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:20:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:20:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-10 13:20:31 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=71.426781, avg_loss=0.649334, seen=110, correct=70, accuracy=0.636364
2025-10-10 13:20:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:20:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:20:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:20:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2036MB allocated=1994MB
2025-10-10 13:20:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:20:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:20:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:20:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:20:37 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.942932, avg_loss=0.673573, seen=40, correct=22, accuracy=0.550000
2025-10-10 13:20:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:20:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:20:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:20:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2036MB allocated=1994MB
2025-10-10 13:20:38 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.550000
2025-10-10 13:20:38 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 13:20:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=526, total=2102)
2025-10-10 13:20:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:20:38 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 13:20:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:20:38 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 13:20:38 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=263, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 13:20:49 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 13:20:49 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 13:20:49 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 13:20:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-10 13:20:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:20:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:20:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-10 13:20:51 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=70.740845, avg_loss=0.643099, seen=110, correct=71, accuracy=0.645455
2025-10-10 13:20:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:20:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:20:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:20:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2080MB allocated=2011MB
2025-10-10 13:20:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:20:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:20:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:20:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:20:57 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.211712, avg_loss=0.680293, seen=40, correct=23, accuracy=0.575000
2025-10-10 13:20:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:20:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:20:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:20:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2080MB allocated=2011MB
2025-10-10 13:20:59 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-10 13:21:07 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 13:21:07 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 13:21:07 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 13:21:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-10 13:21:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:21:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:21:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-10 13:21:09 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=71.528961, avg_loss=0.650263, seen=110, correct=68, accuracy=0.618182
2025-10-10 13:21:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:21:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:21:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:21:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2080MB allocated=2011MB
2025-10-10 13:21:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:21:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:21:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:21:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:21:12 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.069983, avg_loss=0.726750, seen=40, correct=23, accuracy=0.575000
2025-10-10 13:21:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:21:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:21:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:21:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2080MB allocated=2011MB
2025-10-10 13:21:14 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-10 13:21:23 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 13:21:23 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 13:21:23 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 13:21:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-10 13:21:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:21:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:21:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-10 13:21:25 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=70.629738, avg_loss=0.642089, seen=110, correct=71, accuracy=0.645455
2025-10-10 13:21:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:21:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:21:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:21:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2080MB allocated=2011MB
2025-10-10 13:21:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:21:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:21:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:21:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:21:30 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.763371, avg_loss=0.694084, seen=40, correct=25, accuracy=0.625000
2025-10-10 13:21:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:21:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:21:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:21:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2080MB allocated=2011MB
2025-10-10 13:21:32 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-10 13:21:41 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 13:21:41 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 13:21:41 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 13:21:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-10 13:21:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:21:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:21:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-10 13:21:43 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=71.182953, avg_loss=0.647118, seen=110, correct=70, accuracy=0.636364
2025-10-10 13:21:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:21:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:21:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:21:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2080MB allocated=2011MB
2025-10-10 13:21:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:21:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:21:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:21:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:21:46 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.191841, avg_loss=0.679796, seen=40, correct=24, accuracy=0.600000
2025-10-10 13:21:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:21:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:21:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:21:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2080MB allocated=2011MB
2025-10-10 13:21:48 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.625000, curr=0.600000
2025-10-10 13:21:56 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 13:21:56 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 13:21:56 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 13:21:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-10 13:21:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:21:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:21:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-10 13:21:58 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=73.083923, avg_loss=0.664399, seen=110, correct=70, accuracy=0.636364
2025-10-10 13:21:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:21:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:21:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:21:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2080MB allocated=2011MB
2025-10-10 13:22:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:22:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:22:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:22:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:22:01 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.390686, avg_loss=0.659767, seen=40, correct=25, accuracy=0.625000
2025-10-10 13:22:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:22:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:22:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:22:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2080MB allocated=2011MB
2025-10-10 13:22:02 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-10 13:22:12 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 13:22:12 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 13:22:12 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 13:22:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-10 13:22:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:22:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:22:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-10 13:22:14 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=70.688660, avg_loss=0.642624, seen=110, correct=70, accuracy=0.636364
2025-10-10 13:22:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:22:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:22:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:22:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2080MB allocated=2011MB
2025-10-10 13:22:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:22:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:22:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:22:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:22:18 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.150242, avg_loss=0.703756, seen=40, correct=22, accuracy=0.550000
2025-10-10 13:22:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:22:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:22:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:22:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2080MB allocated=2011MB
2025-10-10 13:22:20 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.625000, curr=0.550000
2025-10-10 13:22:27 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 13:22:27 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 13:22:27 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 13:22:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-10 13:22:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:22:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:22:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-10 13:22:29 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=70.197693, avg_loss=0.638161, seen=110, correct=69, accuracy=0.627273
2025-10-10 13:22:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:22:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:22:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:22:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2080MB allocated=2011MB
2025-10-10 13:22:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:22:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:22:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:22:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:22:35 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.819374, avg_loss=0.695484, seen=40, correct=23, accuracy=0.575000
2025-10-10 13:22:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:22:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:22:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:22:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2080MB allocated=2011MB
2025-10-10 13:22:37 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.625000, curr=0.575000
2025-10-10 13:22:46 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 13:22:46 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 13:22:46 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 13:22:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-10 13:22:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:22:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:22:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-10 13:22:48 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=70.666885, avg_loss=0.642426, seen=110, correct=73, accuracy=0.663636
2025-10-10 13:22:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:22:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:22:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:22:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2080MB allocated=2011MB
2025-10-10 13:22:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:22:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:22:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:22:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:22:53 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.710363, avg_loss=0.667759, seen=40, correct=22, accuracy=0.550000
2025-10-10 13:22:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:22:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:22:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:22:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2080MB allocated=2011MB
2025-10-10 13:22:54 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.625000, curr=0.550000
2025-10-10 13:23:05 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 13:23:05 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 13:23:05 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 13:23:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-10 13:23:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:23:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:23:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-10 13:23:06 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=69.766129, avg_loss=0.634238, seen=110, correct=75, accuracy=0.681818
2025-10-10 13:23:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:23:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:23:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:23:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2080MB allocated=2011MB
2025-10-10 13:23:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:23:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:23:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:23:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:23:10 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.540455, avg_loss=0.663511, seen=40, correct=23, accuracy=0.575000
2025-10-10 13:23:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:23:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:23:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:23:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2080MB allocated=2011MB
2025-10-10 13:23:12 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/25), best=0.625000, curr=0.575000
2025-10-10 13:23:21 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 13:23:21 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 13:23:21 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 13:23:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-10 13:23:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:23:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:23:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-10 13:23:23 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=69.817307, avg_loss=0.634703, seen=110, correct=72, accuracy=0.654545
2025-10-10 13:23:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:23:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:23:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:23:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2080MB allocated=2011MB
2025-10-10 13:23:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:23:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:23:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:23:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:23:27 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.920765, avg_loss=0.698019, seen=40, correct=24, accuracy=0.600000
2025-10-10 13:23:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:23:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:23:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:23:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2080MB allocated=2011MB
2025-10-10 13:23:30 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/25), best=0.625000, curr=0.600000
2025-10-10 13:23:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 13:23:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 13:23:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:23:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:23:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2080MB allocated=2011MB
2025-10-10 13:23:31 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #19', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 13:23:31 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #19', 'Round': 0, 'Results_raw': {}}
2025-10-10 13:23:31 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 13:23:32 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 13:23:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:23:33 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 13:23:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:23:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:23:35 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 13:23:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-10-10 13:23:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:23:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:23:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-10-10 13:23:37 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=83, loss_sum=60.402569, avg_loss=0.727742, seen=83, correct=48, accuracy=0.578313
2025-10-10 13:23:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:23:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:23:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:23:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2036MB allocated=2003MB
2025-10-10 13:23:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:23:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:23:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:23:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:23:42 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.503889, avg_loss=0.637597, seen=40, correct=25, accuracy=0.625000
2025-10-10 13:23:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:23:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:23:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:23:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2036MB allocated=2003MB
2025-10-10 13:23:43 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-10 13:23:43 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 13:23:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=395, total=1580)
2025-10-10 13:23:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:23:43 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 13:23:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:23:43 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 13:23:43 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=198, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 13:23:52 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 13:23:52 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 13:23:52 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 13:23:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-10-10 13:23:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:23:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:23:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-10-10 13:23:53 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=83, loss_sum=60.337616, avg_loss=0.726959, seen=83, correct=46, accuracy=0.554217
2025-10-10 13:23:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:23:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:23:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:23:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2070MB allocated=2019MB
2025-10-10 13:23:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:23:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:23:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:23:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:23:56 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.351065, avg_loss=0.608777, seen=40, correct=26, accuracy=0.650000
2025-10-10 13:23:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:23:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:23:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:23:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2070MB allocated=2019MB
2025-10-10 13:23:58 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.650000
2025-10-10 13:24:07 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 13:24:07 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 13:24:07 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 13:24:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-10-10 13:24:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:24:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:24:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-10-10 13:24:08 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=83, loss_sum=60.388397, avg_loss=0.727571, seen=83, correct=48, accuracy=0.578313
2025-10-10 13:24:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:24:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:24:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:24:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2070MB allocated=2019MB
2025-10-10 13:24:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:24:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:24:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:24:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:24:13 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.950632, avg_loss=0.648766, seen=40, correct=28, accuracy=0.700000
2025-10-10 13:24:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:24:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:24:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:24:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2070MB allocated=2019MB
2025-10-10 13:24:14 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.700000
2025-10-10 13:24:24 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 13:24:24 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 13:24:24 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 13:24:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-10-10 13:24:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:24:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:24:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-10-10 13:24:26 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=83, loss_sum=59.963131, avg_loss=0.722447, seen=83, correct=50, accuracy=0.602410
2025-10-10 13:24:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:24:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:24:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:24:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2070MB allocated=2019MB
2025-10-10 13:24:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:24:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:24:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:24:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:24:30 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.663086, avg_loss=0.616577, seen=40, correct=27, accuracy=0.675000
2025-10-10 13:24:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:24:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:24:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:24:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2070MB allocated=2019MB
2025-10-10 13:24:32 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.700000, curr=0.675000
2025-10-10 13:24:41 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 13:24:41 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 13:24:41 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 13:24:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-10-10 13:24:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:24:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:24:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-10-10 13:24:43 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=83, loss_sum=59.014317, avg_loss=0.711016, seen=83, correct=50, accuracy=0.602410
2025-10-10 13:24:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:24:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:24:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:24:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2070MB allocated=2019MB
2025-10-10 13:24:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:24:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:24:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:24:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:24:48 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.547102, avg_loss=0.638678, seen=40, correct=26, accuracy=0.650000
2025-10-10 13:24:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:24:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:24:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:24:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2070MB allocated=2019MB
2025-10-10 13:24:49 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.700000, curr=0.650000
2025-10-10 13:25:01 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 13:25:01 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 13:25:01 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 13:25:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-10-10 13:25:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:25:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:25:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-10-10 13:25:02 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=83, loss_sum=59.256641, avg_loss=0.713935, seen=83, correct=47, accuracy=0.566265
2025-10-10 13:25:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:25:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:25:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:25:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2070MB allocated=2019MB
2025-10-10 13:25:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:25:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:25:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:25:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:25:08 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.528950, avg_loss=0.613224, seen=40, correct=26, accuracy=0.650000
2025-10-10 13:25:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:25:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:25:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:25:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2070MB allocated=2019MB
2025-10-10 13:25:09 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.700000, curr=0.650000
2025-10-10 13:25:18 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 13:25:18 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 13:25:18 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 13:25:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-10-10 13:25:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:25:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:25:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-10-10 13:25:19 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=83, loss_sum=59.915417, avg_loss=0.721872, seen=83, correct=48, accuracy=0.578313
2025-10-10 13:25:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:25:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:25:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:25:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2070MB allocated=2019MB
2025-10-10 13:25:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:25:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:25:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:25:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:25:24 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.534071, avg_loss=0.613352, seen=40, correct=25, accuracy=0.625000
2025-10-10 13:25:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:25:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:25:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:25:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2070MB allocated=2019MB
2025-10-10 13:25:26 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/25), best=0.700000, curr=0.625000
2025-10-10 13:25:35 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 13:25:35 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 13:25:35 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 13:25:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-10-10 13:25:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:25:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:25:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-10-10 13:25:37 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=83, loss_sum=58.140469, avg_loss=0.700488, seen=83, correct=49, accuracy=0.590361
2025-10-10 13:25:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:25:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:25:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:25:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2070MB allocated=2019MB
2025-10-10 13:25:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:25:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:25:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:25:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:25:42 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.113335, avg_loss=0.627833, seen=40, correct=25, accuracy=0.625000
2025-10-10 13:25:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:25:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:25:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:25:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2070MB allocated=2019MB
2025-10-10 13:25:45 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/25), best=0.700000, curr=0.625000
2025-10-10 13:25:54 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 13:25:54 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 13:25:54 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 13:25:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-10-10 13:25:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:25:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:25:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-10-10 13:25:56 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=83, loss_sum=57.558052, avg_loss=0.693471, seen=83, correct=49, accuracy=0.590361
2025-10-10 13:25:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:25:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:25:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:25:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2070MB allocated=2019MB
2025-10-10 13:25:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:25:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:25:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:25:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:25:58 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.869215, avg_loss=0.621730, seen=40, correct=26, accuracy=0.650000
2025-10-10 13:25:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:25:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:26:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:26:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2070MB allocated=2019MB
2025-10-10 13:26:01 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/25), best=0.700000, curr=0.650000
2025-10-10 13:26:10 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 13:26:10 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 13:26:10 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 13:26:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-10-10 13:26:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:26:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:26:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-10-10 13:26:11 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=83, loss_sum=57.350430, avg_loss=0.690969, seen=83, correct=50, accuracy=0.602410
2025-10-10 13:26:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:26:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:26:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:26:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2070MB allocated=2019MB
2025-10-10 13:26:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:26:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:26:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:26:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:26:16 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.887438, avg_loss=0.622186, seen=40, correct=26, accuracy=0.650000
2025-10-10 13:26:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:26:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:26:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:26:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2070MB allocated=2019MB
2025-10-10 13:26:18 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/25), best=0.700000, curr=0.650000
2025-10-10 13:26:27 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 13:26:27 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 13:26:27 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 13:26:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-10-10 13:26:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:26:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:26:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-10-10 13:26:29 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=83, loss_sum=59.533901, avg_loss=0.717276, seen=83, correct=48, accuracy=0.578313
2025-10-10 13:26:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:26:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:26:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:26:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2070MB allocated=2019MB
2025-10-10 13:26:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:26:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:26:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:26:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:26:33 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.373739, avg_loss=0.609343, seen=40, correct=28, accuracy=0.700000
2025-10-10 13:26:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:26:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:26:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:26:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2070MB allocated=2019MB
2025-10-10 13:26:34 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.700000
2025-10-10 13:26:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 13:26:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 13:26:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:26:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:26:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2070MB allocated=2019MB
2025-10-10 13:26:36 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #51', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 13:26:36 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #51', 'Round': 0, 'Results_raw': {}}
2025-10-10 13:26:36 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 13:26:37 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 13:26:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:26:37 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 13:26:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:26:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:26:39 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 13:26:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-10-10 13:26:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:26:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:26:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-10-10 13:26:40 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=54, loss_sum=37.048576, avg_loss=0.686085, seen=54, correct=32, accuracy=0.592593
2025-10-10 13:26:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:26:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:26:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:26:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2036MB allocated=2011MB
2025-10-10 13:26:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:26:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:26:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:26:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:26:44 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.262259, avg_loss=0.631556, seen=40, correct=25, accuracy=0.625000
2025-10-10 13:26:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:26:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:26:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:26:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2036MB allocated=2011MB
2025-10-10 13:26:45 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-10 13:26:46 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 13:26:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=258, total=1030)
2025-10-10 13:26:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:26:46 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 13:26:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:26:46 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 13:26:46 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=129, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 13:26:55 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 13:26:55 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 13:26:55 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 13:26:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-10-10 13:26:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:26:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:26:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-10-10 13:26:56 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=54, loss_sum=36.979637, avg_loss=0.684808, seen=54, correct=31, accuracy=0.574074
2025-10-10 13:26:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:26:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:26:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:26:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2062MB allocated=2028MB
2025-10-10 13:26:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:26:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:26:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:27:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:27:01 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.142149, avg_loss=0.678554, seen=40, correct=23, accuracy=0.575000
2025-10-10 13:27:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:27:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:27:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:27:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2062MB allocated=2028MB
2025-10-10 13:27:02 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.625000, curr=0.575000
2025-10-10 13:27:12 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 13:27:12 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 13:27:12 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 13:27:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-10-10 13:27:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:27:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:27:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-10-10 13:27:13 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=54, loss_sum=36.540794, avg_loss=0.676681, seen=54, correct=31, accuracy=0.574074
2025-10-10 13:27:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:27:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:27:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:27:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2062MB allocated=2028MB
2025-10-10 13:27:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:27:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:27:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:27:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:27:18 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.110474, avg_loss=0.652762, seen=40, correct=24, accuracy=0.600000
2025-10-10 13:27:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:27:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:27:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:27:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2062MB allocated=2028MB
2025-10-10 13:27:19 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.625000, curr=0.600000
2025-10-10 13:27:27 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 13:27:27 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 13:27:27 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 13:27:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-10-10 13:27:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:27:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:27:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-10-10 13:27:28 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=54, loss_sum=35.889782, avg_loss=0.664626, seen=54, correct=32, accuracy=0.592593
2025-10-10 13:27:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:27:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:27:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:27:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2062MB allocated=2028MB
2025-10-10 13:27:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:27:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:27:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:27:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:27:33 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.847500, avg_loss=0.621187, seen=40, correct=27, accuracy=0.675000
2025-10-10 13:27:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:27:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:27:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:27:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2062MB allocated=2028MB
2025-10-10 13:27:35 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.675000
2025-10-10 13:27:43 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 13:27:43 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 13:27:43 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 13:27:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-10-10 13:27:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:27:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:27:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-10-10 13:27:44 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=54, loss_sum=35.068741, avg_loss=0.649421, seen=54, correct=33, accuracy=0.611111
2025-10-10 13:27:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:27:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:27:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:27:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2062MB allocated=2028MB
2025-10-10 13:27:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:27:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:27:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:27:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:27:50 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.384441, avg_loss=0.634611, seen=40, correct=24, accuracy=0.600000
2025-10-10 13:27:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:27:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:27:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:27:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2062MB allocated=2028MB
2025-10-10 13:27:52 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.675000, curr=0.600000
2025-10-10 13:28:00 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 13:28:00 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 13:28:00 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 13:28:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-10-10 13:28:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:28:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:28:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-10-10 13:28:02 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=54, loss_sum=35.549301, avg_loss=0.658320, seen=54, correct=31, accuracy=0.574074
2025-10-10 13:28:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:28:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:28:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:28:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2062MB allocated=2028MB
2025-10-10 13:28:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:28:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:28:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:28:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:28:06 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.961107, avg_loss=0.624028, seen=40, correct=25, accuracy=0.625000
2025-10-10 13:28:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:28:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:28:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:28:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2062MB allocated=2028MB
2025-10-10 13:28:08 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.675000, curr=0.625000
2025-10-10 13:28:17 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 13:28:17 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 13:28:17 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 13:28:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-10-10 13:28:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:28:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:28:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-10-10 13:28:18 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=54, loss_sum=35.964905, avg_loss=0.666017, seen=54, correct=30, accuracy=0.555556
2025-10-10 13:28:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:28:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:28:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:28:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2062MB allocated=2028MB
2025-10-10 13:28:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:28:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:28:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:28:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:28:23 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.831070, avg_loss=0.620777, seen=40, correct=28, accuracy=0.700000
2025-10-10 13:28:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:28:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:28:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:28:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2062MB allocated=2028MB
2025-10-10 13:28:24 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.700000
2025-10-10 13:28:35 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 13:28:35 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 13:28:35 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 13:28:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-10-10 13:28:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:28:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:28:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-10-10 13:28:36 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=54, loss_sum=36.087563, avg_loss=0.668288, seen=54, correct=32, accuracy=0.592593
2025-10-10 13:28:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:28:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:28:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:28:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2062MB allocated=2028MB
2025-10-10 13:28:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:28:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:28:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:28:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:28:39 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.125006, avg_loss=0.653125, seen=40, correct=23, accuracy=0.575000
2025-10-10 13:28:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:28:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:28:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:28:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2062MB allocated=2028MB
2025-10-10 13:28:41 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.700000, curr=0.575000
2025-10-10 13:28:51 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 13:28:51 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 13:28:51 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 13:28:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-10-10 13:28:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:28:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:28:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-10-10 13:28:52 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=54, loss_sum=36.976677, avg_loss=0.684753, seen=54, correct=30, accuracy=0.555556
2025-10-10 13:28:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:28:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:28:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:28:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2062MB allocated=2028MB
2025-10-10 13:28:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:28:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:28:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:28:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:28:58 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.152519, avg_loss=0.678813, seen=40, correct=22, accuracy=0.550000
2025-10-10 13:28:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:28:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:28:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:28:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2062MB allocated=2028MB
2025-10-10 13:28:59 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.700000, curr=0.550000
2025-10-10 13:29:10 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 13:29:10 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 13:29:10 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 13:29:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-10-10 13:29:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:29:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:29:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-10-10 13:29:11 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=54, loss_sum=36.059803, avg_loss=0.667774, seen=54, correct=30, accuracy=0.555556
2025-10-10 13:29:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:29:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:29:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:29:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2062MB allocated=2028MB
2025-10-10 13:29:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:29:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:29:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:29:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:29:15 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.925962, avg_loss=0.623149, seen=40, correct=26, accuracy=0.650000
2025-10-10 13:29:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:29:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:29:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:29:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2062MB allocated=2028MB
2025-10-10 13:29:17 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.700000, curr=0.650000
2025-10-10 13:29:26 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 13:29:26 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 13:29:26 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 13:29:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-10-10 13:29:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:29:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:29:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-10-10 13:29:27 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=54, loss_sum=35.627190, avg_loss=0.659763, seen=54, correct=34, accuracy=0.629630
2025-10-10 13:29:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:29:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:29:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:29:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2062MB allocated=2028MB
2025-10-10 13:29:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:29:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:29:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:29:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:29:31 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.404438, avg_loss=0.610111, seen=40, correct=27, accuracy=0.675000
2025-10-10 13:29:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:29:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:29:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:29:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2062MB allocated=2028MB
2025-10-10 13:29:33 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/25), best=0.700000, curr=0.675000
2025-10-10 13:29:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 13:29:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 13:29:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:29:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:29:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2062MB allocated=2028MB
2025-10-10 13:29:35 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #36', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 13:29:35 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #36', 'Round': 0, 'Results_raw': {}}
2025-10-10 13:29:35 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 13:29:36 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 13:29:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:29:36 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 13:29:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:29:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:29:38 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 13:29:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-10-10 13:29:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:29:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:29:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 13:29:41 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=136, loss_sum=84.176216, avg_loss=0.618943, seen=136, correct=92, accuracy=0.676471
2025-10-10 13:29:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:29:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:29:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:29:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2056MB allocated=2019MB
2025-10-10 13:29:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:29:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:29:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:29:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:29:45 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.796509, avg_loss=0.694913, seen=40, correct=25, accuracy=0.625000
2025-10-10 13:29:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:29:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:29:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:29:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2056MB allocated=2019MB
2025-10-10 13:29:47 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-10 13:29:47 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 13:29:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=648, total=2589)
2025-10-10 13:29:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:29:47 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 13:29:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:29:47 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 13:29:47 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=324, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 13:29:55 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 13:29:55 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 13:29:55 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 13:29:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-10-10 13:29:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:29:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:29:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 13:29:58 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=136, loss_sum=84.888229, avg_loss=0.624178, seen=136, correct=87, accuracy=0.639706
2025-10-10 13:29:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:29:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:29:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:30:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2102MB allocated=2036MB
2025-10-10 13:30:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:30:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:30:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:30:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:30:02 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.272034, avg_loss=0.656801, seen=40, correct=25, accuracy=0.625000
2025-10-10 13:30:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:30:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:30:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:30:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2102MB allocated=2036MB
2025-10-10 13:30:03 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-10 13:30:11 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 13:30:11 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 13:30:11 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 13:30:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-10-10 13:30:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:30:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:30:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 13:30:13 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=136, loss_sum=86.230972, avg_loss=0.634051, seen=136, correct=87, accuracy=0.639706
2025-10-10 13:30:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:30:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:30:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:30:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2102MB allocated=2036MB
2025-10-10 13:30:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:30:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:30:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:30:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:30:18 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.932585, avg_loss=0.748315, seen=40, correct=24, accuracy=0.600000
2025-10-10 13:30:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:30:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:30:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:30:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2102MB allocated=2036MB
2025-10-10 13:30:19 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.625000, curr=0.600000
2025-10-10 13:30:30 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 13:30:30 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 13:30:30 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 13:30:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-10-10 13:30:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:30:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:30:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 13:30:32 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=136, loss_sum=84.750618, avg_loss=0.623166, seen=136, correct=95, accuracy=0.698529
2025-10-10 13:30:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:30:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:30:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:30:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2102MB allocated=2036MB
2025-10-10 13:30:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:30:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:30:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:30:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:30:36 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.451662, avg_loss=0.711292, seen=40, correct=24, accuracy=0.600000
2025-10-10 13:30:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:30:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:30:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:30:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2102MB allocated=2036MB
2025-10-10 13:30:37 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.625000, curr=0.600000
2025-10-10 13:30:44 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 13:30:44 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 13:30:44 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 13:30:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-10-10 13:30:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:30:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:30:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 13:30:46 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=136, loss_sum=83.509491, avg_loss=0.614040, seen=136, correct=88, accuracy=0.647059
2025-10-10 13:30:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:30:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:30:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:30:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2102MB allocated=2036MB
2025-10-10 13:30:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:30:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:30:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:30:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:30:51 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.271227, avg_loss=0.656781, seen=40, correct=24, accuracy=0.600000
2025-10-10 13:30:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:30:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:30:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:30:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2102MB allocated=2036MB
2025-10-10 13:30:53 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.625000, curr=0.600000
2025-10-10 13:31:03 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 13:31:03 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 13:31:03 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 13:31:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-10-10 13:31:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:31:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:31:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 13:31:05 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=136, loss_sum=83.638138, avg_loss=0.614986, seen=136, correct=94, accuracy=0.691176
2025-10-10 13:31:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:31:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:31:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:31:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2102MB allocated=2036MB
2025-10-10 13:31:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:31:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:31:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:31:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:31:11 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.539854, avg_loss=0.688496, seen=40, correct=25, accuracy=0.625000
2025-10-10 13:31:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:31:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:31:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:31:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2102MB allocated=2036MB
2025-10-10 13:31:12 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-10 13:31:22 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 13:31:22 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 13:31:22 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 13:31:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-10-10 13:31:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:31:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:31:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 13:31:24 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=136, loss_sum=84.050201, avg_loss=0.618016, seen=136, correct=90, accuracy=0.661765
2025-10-10 13:31:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:31:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:31:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:31:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2102MB allocated=2036MB
2025-10-10 13:31:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:31:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:31:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:31:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:31:27 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.526665, avg_loss=0.663167, seen=40, correct=24, accuracy=0.600000
2025-10-10 13:31:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:31:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:31:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:31:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2102MB allocated=2036MB
2025-10-10 13:31:28 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.625000, curr=0.600000
2025-10-10 13:31:37 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 13:31:37 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 13:31:37 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 13:31:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-10-10 13:31:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:31:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:31:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 13:31:40 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=136, loss_sum=83.825035, avg_loss=0.616361, seen=136, correct=89, accuracy=0.654412
2025-10-10 13:31:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:31:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:31:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:31:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2102MB allocated=2036MB
2025-10-10 13:31:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:31:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:31:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:31:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:31:45 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.250854, avg_loss=0.656271, seen=40, correct=24, accuracy=0.600000
2025-10-10 13:31:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:31:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:31:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:31:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2102MB allocated=2036MB
2025-10-10 13:31:46 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.625000, curr=0.600000
2025-10-10 13:31:57 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 13:31:57 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 13:31:57 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 13:31:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-10-10 13:31:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:31:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:32:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 13:32:00 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=136, loss_sum=83.091736, avg_loss=0.610969, seen=136, correct=96, accuracy=0.705882
2025-10-10 13:32:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:32:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:32:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:32:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2102MB allocated=2036MB
2025-10-10 13:32:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:32:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:32:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:32:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:32:03 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.302095, avg_loss=0.657552, seen=40, correct=23, accuracy=0.575000
2025-10-10 13:32:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:32:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:32:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:32:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2102MB allocated=2036MB
2025-10-10 13:32:05 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.625000, curr=0.575000
2025-10-10 13:32:12 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 13:32:12 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 13:32:12 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 13:32:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-10-10 13:32:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:32:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:32:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 13:32:15 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=136, loss_sum=83.461494, avg_loss=0.613687, seen=136, correct=93, accuracy=0.683824
2025-10-10 13:32:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:32:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:32:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:32:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2102MB allocated=2036MB
2025-10-10 13:32:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:32:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:32:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:32:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:32:20 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.016144, avg_loss=0.650404, seen=40, correct=25, accuracy=0.625000
2025-10-10 13:32:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:32:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:32:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:32:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2102MB allocated=2036MB
2025-10-10 13:32:21 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-10 13:32:32 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 13:32:32 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 13:32:32 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 13:32:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-10-10 13:32:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:32:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:32:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 13:32:34 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=136, loss_sum=82.939247, avg_loss=0.609847, seen=136, correct=95, accuracy=0.698529
2025-10-10 13:32:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:32:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:32:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:32:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2102MB allocated=2036MB
2025-10-10 13:32:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:32:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:32:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:32:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:32:38 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.775436, avg_loss=0.669386, seen=40, correct=24, accuracy=0.600000
2025-10-10 13:32:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:32:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:32:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:32:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2102MB allocated=2036MB
2025-10-10 13:32:40 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.625000, curr=0.600000
2025-10-10 13:32:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 13:32:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 13:32:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:32:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:32:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2102MB allocated=2036MB
2025-10-10 13:32:41 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #16', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 13:32:41 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #16', 'Round': 0, 'Results_raw': {}}
2025-10-10 13:32:41 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 13:32:42 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 13:32:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:32:42 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 13:32:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:32:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:32:43 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 13:32:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-10-10 13:32:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:32:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:32:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 13:32:46 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=134, loss_sum=85.656860, avg_loss=0.639230, seen=134, correct=78, accuracy=0.582090
2025-10-10 13:32:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:32:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:32:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:32:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2056MB allocated=2028MB
2025-10-10 13:32:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:32:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:32:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:32:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:32:49 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.461769, avg_loss=0.761544, seen=40, correct=18, accuracy=0.450000
2025-10-10 13:32:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:32:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:32:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:32:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2056MB allocated=2028MB
2025-10-10 13:32:51 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.450000
2025-10-10 13:32:51 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 13:32:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=637, total=2547)
2025-10-10 13:32:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:32:51 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 13:32:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:32:51 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 13:32:51 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=319, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 13:33:00 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 13:33:00 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 13:33:00 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 13:33:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-10-10 13:33:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:33:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:33:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 13:33:02 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=134, loss_sum=93.957787, avg_loss=0.701178, seen=134, correct=83, accuracy=0.619403
2025-10-10 13:33:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:33:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:33:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:33:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2086MB allocated=2044MB
2025-10-10 13:33:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:33:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:33:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:33:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:33:07 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=32.570633, avg_loss=0.814266, seen=40, correct=22, accuracy=0.550000
2025-10-10 13:33:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:33:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:33:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:33:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2086MB allocated=2044MB
2025-10-10 13:33:08 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.550000
2025-10-10 13:33:19 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 13:33:19 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 13:33:19 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 13:33:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-10-10 13:33:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:33:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:33:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 13:33:21 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=134, loss_sum=85.357193, avg_loss=0.636994, seen=134, correct=80, accuracy=0.597015
2025-10-10 13:33:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:33:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:33:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:33:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2086MB allocated=2044MB
2025-10-10 13:33:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:33:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:33:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:33:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:33:26 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.809235, avg_loss=0.745231, seen=40, correct=19, accuracy=0.475000
2025-10-10 13:33:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:33:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:33:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:33:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2086MB allocated=2044MB
2025-10-10 13:33:27 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.550000, curr=0.475000
2025-10-10 13:33:37 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 13:33:37 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 13:33:37 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 13:33:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-10-10 13:33:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:33:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:33:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 13:33:40 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=134, loss_sum=84.966095, avg_loss=0.634075, seen=134, correct=77, accuracy=0.574627
2025-10-10 13:33:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:33:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:33:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:33:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2086MB allocated=2044MB
2025-10-10 13:33:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:33:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:33:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:33:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:33:45 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.828880, avg_loss=0.745722, seen=40, correct=20, accuracy=0.500000
2025-10-10 13:33:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:33:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:33:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:33:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2086MB allocated=2044MB
2025-10-10 13:33:47 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.550000, curr=0.500000
2025-10-10 13:33:56 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 13:33:56 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 13:33:56 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 13:33:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-10-10 13:33:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:33:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:33:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 13:33:58 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=134, loss_sum=85.119118, avg_loss=0.635217, seen=134, correct=79, accuracy=0.589552
2025-10-10 13:33:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:33:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:34:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:34:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2086MB allocated=2044MB
2025-10-10 13:34:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:34:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:34:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:34:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:34:03 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.467781, avg_loss=0.736695, seen=40, correct=21, accuracy=0.525000
2025-10-10 13:34:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:34:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:34:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:34:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2086MB allocated=2044MB
2025-10-10 13:34:06 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.550000, curr=0.525000
2025-10-10 13:34:15 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 13:34:15 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 13:34:15 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 13:34:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-10-10 13:34:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:34:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:34:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 13:34:17 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=134, loss_sum=85.310165, avg_loss=0.636643, seen=134, correct=84, accuracy=0.626866
2025-10-10 13:34:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:34:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:34:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:34:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2086MB allocated=2044MB
2025-10-10 13:34:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:34:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:34:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:34:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:34:22 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.396832, avg_loss=0.734921, seen=40, correct=21, accuracy=0.525000
2025-10-10 13:34:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:34:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:34:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:34:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2086MB allocated=2044MB
2025-10-10 13:34:23 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/25), best=0.550000, curr=0.525000
2025-10-10 13:34:32 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 13:34:32 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 13:34:32 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 13:34:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-10-10 13:34:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:34:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:34:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 13:34:34 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=134, loss_sum=85.741013, avg_loss=0.639858, seen=134, correct=85, accuracy=0.634328
2025-10-10 13:34:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:34:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:34:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:34:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2086MB allocated=2044MB
2025-10-10 13:34:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:34:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:34:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:34:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:34:39 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.615711, avg_loss=0.740393, seen=40, correct=20, accuracy=0.500000
2025-10-10 13:34:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:34:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:34:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:34:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2086MB allocated=2044MB
2025-10-10 13:34:42 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/25), best=0.550000, curr=0.500000
2025-10-10 13:34:51 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 13:34:51 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 13:34:51 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 13:34:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-10-10 13:34:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:34:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:34:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 13:34:53 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=134, loss_sum=84.890213, avg_loss=0.633509, seen=134, correct=81, accuracy=0.604478
2025-10-10 13:34:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:34:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:34:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:34:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2086MB allocated=2044MB
2025-10-10 13:34:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:34:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:34:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:34:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:34:56 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.789728, avg_loss=0.719743, seen=40, correct=20, accuracy=0.500000
2025-10-10 13:34:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:34:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:34:56 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:34:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2086MB allocated=2044MB
2025-10-10 13:34:57 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/25), best=0.550000, curr=0.500000
2025-10-10 13:35:06 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 13:35:06 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 13:35:06 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 13:35:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-10-10 13:35:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:35:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:35:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 13:35:08 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=134, loss_sum=84.742752, avg_loss=0.632409, seen=134, correct=84, accuracy=0.626866
2025-10-10 13:35:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:35:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:35:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:35:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2086MB allocated=2044MB
2025-10-10 13:35:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:35:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:35:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:35:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:35:13 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.031273, avg_loss=0.725782, seen=40, correct=22, accuracy=0.550000
2025-10-10 13:35:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:35:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:35:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:35:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2086MB allocated=2044MB
2025-10-10 13:35:14 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.550000
2025-10-10 13:35:23 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 13:35:23 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 13:35:23 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 13:35:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-10-10 13:35:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:35:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:35:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 13:35:25 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=134, loss_sum=84.500984, avg_loss=0.630604, seen=134, correct=85, accuracy=0.634328
2025-10-10 13:35:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:35:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:35:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:35:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2086MB allocated=2044MB
2025-10-10 13:35:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:35:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:35:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:35:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:35:30 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.551155, avg_loss=0.713779, seen=40, correct=22, accuracy=0.550000
2025-10-10 13:35:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:35:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:35:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:35:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2086MB allocated=2044MB
2025-10-10 13:35:31 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.550000
2025-10-10 13:35:39 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 13:35:39 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 13:35:39 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 13:35:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-10-10 13:35:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:35:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:35:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 13:35:41 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=134, loss_sum=85.107376, avg_loss=0.635130, seen=134, correct=82, accuracy=0.611940
2025-10-10 13:35:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:35:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:35:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:35:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2086MB allocated=2044MB
2025-10-10 13:35:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:35:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:35:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:35:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:35:46 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.537560, avg_loss=0.713439, seen=40, correct=21, accuracy=0.525000
2025-10-10 13:35:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:35:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:35:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:35:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2086MB allocated=2044MB
2025-10-10 13:35:48 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.550000, curr=0.525000
2025-10-10 13:35:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 13:35:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 13:35:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:35:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:35:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2086MB allocated=2044MB
2025-10-10 13:35:49 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #6', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 13:35:49 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #6', 'Round': 0, 'Results_raw': {}}
2025-10-10 13:35:50 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 13:35:50 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 13:35:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:35:51 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 13:35:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:35:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:35:53 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 13:35:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 13:35:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:35:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:35:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 13:35:56 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=136.635864, avg_loss=0.683179, seen=200, correct=110, accuracy=0.550000
2025-10-10 13:35:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:35:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:35:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:35:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2076MB allocated=2036MB
2025-10-10 13:35:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:35:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:35:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:36:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:36:00 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.032864, avg_loss=0.675822, seen=40, correct=23, accuracy=0.575000
2025-10-10 13:36:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:36:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:36:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:36:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2076MB allocated=2036MB
2025-10-10 13:36:03 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-10 13:36:03 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 13:36:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1548, total=6191)
2025-10-10 13:36:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:36:03 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 13:36:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:36:03 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 13:36:03 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=774, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 13:36:13 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 13:36:13 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 13:36:13 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 13:36:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 13:36:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:36:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:36:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 13:36:16 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=139.006500, avg_loss=0.695033, seen=200, correct=108, accuracy=0.540000
2025-10-10 13:36:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:36:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:36:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:36:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2110MB allocated=2053MB
2025-10-10 13:36:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:36:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:36:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:36:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:36:19 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.365513, avg_loss=0.709138, seen=40, correct=24, accuracy=0.600000
2025-10-10 13:36:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:36:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:36:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:36:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2110MB allocated=2053MB
2025-10-10 13:36:20 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-10 13:36:30 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 13:36:30 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 13:36:30 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 13:36:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 13:36:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:36:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:36:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 13:36:33 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=137.925079, avg_loss=0.689625, seen=200, correct=116, accuracy=0.580000
2025-10-10 13:36:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:36:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:36:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:36:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2110MB allocated=2053MB
2025-10-10 13:36:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:36:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:36:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:36:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:36:37 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.711536, avg_loss=0.642788, seen=40, correct=24, accuracy=0.600000
2025-10-10 13:36:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:36:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:36:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:36:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2110MB allocated=2053MB
2025-10-10 13:36:38 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-10 13:36:50 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 13:36:50 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 13:36:50 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 13:36:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 13:36:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:36:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:36:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 13:36:53 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=136.587173, avg_loss=0.682936, seen=200, correct=113, accuracy=0.565000
2025-10-10 13:36:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:36:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:36:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:36:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2110MB allocated=2053MB
2025-10-10 13:36:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:36:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:36:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:36:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:36:57 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.903814, avg_loss=0.647595, seen=40, correct=25, accuracy=0.625000
2025-10-10 13:36:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:36:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:36:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:36:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2110MB allocated=2053MB
2025-10-10 13:36:59 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-10 13:37:07 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 13:37:07 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 13:37:07 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 13:37:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 13:37:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:37:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:37:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 13:37:10 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=136.011826, avg_loss=0.680059, seen=200, correct=109, accuracy=0.545000
2025-10-10 13:37:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:37:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:37:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:37:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2110MB allocated=2053MB
2025-10-10 13:37:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:37:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:37:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:37:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:37:14 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.986942, avg_loss=0.674674, seen=40, correct=20, accuracy=0.500000
2025-10-10 13:37:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:37:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:37:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:37:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2110MB allocated=2053MB
2025-10-10 13:37:16 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.625000, curr=0.500000
2025-10-10 13:37:26 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 13:37:26 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 13:37:26 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 13:37:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 13:37:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:37:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:37:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 13:37:29 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=135.764481, avg_loss=0.678822, seen=200, correct=111, accuracy=0.555000
2025-10-10 13:37:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:37:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:37:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:37:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2110MB allocated=2053MB
2025-10-10 13:37:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:37:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:37:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:37:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:37:34 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.304712, avg_loss=0.682618, seen=40, correct=23, accuracy=0.575000
2025-10-10 13:37:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:37:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:37:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:37:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2110MB allocated=2053MB
2025-10-10 13:37:37 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.625000, curr=0.575000
2025-10-10 13:37:44 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 13:37:44 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 13:37:44 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 13:37:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 13:37:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:37:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:37:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 13:37:47 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=135.480057, avg_loss=0.677400, seen=200, correct=113, accuracy=0.565000
2025-10-10 13:37:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:37:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:37:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:37:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2110MB allocated=2053MB
2025-10-10 13:37:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:37:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:37:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:37:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:37:51 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.666229, avg_loss=0.666656, seen=40, correct=24, accuracy=0.600000
2025-10-10 13:37:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:37:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:37:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:37:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2110MB allocated=2053MB
2025-10-10 13:37:53 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.625000, curr=0.600000
2025-10-10 13:38:04 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 13:38:04 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 13:38:04 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 13:38:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 13:38:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:38:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:38:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 13:38:07 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=136.325745, avg_loss=0.681629, seen=200, correct=110, accuracy=0.550000
2025-10-10 13:38:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:38:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:38:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:38:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2110MB allocated=2053MB
2025-10-10 13:38:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:38:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:38:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:38:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:38:11 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.321306, avg_loss=0.683033, seen=40, correct=22, accuracy=0.550000
2025-10-10 13:38:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:38:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:38:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:38:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2110MB allocated=2053MB
2025-10-10 13:38:12 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/25), best=0.625000, curr=0.550000
2025-10-10 13:38:22 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 13:38:22 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 13:38:22 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 13:38:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 13:38:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:38:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:38:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 13:38:25 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=135.722366, avg_loss=0.678612, seen=200, correct=117, accuracy=0.585000
2025-10-10 13:38:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:38:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:38:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:38:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2110MB allocated=2053MB
2025-10-10 13:38:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:38:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:38:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:38:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:38:29 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.116978, avg_loss=0.652924, seen=40, correct=25, accuracy=0.625000
2025-10-10 13:38:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:38:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:38:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:38:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2110MB allocated=2053MB
2025-10-10 13:38:32 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-10 13:38:41 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 13:38:41 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 13:38:41 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 13:38:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 13:38:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:38:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:38:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 13:38:44 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=136.400085, avg_loss=0.682000, seen=200, correct=108, accuracy=0.540000
2025-10-10 13:38:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:38:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:38:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:38:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2110MB allocated=2053MB
2025-10-10 13:38:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:38:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:38:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:38:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:38:50 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.936291, avg_loss=0.673407, seen=40, correct=25, accuracy=0.625000
2025-10-10 13:38:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:38:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:38:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:38:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2110MB allocated=2053MB
2025-10-10 13:38:52 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-10 13:39:00 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 13:39:00 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 13:39:00 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 13:39:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 13:39:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:39:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:39:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 13:39:03 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=136.335342, avg_loss=0.681677, seen=200, correct=108, accuracy=0.540000
2025-10-10 13:39:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:39:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:39:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:39:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2110MB allocated=2053MB
2025-10-10 13:39:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:39:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:39:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:39:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:39:07 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.364285, avg_loss=0.684107, seen=40, correct=23, accuracy=0.575000
2025-10-10 13:39:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:39:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:39:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:39:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2110MB allocated=2053MB
2025-10-10 13:39:08 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.625000, curr=0.575000
2025-10-10 13:39:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 13:39:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 13:39:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:39:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:39:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2110MB allocated=2053MB
2025-10-10 13:39:10 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #29', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 13:39:10 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #29', 'Round': 0, 'Results_raw': {}}
2025-10-10 13:39:10 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 13:39:10 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 13:39:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:39:11 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 13:39:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:39:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:39:13 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 13:39:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 13:39:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:39:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:39:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 13:39:16 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=130.977829, avg_loss=0.654889, seen=200, correct=123, accuracy=0.615000
2025-10-10 13:39:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:39:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:39:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:39:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2076MB allocated=2044MB
2025-10-10 13:39:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:39:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:39:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:39:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:39:20 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.485062, avg_loss=0.662127, seen=40, correct=26, accuracy=0.650000
2025-10-10 13:39:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:39:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:39:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:39:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2076MB allocated=2044MB
2025-10-10 13:39:22 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.650000
2025-10-10 13:39:22 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 13:39:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1471, total=5883)
2025-10-10 13:39:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:39:23 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 13:39:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:39:23 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 13:39:23 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=736, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 13:39:33 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 13:39:33 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 13:39:33 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 13:39:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 13:39:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:39:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:39:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 13:39:36 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=133.873642, avg_loss=0.669368, seen=200, correct=115, accuracy=0.575000
2025-10-10 13:39:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:39:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:39:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:39:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2114MB allocated=2061MB
2025-10-10 13:39:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:39:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:39:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:39:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:39:40 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.603914, avg_loss=0.690098, seen=40, correct=25, accuracy=0.625000
2025-10-10 13:39:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:39:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:39:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:39:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2114MB allocated=2061MB
2025-10-10 13:39:42 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.650000, curr=0.625000
2025-10-10 13:39:49 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 13:39:49 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 13:39:49 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 13:39:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 13:39:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:39:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:39:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 13:39:52 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=131.724457, avg_loss=0.658622, seen=200, correct=126, accuracy=0.630000
2025-10-10 13:39:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:39:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:39:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:39:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2114MB allocated=2061MB
2025-10-10 13:39:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:39:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:39:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:39:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:39:57 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.276764, avg_loss=0.681919, seen=40, correct=23, accuracy=0.575000
2025-10-10 13:39:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:39:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:39:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:39:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2114MB allocated=2061MB
2025-10-10 13:39:58 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.650000, curr=0.575000
2025-10-10 13:40:08 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 13:40:08 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 13:40:09 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 13:40:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 13:40:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:40:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:40:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 13:40:12 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=131.413773, avg_loss=0.657069, seen=200, correct=122, accuracy=0.610000
2025-10-10 13:40:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:40:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:40:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:40:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2114MB allocated=2061MB
2025-10-10 13:40:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:40:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:40:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:40:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:40:15 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.920122, avg_loss=0.673003, seen=40, correct=27, accuracy=0.675000
2025-10-10 13:40:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:40:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:40:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:40:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2114MB allocated=2061MB
2025-10-10 13:40:16 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.675000
2025-10-10 13:40:27 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 13:40:27 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 13:40:27 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 13:40:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 13:40:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:40:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:40:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 13:40:30 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=134.145416, avg_loss=0.670727, seen=200, correct=115, accuracy=0.575000
2025-10-10 13:40:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:40:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:40:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:40:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2114MB allocated=2061MB
2025-10-10 13:40:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:40:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:40:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:40:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:40:34 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.060951, avg_loss=0.701524, seen=40, correct=23, accuracy=0.575000
2025-10-10 13:40:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:40:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:40:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:40:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2114MB allocated=2061MB
2025-10-10 13:40:35 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.675000, curr=0.575000
2025-10-10 13:40:44 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 13:40:44 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 13:40:44 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 13:40:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 13:40:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:40:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:40:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 13:40:47 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=129.202103, avg_loss=0.646011, seen=200, correct=125, accuracy=0.625000
2025-10-10 13:40:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:40:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:40:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:40:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2114MB allocated=2061MB
2025-10-10 13:40:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:40:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:40:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:40:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:40:51 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.490894, avg_loss=0.662272, seen=40, correct=25, accuracy=0.625000
2025-10-10 13:40:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:40:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:40:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:40:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2114MB allocated=2061MB
2025-10-10 13:40:53 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.675000, curr=0.625000
2025-10-10 13:41:03 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 13:41:03 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 13:41:03 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 13:41:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 13:41:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:41:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:41:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 13:41:06 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=128.814957, avg_loss=0.644075, seen=200, correct=121, accuracy=0.605000
2025-10-10 13:41:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:41:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:41:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:41:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2114MB allocated=2061MB
2025-10-10 13:41:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:41:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:41:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:41:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:41:09 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.737461, avg_loss=0.668437, seen=40, correct=26, accuracy=0.650000
2025-10-10 13:41:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:41:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:41:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:41:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2114MB allocated=2061MB
2025-10-10 13:41:10 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.675000, curr=0.650000
2025-10-10 13:41:18 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 13:41:18 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 13:41:18 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 13:41:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 13:41:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:41:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:41:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 13:41:21 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=128.456116, avg_loss=0.642281, seen=200, correct=127, accuracy=0.635000
2025-10-10 13:41:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:41:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:41:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:41:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2114MB allocated=2061MB
2025-10-10 13:41:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:41:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:41:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:41:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:41:26 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.961697, avg_loss=0.674042, seen=40, correct=23, accuracy=0.575000
2025-10-10 13:41:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:41:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:41:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:41:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2114MB allocated=2061MB
2025-10-10 13:41:27 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/25), best=0.675000, curr=0.575000
2025-10-10 13:41:38 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 13:41:38 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 13:41:38 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 13:41:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 13:41:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:41:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:41:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 13:41:41 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=127.455315, avg_loss=0.637277, seen=200, correct=126, accuracy=0.630000
2025-10-10 13:41:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:41:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:41:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:41:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2114MB allocated=2061MB
2025-10-10 13:41:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:41:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:41:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:41:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:41:45 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.360554, avg_loss=0.659014, seen=40, correct=26, accuracy=0.650000
2025-10-10 13:41:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:41:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:41:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:41:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2114MB allocated=2061MB
2025-10-10 13:41:47 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/25), best=0.675000, curr=0.650000
2025-10-10 13:41:53 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 13:41:53 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 13:41:53 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 13:41:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 13:41:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:41:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:41:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 13:41:56 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=127.663200, avg_loss=0.638316, seen=200, correct=124, accuracy=0.620000
2025-10-10 13:41:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:41:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:41:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:41:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2114MB allocated=2061MB
2025-10-10 13:41:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:41:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:41:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:42:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:42:01 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.384705, avg_loss=0.659618, seen=40, correct=25, accuracy=0.625000
2025-10-10 13:42:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:42:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:42:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:42:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2114MB allocated=2061MB
2025-10-10 13:42:02 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/25), best=0.675000, curr=0.625000
2025-10-10 13:42:13 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 13:42:13 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 13:42:13 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 13:42:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 13:42:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:42:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:42:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 13:42:16 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=133.907349, avg_loss=0.669537, seen=200, correct=114, accuracy=0.570000
2025-10-10 13:42:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:42:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:42:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:42:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2114MB allocated=2061MB
2025-10-10 13:42:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:42:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:42:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:42:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:42:20 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.114941, avg_loss=0.702874, seen=40, correct=22, accuracy=0.550000
2025-10-10 13:42:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:42:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:42:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:42:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2114MB allocated=2061MB
2025-10-10 13:42:23 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/25), best=0.675000, curr=0.550000
2025-10-10 13:42:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 13:42:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 13:42:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:42:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:42:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2114MB allocated=2061MB
2025-10-10 13:42:24 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #17', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 13:42:24 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #17', 'Round': 0, 'Results_raw': {}}
2025-10-10 13:42:24 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 13:42:25 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 13:42:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:42:25 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 13:42:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:42:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:42:26 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 13:42:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-10 13:42:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:42:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:42:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-10 13:42:28 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=73.085724, avg_loss=0.664416, seen=110, correct=69, accuracy=0.627273
2025-10-10 13:42:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:42:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:42:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:42:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2076MB allocated=2053MB
2025-10-10 13:42:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:42:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:42:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:42:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:42:32 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.327518, avg_loss=0.608188, seen=40, correct=30, accuracy=0.750000
2025-10-10 13:42:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:42:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:42:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:42:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2076MB allocated=2053MB
2025-10-10 13:42:33 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.750000
2025-10-10 13:42:33 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 13:42:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=525, total=2100)
2025-10-10 13:42:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:42:34 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 13:42:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:42:34 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 13:42:34 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=263, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 13:42:43 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 13:42:43 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 13:42:43 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 13:42:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-10 13:42:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:42:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:42:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-10 13:42:45 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=73.841972, avg_loss=0.671291, seen=110, correct=65, accuracy=0.590909
2025-10-10 13:42:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:42:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:42:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:42:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2106MB allocated=2070MB
2025-10-10 13:42:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:42:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:42:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:42:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:42:50 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.809948, avg_loss=0.620249, seen=40, correct=29, accuracy=0.725000
2025-10-10 13:42:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:42:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:42:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:42:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2106MB allocated=2070MB
2025-10-10 13:42:52 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.750000, curr=0.725000
2025-10-10 13:43:01 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 13:43:01 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 13:43:01 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 13:43:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-10 13:43:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:43:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:43:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-10 13:43:03 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=72.237000, avg_loss=0.656700, seen=110, correct=61, accuracy=0.554545
2025-10-10 13:43:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:43:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:43:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:43:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2106MB allocated=2070MB
2025-10-10 13:43:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:43:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:43:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:43:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:43:06 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.720001, avg_loss=0.593000, seen=40, correct=30, accuracy=0.750000
2025-10-10 13:43:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:43:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:43:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:43:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2106MB allocated=2070MB
2025-10-10 13:43:08 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.750000
2025-10-10 13:43:18 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 13:43:18 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 13:43:18 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 13:43:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-10 13:43:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:43:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:43:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-10 13:43:20 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=72.221535, avg_loss=0.656559, seen=110, correct=68, accuracy=0.618182
2025-10-10 13:43:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:43:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:43:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:43:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2106MB allocated=2070MB
2025-10-10 13:43:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:43:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:43:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:43:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:43:25 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.118439, avg_loss=0.602961, seen=40, correct=28, accuracy=0.700000
2025-10-10 13:43:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:43:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:43:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:43:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2106MB allocated=2070MB
2025-10-10 13:43:28 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.750000, curr=0.700000
2025-10-10 13:43:38 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 13:43:38 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 13:43:38 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 13:43:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-10 13:43:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:43:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:43:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-10 13:43:40 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=72.200348, avg_loss=0.656367, seen=110, correct=69, accuracy=0.627273
2025-10-10 13:43:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:43:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:43:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:43:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2106MB allocated=2070MB
2025-10-10 13:43:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:43:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:43:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:43:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:43:45 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.250111, avg_loss=0.606253, seen=40, correct=30, accuracy=0.750000
2025-10-10 13:43:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:43:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:43:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:43:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2106MB allocated=2070MB
2025-10-10 13:43:47 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.750000
2025-10-10 13:43:55 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 13:43:55 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 13:43:55 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 13:43:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-10 13:43:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:43:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:43:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-10 13:43:57 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=72.326950, avg_loss=0.657518, seen=110, correct=66, accuracy=0.600000
2025-10-10 13:43:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:43:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:43:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:43:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2106MB allocated=2070MB
2025-10-10 13:43:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:43:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:43:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:44:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:44:01 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.994072, avg_loss=0.599852, seen=40, correct=30, accuracy=0.750000
2025-10-10 13:44:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:44:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:44:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:44:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2106MB allocated=2070MB
2025-10-10 13:44:03 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.750000
2025-10-10 13:44:11 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 13:44:11 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 13:44:11 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 13:44:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-10 13:44:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:44:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:44:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-10 13:44:13 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=72.311859, avg_loss=0.657381, seen=110, correct=64, accuracy=0.581818
2025-10-10 13:44:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:44:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:44:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:44:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2106MB allocated=2070MB
2025-10-10 13:44:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:44:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:44:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:44:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:44:18 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.692478, avg_loss=0.592312, seen=40, correct=29, accuracy=0.725000
2025-10-10 13:44:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:44:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:44:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:44:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2106MB allocated=2070MB
2025-10-10 13:44:20 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.750000, curr=0.725000
2025-10-10 13:44:29 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 13:44:29 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 13:44:29 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 13:44:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-10 13:44:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:44:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:44:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-10 13:44:31 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=73.216667, avg_loss=0.665606, seen=110, correct=64, accuracy=0.581818
2025-10-10 13:44:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:44:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:44:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:44:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2106MB allocated=2070MB
2025-10-10 13:44:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:44:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:44:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:44:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:44:36 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.387108, avg_loss=0.584678, seen=40, correct=28, accuracy=0.700000
2025-10-10 13:44:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:44:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:44:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:44:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2106MB allocated=2070MB
2025-10-10 13:44:37 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.750000, curr=0.700000
2025-10-10 13:44:45 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 13:44:45 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 13:44:45 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 13:44:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-10 13:44:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:44:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:44:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-10 13:44:47 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=73.309151, avg_loss=0.666447, seen=110, correct=66, accuracy=0.600000
2025-10-10 13:44:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:44:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:44:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:44:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2106MB allocated=2070MB
2025-10-10 13:44:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:44:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:44:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:44:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:44:51 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.373072, avg_loss=0.584327, seen=40, correct=27, accuracy=0.675000
2025-10-10 13:44:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:44:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:44:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:44:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2106MB allocated=2070MB
2025-10-10 13:44:53 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.750000, curr=0.675000
2025-10-10 13:45:01 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 13:45:01 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 13:45:01 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 13:45:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-10 13:45:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:45:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:45:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-10 13:45:03 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=72.070801, avg_loss=0.655189, seen=110, correct=65, accuracy=0.590909
2025-10-10 13:45:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:45:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:45:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:45:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2106MB allocated=2070MB
2025-10-10 13:45:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:45:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:45:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:45:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:45:09 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.922075, avg_loss=0.598052, seen=40, correct=30, accuracy=0.750000
2025-10-10 13:45:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:45:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:45:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:45:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2106MB allocated=2070MB
2025-10-10 13:45:10 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.750000
2025-10-10 13:45:18 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 13:45:18 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 13:45:18 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 13:45:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-10 13:45:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:45:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:45:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-10 13:45:20 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=71.890854, avg_loss=0.653553, seen=110, correct=67, accuracy=0.609091
2025-10-10 13:45:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:45:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:45:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:45:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2106MB allocated=2070MB
2025-10-10 13:45:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:45:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:45:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:45:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:45:25 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.732630, avg_loss=0.593316, seen=40, correct=28, accuracy=0.700000
2025-10-10 13:45:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:45:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:45:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:45:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2106MB allocated=2070MB
2025-10-10 13:45:27 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.750000, curr=0.700000
2025-10-10 13:45:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 13:45:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 13:45:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:45:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:45:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2106MB allocated=2070MB
2025-10-10 13:45:28 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #46', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 13:45:28 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #46', 'Round': 0, 'Results_raw': {}}
2025-10-10 13:45:29 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 13:45:29 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 13:45:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:45:30 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 13:45:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:45:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:45:32 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 13:45:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-10-10 13:45:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:45:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:45:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-10-10 13:45:34 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=153, loss_sum=105.851639, avg_loss=0.691841, seen=153, correct=89, accuracy=0.581699
2025-10-10 13:45:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:45:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:45:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:45:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2098MB allocated=2061MB
2025-10-10 13:45:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:45:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:45:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:45:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:45:39 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.765299, avg_loss=0.744132, seen=40, correct=19, accuracy=0.475000
2025-10-10 13:45:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:45:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:45:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:45:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2098MB allocated=2061MB
2025-10-10 13:45:41 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.475000
2025-10-10 13:45:41 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 13:45:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=729, total=2915)
2025-10-10 13:45:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:45:41 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 13:45:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:45:41 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 13:45:41 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=365, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 13:45:50 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 13:45:50 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 13:45:50 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 13:45:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-10-10 13:45:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:45:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:45:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-10-10 13:45:53 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=153, loss_sum=106.476746, avg_loss=0.695926, seen=153, correct=78, accuracy=0.509804
2025-10-10 13:45:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:45:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:45:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:45:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2130MB allocated=2078MB
2025-10-10 13:45:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:45:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:45:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:45:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:45:56 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.940180, avg_loss=0.673504, seen=40, correct=23, accuracy=0.575000
2025-10-10 13:45:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:45:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:45:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:45:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2130MB allocated=2078MB
2025-10-10 13:45:58 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-10 13:46:06 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 13:46:06 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 13:46:06 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 13:46:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-10-10 13:46:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:46:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:46:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-10-10 13:46:09 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=153, loss_sum=105.125145, avg_loss=0.687092, seen=153, correct=87, accuracy=0.568627
2025-10-10 13:46:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:46:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:46:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:46:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2130MB allocated=2078MB
2025-10-10 13:46:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:46:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:46:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:46:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:46:13 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.064926, avg_loss=0.676623, seen=40, correct=23, accuracy=0.575000
2025-10-10 13:46:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:46:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:46:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:46:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2130MB allocated=2078MB
2025-10-10 13:46:14 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-10 13:46:24 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 13:46:24 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 13:46:24 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 13:46:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-10-10 13:46:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:46:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:46:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-10-10 13:46:27 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=153, loss_sum=104.961746, avg_loss=0.686024, seen=153, correct=84, accuracy=0.549020
2025-10-10 13:46:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:46:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:46:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:46:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2130MB allocated=2078MB
2025-10-10 13:46:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:46:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:46:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:46:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:46:31 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.113970, avg_loss=0.702849, seen=40, correct=20, accuracy=0.500000
2025-10-10 13:46:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:46:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:46:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:46:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2130MB allocated=2078MB
2025-10-10 13:46:33 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.575000, curr=0.500000
2025-10-10 13:46:40 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 13:46:40 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 13:46:40 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 13:46:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-10-10 13:46:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:46:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:46:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-10-10 13:46:42 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=153, loss_sum=104.818947, avg_loss=0.685091, seen=153, correct=84, accuracy=0.549020
2025-10-10 13:46:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:46:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:46:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:46:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2130MB allocated=2078MB
2025-10-10 13:46:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:46:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:46:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:46:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:46:46 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.461952, avg_loss=0.686549, seen=40, correct=21, accuracy=0.525000
2025-10-10 13:46:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:46:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:46:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:46:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2130MB allocated=2078MB
2025-10-10 13:46:49 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.575000, curr=0.525000
2025-10-10 13:46:59 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 13:46:59 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 13:46:59 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 13:47:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-10-10 13:47:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:47:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:47:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-10-10 13:47:02 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=153, loss_sum=104.653923, avg_loss=0.684013, seen=153, correct=83, accuracy=0.542484
2025-10-10 13:47:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:47:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:47:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:47:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2130MB allocated=2078MB
2025-10-10 13:47:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:47:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:47:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:47:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:47:06 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.847286, avg_loss=0.671182, seen=40, correct=24, accuracy=0.600000
2025-10-10 13:47:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:47:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:47:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:47:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2130MB allocated=2078MB
2025-10-10 13:47:07 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-10 13:47:16 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 13:47:16 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 13:47:16 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 13:47:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-10-10 13:47:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:47:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:47:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-10-10 13:47:18 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=153, loss_sum=104.123993, avg_loss=0.680549, seen=153, correct=93, accuracy=0.607843
2025-10-10 13:47:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:47:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:47:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:47:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2130MB allocated=2078MB
2025-10-10 13:47:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:47:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:47:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:47:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:47:22 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.231003, avg_loss=0.680775, seen=40, correct=25, accuracy=0.625000
2025-10-10 13:47:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:47:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:47:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:47:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2130MB allocated=2078MB
2025-10-10 13:47:23 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-10 13:47:34 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 13:47:34 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 13:47:34 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 13:47:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-10-10 13:47:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:47:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:47:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-10-10 13:47:36 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=153, loss_sum=105.090042, avg_loss=0.686863, seen=153, correct=84, accuracy=0.549020
2025-10-10 13:47:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:47:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:47:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:47:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2130MB allocated=2078MB
2025-10-10 13:47:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:47:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:47:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:47:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:47:41 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.147564, avg_loss=0.653689, seen=40, correct=26, accuracy=0.650000
2025-10-10 13:47:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:47:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:47:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:47:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2130MB allocated=2078MB
2025-10-10 13:47:44 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.650000
2025-10-10 13:47:55 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 13:47:55 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 13:47:55 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 13:47:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-10-10 13:47:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:47:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:47:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-10-10 13:47:57 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=153, loss_sum=105.773071, avg_loss=0.691327, seen=153, correct=81, accuracy=0.529412
2025-10-10 13:47:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:47:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:48:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:48:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2130MB allocated=2078MB
2025-10-10 13:48:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:48:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:48:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:48:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:48:02 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.202099, avg_loss=0.655052, seen=40, correct=26, accuracy=0.650000
2025-10-10 13:48:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:48:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:48:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:48:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2130MB allocated=2078MB
2025-10-10 13:48:03 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.650000
2025-10-10 13:48:14 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 13:48:14 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 13:48:14 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 13:48:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-10-10 13:48:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:48:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:48:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-10-10 13:48:17 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=153, loss_sum=104.184204, avg_loss=0.680943, seen=153, correct=85, accuracy=0.555556
2025-10-10 13:48:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:48:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:48:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:48:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2130MB allocated=2078MB
2025-10-10 13:48:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:48:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:48:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:48:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:48:22 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.494976, avg_loss=0.712374, seen=40, correct=17, accuracy=0.425000
2025-10-10 13:48:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:48:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:48:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:48:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2130MB allocated=2078MB
2025-10-10 13:48:24 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.650000, curr=0.425000
2025-10-10 13:48:34 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 13:48:34 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 13:48:34 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 13:48:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-10-10 13:48:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:48:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:48:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-10-10 13:48:37 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=153, loss_sum=105.106941, avg_loss=0.686973, seen=153, correct=82, accuracy=0.535948
2025-10-10 13:48:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:48:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:48:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:48:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2130MB allocated=2078MB
2025-10-10 13:48:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:48:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:48:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:48:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:48:41 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.639971, avg_loss=0.740999, seen=40, correct=16, accuracy=0.400000
2025-10-10 13:48:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:48:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:48:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:48:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2130MB allocated=2078MB
2025-10-10 13:48:42 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.650000, curr=0.400000
2025-10-10 13:48:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 13:48:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 13:48:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:48:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:48:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2130MB allocated=2078MB
2025-10-10 13:48:43 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #21', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 13:48:43 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #21', 'Round': 0, 'Results_raw': {}}
2025-10-10 13:48:43 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 13:48:44 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 13:48:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:48:45 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 13:48:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:48:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:48:46 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 13:48:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-10-10 13:48:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:48:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:48:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-10 13:48:48 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=147, loss_sum=102.007812, avg_loss=0.693931, seen=147, correct=82, accuracy=0.557823
2025-10-10 13:48:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:48:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:48:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:48:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2098MB allocated=2070MB
2025-10-10 13:48:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:48:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:48:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:48:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:48:53 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.586702, avg_loss=0.689668, seen=40, correct=22, accuracy=0.550000
2025-10-10 13:48:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:48:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:48:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:48:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2098MB allocated=2070MB
2025-10-10 13:48:55 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.550000
2025-10-10 13:48:55 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 13:48:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=703, total=2812)
2025-10-10 13:48:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:48:55 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 13:48:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:48:55 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 13:48:55 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=352, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 13:49:05 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 13:49:05 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 13:49:05 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 13:49:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-10-10 13:49:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:49:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:49:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-10 13:49:08 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=147, loss_sum=102.519112, avg_loss=0.697409, seen=147, correct=80, accuracy=0.544218
2025-10-10 13:49:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:49:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:49:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:49:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2124MB allocated=2086MB
2025-10-10 13:49:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:49:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:49:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:49:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:49:13 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.707800, avg_loss=0.692695, seen=40, correct=21, accuracy=0.525000
2025-10-10 13:49:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:49:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:49:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:49:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2124MB allocated=2086MB
2025-10-10 13:49:15 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.550000, curr=0.525000
2025-10-10 13:49:25 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 13:49:25 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 13:49:25 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 13:49:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-10-10 13:49:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:49:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:49:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-10 13:49:27 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=147, loss_sum=103.258209, avg_loss=0.702437, seen=147, correct=74, accuracy=0.503401
2025-10-10 13:49:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:49:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:49:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:49:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2124MB allocated=2086MB
2025-10-10 13:49:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:49:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:49:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:49:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:49:32 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.956316, avg_loss=0.698908, seen=40, correct=21, accuracy=0.525000
2025-10-10 13:49:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:49:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:49:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:49:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2124MB allocated=2086MB
2025-10-10 13:49:34 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.550000, curr=0.525000
2025-10-10 13:49:43 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 13:49:43 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 13:49:43 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 13:49:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-10-10 13:49:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:49:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:49:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-10 13:49:45 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=147, loss_sum=101.666016, avg_loss=0.691606, seen=147, correct=85, accuracy=0.578231
2025-10-10 13:49:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:49:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:49:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:49:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2124MB allocated=2086MB
2025-10-10 13:49:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:49:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:49:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:49:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:49:50 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.823603, avg_loss=0.695590, seen=40, correct=22, accuracy=0.550000
2025-10-10 13:49:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:49:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:49:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:49:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2124MB allocated=2086MB
2025-10-10 13:49:52 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.550000
2025-10-10 13:50:01 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 13:50:01 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 13:50:01 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 13:50:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-10-10 13:50:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:50:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:50:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-10 13:50:03 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=147, loss_sum=102.999359, avg_loss=0.700676, seen=147, correct=80, accuracy=0.544218
2025-10-10 13:50:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:50:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:50:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:50:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2124MB allocated=2086MB
2025-10-10 13:50:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:50:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:50:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:50:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:50:06 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.465820, avg_loss=0.711646, seen=40, correct=24, accuracy=0.600000
2025-10-10 13:50:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:50:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:50:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:50:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2124MB allocated=2086MB
2025-10-10 13:50:07 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-10 13:50:15 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 13:50:15 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 13:50:15 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 13:50:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-10-10 13:50:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:50:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:50:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-10 13:50:18 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=147, loss_sum=101.597977, avg_loss=0.691143, seen=147, correct=84, accuracy=0.571429
2025-10-10 13:50:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:50:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:50:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:50:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2124MB allocated=2086MB
2025-10-10 13:50:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:50:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:50:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:50:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:50:23 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.460484, avg_loss=0.686512, seen=40, correct=23, accuracy=0.575000
2025-10-10 13:50:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:50:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:50:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:50:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2124MB allocated=2086MB
2025-10-10 13:50:24 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.600000, curr=0.575000
2025-10-10 13:50:34 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 13:50:34 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 13:50:34 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 13:50:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-10-10 13:50:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:50:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:50:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-10 13:50:36 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=147, loss_sum=101.463181, avg_loss=0.690226, seen=147, correct=85, accuracy=0.578231
2025-10-10 13:50:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:50:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:50:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:50:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2124MB allocated=2086MB
2025-10-10 13:50:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:50:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:50:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:50:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:50:41 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.281136, avg_loss=0.682028, seen=40, correct=24, accuracy=0.600000
2025-10-10 13:50:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:50:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:50:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:50:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2124MB allocated=2086MB
2025-10-10 13:50:43 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-10 13:50:51 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 13:50:51 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 13:50:51 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 13:50:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-10-10 13:50:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:50:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:50:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-10 13:50:53 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=147, loss_sum=102.016518, avg_loss=0.693990, seen=147, correct=85, accuracy=0.578231
2025-10-10 13:50:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:50:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:50:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:50:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2124MB allocated=2086MB
2025-10-10 13:50:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:50:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:50:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:50:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:50:57 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.154644, avg_loss=0.703866, seen=40, correct=22, accuracy=0.550000
2025-10-10 13:50:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:50:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:50:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:50:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2124MB allocated=2086MB
2025-10-10 13:50:58 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.600000, curr=0.550000
2025-10-10 13:51:07 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 13:51:07 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 13:51:07 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 13:51:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-10-10 13:51:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:51:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:51:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-10 13:51:09 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=147, loss_sum=101.163925, avg_loss=0.688190, seen=147, correct=86, accuracy=0.585034
2025-10-10 13:51:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:51:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:51:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:51:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2124MB allocated=2086MB
2025-10-10 13:51:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:51:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:51:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:51:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:51:14 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.330338, avg_loss=0.683258, seen=40, correct=22, accuracy=0.550000
2025-10-10 13:51:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:51:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:51:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:51:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2124MB allocated=2086MB
2025-10-10 13:51:16 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.600000, curr=0.550000
2025-10-10 13:51:26 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 13:51:26 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 13:51:26 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 13:51:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-10-10 13:51:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:51:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:51:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-10 13:51:29 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=147, loss_sum=101.747101, avg_loss=0.692157, seen=147, correct=89, accuracy=0.605442
2025-10-10 13:51:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:51:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:51:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:51:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2124MB allocated=2086MB
2025-10-10 13:51:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:51:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:51:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:51:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:51:33 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.333811, avg_loss=0.683345, seen=40, correct=22, accuracy=0.550000
2025-10-10 13:51:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:51:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:51:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:51:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2124MB allocated=2086MB
2025-10-10 13:51:35 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.600000, curr=0.550000
2025-10-10 13:51:45 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 13:51:45 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 13:51:45 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 13:51:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-10-10 13:51:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:51:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:51:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-10 13:51:48 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=147, loss_sum=101.651016, avg_loss=0.691504, seen=147, correct=81, accuracy=0.551020
2025-10-10 13:51:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:51:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:51:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:51:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2124MB allocated=2086MB
2025-10-10 13:51:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:51:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:51:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:51:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:51:52 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.920279, avg_loss=0.698007, seen=40, correct=23, accuracy=0.575000
2025-10-10 13:51:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:51:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:51:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:51:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2124MB allocated=2086MB
2025-10-10 13:51:53 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/25), best=0.600000, curr=0.575000
2025-10-10 13:51:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 13:51:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 13:51:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:51:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:51:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2124MB allocated=2086MB
2025-10-10 13:51:54 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #47', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 13:51:54 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #47', 'Round': 0, 'Results_raw': {}}
2025-10-10 13:51:55 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 13:51:56 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 13:51:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:51:56 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 13:51:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:51:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:51:57 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 13:51:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-10 13:51:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:51:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:52:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-10 13:52:00 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=123.908478, avg_loss=0.659088, seen=188, correct=118, accuracy=0.627660
2025-10-10 13:52:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:52:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:52:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:52:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2116MB allocated=2078MB
2025-10-10 13:52:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:52:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:52:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:52:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:52:04 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.698412, avg_loss=0.692460, seen=40, correct=23, accuracy=0.575000
2025-10-10 13:52:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:52:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:52:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:52:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2116MB allocated=2078MB
2025-10-10 13:52:06 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-10 13:52:06 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 13:52:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=893, total=3572)
2025-10-10 13:52:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:52:06 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 13:52:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:52:06 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 13:52:06 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=447, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 13:52:14 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 13:52:14 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 13:52:14 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 13:52:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-10 13:52:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:52:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:52:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-10 13:52:17 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=122.204437, avg_loss=0.650024, seen=188, correct=106, accuracy=0.563830
2025-10-10 13:52:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:52:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:52:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:52:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2150MB allocated=2095MB
2025-10-10 13:52:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:52:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:52:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:52:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:52:21 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.052773, avg_loss=0.751319, seen=40, correct=19, accuracy=0.475000
2025-10-10 13:52:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:52:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:52:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:52:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2150MB allocated=2095MB
2025-10-10 13:52:22 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.575000, curr=0.475000
2025-10-10 13:52:31 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 13:52:31 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 13:52:31 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 13:52:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-10 13:52:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:52:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:52:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-10 13:52:34 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=122.850197, avg_loss=0.653458, seen=188, correct=112, accuracy=0.595745
2025-10-10 13:52:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:52:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:52:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:52:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2150MB allocated=2095MB
2025-10-10 13:52:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:52:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:52:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:52:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:52:38 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.430731, avg_loss=0.710768, seen=40, correct=22, accuracy=0.550000
2025-10-10 13:52:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:52:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:52:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:52:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2150MB allocated=2095MB
2025-10-10 13:52:39 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.575000, curr=0.550000
2025-10-10 13:52:48 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 13:52:48 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 13:52:48 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 13:52:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-10 13:52:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:52:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:52:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-10 13:52:51 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=126.497223, avg_loss=0.672858, seen=188, correct=117, accuracy=0.622340
2025-10-10 13:52:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:52:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:52:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:52:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2150MB allocated=2095MB
2025-10-10 13:52:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:52:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:52:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:52:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:52:54 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.508911, avg_loss=0.687723, seen=40, correct=21, accuracy=0.525000
2025-10-10 13:52:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:52:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:52:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:52:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2150MB allocated=2095MB
2025-10-10 13:52:56 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.575000, curr=0.525000
2025-10-10 13:53:07 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 13:53:07 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 13:53:07 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 13:53:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-10 13:53:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:53:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:53:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-10 13:53:10 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=121.537712, avg_loss=0.646477, seen=188, correct=112, accuracy=0.595745
2025-10-10 13:53:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:53:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:53:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:53:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2150MB allocated=2095MB
2025-10-10 13:53:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:53:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:53:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:53:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:53:15 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.490112, avg_loss=0.712253, seen=40, correct=21, accuracy=0.525000
2025-10-10 13:53:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:53:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:53:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:53:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2150MB allocated=2095MB
2025-10-10 13:53:17 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/25), best=0.575000, curr=0.525000
2025-10-10 13:53:26 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 13:53:26 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 13:53:26 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 13:53:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-10 13:53:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:53:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:53:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-10 13:53:29 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=124.346146, avg_loss=0.661416, seen=188, correct=114, accuracy=0.606383
2025-10-10 13:53:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:53:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:53:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:53:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2150MB allocated=2095MB
2025-10-10 13:53:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:53:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:53:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:53:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:53:32 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.537910, avg_loss=0.688448, seen=40, correct=21, accuracy=0.525000
2025-10-10 13:53:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:53:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:53:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:53:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2150MB allocated=2095MB
2025-10-10 13:53:33 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/25), best=0.575000, curr=0.525000
2025-10-10 13:53:42 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 13:53:42 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 13:53:42 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 13:53:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-10 13:53:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:53:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:53:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-10 13:53:45 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=125.990784, avg_loss=0.670164, seen=188, correct=117, accuracy=0.622340
2025-10-10 13:53:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:53:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:53:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:53:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2150MB allocated=2095MB
2025-10-10 13:53:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:53:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:53:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:53:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:53:51 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.464493, avg_loss=0.686612, seen=40, correct=22, accuracy=0.550000
2025-10-10 13:53:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:53:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:53:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:53:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2150MB allocated=2095MB
2025-10-10 13:53:52 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/25), best=0.575000, curr=0.550000
2025-10-10 13:54:03 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 13:54:03 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 13:54:03 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 13:54:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-10 13:54:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:54:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:54:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-10 13:54:06 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=123.381050, avg_loss=0.656282, seen=188, correct=103, accuracy=0.547872
2025-10-10 13:54:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:54:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:54:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:54:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2150MB allocated=2095MB
2025-10-10 13:54:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:54:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:54:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:54:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:54:10 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.737055, avg_loss=0.718426, seen=40, correct=21, accuracy=0.525000
2025-10-10 13:54:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:54:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:54:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:54:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2150MB allocated=2095MB
2025-10-10 13:54:11 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/25), best=0.575000, curr=0.525000
2025-10-10 13:54:20 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 13:54:20 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 13:54:20 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 13:54:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-10 13:54:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:54:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:54:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-10 13:54:23 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=122.549179, avg_loss=0.651857, seen=188, correct=110, accuracy=0.585106
2025-10-10 13:54:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:54:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:54:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:54:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2150MB allocated=2095MB
2025-10-10 13:54:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:54:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:54:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:54:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:54:29 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.843943, avg_loss=0.696099, seen=40, correct=21, accuracy=0.525000
2025-10-10 13:54:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:54:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:54:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:54:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2150MB allocated=2095MB
2025-10-10 13:54:30 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/25), best=0.575000, curr=0.525000
2025-10-10 13:54:42 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 13:54:42 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 13:54:42 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 13:54:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-10 13:54:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:54:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:54:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-10 13:54:45 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=125.331406, avg_loss=0.666656, seen=188, correct=117, accuracy=0.622340
2025-10-10 13:54:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:54:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:54:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:54:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2150MB allocated=2095MB
2025-10-10 13:54:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:54:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:54:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:54:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:54:49 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.370544, avg_loss=0.684264, seen=40, correct=22, accuracy=0.550000
2025-10-10 13:54:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:54:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:54:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:54:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2150MB allocated=2095MB
2025-10-10 13:54:51 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=9/25), best=0.575000, curr=0.550000
2025-10-10 13:54:59 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 13:54:59 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 13:54:59 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 13:54:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-10 13:54:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:54:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:55:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-10 13:55:02 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=125.277000, avg_loss=0.666367, seen=188, correct=116, accuracy=0.617021
2025-10-10 13:55:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:55:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:55:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:55:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2150MB allocated=2095MB
2025-10-10 13:55:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:55:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:55:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:55:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:55:07 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.320522, avg_loss=0.683013, seen=40, correct=21, accuracy=0.525000
2025-10-10 13:55:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:55:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:55:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:55:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2150MB allocated=2095MB
2025-10-10 13:55:09 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=10/25), best=0.575000, curr=0.525000
2025-10-10 13:55:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 13:55:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 13:55:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:55:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:55:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2150MB allocated=2095MB
2025-10-10 13:55:10 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #9', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 13:55:10 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #9', 'Round': 0, 'Results_raw': {}}
2025-10-10 13:55:10 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 13:55:11 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 13:55:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:55:11 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 13:55:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:55:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:55:13 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 13:55:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-10-10 13:55:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:55:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:55:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-10-10 13:55:15 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=160, loss_sum=103.749901, avg_loss=0.648437, seen=160, correct=98, accuracy=0.612500
2025-10-10 13:55:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:55:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:55:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:55:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2116MB allocated=2086MB
2025-10-10 13:55:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:55:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:55:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:55:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:55:19 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.084114, avg_loss=0.627103, seen=40, correct=25, accuracy=0.625000
2025-10-10 13:55:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:55:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:55:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:55:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2116MB allocated=2086MB
2025-10-10 13:55:21 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-10 13:55:21 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 13:55:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=764, total=3055)
2025-10-10 13:55:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:55:21 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 13:55:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:55:21 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 13:55:21 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=382, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 13:55:30 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 13:55:30 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 13:55:30 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 13:55:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-10-10 13:55:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:55:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:55:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-10-10 13:55:33 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=160, loss_sum=104.858841, avg_loss=0.655368, seen=160, correct=94, accuracy=0.587500
2025-10-10 13:55:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:55:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:55:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:55:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2140MB allocated=2103MB
2025-10-10 13:55:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:55:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:55:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:55:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:55:36 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.617462, avg_loss=0.665437, seen=40, correct=24, accuracy=0.600000
2025-10-10 13:55:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:55:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:55:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:55:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2140MB allocated=2103MB
2025-10-10 13:55:38 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.625000, curr=0.600000
2025-10-10 13:55:47 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 13:55:47 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 13:55:47 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 13:55:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-10-10 13:55:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:55:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:55:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-10-10 13:55:50 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=160, loss_sum=114.562210, avg_loss=0.716014, seen=160, correct=95, accuracy=0.593750
2025-10-10 13:55:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:55:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:55:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:55:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2140MB allocated=2103MB
2025-10-10 13:55:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:55:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:55:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:55:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:55:55 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.667332, avg_loss=0.616683, seen=40, correct=25, accuracy=0.625000
2025-10-10 13:55:55 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:55:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:55:56 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:55:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2140MB allocated=2103MB
2025-10-10 13:55:56 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-10 13:56:08 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 13:56:08 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 13:56:08 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 13:56:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-10-10 13:56:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:56:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:56:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-10-10 13:56:10 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=160, loss_sum=117.169357, avg_loss=0.732308, seen=160, correct=92, accuracy=0.575000
2025-10-10 13:56:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:56:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:56:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:56:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2140MB allocated=2103MB
2025-10-10 13:56:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:56:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:56:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:56:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:56:15 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.887093, avg_loss=0.622177, seen=40, correct=24, accuracy=0.600000
2025-10-10 13:56:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:56:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:56:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:56:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2140MB allocated=2103MB
2025-10-10 13:56:17 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.625000, curr=0.600000
2025-10-10 13:56:25 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 13:56:25 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 13:56:25 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 13:56:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-10-10 13:56:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:56:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:56:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-10-10 13:56:28 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=160, loss_sum=102.997818, avg_loss=0.643736, seen=160, correct=95, accuracy=0.593750
2025-10-10 13:56:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:56:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:56:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:56:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2140MB allocated=2103MB
2025-10-10 13:56:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:56:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:56:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:56:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:56:32 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.155128, avg_loss=0.628878, seen=40, correct=26, accuracy=0.650000
2025-10-10 13:56:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:56:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:56:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:56:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2140MB allocated=2103MB
2025-10-10 13:56:34 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.650000
2025-10-10 13:56:45 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 13:56:45 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 13:56:45 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 13:56:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-10-10 13:56:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:56:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:56:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-10-10 13:56:48 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=160, loss_sum=103.513855, avg_loss=0.646962, seen=160, correct=92, accuracy=0.575000
2025-10-10 13:56:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:56:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:56:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:56:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2140MB allocated=2103MB
2025-10-10 13:56:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:56:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:56:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:56:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:56:53 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.971134, avg_loss=0.649278, seen=40, correct=24, accuracy=0.600000
2025-10-10 13:56:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:56:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:56:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:56:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2140MB allocated=2103MB
2025-10-10 13:56:55 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.650000, curr=0.600000
2025-10-10 13:57:03 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 13:57:03 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 13:57:03 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 13:57:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-10-10 13:57:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:57:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:57:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-10-10 13:57:06 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=160, loss_sum=103.829033, avg_loss=0.648931, seen=160, correct=100, accuracy=0.625000
2025-10-10 13:57:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:57:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:57:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:57:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2140MB allocated=2103MB
2025-10-10 13:57:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:57:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:57:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:57:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:57:10 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.371357, avg_loss=0.609284, seen=40, correct=28, accuracy=0.700000
2025-10-10 13:57:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:57:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:57:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:57:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2140MB allocated=2103MB
2025-10-10 13:57:11 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.700000
2025-10-10 13:57:19 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 13:57:19 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 13:57:19 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 13:57:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-10-10 13:57:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:57:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:57:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-10-10 13:57:22 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=160, loss_sum=103.112328, avg_loss=0.644452, seen=160, correct=100, accuracy=0.625000
2025-10-10 13:57:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:57:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:57:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:57:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2140MB allocated=2103MB
2025-10-10 13:57:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:57:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:57:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:57:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:57:27 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.091955, avg_loss=0.602299, seen=40, correct=28, accuracy=0.700000
2025-10-10 13:57:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:57:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:57:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:57:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2140MB allocated=2103MB
2025-10-10 13:57:28 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.700000
2025-10-10 13:57:38 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 13:57:38 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 13:57:38 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 13:57:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-10-10 13:57:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:57:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:57:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-10-10 13:57:40 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=160, loss_sum=102.942497, avg_loss=0.643391, seen=160, correct=101, accuracy=0.631250
2025-10-10 13:57:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:57:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:57:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:57:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2140MB allocated=2103MB
2025-10-10 13:57:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:57:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:57:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:57:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:57:46 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.216156, avg_loss=0.605404, seen=40, correct=26, accuracy=0.650000
2025-10-10 13:57:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:57:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:57:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:57:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2140MB allocated=2103MB
2025-10-10 13:57:47 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.700000, curr=0.650000
2025-10-10 13:57:57 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 13:57:57 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 13:57:57 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 13:57:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-10-10 13:57:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:57:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:58:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-10-10 13:58:00 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=160, loss_sum=102.040115, avg_loss=0.637751, seen=160, correct=94, accuracy=0.587500
2025-10-10 13:58:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:58:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:58:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:58:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2140MB allocated=2103MB
2025-10-10 13:58:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:58:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:58:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:58:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:58:05 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.721199, avg_loss=0.618030, seen=40, correct=28, accuracy=0.700000
2025-10-10 13:58:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:58:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:58:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:58:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2140MB allocated=2103MB
2025-10-10 13:58:07 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.700000
2025-10-10 13:58:16 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 13:58:16 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 13:58:16 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 13:58:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-10-10 13:58:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:58:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:58:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-10-10 13:58:19 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=160, loss_sum=102.815018, avg_loss=0.642594, seen=160, correct=91, accuracy=0.568750
2025-10-10 13:58:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:58:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:58:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:58:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2140MB allocated=2103MB
2025-10-10 13:58:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:58:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:58:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:58:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:58:24 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.293514, avg_loss=0.632338, seen=40, correct=27, accuracy=0.675000
2025-10-10 13:58:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:58:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:58:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:58:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2140MB allocated=2103MB
2025-10-10 13:58:26 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.700000, curr=0.675000
2025-10-10 13:58:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 13:58:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 13:58:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:58:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:58:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2140MB allocated=2103MB
2025-10-10 13:58:27 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #14', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 13:58:27 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #14', 'Round': 0, 'Results_raw': {}}
2025-10-10 13:58:28 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 13:58:28 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 13:58:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:58:29 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 13:58:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:58:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:58:31 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 13:58:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-10-10 13:58:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:58:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:58:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-10-10 13:58:34 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=161, loss_sum=106.388985, avg_loss=0.660801, seen=161, correct=99, accuracy=0.614907
2025-10-10 13:58:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:58:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:58:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:58:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2138MB allocated=2095MB
2025-10-10 13:58:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:58:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:58:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:58:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:58:38 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.944675, avg_loss=0.673617, seen=40, correct=24, accuracy=0.600000
2025-10-10 13:58:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:58:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:58:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:58:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2138MB allocated=2095MB
2025-10-10 13:58:40 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-10 13:58:40 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 13:58:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=766, total=3063)
2025-10-10 13:58:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:58:40 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 13:58:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:58:40 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 13:58:40 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=383, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 13:58:50 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 13:58:50 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 13:58:50 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 13:58:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-10-10 13:58:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:58:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:58:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-10-10 13:58:53 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=161, loss_sum=108.476547, avg_loss=0.673767, seen=161, correct=96, accuracy=0.596273
2025-10-10 13:58:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:58:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:58:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:58:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2158MB allocated=2112MB
2025-10-10 13:58:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:58:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:58:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:58:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:58:57 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.127907, avg_loss=0.703198, seen=40, correct=22, accuracy=0.550000
2025-10-10 13:58:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:58:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:58:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:58:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2158MB allocated=2112MB
2025-10-10 13:58:58 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.600000, curr=0.550000
2025-10-10 13:59:08 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 13:59:08 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 13:59:08 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 13:59:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-10-10 13:59:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:59:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:59:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-10-10 13:59:10 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=161, loss_sum=107.304649, avg_loss=0.666489, seen=161, correct=100, accuracy=0.621118
2025-10-10 13:59:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:59:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:59:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:59:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2158MB allocated=2112MB
2025-10-10 13:59:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:59:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:59:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:59:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:59:16 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.856970, avg_loss=0.696424, seen=40, correct=21, accuracy=0.525000
2025-10-10 13:59:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:59:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:59:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:59:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2158MB allocated=2112MB
2025-10-10 13:59:17 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.600000, curr=0.525000
2025-10-10 13:59:27 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 13:59:27 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 13:59:27 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 13:59:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-10-10 13:59:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:59:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:59:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-10-10 13:59:30 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=161, loss_sum=107.269722, avg_loss=0.666272, seen=161, correct=92, accuracy=0.571429
2025-10-10 13:59:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:59:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:59:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:59:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2158MB allocated=2112MB
2025-10-10 13:59:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:59:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:59:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:59:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:59:34 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.523523, avg_loss=0.688088, seen=40, correct=23, accuracy=0.575000
2025-10-10 13:59:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:59:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:59:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:59:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2158MB allocated=2112MB
2025-10-10 13:59:35 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.600000, curr=0.575000
2025-10-10 13:59:42 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 13:59:42 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 13:59:42 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 13:59:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-10-10 13:59:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:59:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:59:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-10-10 13:59:44 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=161, loss_sum=106.885826, avg_loss=0.663887, seen=161, correct=91, accuracy=0.565217
2025-10-10 13:59:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:59:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:59:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:59:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2158MB allocated=2112MB
2025-10-10 13:59:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 13:59:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:59:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 13:59:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 13:59:48 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.349800, avg_loss=0.683745, seen=40, correct=23, accuracy=0.575000
2025-10-10 13:59:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 13:59:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 13:59:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 13:59:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2158MB allocated=2112MB
2025-10-10 13:59:50 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/25), best=0.600000, curr=0.575000
2025-10-10 14:00:01 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 14:00:01 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 14:00:01 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 14:00:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-10-10 14:00:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:00:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:00:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-10-10 14:00:04 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=161, loss_sum=107.424271, avg_loss=0.667231, seen=161, correct=100, accuracy=0.621118
2025-10-10 14:00:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:00:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:00:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:00:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2158MB allocated=2112MB
2025-10-10 14:00:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:00:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:00:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:00:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:00:09 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.359964, avg_loss=0.683999, seen=40, correct=21, accuracy=0.525000
2025-10-10 14:00:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:00:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:00:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:00:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2158MB allocated=2112MB
2025-10-10 14:00:10 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/25), best=0.600000, curr=0.525000
2025-10-10 14:00:19 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 14:00:19 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 14:00:19 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 14:00:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-10-10 14:00:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:00:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:00:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-10-10 14:00:22 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=161, loss_sum=105.536690, avg_loss=0.655507, seen=161, correct=97, accuracy=0.602484
2025-10-10 14:00:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:00:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:00:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:00:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2158MB allocated=2112MB
2025-10-10 14:00:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:00:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:00:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:00:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:00:27 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.083452, avg_loss=0.677086, seen=40, correct=21, accuracy=0.525000
2025-10-10 14:00:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:00:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:00:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:00:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2158MB allocated=2112MB
2025-10-10 14:00:30 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/25), best=0.600000, curr=0.525000
2025-10-10 14:00:40 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 14:00:40 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 14:00:40 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 14:00:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-10-10 14:00:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:00:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:00:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-10-10 14:00:42 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=161, loss_sum=105.430397, avg_loss=0.654847, seen=161, correct=96, accuracy=0.596273
2025-10-10 14:00:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:00:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:00:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:00:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2158MB allocated=2112MB
2025-10-10 14:00:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:00:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:00:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:00:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:00:46 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.758780, avg_loss=0.668969, seen=40, correct=22, accuracy=0.550000
2025-10-10 14:00:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:00:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:00:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:00:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2158MB allocated=2112MB
2025-10-10 14:00:47 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/25), best=0.600000, curr=0.550000
2025-10-10 14:00:57 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 14:00:57 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 14:00:57 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 14:00:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-10-10 14:00:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:00:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:00:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-10-10 14:00:59 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=161, loss_sum=105.429138, avg_loss=0.654839, seen=161, correct=94, accuracy=0.583851
2025-10-10 14:00:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:00:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:01:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:01:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2158MB allocated=2112MB
2025-10-10 14:01:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:01:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:01:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:01:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:01:03 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.841259, avg_loss=0.671031, seen=40, correct=24, accuracy=0.600000
2025-10-10 14:01:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:01:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:01:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:01:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2158MB allocated=2112MB
2025-10-10 14:01:05 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-10 14:01:15 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 14:01:15 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 14:01:15 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 14:01:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-10-10 14:01:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:01:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:01:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-10-10 14:01:18 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=161, loss_sum=105.326668, avg_loss=0.654203, seen=161, correct=92, accuracy=0.571429
2025-10-10 14:01:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:01:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:01:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:01:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2158MB allocated=2112MB
2025-10-10 14:01:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:01:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:01:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:01:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:01:23 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.982162, avg_loss=0.674554, seen=40, correct=23, accuracy=0.575000
2025-10-10 14:01:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:01:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:01:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:01:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2158MB allocated=2112MB
2025-10-10 14:01:24 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.600000, curr=0.575000
2025-10-10 14:01:36 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 14:01:36 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 14:01:36 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 14:01:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-10-10 14:01:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:01:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:01:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-10-10 14:01:38 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=161, loss_sum=105.283272, avg_loss=0.653933, seen=161, correct=102, accuracy=0.633540
2025-10-10 14:01:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:01:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:01:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:01:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2158MB allocated=2112MB
2025-10-10 14:01:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:01:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:01:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:01:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:01:42 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.904459, avg_loss=0.672611, seen=40, correct=24, accuracy=0.600000
2025-10-10 14:01:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:01:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:01:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:01:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2158MB allocated=2112MB
2025-10-10 14:01:44 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-10 14:01:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 14:01:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 14:01:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:01:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:01:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2158MB allocated=2112MB
2025-10-10 14:01:45 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #26', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 14:01:45 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #26', 'Round': 0, 'Results_raw': {}}
2025-10-10 14:01:45 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 14:01:46 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 14:01:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:01:47 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 14:01:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:01:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:01:48 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 14:01:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-10-10 14:01:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:01:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:01:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 14:01:51 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=135, loss_sum=91.404449, avg_loss=0.677070, seen=135, correct=76, accuracy=0.562963
2025-10-10 14:01:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:01:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:01:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:01:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2138MB allocated=2103MB
2025-10-10 14:01:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:01:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:01:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:01:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:01:56 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.637848, avg_loss=0.640946, seen=40, correct=23, accuracy=0.575000
2025-10-10 14:01:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:01:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:01:56 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:01:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2138MB allocated=2103MB
2025-10-10 14:01:57 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-10 14:01:57 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 14:01:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=644, total=2576)
2025-10-10 14:01:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:01:57 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 14:01:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:01:57 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 14:01:57 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=322, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 14:02:08 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 14:02:08 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 14:02:08 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 14:02:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-10-10 14:02:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:02:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:02:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 14:02:11 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=135, loss_sum=92.603806, avg_loss=0.685954, seen=135, correct=82, accuracy=0.607407
2025-10-10 14:02:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:02:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:02:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:02:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2120MB
2025-10-10 14:02:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:02:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:02:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:02:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:02:16 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.453508, avg_loss=0.586338, seen=40, correct=30, accuracy=0.750000
2025-10-10 14:02:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:02:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:02:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:02:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2120MB
2025-10-10 14:02:18 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.750000
2025-10-10 14:02:26 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 14:02:26 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 14:02:26 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 14:02:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-10-10 14:02:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:02:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:02:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 14:02:28 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=135, loss_sum=96.326157, avg_loss=0.713527, seen=135, correct=70, accuracy=0.518519
2025-10-10 14:02:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:02:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:02:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:02:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2120MB
2025-10-10 14:02:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:02:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:02:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:02:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:02:31 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.753922, avg_loss=0.718848, seen=40, correct=19, accuracy=0.475000
2025-10-10 14:02:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:02:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:02:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:02:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2120MB
2025-10-10 14:02:33 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.750000, curr=0.475000
2025-10-10 14:02:42 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 14:02:42 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 14:02:42 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 14:02:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-10-10 14:02:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:02:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:02:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 14:02:44 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=135, loss_sum=95.605858, avg_loss=0.708192, seen=135, correct=74, accuracy=0.548148
2025-10-10 14:02:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:02:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:02:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:02:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2120MB
2025-10-10 14:02:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:02:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:02:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:02:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:02:49 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.045704, avg_loss=0.701143, seen=40, correct=19, accuracy=0.475000
2025-10-10 14:02:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:02:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:02:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:02:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2120MB
2025-10-10 14:02:51 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.750000, curr=0.475000
2025-10-10 14:03:01 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 14:03:01 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 14:03:01 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 14:03:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-10-10 14:03:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:03:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:03:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 14:03:03 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=135, loss_sum=91.481049, avg_loss=0.677637, seen=135, correct=77, accuracy=0.570370
2025-10-10 14:03:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:03:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:03:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:03:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2120MB
2025-10-10 14:03:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:03:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:03:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:03:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:03:07 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.014050, avg_loss=0.625351, seen=40, correct=28, accuracy=0.700000
2025-10-10 14:03:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:03:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:03:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:03:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2120MB
2025-10-10 14:03:08 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.750000, curr=0.700000
2025-10-10 14:03:19 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 14:03:19 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 14:03:19 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 14:03:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-10-10 14:03:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:03:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:03:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 14:03:21 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=135, loss_sum=91.306595, avg_loss=0.676345, seen=135, correct=80, accuracy=0.592593
2025-10-10 14:03:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:03:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:03:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:03:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2120MB
2025-10-10 14:03:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:03:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:03:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:03:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:03:24 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.935562, avg_loss=0.623389, seen=40, correct=26, accuracy=0.650000
2025-10-10 14:03:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:03:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:03:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:03:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2120MB
2025-10-10 14:03:27 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/25), best=0.750000, curr=0.650000
2025-10-10 14:03:35 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 14:03:35 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 14:03:35 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 14:03:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-10-10 14:03:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:03:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:03:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 14:03:37 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=135, loss_sum=90.874237, avg_loss=0.673142, seen=135, correct=82, accuracy=0.607407
2025-10-10 14:03:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:03:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:03:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:03:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2120MB
2025-10-10 14:03:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:03:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:03:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:03:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:03:41 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.732405, avg_loss=0.593310, seen=40, correct=30, accuracy=0.750000
2025-10-10 14:03:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:03:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:03:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:03:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2120MB
2025-10-10 14:03:43 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.750000
2025-10-10 14:03:51 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 14:03:51 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 14:03:51 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 14:03:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-10-10 14:03:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:03:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:03:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 14:03:53 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=135, loss_sum=90.890160, avg_loss=0.673260, seen=135, correct=83, accuracy=0.614815
2025-10-10 14:03:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:03:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:03:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:03:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2120MB
2025-10-10 14:03:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:03:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:03:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:03:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:03:59 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.609932, avg_loss=0.590248, seen=40, correct=31, accuracy=0.775000
2025-10-10 14:03:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:03:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:03:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:04:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2120MB
2025-10-10 14:04:00 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.775000
2025-10-10 14:04:10 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 14:04:10 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 14:04:10 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 14:04:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-10-10 14:04:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:04:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:04:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 14:04:12 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=135, loss_sum=90.532852, avg_loss=0.670614, seen=135, correct=77, accuracy=0.570370
2025-10-10 14:04:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:04:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:04:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:04:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2120MB
2025-10-10 14:04:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:04:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:04:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:04:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:04:16 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.170952, avg_loss=0.629274, seen=40, correct=24, accuracy=0.600000
2025-10-10 14:04:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:04:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:04:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:04:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2120MB
2025-10-10 14:04:17 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.775000, curr=0.600000
2025-10-10 14:04:25 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 14:04:25 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 14:04:25 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 14:04:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-10-10 14:04:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:04:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:04:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 14:04:28 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=135, loss_sum=91.374901, avg_loss=0.676851, seen=135, correct=79, accuracy=0.585185
2025-10-10 14:04:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:04:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:04:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:04:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2120MB
2025-10-10 14:04:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:04:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:04:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:04:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:04:32 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.035093, avg_loss=0.650877, seen=40, correct=25, accuracy=0.625000
2025-10-10 14:04:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:04:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:04:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:04:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2120MB
2025-10-10 14:04:34 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.775000, curr=0.625000
2025-10-10 14:04:44 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 14:04:44 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 14:04:44 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 14:04:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-10-10 14:04:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:04:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:04:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 14:04:46 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=135, loss_sum=90.397171, avg_loss=0.669609, seen=135, correct=79, accuracy=0.585185
2025-10-10 14:04:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:04:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:04:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:04:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2120MB
2025-10-10 14:04:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:04:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:04:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:04:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:04:52 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.042616, avg_loss=0.601065, seen=40, correct=30, accuracy=0.750000
2025-10-10 14:04:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:04:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:04:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:04:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2120MB
2025-10-10 14:04:53 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.775000, curr=0.750000
2025-10-10 14:04:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 14:04:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 14:04:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:04:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:04:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2120MB
2025-10-10 14:04:54 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #18', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 14:04:54 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #18', 'Round': 0, 'Results_raw': {}}
2025-10-10 14:04:55 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 14:04:56 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 14:04:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:04:56 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 14:04:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:04:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:04:57 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 14:04:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-10 14:04:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:04:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:05:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-10 14:05:01 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=126.769203, avg_loss=0.674304, seen=188, correct=108, accuracy=0.574468
2025-10-10 14:05:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:05:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:05:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:05:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2138MB allocated=2112MB
2025-10-10 14:05:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:05:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:05:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:05:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:05:04 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.027657, avg_loss=0.625691, seen=40, correct=26, accuracy=0.650000
2025-10-10 14:05:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:05:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:05:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:05:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2138MB allocated=2112MB
2025-10-10 14:05:06 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.650000
2025-10-10 14:05:06 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 14:05:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=898, total=3589)
2025-10-10 14:05:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:05:06 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 14:05:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:05:06 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 14:05:06 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=449, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 14:05:17 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 14:05:17 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 14:05:17 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 14:05:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-10 14:05:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:05:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:05:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-10 14:05:20 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=126.291519, avg_loss=0.671763, seen=188, correct=109, accuracy=0.579787
2025-10-10 14:05:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:05:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:05:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:05:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2170MB allocated=2128MB
2025-10-10 14:05:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:05:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:05:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:05:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:05:26 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.497787, avg_loss=0.637445, seen=40, correct=27, accuracy=0.675000
2025-10-10 14:05:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:05:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:05:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:05:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2170MB allocated=2128MB
2025-10-10 14:05:29 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.675000
2025-10-10 14:05:39 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 14:05:39 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 14:05:39 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 14:05:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-10 14:05:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:05:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:05:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-10 14:05:42 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=126.811554, avg_loss=0.674530, seen=188, correct=111, accuracy=0.590426
2025-10-10 14:05:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:05:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:05:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:05:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2170MB allocated=2128MB
2025-10-10 14:05:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:05:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:05:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:05:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:05:45 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.041714, avg_loss=0.626043, seen=40, correct=26, accuracy=0.650000
2025-10-10 14:05:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:05:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:05:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:05:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2170MB allocated=2128MB
2025-10-10 14:05:46 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.675000, curr=0.650000
2025-10-10 14:05:56 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 14:05:56 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 14:05:56 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 14:05:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-10 14:05:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:05:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:05:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-10 14:05:59 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=125.875290, avg_loss=0.669549, seen=188, correct=111, accuracy=0.590426
2025-10-10 14:05:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:05:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:06:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:06:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2170MB allocated=2128MB
2025-10-10 14:06:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:06:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:06:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:06:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:06:04 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.149059, avg_loss=0.628726, seen=40, correct=29, accuracy=0.725000
2025-10-10 14:06:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:06:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:06:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:06:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2170MB allocated=2128MB
2025-10-10 14:06:05 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.725000
2025-10-10 14:06:16 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 14:06:16 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 14:06:16 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 14:06:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-10 14:06:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:06:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:06:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-10 14:06:19 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=125.438622, avg_loss=0.667227, seen=188, correct=110, accuracy=0.585106
2025-10-10 14:06:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:06:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:06:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:06:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2170MB allocated=2128MB
2025-10-10 14:06:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:06:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:06:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:06:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:06:23 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.276054, avg_loss=0.631901, seen=40, correct=27, accuracy=0.675000
2025-10-10 14:06:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:06:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:06:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:06:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2170MB allocated=2128MB
2025-10-10 14:06:25 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.725000, curr=0.675000
2025-10-10 14:06:32 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 14:06:32 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 14:06:32 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 14:06:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-10 14:06:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:06:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:06:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-10 14:06:35 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=125.545250, avg_loss=0.667794, seen=188, correct=112, accuracy=0.595745
2025-10-10 14:06:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:06:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:06:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:06:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2170MB allocated=2128MB
2025-10-10 14:06:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:06:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:06:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:06:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:06:41 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.839397, avg_loss=0.620985, seen=40, correct=27, accuracy=0.675000
2025-10-10 14:06:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:06:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:06:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:06:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2170MB allocated=2128MB
2025-10-10 14:06:42 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.725000, curr=0.675000
2025-10-10 14:06:53 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 14:06:53 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 14:06:53 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 14:06:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-10 14:06:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:06:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:06:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-10 14:06:55 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=125.802681, avg_loss=0.669163, seen=188, correct=109, accuracy=0.579787
2025-10-10 14:06:55 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:06:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:06:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:06:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2170MB allocated=2128MB
2025-10-10 14:06:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:06:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:06:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:07:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:07:00 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.027964, avg_loss=0.625699, seen=40, correct=27, accuracy=0.675000
2025-10-10 14:07:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:07:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:07:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:07:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2170MB allocated=2128MB
2025-10-10 14:07:03 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.725000, curr=0.675000
2025-10-10 14:07:11 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 14:07:11 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 14:07:11 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 14:07:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-10 14:07:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:07:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:07:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-10 14:07:14 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=126.038231, avg_loss=0.670416, seen=188, correct=110, accuracy=0.585106
2025-10-10 14:07:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:07:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:07:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:07:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2170MB allocated=2128MB
2025-10-10 14:07:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:07:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:07:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:07:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:07:19 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.312723, avg_loss=0.632818, seen=40, correct=29, accuracy=0.725000
2025-10-10 14:07:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:07:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:07:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:07:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2170MB allocated=2128MB
2025-10-10 14:07:20 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.725000
2025-10-10 14:07:30 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 14:07:30 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 14:07:30 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 14:07:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-10 14:07:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:07:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:07:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-10 14:07:33 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=125.103958, avg_loss=0.665447, seen=188, correct=108, accuracy=0.574468
2025-10-10 14:07:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:07:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:07:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:07:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2170MB allocated=2128MB
2025-10-10 14:07:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:07:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:07:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:07:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:07:39 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.188332, avg_loss=0.629708, seen=40, correct=27, accuracy=0.675000
2025-10-10 14:07:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:07:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:07:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:07:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2170MB allocated=2128MB
2025-10-10 14:07:40 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.725000, curr=0.675000
2025-10-10 14:07:51 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 14:07:51 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 14:07:51 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 14:07:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-10 14:07:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:07:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:07:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-10 14:07:54 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=125.941711, avg_loss=0.669903, seen=188, correct=114, accuracy=0.606383
2025-10-10 14:07:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:07:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:07:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:07:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2170MB allocated=2128MB
2025-10-10 14:07:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:07:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:07:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:07:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:07:59 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.931284, avg_loss=0.623282, seen=40, correct=25, accuracy=0.625000
2025-10-10 14:07:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:07:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:08:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:08:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2170MB allocated=2128MB
2025-10-10 14:08:01 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.725000, curr=0.625000
2025-10-10 14:08:11 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 14:08:11 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 14:08:11 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 14:08:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-10 14:08:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:08:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:08:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-10 14:08:14 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=124.872292, avg_loss=0.664214, seen=188, correct=106, accuracy=0.563830
2025-10-10 14:08:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:08:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:08:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:08:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2170MB allocated=2128MB
2025-10-10 14:08:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:08:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:08:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:08:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:08:19 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.044798, avg_loss=0.626120, seen=40, correct=27, accuracy=0.675000
2025-10-10 14:08:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:08:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:08:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:08:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2170MB allocated=2128MB
2025-10-10 14:08:21 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.725000, curr=0.675000
2025-10-10 14:08:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 14:08:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 14:08:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:08:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:08:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2170MB allocated=2128MB
2025-10-10 14:08:22 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #52', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 14:08:22 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #52', 'Round': 0, 'Results_raw': {}}
2025-10-10 14:08:22 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 14:08:23 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 14:08:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:08:24 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 14:08:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:08:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:08:25 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 14:08:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-10-10 14:08:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:08:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:08:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-10-10 14:08:27 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=89, loss_sum=64.182465, avg_loss=0.721151, seen=89, correct=54, accuracy=0.606742
2025-10-10 14:08:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:08:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:08:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:08:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2158MB allocated=2120MB
2025-10-10 14:08:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:08:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:08:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:08:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:08:31 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.937050, avg_loss=0.673426, seen=40, correct=21, accuracy=0.525000
2025-10-10 14:08:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:08:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:08:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:08:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2158MB allocated=2120MB
2025-10-10 14:08:33 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.525000
2025-10-10 14:08:33 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 14:08:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=424, total=1694)
2025-10-10 14:08:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:08:33 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 14:08:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:08:33 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 14:08:33 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=212, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 14:08:42 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 14:08:42 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 14:08:42 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 14:08:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-10-10 14:08:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:08:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:08:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-10-10 14:08:44 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=89, loss_sum=65.857292, avg_loss=0.739970, seen=89, correct=44, accuracy=0.494382
2025-10-10 14:08:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:08:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:08:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:08:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2188MB allocated=2137MB
2025-10-10 14:08:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:08:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:08:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:08:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:08:50 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.537664, avg_loss=0.688442, seen=40, correct=22, accuracy=0.550000
2025-10-10 14:08:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:08:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:08:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:08:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2188MB allocated=2137MB
2025-10-10 14:08:51 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.550000
2025-10-10 14:09:01 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 14:09:01 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 14:09:01 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 14:09:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-10-10 14:09:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:09:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:09:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-10-10 14:09:02 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=89, loss_sum=66.697632, avg_loss=0.749412, seen=89, correct=48, accuracy=0.539326
2025-10-10 14:09:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:09:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:09:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:09:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2188MB allocated=2137MB
2025-10-10 14:09:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:09:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:09:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:09:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:09:07 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.893000, avg_loss=0.747325, seen=40, correct=20, accuracy=0.500000
2025-10-10 14:09:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:09:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:09:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:09:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2188MB allocated=2137MB
2025-10-10 14:09:10 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.550000, curr=0.500000
2025-10-10 14:09:18 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 14:09:18 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 14:09:18 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 14:09:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-10-10 14:09:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:09:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:09:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-10-10 14:09:20 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=89, loss_sum=66.036415, avg_loss=0.741982, seen=89, correct=47, accuracy=0.528090
2025-10-10 14:09:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:09:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:09:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:09:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2188MB allocated=2137MB
2025-10-10 14:09:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:09:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:09:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:09:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:09:25 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.354816, avg_loss=0.683870, seen=40, correct=23, accuracy=0.575000
2025-10-10 14:09:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:09:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:09:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:09:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2188MB allocated=2137MB
2025-10-10 14:09:26 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-10 14:09:36 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 14:09:36 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 14:09:36 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 14:09:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-10-10 14:09:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:09:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:09:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-10-10 14:09:38 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=89, loss_sum=64.927170, avg_loss=0.729519, seen=89, correct=48, accuracy=0.539326
2025-10-10 14:09:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:09:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:09:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:09:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2188MB allocated=2137MB
2025-10-10 14:09:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:09:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:09:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:09:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:09:43 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.165222, avg_loss=0.679131, seen=40, correct=23, accuracy=0.575000
2025-10-10 14:09:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:09:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:09:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:09:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2188MB allocated=2137MB
2025-10-10 14:09:45 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-10 14:09:56 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 14:09:56 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 14:09:56 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 14:09:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-10-10 14:09:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:09:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:09:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-10-10 14:09:58 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=89, loss_sum=63.598427, avg_loss=0.714589, seen=89, correct=51, accuracy=0.573034
2025-10-10 14:09:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:09:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:09:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:10:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2188MB allocated=2137MB
2025-10-10 14:10:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:10:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:10:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:10:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:10:04 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.798519, avg_loss=0.669963, seen=40, correct=21, accuracy=0.525000
2025-10-10 14:10:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:10:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:10:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:10:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2188MB allocated=2137MB
2025-10-10 14:10:05 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.575000, curr=0.525000
2025-10-10 14:10:15 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 14:10:15 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 14:10:15 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 14:10:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-10-10 14:10:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:10:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:10:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-10-10 14:10:17 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=89, loss_sum=62.692436, avg_loss=0.704409, seen=89, correct=50, accuracy=0.561798
2025-10-10 14:10:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:10:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:10:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:10:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2188MB allocated=2137MB
2025-10-10 14:10:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:10:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:10:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:10:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:10:23 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.020355, avg_loss=0.675509, seen=40, correct=22, accuracy=0.550000
2025-10-10 14:10:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:10:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:10:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:10:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2188MB allocated=2137MB
2025-10-10 14:10:24 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.575000, curr=0.550000
2025-10-10 14:10:35 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 14:10:35 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 14:10:35 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 14:10:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-10-10 14:10:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:10:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:10:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-10-10 14:10:37 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=89, loss_sum=62.974960, avg_loss=0.707584, seen=89, correct=49, accuracy=0.550562
2025-10-10 14:10:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:10:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:10:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:10:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2188MB allocated=2137MB
2025-10-10 14:10:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:10:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:10:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:10:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:10:40 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.757790, avg_loss=0.668945, seen=40, correct=24, accuracy=0.600000
2025-10-10 14:10:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:10:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:10:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:10:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2188MB allocated=2137MB
2025-10-10 14:10:42 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-10 14:10:52 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 14:10:52 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 14:10:52 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 14:10:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-10-10 14:10:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:10:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:10:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-10-10 14:10:53 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=89, loss_sum=62.254951, avg_loss=0.699494, seen=89, correct=51, accuracy=0.573034
2025-10-10 14:10:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:10:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:10:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:10:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2188MB allocated=2137MB
2025-10-10 14:10:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:10:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:10:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:10:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:10:59 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.257040, avg_loss=0.656426, seen=40, correct=24, accuracy=0.600000
2025-10-10 14:10:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:10:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:11:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:11:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2188MB allocated=2137MB
2025-10-10 14:11:01 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-10 14:11:11 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 14:11:11 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 14:11:11 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 14:11:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-10-10 14:11:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:11:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:11:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-10-10 14:11:13 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=89, loss_sum=62.600204, avg_loss=0.703373, seen=89, correct=51, accuracy=0.573034
2025-10-10 14:11:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:11:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:11:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:11:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2188MB allocated=2137MB
2025-10-10 14:11:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:11:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:11:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:11:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:11:16 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.302471, avg_loss=0.657562, seen=40, correct=25, accuracy=0.625000
2025-10-10 14:11:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:11:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:11:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:11:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2188MB allocated=2137MB
2025-10-10 14:11:18 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-10 14:11:27 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 14:11:27 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 14:11:27 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 14:11:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-10-10 14:11:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:11:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:11:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-10-10 14:11:29 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=89, loss_sum=63.549774, avg_loss=0.714042, seen=89, correct=50, accuracy=0.561798
2025-10-10 14:11:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:11:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:11:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:11:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2188MB allocated=2137MB
2025-10-10 14:11:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:11:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:11:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:11:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:11:33 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.025574, avg_loss=0.650639, seen=40, correct=25, accuracy=0.625000
2025-10-10 14:11:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:11:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:11:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:11:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2188MB allocated=2137MB
2025-10-10 14:11:34 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-10 14:11:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 14:11:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 14:11:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:11:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:11:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2188MB allocated=2137MB
2025-10-10 14:11:36 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #43', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 14:11:36 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #43', 'Round': 0, 'Results_raw': {}}
2025-10-10 14:11:36 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 14:11:37 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 14:11:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:11:37 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 14:11:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:11:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:11:40 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 14:11:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-10 14:11:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:11:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:11:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-10 14:11:40 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=9.713543, avg_loss=0.883049, seen=11, correct=6, accuracy=0.545455
2025-10-10 14:11:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:11:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:11:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:11:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2158MB allocated=2128MB
2025-10-10 14:11:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:11:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:11:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:11:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:11:45 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=31.482134, avg_loss=0.787053, seen=40, correct=23, accuracy=0.575000
2025-10-10 14:11:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:11:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:11:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:11:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2158MB allocated=2128MB
2025-10-10 14:11:47 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-10 14:11:47 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 14:11:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=54, total=214)
2025-10-10 14:11:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:11:47 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 14:11:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:11:47 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 14:11:47 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=27, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 14:11:57 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 14:11:57 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 14:11:57 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 14:11:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-10 14:11:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:11:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:11:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-10 14:11:57 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=10.168329, avg_loss=0.924394, seen=11, correct=6, accuracy=0.545455
2025-10-10 14:11:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:11:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:11:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:12:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2190MB allocated=2145MB
2025-10-10 14:12:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:12:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:12:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:12:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:12:02 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=32.958374, avg_loss=0.823959, seen=40, correct=23, accuracy=0.575000
2025-10-10 14:12:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:12:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:12:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:12:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2190MB allocated=2145MB
2025-10-10 14:12:04 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-10 14:12:13 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 14:12:13 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 14:12:13 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 14:12:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-10 14:12:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:12:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:12:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-10 14:12:14 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=10.316586, avg_loss=0.937871, seen=11, correct=6, accuracy=0.545455
2025-10-10 14:12:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:12:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:12:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:12:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2190MB allocated=2145MB
2025-10-10 14:12:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:12:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:12:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:12:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:12:18 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=32.516434, avg_loss=0.812911, seen=40, correct=23, accuracy=0.575000
2025-10-10 14:12:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:12:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:12:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:12:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2190MB allocated=2145MB
2025-10-10 14:12:20 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-10 14:12:30 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 14:12:30 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 14:12:30 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 14:12:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-10 14:12:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:12:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:12:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-10 14:12:31 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=10.672482, avg_loss=0.970226, seen=11, correct=7, accuracy=0.636364
2025-10-10 14:12:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:12:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:12:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:12:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2190MB allocated=2145MB
2025-10-10 14:12:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:12:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:12:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:12:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:12:36 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=33.317726, avg_loss=0.832943, seen=40, correct=23, accuracy=0.575000
2025-10-10 14:12:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:12:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:12:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:12:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2190MB allocated=2145MB
2025-10-10 14:12:38 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-10 14:12:45 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 14:12:45 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 14:12:45 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 14:12:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-10 14:12:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:12:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:12:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-10 14:12:46 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=11.480984, avg_loss=1.043726, seen=11, correct=7, accuracy=0.636364
2025-10-10 14:12:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:12:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:12:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:12:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2190MB allocated=2145MB
2025-10-10 14:12:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:12:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:12:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:12:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:12:49 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=35.488045, avg_loss=0.887201, seen=40, correct=21, accuracy=0.525000
2025-10-10 14:12:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:12:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:12:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:12:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2190MB allocated=2145MB
2025-10-10 14:12:51 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.575000, curr=0.525000
2025-10-10 14:13:01 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 14:13:01 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 14:13:01 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 14:13:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-10 14:13:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:13:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:13:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-10 14:13:02 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=12.005400, avg_loss=1.091400, seen=11, correct=7, accuracy=0.636364
2025-10-10 14:13:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:13:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:13:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:13:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2190MB allocated=2145MB
2025-10-10 14:13:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:13:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:13:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:13:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:13:07 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=36.147816, avg_loss=0.903695, seen=40, correct=23, accuracy=0.575000
2025-10-10 14:13:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:13:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:13:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:13:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2190MB allocated=2145MB
2025-10-10 14:13:08 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-10 14:13:19 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 14:13:19 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 14:13:19 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 14:13:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-10 14:13:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:13:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:13:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-10 14:13:19 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=12.449075, avg_loss=1.131734, seen=11, correct=6, accuracy=0.545455
2025-10-10 14:13:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:13:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:13:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:13:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2190MB allocated=2145MB
2025-10-10 14:13:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:13:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:13:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:13:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:13:23 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=36.961876, avg_loss=0.924047, seen=40, correct=22, accuracy=0.550000
2025-10-10 14:13:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:13:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:13:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:13:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2190MB allocated=2145MB
2025-10-10 14:13:26 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.575000, curr=0.550000
2025-10-10 14:13:35 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 14:13:35 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 14:13:35 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 14:13:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-10 14:13:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:13:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:13:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-10 14:13:35 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=11.991007, avg_loss=1.090092, seen=11, correct=7, accuracy=0.636364
2025-10-10 14:13:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:13:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:13:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:13:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2190MB allocated=2145MB
2025-10-10 14:13:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:13:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:13:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:13:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:13:40 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=37.219212, avg_loss=0.930480, seen=40, correct=23, accuracy=0.575000
2025-10-10 14:13:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:13:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:13:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:13:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2190MB allocated=2145MB
2025-10-10 14:13:43 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-10 14:13:51 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 14:13:51 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 14:13:51 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 14:13:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-10 14:13:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:13:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:13:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-10 14:13:51 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=12.965174, avg_loss=1.178652, seen=11, correct=6, accuracy=0.545455
2025-10-10 14:13:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:13:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:13:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:13:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2190MB allocated=2145MB
2025-10-10 14:13:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:13:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:13:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:13:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:13:55 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=38.475147, avg_loss=0.961879, seen=40, correct=23, accuracy=0.575000
2025-10-10 14:13:55 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:13:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:13:56 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:13:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2190MB allocated=2145MB
2025-10-10 14:13:56 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-10 14:14:04 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 14:14:04 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 14:14:04 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 14:14:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-10 14:14:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:14:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:14:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-10 14:14:05 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=14.500826, avg_loss=1.318257, seen=11, correct=6, accuracy=0.545455
2025-10-10 14:14:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:14:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:14:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:14:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2190MB allocated=2145MB
2025-10-10 14:14:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:14:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:14:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:14:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:14:09 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=40.747990, avg_loss=1.018700, seen=40, correct=22, accuracy=0.550000
2025-10-10 14:14:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:14:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:14:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:14:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2190MB allocated=2145MB
2025-10-10 14:14:10 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.575000, curr=0.550000
2025-10-10 14:14:20 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 14:14:20 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 14:14:20 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 14:14:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-10 14:14:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:14:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:14:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-10 14:14:20 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=14.175169, avg_loss=1.288652, seen=11, correct=6, accuracy=0.545455
2025-10-10 14:14:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:14:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:14:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:14:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2190MB allocated=2145MB
2025-10-10 14:14:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:14:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:14:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:14:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:14:25 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=44.481453, avg_loss=1.112036, seen=40, correct=22, accuracy=0.550000
2025-10-10 14:14:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:14:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:14:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:14:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2190MB allocated=2145MB
2025-10-10 14:14:27 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.575000, curr=0.550000
2025-10-10 14:14:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 14:14:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 14:14:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:14:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:14:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2190MB allocated=2145MB
2025-10-10 14:14:28 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #2', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 14:14:28 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #2', 'Round': 0, 'Results_raw': {}}
2025-10-10 14:14:28 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 14:14:29 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 14:14:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:14:30 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 14:14:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:14:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:14:32 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 14:14:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-10-10 14:14:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:14:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:14:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-10 14:14:34 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=72, loss_sum=47.724579, avg_loss=0.662841, seen=72, correct=44, accuracy=0.611111
2025-10-10 14:14:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:14:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:14:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:14:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2178MB allocated=2137MB
2025-10-10 14:14:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:14:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:14:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:14:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:14:37 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.387751, avg_loss=0.734694, seen=40, correct=23, accuracy=0.575000
2025-10-10 14:14:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:14:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:14:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:14:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2178MB allocated=2137MB
2025-10-10 14:14:38 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-10 14:14:38 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 14:14:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=343, total=1372)
2025-10-10 14:14:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:14:39 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 14:14:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:14:39 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 14:14:39 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=172, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 14:14:47 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 14:14:47 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 14:14:47 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 14:14:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-10-10 14:14:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:14:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:14:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-10 14:14:48 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=72, loss_sum=47.630527, avg_loss=0.661535, seen=72, correct=45, accuracy=0.625000
2025-10-10 14:14:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:14:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:14:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:14:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2206MB allocated=2154MB
2025-10-10 14:14:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:14:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:14:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:14:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:14:51 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.094769, avg_loss=0.727369, seen=40, correct=25, accuracy=0.625000
2025-10-10 14:14:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:14:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:14:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:14:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2206MB allocated=2154MB
2025-10-10 14:14:53 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-10 14:15:01 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 14:15:01 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 14:15:01 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 14:15:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-10-10 14:15:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:15:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:15:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-10 14:15:03 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=72, loss_sum=47.787060, avg_loss=0.663709, seen=72, correct=44, accuracy=0.611111
2025-10-10 14:15:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:15:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:15:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:15:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2206MB allocated=2154MB
2025-10-10 14:15:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:15:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:15:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:15:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:15:08 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.959682, avg_loss=0.723992, seen=40, correct=22, accuracy=0.550000
2025-10-10 14:15:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:15:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:15:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:15:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2206MB allocated=2154MB
2025-10-10 14:15:09 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.625000, curr=0.550000
2025-10-10 14:15:21 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 14:15:21 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 14:15:21 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 14:15:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-10-10 14:15:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:15:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:15:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-10 14:15:22 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=72, loss_sum=49.347179, avg_loss=0.685377, seen=72, correct=45, accuracy=0.625000
2025-10-10 14:15:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:15:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:15:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:15:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2206MB allocated=2154MB
2025-10-10 14:15:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:15:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:15:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:15:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:15:25 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.627352, avg_loss=0.715684, seen=40, correct=20, accuracy=0.500000
2025-10-10 14:15:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:15:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:15:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:15:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2206MB allocated=2154MB
2025-10-10 14:15:28 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.625000, curr=0.500000
2025-10-10 14:15:38 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 14:15:38 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 14:15:38 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 14:15:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-10-10 14:15:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:15:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:15:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-10 14:15:40 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=72, loss_sum=47.409683, avg_loss=0.658468, seen=72, correct=46, accuracy=0.638889
2025-10-10 14:15:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:15:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:15:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:15:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2206MB allocated=2154MB
2025-10-10 14:15:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:15:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:15:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:15:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:15:45 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.118984, avg_loss=0.727975, seen=40, correct=25, accuracy=0.625000
2025-10-10 14:15:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:15:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:15:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:15:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2206MB allocated=2154MB
2025-10-10 14:15:47 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-10 14:15:57 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 14:15:57 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 14:15:57 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 14:15:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-10-10 14:15:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:15:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:15:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-10 14:15:58 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=72, loss_sum=47.890533, avg_loss=0.665146, seen=72, correct=46, accuracy=0.638889
2025-10-10 14:15:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:15:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:16:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:16:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2206MB allocated=2154MB
2025-10-10 14:16:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:16:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:16:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:16:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:16:02 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.695143, avg_loss=0.717379, seen=40, correct=21, accuracy=0.525000
2025-10-10 14:16:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:16:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:16:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:16:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2206MB allocated=2154MB
2025-10-10 14:16:04 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.625000, curr=0.525000
2025-10-10 14:16:14 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 14:16:14 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 14:16:14 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 14:16:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-10-10 14:16:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:16:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:16:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-10 14:16:15 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=72, loss_sum=47.604977, avg_loss=0.661180, seen=72, correct=45, accuracy=0.625000
2025-10-10 14:16:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:16:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:16:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:16:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2206MB allocated=2154MB
2025-10-10 14:16:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:16:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:16:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:16:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:16:20 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.971855, avg_loss=0.724296, seen=40, correct=24, accuracy=0.600000
2025-10-10 14:16:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:16:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:16:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:16:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2206MB allocated=2154MB
2025-10-10 14:16:21 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.625000, curr=0.600000
2025-10-10 14:16:30 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 14:16:30 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 14:16:30 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 14:16:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-10-10 14:16:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:16:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:16:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-10 14:16:31 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=72, loss_sum=48.150146, avg_loss=0.668752, seen=72, correct=44, accuracy=0.611111
2025-10-10 14:16:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:16:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:16:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:16:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2206MB allocated=2154MB
2025-10-10 14:16:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:16:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:16:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:16:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:16:35 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.153351, avg_loss=0.703834, seen=40, correct=22, accuracy=0.550000
2025-10-10 14:16:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:16:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:16:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:16:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2206MB allocated=2154MB
2025-10-10 14:16:37 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.625000, curr=0.550000
2025-10-10 14:16:46 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 14:16:46 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 14:16:46 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 14:16:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-10-10 14:16:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:16:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:16:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-10 14:16:47 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=72, loss_sum=48.036446, avg_loss=0.667173, seen=72, correct=46, accuracy=0.638889
2025-10-10 14:16:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:16:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:16:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:16:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2206MB allocated=2154MB
2025-10-10 14:16:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:16:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:16:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:16:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:16:51 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.978889, avg_loss=0.699472, seen=40, correct=20, accuracy=0.500000
2025-10-10 14:16:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:16:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:16:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:16:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2206MB allocated=2154MB
2025-10-10 14:16:52 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/25), best=0.625000, curr=0.500000
2025-10-10 14:17:03 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 14:17:03 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 14:17:03 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 14:17:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-10-10 14:17:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:17:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:17:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-10 14:17:05 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=72, loss_sum=46.590324, avg_loss=0.647088, seen=72, correct=46, accuracy=0.638889
2025-10-10 14:17:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:17:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:17:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:17:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2206MB allocated=2154MB
2025-10-10 14:17:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:17:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:17:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:17:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:17:08 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.319471, avg_loss=0.707987, seen=40, correct=23, accuracy=0.575000
2025-10-10 14:17:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:17:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:17:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:17:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2206MB allocated=2154MB
2025-10-10 14:17:10 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/25), best=0.625000, curr=0.575000
2025-10-10 14:17:18 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 14:17:18 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 14:17:18 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 14:17:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-10-10 14:17:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:17:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:17:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-10 14:17:19 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=72, loss_sum=47.212479, avg_loss=0.655729, seen=72, correct=44, accuracy=0.611111
2025-10-10 14:17:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:17:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:17:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:17:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2206MB allocated=2154MB
2025-10-10 14:17:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:17:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:17:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:17:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:17:23 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.116039, avg_loss=0.727901, seen=40, correct=24, accuracy=0.600000
2025-10-10 14:17:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:17:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:17:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:17:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2206MB allocated=2154MB
2025-10-10 14:17:25 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/25), best=0.625000, curr=0.600000
2025-10-10 14:17:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 14:17:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 14:17:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:17:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:17:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2206MB allocated=2154MB
2025-10-10 14:17:27 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #13', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 14:17:27 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #13', 'Round': 0, 'Results_raw': {}}
2025-10-10 14:17:27 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 14:17:28 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 14:17:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:17:28 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 14:17:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:17:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:17:30 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 14:17:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-10-10 14:17:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:17:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:17:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-10-10 14:17:32 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=119, loss_sum=85.037033, avg_loss=0.714597, seen=119, correct=66, accuracy=0.554622
2025-10-10 14:17:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:17:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:17:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:17:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2178MB allocated=2145MB
2025-10-10 14:17:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:17:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:17:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:17:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:17:36 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.439432, avg_loss=0.735986, seen=40, correct=20, accuracy=0.500000
2025-10-10 14:17:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:17:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:17:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:17:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2178MB allocated=2145MB
2025-10-10 14:17:38 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.500000
2025-10-10 14:17:38 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 14:17:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=569, total=2275)
2025-10-10 14:17:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:17:38 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 14:17:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:17:38 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 14:17:38 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=285, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 14:17:47 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 14:17:47 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 14:17:47 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 14:17:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-10-10 14:17:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:17:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:17:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-10-10 14:17:50 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=119, loss_sum=84.303955, avg_loss=0.708437, seen=119, correct=62, accuracy=0.521008
2025-10-10 14:17:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:17:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:17:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:17:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2198MB allocated=2162MB
2025-10-10 14:17:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:17:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:17:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:17:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:17:53 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.766773, avg_loss=0.719169, seen=40, correct=16, accuracy=0.400000
2025-10-10 14:17:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:17:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:17:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:17:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2198MB allocated=2162MB
2025-10-10 14:17:56 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.500000, curr=0.400000
2025-10-10 14:18:05 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 14:18:05 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 14:18:05 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 14:18:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-10-10 14:18:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:18:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:18:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-10-10 14:18:07 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=119, loss_sum=85.035454, avg_loss=0.714584, seen=119, correct=55, accuracy=0.462185
2025-10-10 14:18:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:18:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:18:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:18:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2198MB allocated=2162MB
2025-10-10 14:18:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:18:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:18:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:18:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:18:10 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.342566, avg_loss=0.683564, seen=40, correct=20, accuracy=0.500000
2025-10-10 14:18:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:18:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:18:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:18:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2198MB allocated=2162MB
2025-10-10 14:18:11 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.500000
2025-10-10 14:18:20 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 14:18:20 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 14:18:20 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 14:18:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-10-10 14:18:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:18:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:18:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-10-10 14:18:22 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=119, loss_sum=85.130852, avg_loss=0.715385, seen=119, correct=52, accuracy=0.436975
2025-10-10 14:18:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:18:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:18:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:18:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2198MB allocated=2162MB
2025-10-10 14:18:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:18:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:18:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:18:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:18:26 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.955465, avg_loss=0.673887, seen=40, correct=23, accuracy=0.575000
2025-10-10 14:18:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:18:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:18:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:18:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2198MB allocated=2162MB
2025-10-10 14:18:29 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-10 14:18:38 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 14:18:38 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 14:18:38 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 14:18:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-10-10 14:18:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:18:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:18:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-10-10 14:18:39 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=119, loss_sum=83.652435, avg_loss=0.702962, seen=119, correct=63, accuracy=0.529412
2025-10-10 14:18:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:18:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:18:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:18:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2198MB allocated=2162MB
2025-10-10 14:18:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:18:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:18:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:18:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:18:45 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.428337, avg_loss=0.710708, seen=40, correct=17, accuracy=0.425000
2025-10-10 14:18:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:18:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:18:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:18:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2198MB allocated=2162MB
2025-10-10 14:18:46 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.575000, curr=0.425000
2025-10-10 14:18:56 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 14:18:56 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 14:18:56 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 14:18:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-10-10 14:18:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:18:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:18:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-10-10 14:18:58 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=119, loss_sum=83.099747, avg_loss=0.698317, seen=119, correct=59, accuracy=0.495798
2025-10-10 14:18:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:18:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:19:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:19:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2198MB allocated=2162MB
2025-10-10 14:19:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:19:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:19:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:19:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:19:04 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.264429, avg_loss=0.681611, seen=40, correct=22, accuracy=0.550000
2025-10-10 14:19:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:19:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:19:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:19:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2198MB allocated=2162MB
2025-10-10 14:19:06 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.575000, curr=0.550000
2025-10-10 14:19:14 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 14:19:14 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 14:19:14 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 14:19:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-10-10 14:19:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:19:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:19:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-10-10 14:19:17 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=119, loss_sum=84.773636, avg_loss=0.712383, seen=119, correct=56, accuracy=0.470588
2025-10-10 14:19:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:19:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:19:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:19:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2198MB allocated=2162MB
2025-10-10 14:19:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:19:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:19:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:19:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:19:21 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.738813, avg_loss=0.668470, seen=40, correct=21, accuracy=0.525000
2025-10-10 14:19:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:19:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:19:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:19:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2198MB allocated=2162MB
2025-10-10 14:19:23 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.575000, curr=0.525000
2025-10-10 14:19:32 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 14:19:32 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 14:19:32 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 14:19:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-10-10 14:19:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:19:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:19:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-10-10 14:19:35 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=119, loss_sum=86.158180, avg_loss=0.724018, seen=119, correct=54, accuracy=0.453782
2025-10-10 14:19:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:19:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:19:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:19:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2198MB allocated=2162MB
2025-10-10 14:19:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:19:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:19:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:19:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:19:38 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.690115, avg_loss=0.667253, seen=40, correct=24, accuracy=0.600000
2025-10-10 14:19:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:19:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:19:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:19:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2198MB allocated=2162MB
2025-10-10 14:19:40 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-10 14:19:50 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 14:19:50 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 14:19:50 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 14:19:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-10-10 14:19:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:19:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:19:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-10-10 14:19:52 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=119, loss_sum=83.212624, avg_loss=0.699266, seen=119, correct=65, accuracy=0.546218
2025-10-10 14:19:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:19:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:19:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:19:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2198MB allocated=2162MB
2025-10-10 14:19:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:19:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:19:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:19:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:19:57 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.816982, avg_loss=0.695425, seen=40, correct=19, accuracy=0.475000
2025-10-10 14:19:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:19:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:19:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:19:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2198MB allocated=2162MB
2025-10-10 14:19:58 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.600000, curr=0.475000
2025-10-10 14:20:09 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 14:20:09 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 14:20:09 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 14:20:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-10-10 14:20:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:20:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:20:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-10-10 14:20:11 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=119, loss_sum=85.271957, avg_loss=0.716571, seen=119, correct=62, accuracy=0.521008
2025-10-10 14:20:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:20:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:20:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:20:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2198MB allocated=2162MB
2025-10-10 14:20:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:20:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:20:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:20:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:20:15 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.765121, avg_loss=0.744128, seen=40, correct=20, accuracy=0.500000
2025-10-10 14:20:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:20:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:20:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:20:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2198MB allocated=2162MB
2025-10-10 14:20:16 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.600000, curr=0.500000
2025-10-10 14:20:23 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 14:20:23 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 14:20:23 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 14:20:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-10-10 14:20:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:20:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:20:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-10-10 14:20:25 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=119, loss_sum=82.996140, avg_loss=0.697447, seen=119, correct=67, accuracy=0.563025
2025-10-10 14:20:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:20:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:20:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:20:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2198MB allocated=2162MB
2025-10-10 14:20:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:20:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:20:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:20:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:20:29 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.959574, avg_loss=0.698989, seen=40, correct=20, accuracy=0.500000
2025-10-10 14:20:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:20:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:20:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:20:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2198MB allocated=2162MB
2025-10-10 14:20:31 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.600000, curr=0.500000
2025-10-10 14:20:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 14:20:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 14:20:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:20:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:20:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2198MB allocated=2162MB
2025-10-10 14:20:32 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #41', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 14:20:32 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #41', 'Round': 0, 'Results_raw': {}}
2025-10-10 14:20:33 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 14:20:34 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 14:20:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:20:34 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 14:20:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:20:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:20:37 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 14:20:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 14:20:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:20:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:20:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 14:20:40 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=140.756836, avg_loss=0.703784, seen=200, correct=100, accuracy=0.500000
2025-10-10 14:20:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:20:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:20:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:20:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2198MB allocated=2154MB
2025-10-10 14:20:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:20:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:20:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:20:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:20:44 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.894348, avg_loss=0.772359, seen=40, correct=15, accuracy=0.375000
2025-10-10 14:20:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:20:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:20:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:20:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2198MB allocated=2154MB
2025-10-10 14:20:46 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.375000
2025-10-10 14:20:46 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 14:20:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1162, total=4647)
2025-10-10 14:20:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:20:46 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 14:20:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:20:46 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 14:20:46 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=581, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 14:20:54 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 14:20:54 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 14:20:54 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 14:20:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 14:20:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:20:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:20:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 14:20:57 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=140.913284, avg_loss=0.704566, seen=200, correct=97, accuracy=0.485000
2025-10-10 14:20:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:20:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:20:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:20:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2218MB allocated=2170MB
2025-10-10 14:20:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:20:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:20:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:21:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:21:01 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=31.097466, avg_loss=0.777437, seen=40, correct=15, accuracy=0.375000
2025-10-10 14:21:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:21:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:21:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:21:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2218MB allocated=2170MB
2025-10-10 14:21:02 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.375000
2025-10-10 14:21:12 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 14:21:12 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 14:21:12 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 14:21:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 14:21:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:21:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:21:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 14:21:15 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=145.624344, avg_loss=0.728122, seen=200, correct=96, accuracy=0.480000
2025-10-10 14:21:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:21:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:21:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:21:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2218MB allocated=2170MB
2025-10-10 14:21:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:21:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:21:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:21:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:21:20 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.173765, avg_loss=0.704344, seen=40, correct=21, accuracy=0.525000
2025-10-10 14:21:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:21:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:21:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:21:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2218MB allocated=2170MB
2025-10-10 14:21:22 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.525000
2025-10-10 14:21:32 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 14:21:32 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 14:21:32 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 14:21:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 14:21:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:21:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:21:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 14:21:35 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=141.188293, avg_loss=0.705941, seen=200, correct=114, accuracy=0.570000
2025-10-10 14:21:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:21:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:21:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:21:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2218MB allocated=2170MB
2025-10-10 14:21:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:21:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:21:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:21:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:21:39 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.050749, avg_loss=0.751269, seen=40, correct=16, accuracy=0.400000
2025-10-10 14:21:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:21:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:21:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:21:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2218MB allocated=2170MB
2025-10-10 14:21:41 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.525000, curr=0.400000
2025-10-10 14:21:51 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 14:21:51 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 14:21:51 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 14:21:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 14:21:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:21:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:21:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 14:21:54 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=140.131638, avg_loss=0.700658, seen=200, correct=99, accuracy=0.495000
2025-10-10 14:21:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:21:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:21:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:21:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2218MB allocated=2170MB
2025-10-10 14:21:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:21:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:21:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:21:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:21:59 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=31.032734, avg_loss=0.775818, seen=40, correct=12, accuracy=0.300000
2025-10-10 14:21:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:21:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:22:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:22:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2218MB allocated=2170MB
2025-10-10 14:22:00 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.525000, curr=0.300000
2025-10-10 14:22:12 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 14:22:12 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 14:22:12 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 14:22:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 14:22:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:22:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:22:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 14:22:15 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=141.684784, avg_loss=0.708424, seen=200, correct=108, accuracy=0.540000
2025-10-10 14:22:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:22:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:22:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:22:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2218MB allocated=2170MB
2025-10-10 14:22:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:22:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:22:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:22:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:22:18 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.693241, avg_loss=0.742331, seen=40, correct=17, accuracy=0.425000
2025-10-10 14:22:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:22:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:22:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:22:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2218MB allocated=2170MB
2025-10-10 14:22:20 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.525000, curr=0.425000
2025-10-10 14:22:30 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 14:22:30 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 14:22:30 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 14:22:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 14:22:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:22:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:22:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 14:22:33 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=142.871948, avg_loss=0.714360, seen=200, correct=108, accuracy=0.540000
2025-10-10 14:22:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:22:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:22:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:22:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2218MB allocated=2170MB
2025-10-10 14:22:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:22:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:22:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:22:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:22:37 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.011559, avg_loss=0.725289, seen=40, correct=19, accuracy=0.475000
2025-10-10 14:22:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:22:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:22:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:22:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2218MB allocated=2170MB
2025-10-10 14:22:38 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/25), best=0.525000, curr=0.475000
2025-10-10 14:22:47 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 14:22:47 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 14:22:47 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 14:22:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 14:22:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:22:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:22:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 14:22:50 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=141.269989, avg_loss=0.706350, seen=200, correct=107, accuracy=0.535000
2025-10-10 14:22:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:22:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:22:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:22:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2218MB allocated=2170MB
2025-10-10 14:22:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:22:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:22:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:22:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:22:54 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.440248, avg_loss=0.736006, seen=40, correct=16, accuracy=0.400000
2025-10-10 14:22:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:22:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:22:56 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:22:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2218MB allocated=2170MB
2025-10-10 14:22:56 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/25), best=0.525000, curr=0.400000
2025-10-10 14:23:06 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 14:23:06 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 14:23:06 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 14:23:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 14:23:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:23:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:23:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 14:23:09 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=139.556488, avg_loss=0.697782, seen=200, correct=100, accuracy=0.500000
2025-10-10 14:23:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:23:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:23:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:23:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2218MB allocated=2170MB
2025-10-10 14:23:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:23:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:23:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:23:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:23:13 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=31.091812, avg_loss=0.777295, seen=40, correct=12, accuracy=0.300000
2025-10-10 14:23:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:23:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:23:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:23:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2218MB allocated=2170MB
2025-10-10 14:23:15 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/25), best=0.525000, curr=0.300000
2025-10-10 14:23:25 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 14:23:25 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 14:23:25 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 14:23:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 14:23:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:23:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:23:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 14:23:28 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=140.115021, avg_loss=0.700575, seen=200, correct=98, accuracy=0.490000
2025-10-10 14:23:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:23:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:23:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:23:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2218MB allocated=2170MB
2025-10-10 14:23:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:23:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:23:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:23:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:23:32 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=32.229584, avg_loss=0.805740, seen=40, correct=14, accuracy=0.350000
2025-10-10 14:23:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:23:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:23:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:23:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2218MB allocated=2170MB
2025-10-10 14:23:35 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/25), best=0.525000, curr=0.350000
2025-10-10 14:23:43 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 14:23:43 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 14:23:43 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 14:23:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 14:23:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:23:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:23:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 14:23:46 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=139.739365, avg_loss=0.698697, seen=200, correct=103, accuracy=0.515000
2025-10-10 14:23:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:23:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:23:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:23:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2218MB allocated=2170MB
2025-10-10 14:23:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:23:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:23:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:23:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:23:51 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.688316, avg_loss=0.767208, seen=40, correct=12, accuracy=0.300000
2025-10-10 14:23:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:23:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:23:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:23:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2218MB allocated=2170MB
2025-10-10 14:23:53 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/25), best=0.525000, curr=0.300000
2025-10-10 14:23:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 14:23:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 14:23:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:23:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:23:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2218MB allocated=2170MB
2025-10-10 14:23:54 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #25', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 14:23:54 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #25', 'Round': 0, 'Results_raw': {}}
2025-10-10 14:23:55 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 14:23:55 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 14:23:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:23:56 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 14:23:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:23:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:23:58 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 14:23:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-10-10 14:23:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:23:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:23:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-10-10 14:23:59 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=57, loss_sum=38.726170, avg_loss=0.679406, seen=57, correct=31, accuracy=0.543860
2025-10-10 14:23:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:23:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:24:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:24:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2198MB allocated=2162MB
2025-10-10 14:24:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:24:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:24:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:24:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:24:03 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.329800, avg_loss=0.608245, seen=40, correct=26, accuracy=0.650000
2025-10-10 14:24:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:24:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:24:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:24:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2198MB allocated=2162MB
2025-10-10 14:24:05 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.650000
2025-10-10 14:24:05 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 14:24:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=272, total=1088)
2025-10-10 14:24:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:24:05 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 14:24:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:24:05 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 14:24:05 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=136, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 14:24:14 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 14:24:14 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 14:24:14 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 14:24:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-10-10 14:24:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:24:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:24:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-10-10 14:24:15 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=57, loss_sum=38.655846, avg_loss=0.678173, seen=57, correct=32, accuracy=0.561404
2025-10-10 14:24:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:24:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:24:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:24:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2238MB allocated=2179MB
2025-10-10 14:24:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:24:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:24:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:24:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:24:19 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.035568, avg_loss=0.600889, seen=40, correct=27, accuracy=0.675000
2025-10-10 14:24:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:24:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:24:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:24:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2238MB allocated=2179MB
2025-10-10 14:24:20 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.675000
2025-10-10 14:24:28 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 14:24:28 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 14:24:28 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 14:24:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-10-10 14:24:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:24:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:24:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-10-10 14:24:29 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=57, loss_sum=38.958546, avg_loss=0.683483, seen=57, correct=32, accuracy=0.561404
2025-10-10 14:24:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:24:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:24:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:24:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2238MB allocated=2179MB
2025-10-10 14:24:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:24:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:24:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:24:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:24:34 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.101727, avg_loss=0.602543, seen=40, correct=28, accuracy=0.700000
2025-10-10 14:24:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:24:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:24:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:24:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2238MB allocated=2179MB
2025-10-10 14:24:37 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.700000
2025-10-10 14:24:46 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 14:24:46 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 14:24:46 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 14:24:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-10-10 14:24:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:24:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:24:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-10-10 14:24:47 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=57, loss_sum=39.042011, avg_loss=0.684948, seen=57, correct=31, accuracy=0.543860
2025-10-10 14:24:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:24:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:24:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:24:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2238MB allocated=2179MB
2025-10-10 14:24:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:24:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:24:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:24:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:24:53 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.528877, avg_loss=0.613222, seen=40, correct=28, accuracy=0.700000
2025-10-10 14:24:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:24:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:24:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:24:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2238MB allocated=2179MB
2025-10-10 14:24:54 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.700000
2025-10-10 14:25:02 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 14:25:02 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 14:25:02 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 14:25:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-10-10 14:25:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:25:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:25:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-10-10 14:25:04 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=57, loss_sum=38.926701, avg_loss=0.682925, seen=57, correct=33, accuracy=0.578947
2025-10-10 14:25:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:25:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:25:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:25:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2238MB allocated=2179MB
2025-10-10 14:25:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:25:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:25:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:25:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:25:08 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.075743, avg_loss=0.601894, seen=40, correct=26, accuracy=0.650000
2025-10-10 14:25:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:25:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:25:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:25:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2238MB allocated=2179MB
2025-10-10 14:25:09 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.700000, curr=0.650000
2025-10-10 14:25:20 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 14:25:20 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 14:25:20 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 14:25:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-10-10 14:25:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:25:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:25:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-10-10 14:25:21 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=57, loss_sum=38.443260, avg_loss=0.674443, seen=57, correct=33, accuracy=0.578947
2025-10-10 14:25:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:25:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:25:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:25:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2238MB allocated=2179MB
2025-10-10 14:25:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:25:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:25:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:25:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:25:26 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.893763, avg_loss=0.597344, seen=40, correct=26, accuracy=0.650000
2025-10-10 14:25:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:25:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:25:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:25:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2238MB allocated=2179MB
2025-10-10 14:25:28 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.700000, curr=0.650000
2025-10-10 14:25:37 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 14:25:37 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 14:25:37 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 14:25:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-10-10 14:25:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:25:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:25:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-10-10 14:25:38 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=57, loss_sum=39.209560, avg_loss=0.687887, seen=57, correct=34, accuracy=0.596491
2025-10-10 14:25:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:25:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:25:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:25:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2238MB allocated=2179MB
2025-10-10 14:25:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:25:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:25:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:25:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:25:42 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.738338, avg_loss=0.618458, seen=40, correct=27, accuracy=0.675000
2025-10-10 14:25:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:25:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:25:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:25:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2238MB allocated=2179MB
2025-10-10 14:25:44 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.700000, curr=0.675000
2025-10-10 14:25:56 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 14:25:56 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 14:25:56 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 14:25:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-10-10 14:25:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:25:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:25:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-10-10 14:25:57 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=57, loss_sum=38.609219, avg_loss=0.677355, seen=57, correct=32, accuracy=0.561404
2025-10-10 14:25:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:25:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:25:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:26:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2238MB allocated=2179MB
2025-10-10 14:26:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:26:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:26:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:26:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:26:01 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.565174, avg_loss=0.614129, seen=40, correct=26, accuracy=0.650000
2025-10-10 14:26:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:26:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:26:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:26:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2238MB allocated=2179MB
2025-10-10 14:26:03 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/25), best=0.700000, curr=0.650000
2025-10-10 14:26:14 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 14:26:14 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 14:26:14 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 14:26:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-10-10 14:26:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:26:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:26:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-10-10 14:26:15 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=57, loss_sum=38.434513, avg_loss=0.674290, seen=57, correct=35, accuracy=0.614035
2025-10-10 14:26:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:26:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:26:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:26:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2238MB allocated=2179MB
2025-10-10 14:26:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:26:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:26:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:26:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:26:18 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.416126, avg_loss=0.610403, seen=40, correct=27, accuracy=0.675000
2025-10-10 14:26:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:26:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:26:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:26:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2238MB allocated=2179MB
2025-10-10 14:26:20 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/25), best=0.700000, curr=0.675000
2025-10-10 14:26:28 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 14:26:28 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 14:26:28 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 14:26:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-10-10 14:26:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:26:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:26:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-10-10 14:26:30 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=57, loss_sum=38.359364, avg_loss=0.672971, seen=57, correct=33, accuracy=0.578947
2025-10-10 14:26:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:26:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:26:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:26:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2238MB allocated=2179MB
2025-10-10 14:26:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:26:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:26:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:26:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:26:35 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.076708, avg_loss=0.601918, seen=40, correct=26, accuracy=0.650000
2025-10-10 14:26:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:26:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:26:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:26:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2238MB allocated=2179MB
2025-10-10 14:26:36 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/25), best=0.700000, curr=0.650000
2025-10-10 14:26:46 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 14:26:46 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 14:26:46 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 14:26:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-10-10 14:26:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:26:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:26:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-10-10 14:26:47 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=57, loss_sum=38.807213, avg_loss=0.680828, seen=57, correct=36, accuracy=0.631579
2025-10-10 14:26:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:26:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:26:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:26:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2238MB allocated=2179MB
2025-10-10 14:26:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:26:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:26:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:26:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:26:50 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.729229, avg_loss=0.618231, seen=40, correct=24, accuracy=0.600000
2025-10-10 14:26:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:26:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:26:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:26:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2238MB allocated=2179MB
2025-10-10 14:26:53 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/25), best=0.700000, curr=0.600000
2025-10-10 14:26:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 14:26:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 14:26:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:26:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:26:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2238MB allocated=2179MB
2025-10-10 14:26:54 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #7', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 14:26:54 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #7', 'Round': 0, 'Results_raw': {}}
2025-10-10 14:26:54 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 14:26:55 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 14:26:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:26:56 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 14:26:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:26:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:26:58 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 14:26:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 14:26:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:26:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:27:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 14:27:01 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=133.361115, avg_loss=0.666806, seen=200, correct=116, accuracy=0.580000
2025-10-10 14:27:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:27:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:27:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:27:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2198MB allocated=2170MB
2025-10-10 14:27:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:27:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:27:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:27:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:27:05 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.775003, avg_loss=0.744375, seen=40, correct=13, accuracy=0.325000
2025-10-10 14:27:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:27:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:27:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:27:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2198MB allocated=2170MB
2025-10-10 14:27:07 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.325000
2025-10-10 14:27:07 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 14:27:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1236, total=4944)
2025-10-10 14:27:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:27:07 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 14:27:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:27:07 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 14:27:08 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=618, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 14:27:16 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 14:27:16 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 14:27:16 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 14:27:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 14:27:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:27:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:27:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 14:27:20 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=132.856995, avg_loss=0.664285, seen=200, correct=117, accuracy=0.585000
2025-10-10 14:27:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:27:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:27:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:27:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2216MB allocated=2187MB
2025-10-10 14:27:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:27:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:27:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:27:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:27:25 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.471033, avg_loss=0.736776, seen=40, correct=13, accuracy=0.325000
2025-10-10 14:27:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:27:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:27:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:27:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2216MB allocated=2187MB
2025-10-10 14:27:27 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.325000
2025-10-10 14:27:36 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 14:27:36 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 14:27:36 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 14:27:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 14:27:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:27:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:27:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 14:27:39 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=133.999924, avg_loss=0.670000, seen=200, correct=122, accuracy=0.610000
2025-10-10 14:27:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:27:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:27:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:27:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2216MB allocated=2187MB
2025-10-10 14:27:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:27:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:27:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:27:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:27:44 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.109085, avg_loss=0.727727, seen=40, correct=22, accuracy=0.550000
2025-10-10 14:27:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:27:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:27:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:27:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2216MB allocated=2187MB
2025-10-10 14:27:46 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.550000
2025-10-10 14:27:56 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 14:27:56 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 14:27:56 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 14:27:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 14:27:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:27:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:27:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 14:27:59 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=132.389374, avg_loss=0.661947, seen=200, correct=122, accuracy=0.610000
2025-10-10 14:27:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:27:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:28:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:28:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2216MB allocated=2187MB
2025-10-10 14:28:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:28:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:28:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:28:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:28:03 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.768269, avg_loss=0.744207, seen=40, correct=14, accuracy=0.350000
2025-10-10 14:28:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:28:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:28:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:28:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2216MB allocated=2187MB
2025-10-10 14:28:04 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.550000, curr=0.350000
2025-10-10 14:28:13 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 14:28:13 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 14:28:13 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 14:28:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 14:28:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:28:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:28:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 14:28:16 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=132.415604, avg_loss=0.662078, seen=200, correct=117, accuracy=0.585000
2025-10-10 14:28:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:28:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:28:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:28:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2216MB allocated=2187MB
2025-10-10 14:28:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:28:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:28:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:28:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:28:20 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.402435, avg_loss=0.735061, seen=40, correct=14, accuracy=0.350000
2025-10-10 14:28:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:28:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:28:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:28:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2216MB allocated=2187MB
2025-10-10 14:28:22 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.550000, curr=0.350000
2025-10-10 14:28:31 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 14:28:31 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 14:28:31 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 14:28:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 14:28:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:28:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:28:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 14:28:35 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=133.670074, avg_loss=0.668350, seen=200, correct=115, accuracy=0.575000
2025-10-10 14:28:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:28:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:28:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:28:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2216MB allocated=2187MB
2025-10-10 14:28:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:28:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:28:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:28:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:28:39 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.928432, avg_loss=0.723211, seen=40, correct=16, accuracy=0.400000
2025-10-10 14:28:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:28:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:28:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:28:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2216MB allocated=2187MB
2025-10-10 14:28:41 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.550000, curr=0.400000
2025-10-10 14:28:51 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 14:28:51 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 14:28:51 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 14:28:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 14:28:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:28:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:28:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 14:28:54 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=133.569412, avg_loss=0.667847, seen=200, correct=118, accuracy=0.590000
2025-10-10 14:28:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:28:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:28:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:28:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2216MB allocated=2187MB
2025-10-10 14:28:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:28:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:28:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:29:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:29:00 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.950735, avg_loss=0.723768, seen=40, correct=17, accuracy=0.425000
2025-10-10 14:29:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:29:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:29:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:29:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2216MB allocated=2187MB
2025-10-10 14:29:01 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/25), best=0.550000, curr=0.425000
2025-10-10 14:29:09 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 14:29:09 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 14:29:09 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 14:29:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 14:29:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:29:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:29:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 14:29:13 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=134.233047, avg_loss=0.671165, seen=200, correct=120, accuracy=0.600000
2025-10-10 14:29:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:29:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:29:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:29:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2216MB allocated=2187MB
2025-10-10 14:29:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:29:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:29:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:29:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:29:18 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.984722, avg_loss=0.724618, seen=40, correct=15, accuracy=0.375000
2025-10-10 14:29:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:29:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:29:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:29:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2216MB allocated=2187MB
2025-10-10 14:29:19 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/25), best=0.550000, curr=0.375000
2025-10-10 14:29:28 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 14:29:28 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 14:29:28 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 14:29:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 14:29:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:29:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:29:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 14:29:31 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=135.138840, avg_loss=0.675694, seen=200, correct=114, accuracy=0.570000
2025-10-10 14:29:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:29:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:29:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:29:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2216MB allocated=2187MB
2025-10-10 14:29:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:29:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:29:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:29:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:29:35 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.796562, avg_loss=0.719914, seen=40, correct=16, accuracy=0.400000
2025-10-10 14:29:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:29:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:29:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:29:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2216MB allocated=2187MB
2025-10-10 14:29:38 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/25), best=0.550000, curr=0.400000
2025-10-10 14:29:47 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 14:29:47 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 14:29:47 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 14:29:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 14:29:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:29:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:29:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 14:29:50 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=134.513824, avg_loss=0.672569, seen=200, correct=119, accuracy=0.595000
2025-10-10 14:29:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:29:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:29:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:29:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2216MB allocated=2187MB
2025-10-10 14:29:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:29:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:29:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:29:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:29:53 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.504406, avg_loss=0.712610, seen=40, correct=18, accuracy=0.450000
2025-10-10 14:29:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:29:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:29:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:29:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2216MB allocated=2187MB
2025-10-10 14:29:54 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/25), best=0.550000, curr=0.450000
2025-10-10 14:30:03 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 14:30:03 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 14:30:03 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 14:30:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 14:30:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:30:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:30:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 14:30:06 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=135.133179, avg_loss=0.675666, seen=200, correct=117, accuracy=0.585000
2025-10-10 14:30:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:30:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:30:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:30:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2216MB allocated=2187MB
2025-10-10 14:30:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:30:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:30:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:30:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:30:10 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.581287, avg_loss=0.714532, seen=40, correct=18, accuracy=0.450000
2025-10-10 14:30:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:30:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:30:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:30:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2216MB allocated=2187MB
2025-10-10 14:30:12 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/25), best=0.550000, curr=0.450000
2025-10-10 14:30:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 14:30:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 14:30:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:30:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:30:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2216MB allocated=2187MB
2025-10-10 14:30:13 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #24', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 14:30:13 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #24', 'Round': 0, 'Results_raw': {}}
2025-10-10 14:30:13 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 14:30:15 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 14:30:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:30:15 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 14:30:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:30:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:30:17 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 14:30:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 14:30:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:30:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:30:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 14:30:21 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=143.669754, avg_loss=0.718349, seen=200, correct=98, accuracy=0.490000
2025-10-10 14:30:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:30:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:30:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:30:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2218MB allocated=2179MB
2025-10-10 14:30:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:30:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:30:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:30:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:30:25 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.699110, avg_loss=0.742478, seen=40, correct=19, accuracy=0.475000
2025-10-10 14:30:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:30:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:30:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:30:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2218MB allocated=2179MB
2025-10-10 14:30:27 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.475000
2025-10-10 14:30:27 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 14:30:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1069, total=4273)
2025-10-10 14:30:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:30:27 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 14:30:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:30:27 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 14:30:27 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=535, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 14:30:36 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 14:30:36 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 14:30:36 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 14:30:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 14:30:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:30:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:30:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 14:30:39 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=139.389389, avg_loss=0.696947, seen=200, correct=103, accuracy=0.515000
2025-10-10 14:30:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:30:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:30:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:30:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2242MB allocated=2196MB
2025-10-10 14:30:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:30:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:30:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:30:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:30:43 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.386091, avg_loss=0.709652, seen=40, correct=20, accuracy=0.500000
2025-10-10 14:30:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:30:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:30:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:30:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2242MB allocated=2196MB
2025-10-10 14:30:44 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.500000
2025-10-10 14:30:55 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 14:30:55 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 14:30:55 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 14:30:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 14:30:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:30:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:30:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 14:30:58 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=145.719299, avg_loss=0.728596, seen=200, correct=99, accuracy=0.495000
2025-10-10 14:30:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:30:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:31:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:31:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2242MB allocated=2196MB
2025-10-10 14:31:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:31:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:31:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:31:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:31:03 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.468441, avg_loss=0.761711, seen=40, correct=19, accuracy=0.475000
2025-10-10 14:31:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:31:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:31:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:31:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2242MB allocated=2196MB
2025-10-10 14:31:04 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.500000, curr=0.475000
2025-10-10 14:31:15 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 14:31:15 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 14:31:15 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 14:31:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 14:31:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:31:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:31:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 14:31:18 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=144.399872, avg_loss=0.721999, seen=200, correct=97, accuracy=0.485000
2025-10-10 14:31:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:31:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:31:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:31:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2242MB allocated=2196MB
2025-10-10 14:31:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:31:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:31:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:31:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:31:22 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.904699, avg_loss=0.747617, seen=40, correct=19, accuracy=0.475000
2025-10-10 14:31:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:31:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:31:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:31:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2242MB allocated=2196MB
2025-10-10 14:31:24 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.500000, curr=0.475000
2025-10-10 14:31:33 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 14:31:33 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 14:31:33 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 14:31:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 14:31:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:31:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:31:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 14:31:36 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=138.902618, avg_loss=0.694513, seen=200, correct=108, accuracy=0.540000
2025-10-10 14:31:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:31:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:31:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:31:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2242MB allocated=2196MB
2025-10-10 14:31:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:31:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:31:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:31:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:31:42 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.793072, avg_loss=0.694827, seen=40, correct=18, accuracy=0.450000
2025-10-10 14:31:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:31:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:31:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:31:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2242MB allocated=2196MB
2025-10-10 14:31:44 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.500000, curr=0.450000
2025-10-10 14:31:54 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 14:31:54 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 14:31:54 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 14:31:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 14:31:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:31:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:31:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 14:31:57 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=138.376633, avg_loss=0.691883, seen=200, correct=111, accuracy=0.555000
2025-10-10 14:31:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:31:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:31:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:31:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2242MB allocated=2196MB
2025-10-10 14:31:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:31:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:31:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:32:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:32:00 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.808689, avg_loss=0.695217, seen=40, correct=19, accuracy=0.475000
2025-10-10 14:32:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:32:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:32:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:32:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2242MB allocated=2196MB
2025-10-10 14:32:02 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/25), best=0.500000, curr=0.475000
2025-10-10 14:32:10 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 14:32:10 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 14:32:10 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 14:32:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 14:32:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:32:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:32:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 14:32:13 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=141.364212, avg_loss=0.706821, seen=200, correct=102, accuracy=0.510000
2025-10-10 14:32:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:32:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:32:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:32:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2242MB allocated=2196MB
2025-10-10 14:32:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:32:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:32:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:32:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:32:17 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.785387, avg_loss=0.719635, seen=40, correct=20, accuracy=0.500000
2025-10-10 14:32:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:32:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:32:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:32:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2242MB allocated=2196MB
2025-10-10 14:32:19 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.500000
2025-10-10 14:32:28 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 14:32:28 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 14:32:28 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 14:32:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 14:32:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:32:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:32:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 14:32:31 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=138.393250, avg_loss=0.691966, seen=200, correct=104, accuracy=0.520000
2025-10-10 14:32:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:32:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:32:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:32:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2242MB allocated=2196MB
2025-10-10 14:32:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:32:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:32:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:32:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:32:34 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.684437, avg_loss=0.692111, seen=40, correct=19, accuracy=0.475000
2025-10-10 14:32:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:32:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:32:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:32:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2242MB allocated=2196MB
2025-10-10 14:32:36 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.500000, curr=0.475000
2025-10-10 14:32:46 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 14:32:46 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 14:32:46 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 14:32:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 14:32:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:32:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:32:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 14:32:49 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=138.133698, avg_loss=0.690668, seen=200, correct=98, accuracy=0.490000
2025-10-10 14:32:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:32:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:32:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:32:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2242MB allocated=2196MB
2025-10-10 14:32:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:32:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:32:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:32:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:32:53 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.797348, avg_loss=0.694934, seen=40, correct=19, accuracy=0.475000
2025-10-10 14:32:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:32:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:32:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:32:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2242MB allocated=2196MB
2025-10-10 14:32:54 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.500000, curr=0.475000
2025-10-10 14:33:05 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 14:33:05 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 14:33:05 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 14:33:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 14:33:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:33:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:33:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 14:33:08 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=139.681931, avg_loss=0.698410, seen=200, correct=108, accuracy=0.540000
2025-10-10 14:33:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:33:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:33:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:33:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2242MB allocated=2196MB
2025-10-10 14:33:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:33:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:33:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:33:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:33:14 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.579508, avg_loss=0.714488, seen=40, correct=18, accuracy=0.450000
2025-10-10 14:33:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:33:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:33:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:33:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2242MB allocated=2196MB
2025-10-10 14:33:16 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.500000, curr=0.450000
2025-10-10 14:33:22 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 14:33:22 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 14:33:22 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 14:33:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 14:33:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:33:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:33:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 14:33:25 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=138.899445, avg_loss=0.694497, seen=200, correct=106, accuracy=0.530000
2025-10-10 14:33:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:33:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:33:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:33:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2242MB allocated=2196MB
2025-10-10 14:33:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:33:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:33:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:33:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:33:31 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.466766, avg_loss=0.711669, seen=40, correct=20, accuracy=0.500000
2025-10-10 14:33:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:33:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:33:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:33:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2242MB allocated=2196MB
2025-10-10 14:33:32 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.500000
2025-10-10 14:33:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 14:33:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 14:33:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:33:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:33:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2242MB allocated=2196MB
2025-10-10 14:33:33 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #37', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 14:33:33 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #37', 'Round': 0, 'Results_raw': {}}
2025-10-10 14:33:33 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 14:33:34 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 14:33:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:33:35 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 14:33:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:33:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:33:38 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 14:33:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-10 14:33:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:33:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:33:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-10 14:33:38 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=9.712322, avg_loss=0.882938, seen=11, correct=4, accuracy=0.363636
2025-10-10 14:33:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:33:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:33:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:33:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2218MB allocated=2187MB
2025-10-10 14:33:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:33:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:33:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:33:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:33:43 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.503727, avg_loss=0.662593, seen=40, correct=24, accuracy=0.600000
2025-10-10 14:33:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:33:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:33:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:33:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2218MB allocated=2187MB
2025-10-10 14:33:44 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-10 14:33:45 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 14:33:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=56, total=224)
2025-10-10 14:33:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:33:45 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 14:33:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:33:45 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 14:33:45 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=28, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 14:33:52 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 14:33:52 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 14:33:52 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 14:33:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-10 14:33:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:33:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:33:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-10 14:33:53 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=9.003428, avg_loss=0.818493, seen=11, correct=5, accuracy=0.454545
2025-10-10 14:33:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:33:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:33:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:33:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2254MB allocated=2204MB
2025-10-10 14:33:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:33:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:33:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:33:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:33:57 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.273659, avg_loss=0.656841, seen=40, correct=25, accuracy=0.625000
2025-10-10 14:33:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:33:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:33:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:33:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2254MB allocated=2204MB
2025-10-10 14:33:59 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-10 14:34:07 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 14:34:07 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 14:34:07 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 14:34:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-10 14:34:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:34:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:34:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-10 14:34:08 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=9.413877, avg_loss=0.855807, seen=11, correct=4, accuracy=0.363636
2025-10-10 14:34:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:34:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:34:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:34:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2254MB allocated=2204MB
2025-10-10 14:34:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:34:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:34:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:34:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:34:11 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.837463, avg_loss=0.645937, seen=40, correct=27, accuracy=0.675000
2025-10-10 14:34:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:34:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:34:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:34:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2254MB allocated=2204MB
2025-10-10 14:34:12 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.675000
2025-10-10 14:34:21 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 14:34:21 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 14:34:21 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 14:34:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-10 14:34:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:34:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:34:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-10 14:34:22 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=8.637739, avg_loss=0.785249, seen=11, correct=6, accuracy=0.545455
2025-10-10 14:34:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:34:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:34:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:34:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2254MB allocated=2204MB
2025-10-10 14:34:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:34:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:34:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:34:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:34:26 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.470558, avg_loss=0.661764, seen=40, correct=25, accuracy=0.625000
2025-10-10 14:34:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:34:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:34:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:34:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2254MB allocated=2204MB
2025-10-10 14:34:27 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.675000, curr=0.625000
2025-10-10 14:34:34 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 14:34:34 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 14:34:34 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 14:34:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-10 14:34:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:34:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:34:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-10 14:34:35 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=8.763128, avg_loss=0.796648, seen=11, correct=6, accuracy=0.545455
2025-10-10 14:34:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:34:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:34:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:34:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2254MB allocated=2204MB
2025-10-10 14:34:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:34:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:34:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:34:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:34:39 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.054405, avg_loss=0.651360, seen=40, correct=25, accuracy=0.625000
2025-10-10 14:34:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:34:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:34:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:34:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2254MB allocated=2204MB
2025-10-10 14:34:40 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.675000, curr=0.625000
2025-10-10 14:34:49 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 14:34:49 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 14:34:49 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 14:34:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-10 14:34:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:34:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:34:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-10 14:34:50 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=8.878122, avg_loss=0.807102, seen=11, correct=6, accuracy=0.545455
2025-10-10 14:34:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:34:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:34:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:34:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2254MB allocated=2204MB
2025-10-10 14:34:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:34:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:34:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:34:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:34:54 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.002245, avg_loss=0.650056, seen=40, correct=25, accuracy=0.625000
2025-10-10 14:34:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:34:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:34:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:34:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2254MB allocated=2204MB
2025-10-10 14:34:55 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.675000, curr=0.625000
2025-10-10 14:35:04 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 14:35:04 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 14:35:04 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 14:35:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-10 14:35:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:35:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:35:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-10 14:35:04 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=9.346041, avg_loss=0.849640, seen=11, correct=6, accuracy=0.545455
2025-10-10 14:35:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:35:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:35:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:35:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2254MB allocated=2204MB
2025-10-10 14:35:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:35:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:35:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:35:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:35:08 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.734325, avg_loss=0.643358, seen=40, correct=26, accuracy=0.650000
2025-10-10 14:35:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:35:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:35:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:35:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2254MB allocated=2204MB
2025-10-10 14:35:10 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/25), best=0.675000, curr=0.650000
2025-10-10 14:35:19 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 14:35:19 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 14:35:19 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 14:35:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-10 14:35:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:35:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:35:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-10 14:35:20 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=9.103990, avg_loss=0.827635, seen=11, correct=6, accuracy=0.545455
2025-10-10 14:35:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:35:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:35:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:35:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2254MB allocated=2204MB
2025-10-10 14:35:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:35:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:35:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:35:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:35:24 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.164484, avg_loss=0.654112, seen=40, correct=27, accuracy=0.675000
2025-10-10 14:35:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:35:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:35:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:35:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2254MB allocated=2204MB
2025-10-10 14:35:26 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.675000
2025-10-10 14:35:34 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 14:35:34 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 14:35:34 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 14:35:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-10 14:35:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:35:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:35:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-10 14:35:35 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=9.469859, avg_loss=0.860896, seen=11, correct=6, accuracy=0.545455
2025-10-10 14:35:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:35:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:35:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:35:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2254MB allocated=2204MB
2025-10-10 14:35:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:35:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:35:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:35:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:35:39 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.394842, avg_loss=0.659871, seen=40, correct=24, accuracy=0.600000
2025-10-10 14:35:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:35:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:35:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:35:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2254MB allocated=2204MB
2025-10-10 14:35:40 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.675000, curr=0.600000
2025-10-10 14:35:48 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 14:35:48 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 14:35:48 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 14:35:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-10 14:35:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:35:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:35:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-10 14:35:48 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=9.949095, avg_loss=0.904463, seen=11, correct=6, accuracy=0.545455
2025-10-10 14:35:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:35:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:35:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:35:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2254MB allocated=2204MB
2025-10-10 14:35:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:35:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:35:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:35:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:35:52 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.663910, avg_loss=0.666598, seen=40, correct=25, accuracy=0.625000
2025-10-10 14:35:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:35:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:35:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:35:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2254MB allocated=2204MB
2025-10-10 14:35:55 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.675000, curr=0.625000
2025-10-10 14:36:03 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 14:36:03 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 14:36:03 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 14:36:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-10 14:36:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:36:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:36:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-10 14:36:03 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=9.637850, avg_loss=0.876168, seen=11, correct=6, accuracy=0.545455
2025-10-10 14:36:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:36:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:36:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:36:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2254MB allocated=2204MB
2025-10-10 14:36:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:36:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:36:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:36:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:36:08 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.814884, avg_loss=0.645372, seen=40, correct=25, accuracy=0.625000
2025-10-10 14:36:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:36:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:36:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:36:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2254MB allocated=2204MB
2025-10-10 14:36:10 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.675000, curr=0.625000
2025-10-10 14:36:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 14:36:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 14:36:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:36:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:36:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2254MB allocated=2204MB
2025-10-10 14:36:12 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #22', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 14:36:12 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #22', 'Round': 0, 'Results_raw': {}}
2025-10-10 14:36:12 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 14:36:13 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 14:36:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:36:13 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 14:36:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:36:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:36:15 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 14:36:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-10-10 14:36:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:36:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:36:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-10-10 14:36:17 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=126, loss_sum=90.107315, avg_loss=0.715137, seen=126, correct=72, accuracy=0.571429
2025-10-10 14:36:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:36:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:36:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:36:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2238MB allocated=2196MB
2025-10-10 14:36:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:36:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:36:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:36:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:36:22 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.433937, avg_loss=0.610848, seen=40, correct=26, accuracy=0.650000
2025-10-10 14:36:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:36:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:36:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:36:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2238MB allocated=2196MB
2025-10-10 14:36:25 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.650000
2025-10-10 14:36:25 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 14:36:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=600, total=2399)
2025-10-10 14:36:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:36:25 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 14:36:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:36:25 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 14:36:25 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=300, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 14:36:34 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 14:36:34 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 14:36:34 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 14:36:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-10-10 14:36:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:36:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:36:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-10-10 14:36:36 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=126, loss_sum=93.968002, avg_loss=0.745778, seen=126, correct=74, accuracy=0.587302
2025-10-10 14:36:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:36:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:36:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:36:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2278MB allocated=2212MB
2025-10-10 14:36:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:36:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:36:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:36:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:36:39 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.443480, avg_loss=0.586087, seen=40, correct=26, accuracy=0.650000
2025-10-10 14:36:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:36:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:36:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:36:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2278MB allocated=2212MB
2025-10-10 14:36:41 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.650000
2025-10-10 14:36:49 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 14:36:49 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 14:36:49 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 14:36:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-10-10 14:36:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:36:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:36:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-10-10 14:36:51 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=126, loss_sum=93.923538, avg_loss=0.745425, seen=126, correct=77, accuracy=0.611111
2025-10-10 14:36:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:36:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:36:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:36:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2278MB allocated=2212MB
2025-10-10 14:36:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:36:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:36:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:36:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:36:54 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.183247, avg_loss=0.579581, seen=40, correct=26, accuracy=0.650000
2025-10-10 14:36:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:36:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:36:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:36:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2278MB allocated=2212MB
2025-10-10 14:36:56 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.650000
2025-10-10 14:37:06 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 14:37:06 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 14:37:06 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 14:37:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-10-10 14:37:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:37:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:37:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-10-10 14:37:08 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=126, loss_sum=92.487228, avg_loss=0.734026, seen=126, correct=78, accuracy=0.619048
2025-10-10 14:37:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:37:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:37:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:37:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2278MB allocated=2212MB
2025-10-10 14:37:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:37:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:37:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:37:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:37:13 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.536764, avg_loss=0.588419, seen=40, correct=27, accuracy=0.675000
2025-10-10 14:37:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:37:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:37:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:37:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2278MB allocated=2212MB
2025-10-10 14:37:14 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.675000
2025-10-10 14:37:23 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 14:37:23 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 14:37:23 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 14:37:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-10-10 14:37:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:37:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:37:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-10-10 14:37:25 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=126, loss_sum=91.924683, avg_loss=0.729561, seen=126, correct=73, accuracy=0.579365
2025-10-10 14:37:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:37:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:37:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:37:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2278MB allocated=2212MB
2025-10-10 14:37:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:37:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:37:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:37:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:37:31 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.951691, avg_loss=0.598792, seen=40, correct=27, accuracy=0.675000
2025-10-10 14:37:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:37:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:37:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:37:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2278MB allocated=2212MB
2025-10-10 14:37:32 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.675000
2025-10-10 14:37:42 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 14:37:42 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 14:37:42 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 14:37:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-10-10 14:37:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:37:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:37:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-10-10 14:37:44 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=126, loss_sum=91.588379, avg_loss=0.726892, seen=126, correct=69, accuracy=0.547619
2025-10-10 14:37:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:37:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:37:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:37:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2278MB allocated=2212MB
2025-10-10 14:37:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:37:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:37:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:37:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:37:50 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.094627, avg_loss=0.627366, seen=40, correct=27, accuracy=0.675000
2025-10-10 14:37:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:37:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:37:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:37:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2278MB allocated=2212MB
2025-10-10 14:37:51 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.675000
2025-10-10 14:38:02 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 14:38:02 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 14:38:02 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 14:38:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-10-10 14:38:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:38:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:38:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-10-10 14:38:04 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=126, loss_sum=89.538734, avg_loss=0.710625, seen=126, correct=71, accuracy=0.563492
2025-10-10 14:38:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:38:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:38:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:38:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2278MB allocated=2212MB
2025-10-10 14:38:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:38:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:38:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:38:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:38:09 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.470814, avg_loss=0.586770, seen=40, correct=27, accuracy=0.675000
2025-10-10 14:38:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:38:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:38:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:38:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2278MB allocated=2212MB
2025-10-10 14:38:11 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.675000
2025-10-10 14:38:21 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 14:38:21 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 14:38:21 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 14:38:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-10-10 14:38:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:38:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:38:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-10-10 14:38:23 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=126, loss_sum=88.843483, avg_loss=0.705107, seen=126, correct=75, accuracy=0.595238
2025-10-10 14:38:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:38:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:38:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:38:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2278MB allocated=2212MB
2025-10-10 14:38:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:38:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:38:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:38:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:38:29 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.527742, avg_loss=0.588194, seen=40, correct=27, accuracy=0.675000
2025-10-10 14:38:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:38:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:38:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:38:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2278MB allocated=2212MB
2025-10-10 14:38:31 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.675000
2025-10-10 14:38:39 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 14:38:39 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 14:38:39 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 14:38:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-10-10 14:38:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:38:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:38:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-10-10 14:38:41 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=126, loss_sum=88.810753, avg_loss=0.704847, seen=126, correct=71, accuracy=0.563492
2025-10-10 14:38:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:38:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:38:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:38:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2278MB allocated=2212MB
2025-10-10 14:38:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:38:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:38:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:38:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:38:46 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.464458, avg_loss=0.586611, seen=40, correct=28, accuracy=0.700000
2025-10-10 14:38:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:38:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:38:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:38:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2278MB allocated=2212MB
2025-10-10 14:38:48 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.700000
2025-10-10 14:38:59 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 14:38:59 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 14:38:59 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 14:38:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-10-10 14:38:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:38:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:39:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-10-10 14:39:01 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=126, loss_sum=89.398560, avg_loss=0.709512, seen=126, correct=71, accuracy=0.563492
2025-10-10 14:39:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:39:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:39:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:39:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2278MB allocated=2212MB
2025-10-10 14:39:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:39:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:39:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:39:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:39:05 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.142719, avg_loss=0.603568, seen=40, correct=26, accuracy=0.650000
2025-10-10 14:39:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:39:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:39:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:39:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2278MB allocated=2212MB
2025-10-10 14:39:06 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.700000, curr=0.650000
2025-10-10 14:39:15 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 14:39:15 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 14:39:15 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 14:39:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-10-10 14:39:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:39:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:39:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-10-10 14:39:17 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=126, loss_sum=88.845581, avg_loss=0.705124, seen=126, correct=78, accuracy=0.619048
2025-10-10 14:39:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:39:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:39:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:39:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2278MB allocated=2212MB
2025-10-10 14:39:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:39:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:39:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:39:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:39:21 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.339737, avg_loss=0.583493, seen=40, correct=27, accuracy=0.675000
2025-10-10 14:39:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:39:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:39:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:39:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2278MB allocated=2212MB
2025-10-10 14:39:24 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.700000, curr=0.675000
2025-10-10 14:39:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 14:39:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 14:39:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:39:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:39:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2278MB allocated=2212MB
2025-10-10 14:39:25 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #20', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 14:39:25 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #20', 'Round': 0, 'Results_raw': {}}
2025-10-10 14:39:25 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 14:39:26 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 14:39:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:39:26 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 14:39:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:39:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:39:28 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 14:39:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-10-10 14:39:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:39:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:39:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-10-10 14:39:30 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=63, loss_sum=42.496849, avg_loss=0.674553, seen=63, correct=39, accuracy=0.619048
2025-10-10 14:39:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:39:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:39:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:39:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-10-10 14:39:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:39:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:39:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:39:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:39:33 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.834694, avg_loss=0.595867, seen=40, correct=25, accuracy=0.625000
2025-10-10 14:39:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:39:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:39:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:39:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-10-10 14:39:35 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-10 14:39:35 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 14:39:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=303, total=1209)
2025-10-10 14:39:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:39:35 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 14:39:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:39:35 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 14:39:35 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=152, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 14:39:45 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 14:39:45 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 14:39:45 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 14:39:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-10-10 14:39:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:39:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:39:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-10-10 14:39:46 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=63, loss_sum=42.282162, avg_loss=0.671145, seen=63, correct=41, accuracy=0.650794
2025-10-10 14:39:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:39:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:39:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:39:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2276MB allocated=2221MB
2025-10-10 14:39:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:39:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:39:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:39:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:39:51 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.833298, avg_loss=0.595832, seen=40, correct=26, accuracy=0.650000
2025-10-10 14:39:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:39:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:39:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:39:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2276MB allocated=2221MB
2025-10-10 14:39:52 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.650000
2025-10-10 14:40:02 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 14:40:02 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 14:40:02 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 14:40:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-10-10 14:40:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:40:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:40:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-10-10 14:40:03 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=63, loss_sum=40.383064, avg_loss=0.641001, seen=63, correct=39, accuracy=0.619048
2025-10-10 14:40:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:40:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:40:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:40:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2276MB allocated=2221MB
2025-10-10 14:40:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:40:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:40:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:40:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:40:08 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=23.128723, avg_loss=0.578218, seen=40, correct=30, accuracy=0.750000
2025-10-10 14:40:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:40:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:40:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:40:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2276MB allocated=2221MB
2025-10-10 14:40:10 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.750000
2025-10-10 14:40:18 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 14:40:18 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 14:40:18 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 14:40:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-10-10 14:40:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:40:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:40:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-10-10 14:40:19 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=63, loss_sum=42.322269, avg_loss=0.671782, seen=63, correct=40, accuracy=0.634921
2025-10-10 14:40:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:40:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:40:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:40:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2276MB allocated=2221MB
2025-10-10 14:40:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:40:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:40:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:40:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:40:24 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=22.830988, avg_loss=0.570775, seen=40, correct=26, accuracy=0.650000
2025-10-10 14:40:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:40:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:40:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:40:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2276MB allocated=2221MB
2025-10-10 14:40:26 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.750000, curr=0.650000
2025-10-10 14:40:37 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 14:40:37 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 14:40:37 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 14:40:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-10-10 14:40:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:40:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:40:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-10-10 14:40:38 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=63, loss_sum=41.578602, avg_loss=0.659978, seen=63, correct=39, accuracy=0.619048
2025-10-10 14:40:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:40:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:40:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:40:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2276MB allocated=2221MB
2025-10-10 14:40:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:40:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:40:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:40:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:40:42 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=22.685734, avg_loss=0.567143, seen=40, correct=25, accuracy=0.625000
2025-10-10 14:40:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:40:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:40:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:40:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2276MB allocated=2221MB
2025-10-10 14:40:44 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.750000, curr=0.625000
2025-10-10 14:40:52 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 14:40:52 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 14:40:52 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 14:40:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-10-10 14:40:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:40:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:40:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-10-10 14:40:54 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=63, loss_sum=43.327183, avg_loss=0.687733, seen=63, correct=40, accuracy=0.634921
2025-10-10 14:40:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:40:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:40:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:40:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2276MB allocated=2221MB
2025-10-10 14:40:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:40:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:40:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:40:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:40:59 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=22.805569, avg_loss=0.570139, seen=40, correct=26, accuracy=0.650000
2025-10-10 14:40:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:40:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:41:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:41:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2276MB allocated=2221MB
2025-10-10 14:41:01 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.750000, curr=0.650000
2025-10-10 14:41:10 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 14:41:10 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 14:41:10 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 14:41:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-10-10 14:41:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:41:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:41:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-10-10 14:41:11 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=63, loss_sum=41.228378, avg_loss=0.654419, seen=63, correct=43, accuracy=0.682540
2025-10-10 14:41:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:41:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:41:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:41:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2276MB allocated=2221MB
2025-10-10 14:41:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:41:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:41:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:41:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:41:17 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=22.172293, avg_loss=0.554307, seen=40, correct=28, accuracy=0.700000
2025-10-10 14:41:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:41:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:41:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:41:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2276MB allocated=2221MB
2025-10-10 14:41:19 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/25), best=0.750000, curr=0.700000
2025-10-10 14:41:26 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 14:41:26 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 14:41:26 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 14:41:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-10-10 14:41:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:41:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:41:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-10-10 14:41:27 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=63, loss_sum=40.017834, avg_loss=0.635204, seen=63, correct=42, accuracy=0.666667
2025-10-10 14:41:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:41:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:41:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:41:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2276MB allocated=2221MB
2025-10-10 14:41:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:41:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:41:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:41:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:41:33 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=21.594290, avg_loss=0.539857, seen=40, correct=29, accuracy=0.725000
2025-10-10 14:41:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:41:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:41:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:41:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2276MB allocated=2221MB
2025-10-10 14:41:34 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/25), best=0.750000, curr=0.725000
2025-10-10 14:41:45 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 14:41:45 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 14:41:45 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 14:41:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-10-10 14:41:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:41:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:41:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-10-10 14:41:47 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=63, loss_sum=41.024876, avg_loss=0.651189, seen=63, correct=41, accuracy=0.650794
2025-10-10 14:41:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:41:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:41:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:41:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2276MB allocated=2221MB
2025-10-10 14:41:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:41:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:41:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:41:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:41:51 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=21.683790, avg_loss=0.542095, seen=40, correct=27, accuracy=0.675000
2025-10-10 14:41:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:41:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:41:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:41:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2276MB allocated=2221MB
2025-10-10 14:41:53 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/25), best=0.750000, curr=0.675000
2025-10-10 14:42:03 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 14:42:03 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 14:42:03 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 14:42:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-10-10 14:42:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:42:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:42:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-10-10 14:42:04 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=63, loss_sum=42.043827, avg_loss=0.667362, seen=63, correct=41, accuracy=0.650794
2025-10-10 14:42:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:42:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:42:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:42:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2276MB allocated=2221MB
2025-10-10 14:42:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:42:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:42:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:42:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:42:07 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=22.146288, avg_loss=0.553657, seen=40, correct=26, accuracy=0.650000
2025-10-10 14:42:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:42:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:42:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:42:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2276MB allocated=2221MB
2025-10-10 14:42:08 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/25), best=0.750000, curr=0.650000
2025-10-10 14:42:16 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 14:42:16 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 14:42:16 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 14:42:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-10-10 14:42:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:42:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:42:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-10-10 14:42:17 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=63, loss_sum=40.105392, avg_loss=0.636594, seen=63, correct=43, accuracy=0.682540
2025-10-10 14:42:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:42:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:42:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:42:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2276MB allocated=2221MB
2025-10-10 14:42:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:42:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:42:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:42:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:42:21 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=21.535606, avg_loss=0.538390, seen=40, correct=29, accuracy=0.725000
2025-10-10 14:42:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:42:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:42:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:42:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2276MB allocated=2221MB
2025-10-10 14:42:22 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/25), best=0.750000, curr=0.725000
2025-10-10 14:42:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 14:42:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 14:42:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:42:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:42:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2276MB allocated=2221MB
2025-10-10 14:42:24 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #10', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 14:42:24 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #10', 'Round': 0, 'Results_raw': {}}
2025-10-10 14:42:24 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 14:42:25 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 14:42:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:42:26 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 14:42:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:42:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:42:28 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 14:42:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 14:42:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:42:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:42:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 14:42:31 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=127.836243, avg_loss=0.639181, seen=200, correct=118, accuracy=0.590000
2025-10-10 14:42:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:42:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:42:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:42:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2238MB allocated=2212MB
2025-10-10 14:42:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:42:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:42:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:42:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:42:36 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.013260, avg_loss=0.700331, seen=40, correct=26, accuracy=0.650000
2025-10-10 14:42:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:42:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:42:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:42:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2238MB allocated=2212MB
2025-10-10 14:42:37 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.650000
2025-10-10 14:42:37 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 14:42:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1002, total=4005)
2025-10-10 14:42:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:42:37 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 14:42:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:42:37 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 14:42:37 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=501, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 14:42:47 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 14:42:47 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 14:42:47 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 14:42:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 14:42:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:42:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:42:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 14:42:50 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=129.901459, avg_loss=0.649507, seen=200, correct=110, accuracy=0.550000
2025-10-10 14:42:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:42:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:42:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:42:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2258MB allocated=2229MB
2025-10-10 14:42:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:42:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:42:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:42:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:42:54 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.228868, avg_loss=0.730722, seen=40, correct=25, accuracy=0.625000
2025-10-10 14:42:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:42:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:42:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:42:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2258MB allocated=2229MB
2025-10-10 14:42:55 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.650000, curr=0.625000
2025-10-10 14:43:05 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 14:43:05 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 14:43:05 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 14:43:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 14:43:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:43:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:43:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 14:43:09 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=126.623100, avg_loss=0.633116, seen=200, correct=132, accuracy=0.660000
2025-10-10 14:43:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:43:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:43:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:43:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2258MB allocated=2229MB
2025-10-10 14:43:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:43:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:43:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:43:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:43:13 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.799160, avg_loss=0.744979, seen=40, correct=23, accuracy=0.575000
2025-10-10 14:43:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:43:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:43:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:43:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2258MB allocated=2229MB
2025-10-10 14:43:16 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.650000, curr=0.575000
2025-10-10 14:43:26 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 14:43:26 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 14:43:26 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 14:43:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 14:43:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:43:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:43:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 14:43:29 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=129.013596, avg_loss=0.645068, seen=200, correct=116, accuracy=0.580000
2025-10-10 14:43:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:43:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:43:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:43:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2258MB allocated=2229MB
2025-10-10 14:43:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:43:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:43:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:43:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:43:34 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.741077, avg_loss=0.718527, seen=40, correct=27, accuracy=0.675000
2025-10-10 14:43:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:43:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:43:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:43:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2258MB allocated=2229MB
2025-10-10 14:43:35 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.675000
2025-10-10 14:43:44 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 14:43:44 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 14:43:44 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 14:43:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 14:43:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:43:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:43:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 14:43:47 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=131.944748, avg_loss=0.659724, seen=200, correct=114, accuracy=0.570000
2025-10-10 14:43:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:43:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:43:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:43:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2258MB allocated=2229MB
2025-10-10 14:43:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:43:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:43:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:43:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:43:51 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.729782, avg_loss=0.718245, seen=40, correct=23, accuracy=0.575000
2025-10-10 14:43:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:43:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:43:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:43:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2258MB allocated=2229MB
2025-10-10 14:43:54 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.675000, curr=0.575000
2025-10-10 14:44:02 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 14:44:02 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 14:44:02 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 14:44:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 14:44:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:44:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:44:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 14:44:05 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=125.486801, avg_loss=0.627434, seen=200, correct=132, accuracy=0.660000
2025-10-10 14:44:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:44:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:44:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:44:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2258MB allocated=2229MB
2025-10-10 14:44:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:44:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:44:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:44:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:44:08 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.455803, avg_loss=0.711395, seen=40, correct=25, accuracy=0.625000
2025-10-10 14:44:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:44:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:44:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:44:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2258MB allocated=2229MB
2025-10-10 14:44:11 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.675000, curr=0.625000
2025-10-10 14:44:19 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 14:44:19 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 14:44:19 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 14:44:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 14:44:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:44:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:44:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 14:44:22 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=125.231895, avg_loss=0.626159, seen=200, correct=129, accuracy=0.645000
2025-10-10 14:44:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:44:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:44:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:44:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2258MB allocated=2229MB
2025-10-10 14:44:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:44:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:44:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:44:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:44:28 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.580368, avg_loss=0.689509, seen=40, correct=25, accuracy=0.625000
2025-10-10 14:44:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:44:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:44:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:44:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2258MB allocated=2229MB
2025-10-10 14:44:29 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.675000, curr=0.625000
2025-10-10 14:44:40 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 14:44:40 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 14:44:40 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 14:44:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 14:44:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:44:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:44:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 14:44:43 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=127.403046, avg_loss=0.637015, seen=200, correct=119, accuracy=0.595000
2025-10-10 14:44:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:44:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:44:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:44:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2258MB allocated=2229MB
2025-10-10 14:44:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:44:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:44:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:44:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:44:48 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.797428, avg_loss=0.694936, seen=40, correct=27, accuracy=0.675000
2025-10-10 14:44:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:44:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:44:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:44:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2258MB allocated=2229MB
2025-10-10 14:44:50 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.675000
2025-10-10 14:44:59 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 14:44:59 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 14:44:59 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 14:44:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 14:44:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:44:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:45:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 14:45:02 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=126.960907, avg_loss=0.634805, seen=200, correct=121, accuracy=0.605000
2025-10-10 14:45:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:45:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:45:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:45:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2258MB allocated=2229MB
2025-10-10 14:45:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:45:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:45:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:45:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:45:06 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.766315, avg_loss=0.694158, seen=40, correct=26, accuracy=0.650000
2025-10-10 14:45:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:45:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:45:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:45:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2258MB allocated=2229MB
2025-10-10 14:45:07 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.675000, curr=0.650000
2025-10-10 14:45:15 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 14:45:15 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 14:45:15 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 14:45:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 14:45:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:45:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:45:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 14:45:18 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=127.356705, avg_loss=0.636784, seen=200, correct=120, accuracy=0.600000
2025-10-10 14:45:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:45:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:45:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:45:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2258MB allocated=2229MB
2025-10-10 14:45:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:45:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:45:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:45:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:45:24 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.270077, avg_loss=0.681752, seen=40, correct=26, accuracy=0.650000
2025-10-10 14:45:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:45:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:45:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:45:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2258MB allocated=2229MB
2025-10-10 14:45:25 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.675000, curr=0.650000
2025-10-10 14:45:34 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 14:45:34 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 14:45:34 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 14:45:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-10 14:45:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:45:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:45:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-10 14:45:37 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=131.524414, avg_loss=0.657622, seen=200, correct=116, accuracy=0.580000
2025-10-10 14:45:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:45:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:45:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:45:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2258MB allocated=2229MB
2025-10-10 14:45:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:45:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:45:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:45:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:45:42 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.448406, avg_loss=0.686210, seen=40, correct=21, accuracy=0.525000
2025-10-10 14:45:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:45:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:45:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:45:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2258MB allocated=2229MB
2025-10-10 14:45:45 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.675000, curr=0.525000
2025-10-10 14:45:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 14:45:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 14:45:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:45:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:45:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2258MB allocated=2229MB
2025-10-10 14:45:46 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #40', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 14:45:46 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #40', 'Round': 0, 'Results_raw': {}}
2025-10-10 14:45:46 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 14:45:47 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 14:45:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:45:47 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 14:45:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:45:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:45:50 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 14:45:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-10-10 14:45:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:45:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:45:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 14:45:52 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=133, loss_sum=90.480827, avg_loss=0.680307, seen=133, correct=75, accuracy=0.563910
2025-10-10 14:45:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:45:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:45:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:45:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2258MB allocated=2221MB
2025-10-10 14:45:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:45:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:45:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:45:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:45:56 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.186470, avg_loss=0.729662, seen=40, correct=22, accuracy=0.550000
2025-10-10 14:45:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:45:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:45:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:45:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2258MB allocated=2221MB
2025-10-10 14:45:59 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.550000
2025-10-10 14:45:59 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 14:45:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=632, total=2527)
2025-10-10 14:45:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:45:59 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 14:45:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:45:59 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 14:45:59 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=316, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 14:46:09 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 14:46:09 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 14:46:09 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 14:46:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-10-10 14:46:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:46:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:46:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 14:46:11 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=133, loss_sum=90.440033, avg_loss=0.680000, seen=133, correct=78, accuracy=0.586466
2025-10-10 14:46:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:46:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:46:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:46:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2280MB allocated=2238MB
2025-10-10 14:46:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:46:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:46:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:46:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:46:15 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.582573, avg_loss=0.739564, seen=40, correct=20, accuracy=0.500000
2025-10-10 14:46:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:46:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:46:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:46:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2280MB allocated=2238MB
2025-10-10 14:46:16 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.550000, curr=0.500000
2025-10-10 14:46:27 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 14:46:27 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 14:46:27 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 14:46:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-10-10 14:46:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:46:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:46:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 14:46:29 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=133, loss_sum=91.533249, avg_loss=0.688220, seen=133, correct=78, accuracy=0.586466
2025-10-10 14:46:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:46:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:46:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:46:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2280MB allocated=2238MB
2025-10-10 14:46:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:46:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:46:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:46:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:46:34 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.099861, avg_loss=0.752497, seen=40, correct=21, accuracy=0.525000
2025-10-10 14:46:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:46:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:46:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:46:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2280MB allocated=2238MB
2025-10-10 14:46:35 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.550000, curr=0.525000
2025-10-10 14:46:48 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 14:46:48 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 14:46:48 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 14:46:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-10-10 14:46:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:46:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:46:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 14:46:50 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=133, loss_sum=91.813461, avg_loss=0.690327, seen=133, correct=74, accuracy=0.556391
2025-10-10 14:46:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:46:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:46:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:46:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2280MB allocated=2238MB
2025-10-10 14:46:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:46:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:46:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:46:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:46:55 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.230125, avg_loss=0.755753, seen=40, correct=20, accuracy=0.500000
2025-10-10 14:46:55 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:46:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:46:56 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:46:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2280MB allocated=2238MB
2025-10-10 14:46:56 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.550000, curr=0.500000
2025-10-10 14:47:06 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 14:47:06 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 14:47:06 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 14:47:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-10-10 14:47:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:47:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:47:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 14:47:08 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=133, loss_sum=89.540085, avg_loss=0.673234, seen=133, correct=77, accuracy=0.578947
2025-10-10 14:47:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:47:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:47:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:47:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2280MB allocated=2238MB
2025-10-10 14:47:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:47:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:47:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:47:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:47:12 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.770433, avg_loss=0.719261, seen=40, correct=21, accuracy=0.525000
2025-10-10 14:47:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:47:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:47:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:47:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2280MB allocated=2238MB
2025-10-10 14:47:13 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/25), best=0.550000, curr=0.525000
2025-10-10 14:47:22 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 14:47:22 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 14:47:22 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 14:47:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-10-10 14:47:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:47:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:47:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 14:47:24 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=133, loss_sum=90.275200, avg_loss=0.678761, seen=133, correct=82, accuracy=0.616541
2025-10-10 14:47:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:47:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:47:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:47:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2280MB allocated=2238MB
2025-10-10 14:47:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:47:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:47:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:47:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:47:28 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.744263, avg_loss=0.718607, seen=40, correct=20, accuracy=0.500000
2025-10-10 14:47:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:47:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:47:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:47:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2280MB allocated=2238MB
2025-10-10 14:47:30 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/25), best=0.550000, curr=0.500000
2025-10-10 14:47:37 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 14:47:37 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 14:47:37 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 14:47:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-10-10 14:47:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:47:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:47:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 14:47:39 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=133, loss_sum=89.539017, avg_loss=0.673226, seen=133, correct=81, accuracy=0.609023
2025-10-10 14:47:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:47:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:47:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:47:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2280MB allocated=2238MB
2025-10-10 14:47:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:47:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:47:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:47:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:47:43 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.994556, avg_loss=0.724864, seen=40, correct=21, accuracy=0.525000
2025-10-10 14:47:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:47:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:47:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:47:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2280MB allocated=2238MB
2025-10-10 14:47:46 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/25), best=0.550000, curr=0.525000
2025-10-10 14:47:55 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 14:47:55 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 14:47:55 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 14:47:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-10-10 14:47:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:47:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:47:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 14:47:58 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=133, loss_sum=89.599800, avg_loss=0.673683, seen=133, correct=80, accuracy=0.601504
2025-10-10 14:47:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:47:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:48:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:48:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2280MB allocated=2238MB
2025-10-10 14:48:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:48:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:48:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:48:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:48:03 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.599869, avg_loss=0.739997, seen=40, correct=20, accuracy=0.500000
2025-10-10 14:48:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:48:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:48:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:48:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2280MB allocated=2238MB
2025-10-10 14:48:05 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/25), best=0.550000, curr=0.500000
2025-10-10 14:48:15 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 14:48:15 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 14:48:15 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 14:48:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-10-10 14:48:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:48:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:48:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 14:48:17 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=133, loss_sum=89.910088, avg_loss=0.676016, seen=133, correct=80, accuracy=0.601504
2025-10-10 14:48:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:48:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:48:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:48:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2280MB allocated=2238MB
2025-10-10 14:48:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:48:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:48:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:48:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:48:23 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.262079, avg_loss=0.731552, seen=40, correct=18, accuracy=0.450000
2025-10-10 14:48:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:48:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:48:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:48:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2280MB allocated=2238MB
2025-10-10 14:48:25 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/25), best=0.550000, curr=0.450000
2025-10-10 14:48:34 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 14:48:34 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 14:48:34 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 14:48:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-10-10 14:48:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:48:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:48:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 14:48:37 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=133, loss_sum=88.568611, avg_loss=0.665929, seen=133, correct=83, accuracy=0.624060
2025-10-10 14:48:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:48:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:48:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:48:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2280MB allocated=2238MB
2025-10-10 14:48:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:48:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:48:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:48:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:48:41 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.470314, avg_loss=0.736758, seen=40, correct=18, accuracy=0.450000
2025-10-10 14:48:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:48:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:48:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:48:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2280MB allocated=2238MB
2025-10-10 14:48:43 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=9/25), best=0.550000, curr=0.450000
2025-10-10 14:48:51 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 14:48:51 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 14:48:51 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 14:48:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-10-10 14:48:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:48:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:48:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-10 14:48:53 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=133, loss_sum=89.318184, avg_loss=0.671565, seen=133, correct=81, accuracy=0.609023
2025-10-10 14:48:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:48:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:48:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:48:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2280MB allocated=2238MB
2025-10-10 14:48:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:48:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:48:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:48:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:48:56 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.215796, avg_loss=0.730395, seen=40, correct=18, accuracy=0.450000
2025-10-10 14:48:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:48:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:48:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:48:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2280MB allocated=2238MB
2025-10-10 14:48:57 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=10/25), best=0.550000, curr=0.450000
2025-10-10 14:48:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 14:48:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 14:48:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:48:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:48:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2280MB allocated=2238MB
2025-10-10 14:48:59 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #50', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 14:48:59 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #50', 'Round': 0, 'Results_raw': {}}
2025-10-10 14:48:59 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 14:49:00 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 14:49:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:49:00 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 14:49:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:49:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:49:02 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 14:49:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-10 14:49:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:49:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:49:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-10 14:49:02 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=9.457705, avg_loss=0.859791, seen=11, correct=8, accuracy=0.727273
2025-10-10 14:49:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:49:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:49:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:49:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2258MB allocated=2229MB
2025-10-10 14:49:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:49:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:49:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:49:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:49:07 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=64.497055, avg_loss=1.612426, seen=40, correct=24, accuracy=0.600000
2025-10-10 14:49:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:49:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:49:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:49:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2258MB allocated=2229MB
2025-10-10 14:49:09 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-10 14:49:09 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 14:49:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=54, total=213)
2025-10-10 14:49:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:49:09 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 14:49:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:49:09 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 14:49:09 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=27, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 14:49:18 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 14:49:18 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 14:49:18 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 14:49:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-10 14:49:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:49:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:49:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-10 14:49:19 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=12.508948, avg_loss=1.137177, seen=11, correct=6, accuracy=0.545455
2025-10-10 14:49:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:49:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:49:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:49:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2296MB allocated=2246MB
2025-10-10 14:49:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:49:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:49:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:49:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:49:22 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=67.107018, avg_loss=1.677675, seen=40, correct=22, accuracy=0.550000
2025-10-10 14:49:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:49:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:49:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:49:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2296MB allocated=2246MB
2025-10-10 14:49:23 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.600000, curr=0.550000
2025-10-10 14:49:33 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 14:49:33 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 14:49:33 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 14:49:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-10 14:49:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:49:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:49:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-10 14:49:33 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=13.493699, avg_loss=1.226700, seen=11, correct=7, accuracy=0.636364
2025-10-10 14:49:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:49:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:49:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:49:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2296MB allocated=2246MB
2025-10-10 14:49:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:49:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:49:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:49:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:49:37 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=63.633282, avg_loss=1.590832, seen=40, correct=23, accuracy=0.575000
2025-10-10 14:49:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:49:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:49:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:49:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2296MB allocated=2246MB
2025-10-10 14:49:39 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.600000, curr=0.575000
2025-10-10 14:49:50 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 14:49:50 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 14:49:50 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 14:49:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-10 14:49:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:49:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:49:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-10 14:49:50 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=10.719757, avg_loss=0.974523, seen=11, correct=7, accuracy=0.636364
2025-10-10 14:49:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:49:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:49:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:49:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2296MB allocated=2246MB
2025-10-10 14:49:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:49:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:49:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:49:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:49:54 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=75.935089, avg_loss=1.898377, seen=40, correct=21, accuracy=0.525000
2025-10-10 14:49:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:49:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:49:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:49:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2296MB allocated=2246MB
2025-10-10 14:49:56 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.600000, curr=0.525000
2025-10-10 14:50:05 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 14:50:05 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 14:50:05 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 14:50:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-10 14:50:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:50:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:50:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-10 14:50:05 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=14.131334, avg_loss=1.284667, seen=11, correct=7, accuracy=0.636364
2025-10-10 14:50:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:50:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:50:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:50:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2296MB allocated=2246MB
2025-10-10 14:50:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:50:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:50:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:50:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:50:10 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=68.336464, avg_loss=1.708412, seen=40, correct=24, accuracy=0.600000
2025-10-10 14:50:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:50:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:50:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:50:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2296MB allocated=2246MB
2025-10-10 14:50:12 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-10 14:50:21 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 14:50:21 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 14:50:21 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 14:50:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-10 14:50:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:50:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:50:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-10 14:50:21 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=12.965096, avg_loss=1.178645, seen=11, correct=8, accuracy=0.727273
2025-10-10 14:50:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:50:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:50:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:50:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2296MB allocated=2246MB
2025-10-10 14:50:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:50:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:50:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:50:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:50:25 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=68.543709, avg_loss=1.713593, seen=40, correct=21, accuracy=0.525000
2025-10-10 14:50:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:50:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:50:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:50:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2296MB allocated=2246MB
2025-10-10 14:50:27 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.600000, curr=0.525000
2025-10-10 14:50:36 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 14:50:36 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 14:50:36 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 14:50:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-10 14:50:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:50:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:50:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-10 14:50:37 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=11.269144, avg_loss=1.024468, seen=11, correct=7, accuracy=0.636364
2025-10-10 14:50:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:50:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:50:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:50:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2296MB allocated=2246MB
2025-10-10 14:50:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:50:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:50:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:50:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:50:42 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=74.219170, avg_loss=1.855479, seen=40, correct=20, accuracy=0.500000
2025-10-10 14:50:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:50:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:50:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:50:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2296MB allocated=2246MB
2025-10-10 14:50:44 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.600000, curr=0.500000
2025-10-10 14:50:54 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 14:50:54 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 14:50:54 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 14:50:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-10 14:50:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:50:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:50:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-10 14:50:55 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=11.474161, avg_loss=1.043106, seen=11, correct=7, accuracy=0.636364
2025-10-10 14:50:55 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:50:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:50:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:50:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2296MB allocated=2246MB
2025-10-10 14:50:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:50:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:50:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:50:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:50:59 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=74.134010, avg_loss=1.853350, seen=40, correct=23, accuracy=0.575000
2025-10-10 14:50:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:50:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:50:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:51:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2296MB allocated=2246MB
2025-10-10 14:51:00 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.600000, curr=0.575000
2025-10-10 14:51:10 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 14:51:10 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 14:51:10 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 14:51:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-10 14:51:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:51:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:51:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-10 14:51:11 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=12.702243, avg_loss=1.154749, seen=11, correct=7, accuracy=0.636364
2025-10-10 14:51:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:51:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:51:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:51:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2296MB allocated=2246MB
2025-10-10 14:51:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:51:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:51:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:51:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:51:16 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=79.474632, avg_loss=1.986866, seen=40, correct=22, accuracy=0.550000
2025-10-10 14:51:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:51:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:51:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:51:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2296MB allocated=2246MB
2025-10-10 14:51:17 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/25), best=0.600000, curr=0.550000
2025-10-10 14:51:27 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 14:51:27 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 14:51:27 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 14:51:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-10 14:51:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:51:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:51:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-10 14:51:28 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=9.234312, avg_loss=0.839483, seen=11, correct=7, accuracy=0.636364
2025-10-10 14:51:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:51:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:51:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:51:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2296MB allocated=2246MB
2025-10-10 14:51:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:51:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:51:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:51:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:51:31 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=89.300858, avg_loss=2.232521, seen=40, correct=18, accuracy=0.450000
2025-10-10 14:51:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:51:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:51:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:51:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2296MB allocated=2246MB
2025-10-10 14:51:33 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/25), best=0.600000, curr=0.450000
2025-10-10 14:51:42 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 14:51:42 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 14:51:42 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 14:51:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-10 14:51:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:51:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:51:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-10 14:51:43 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=25.878569, avg_loss=2.352597, seen=11, correct=7, accuracy=0.636364
2025-10-10 14:51:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:51:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:51:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:51:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2296MB allocated=2246MB
2025-10-10 14:51:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:51:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:51:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:51:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:51:48 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=70.752274, avg_loss=1.768807, seen=40, correct=23, accuracy=0.575000
2025-10-10 14:51:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:51:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:51:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:51:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2296MB allocated=2246MB
2025-10-10 14:51:49 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/25), best=0.600000, curr=0.575000
2025-10-10 14:51:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 14:51:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 14:51:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:51:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:51:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2296MB allocated=2246MB
2025-10-10 14:51:50 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #4', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 14:51:50 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #4', 'Round': 0, 'Results_raw': {}}
2025-10-10 14:51:50 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 14:51:51 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 14:51:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:51:52 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 14:51:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:51:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:51:55 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 14:51:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-10-10 14:51:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:51:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:51:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-10 14:51:57 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=146, loss_sum=96.135078, avg_loss=0.658459, seen=146, correct=90, accuracy=0.616438
2025-10-10 14:51:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:51:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:51:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:52:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2278MB allocated=2238MB
2025-10-10 14:52:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:52:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:52:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:52:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:52:02 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.852890, avg_loss=0.671322, seen=40, correct=25, accuracy=0.625000
2025-10-10 14:52:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:52:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:52:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:52:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2278MB allocated=2238MB
2025-10-10 14:52:04 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-10 14:52:04 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 14:52:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=699, total=2793)
2025-10-10 14:52:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:52:04 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 14:52:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:52:04 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 14:52:04 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=350, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 14:52:13 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 14:52:13 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 14:52:13 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 14:52:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-10-10 14:52:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:52:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:52:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-10 14:52:16 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=146, loss_sum=96.877533, avg_loss=0.663545, seen=146, correct=84, accuracy=0.575342
2025-10-10 14:52:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:52:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:52:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:52:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2298MB allocated=2254MB
2025-10-10 14:52:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:52:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:52:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:52:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:52:20 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.884624, avg_loss=0.672116, seen=40, correct=24, accuracy=0.600000
2025-10-10 14:52:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:52:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:52:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:52:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2298MB allocated=2254MB
2025-10-10 14:52:21 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.625000, curr=0.600000
2025-10-10 14:52:29 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 14:52:29 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 14:52:29 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 14:52:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-10-10 14:52:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:52:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:52:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-10 14:52:32 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=146, loss_sum=96.756126, avg_loss=0.662713, seen=146, correct=87, accuracy=0.595890
2025-10-10 14:52:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:52:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:52:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:52:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2298MB allocated=2254MB
2025-10-10 14:52:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:52:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:52:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:52:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:52:36 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.834482, avg_loss=0.670862, seen=40, correct=23, accuracy=0.575000
2025-10-10 14:52:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:52:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:52:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:52:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2298MB allocated=2254MB
2025-10-10 14:52:37 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.625000, curr=0.575000
2025-10-10 14:52:47 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 14:52:47 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 14:52:47 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 14:52:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-10-10 14:52:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:52:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:52:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-10 14:52:49 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=146, loss_sum=95.936417, avg_loss=0.657099, seen=146, correct=89, accuracy=0.609589
2025-10-10 14:52:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:52:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:52:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:52:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2298MB allocated=2254MB
2025-10-10 14:52:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:52:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:52:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:52:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:52:54 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.618538, avg_loss=0.665463, seen=40, correct=25, accuracy=0.625000
2025-10-10 14:52:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:52:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:52:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:52:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2298MB allocated=2254MB
2025-10-10 14:52:55 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-10 14:53:05 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 14:53:05 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 14:53:05 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 14:53:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-10-10 14:53:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:53:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:53:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-10 14:53:08 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=146, loss_sum=96.184464, avg_loss=0.658798, seen=146, correct=87, accuracy=0.595890
2025-10-10 14:53:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:53:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:53:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:53:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2298MB allocated=2254MB
2025-10-10 14:53:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:53:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:53:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:53:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:53:13 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.467941, avg_loss=0.661699, seen=40, correct=24, accuracy=0.600000
2025-10-10 14:53:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:53:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:53:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:53:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2298MB allocated=2254MB
2025-10-10 14:53:16 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.625000, curr=0.600000
2025-10-10 14:53:25 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 14:53:25 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 14:53:25 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 14:53:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-10-10 14:53:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:53:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:53:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-10 14:53:27 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=146, loss_sum=96.269363, avg_loss=0.659379, seen=146, correct=90, accuracy=0.616438
2025-10-10 14:53:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:53:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:53:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:53:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2298MB allocated=2254MB
2025-10-10 14:53:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:53:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:53:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:53:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:53:33 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.344770, avg_loss=0.658619, seen=40, correct=24, accuracy=0.600000
2025-10-10 14:53:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:53:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:53:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:53:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2298MB allocated=2254MB
2025-10-10 14:53:34 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.625000, curr=0.600000
2025-10-10 14:53:43 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 14:53:43 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 14:53:43 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 14:53:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-10-10 14:53:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:53:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:53:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-10 14:53:45 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=146, loss_sum=96.385963, avg_loss=0.660178, seen=146, correct=91, accuracy=0.623288
2025-10-10 14:53:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:53:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:53:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:53:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2298MB allocated=2254MB
2025-10-10 14:53:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:53:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:53:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:53:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:53:49 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.394855, avg_loss=0.659871, seen=40, correct=26, accuracy=0.650000
2025-10-10 14:53:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:53:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:53:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:53:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2298MB allocated=2254MB
2025-10-10 14:53:50 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.650000
2025-10-10 14:53:59 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 14:53:59 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 14:53:59 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 14:53:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-10-10 14:53:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:53:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:54:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-10 14:54:01 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=146, loss_sum=95.394913, avg_loss=0.653390, seen=146, correct=95, accuracy=0.650685
2025-10-10 14:54:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:54:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:54:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:54:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2298MB allocated=2254MB
2025-10-10 14:54:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:54:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:54:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:54:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:54:05 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.100456, avg_loss=0.652511, seen=40, correct=27, accuracy=0.675000
2025-10-10 14:54:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:54:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:54:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:54:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2298MB allocated=2254MB
2025-10-10 14:54:07 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.675000
2025-10-10 14:54:15 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 14:54:15 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 14:54:15 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 14:54:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-10-10 14:54:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:54:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:54:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-10 14:54:18 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=146, loss_sum=96.625107, avg_loss=0.661816, seen=146, correct=84, accuracy=0.575342
2025-10-10 14:54:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:54:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:54:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:54:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2298MB allocated=2254MB
2025-10-10 14:54:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:54:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:54:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:54:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:54:21 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.527649, avg_loss=0.663191, seen=40, correct=24, accuracy=0.600000
2025-10-10 14:54:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:54:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:54:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:54:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2298MB allocated=2254MB
2025-10-10 14:54:23 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.675000, curr=0.600000
2025-10-10 14:54:32 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 14:54:32 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 14:54:32 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 14:54:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-10-10 14:54:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:54:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:54:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-10 14:54:34 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=146, loss_sum=95.383659, avg_loss=0.653313, seen=146, correct=90, accuracy=0.616438
2025-10-10 14:54:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:54:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:54:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:54:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2298MB allocated=2254MB
2025-10-10 14:54:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:54:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:54:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:54:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:54:39 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.246964, avg_loss=0.656174, seen=40, correct=24, accuracy=0.600000
2025-10-10 14:54:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:54:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:54:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:54:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2298MB allocated=2254MB
2025-10-10 14:54:40 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.675000, curr=0.600000
2025-10-10 14:54:51 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 14:54:51 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 14:54:51 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 14:54:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-10-10 14:54:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:54:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:54:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-10 14:54:53 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=146, loss_sum=98.084053, avg_loss=0.671809, seen=146, correct=83, accuracy=0.568493
2025-10-10 14:54:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:54:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:54:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:54:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2298MB allocated=2254MB
2025-10-10 14:54:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:54:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:54:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:54:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:54:58 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.923267, avg_loss=0.673082, seen=40, correct=23, accuracy=0.575000
2025-10-10 14:54:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:54:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:54:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:55:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2298MB allocated=2254MB
2025-10-10 14:55:00 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.675000, curr=0.575000
2025-10-10 14:55:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 14:55:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 14:55:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:55:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:55:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2298MB allocated=2254MB
2025-10-10 14:55:01 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #1', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 14:55:01 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #1', 'Round': 0, 'Results_raw': {}}
2025-10-10 14:55:02 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 14:55:02 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 14:55:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:55:03 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 14:55:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:55:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:55:04 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 14:55:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-10-10 14:55:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:55:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:55:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-10-10 14:55:05 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=46, loss_sum=29.066753, avg_loss=0.631886, seen=46, correct=33, accuracy=0.717391
2025-10-10 14:55:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:55:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:55:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:55:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2278MB allocated=2246MB
2025-10-10 14:55:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:55:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:55:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:55:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:55:09 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.540087, avg_loss=0.713502, seen=40, correct=22, accuracy=0.550000
2025-10-10 14:55:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:55:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:55:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:55:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2278MB allocated=2246MB
2025-10-10 14:55:10 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.550000
2025-10-10 14:55:10 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 14:55:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=220, total=880)
2025-10-10 14:55:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:55:10 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 14:55:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:55:10 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 14:55:10 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=110, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 14:55:20 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 14:55:20 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 14:55:20 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 14:55:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-10-10 14:55:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:55:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:55:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-10-10 14:55:21 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=46, loss_sum=29.436546, avg_loss=0.639925, seen=46, correct=31, accuracy=0.673913
2025-10-10 14:55:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:55:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:55:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:55:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2308MB allocated=2263MB
2025-10-10 14:55:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:55:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:55:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:55:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:55:26 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.542088, avg_loss=0.713552, seen=40, correct=22, accuracy=0.550000
2025-10-10 14:55:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:55:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:55:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:55:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2308MB allocated=2263MB
2025-10-10 14:55:27 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.550000
2025-10-10 14:55:37 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 14:55:37 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 14:55:37 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 14:55:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-10-10 14:55:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:55:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:55:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-10-10 14:55:38 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=46, loss_sum=29.231474, avg_loss=0.635467, seen=46, correct=31, accuracy=0.673913
2025-10-10 14:55:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:55:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:55:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:55:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2308MB allocated=2263MB
2025-10-10 14:55:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:55:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:55:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:55:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:55:42 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.020332, avg_loss=0.725508, seen=40, correct=22, accuracy=0.550000
2025-10-10 14:55:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:55:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:55:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:55:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2308MB allocated=2263MB
2025-10-10 14:55:43 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.550000
2025-10-10 14:55:54 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 14:55:54 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 14:55:54 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 14:55:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-10-10 14:55:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:55:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:55:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-10-10 14:55:55 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=46, loss_sum=30.158319, avg_loss=0.655616, seen=46, correct=26, accuracy=0.565217
2025-10-10 14:55:55 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:55:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:55:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:55:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2308MB allocated=2263MB
2025-10-10 14:55:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:55:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:55:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:56:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:56:01 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.175859, avg_loss=0.704396, seen=40, correct=24, accuracy=0.600000
2025-10-10 14:56:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:56:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:56:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:56:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2308MB allocated=2263MB
2025-10-10 14:56:02 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-10 14:56:12 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 14:56:12 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 14:56:12 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 14:56:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-10-10 14:56:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:56:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:56:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-10-10 14:56:13 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=46, loss_sum=29.186813, avg_loss=0.634496, seen=46, correct=33, accuracy=0.717391
2025-10-10 14:56:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:56:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:56:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:56:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2308MB allocated=2263MB
2025-10-10 14:56:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:56:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:56:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:56:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:56:19 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.836884, avg_loss=0.720922, seen=40, correct=21, accuracy=0.525000
2025-10-10 14:56:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:56:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:56:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:56:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2308MB allocated=2263MB
2025-10-10 14:56:20 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.600000, curr=0.525000
2025-10-10 14:56:29 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 14:56:29 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 14:56:29 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 14:56:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-10-10 14:56:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:56:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:56:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-10-10 14:56:30 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=46, loss_sum=29.866508, avg_loss=0.649272, seen=46, correct=30, accuracy=0.652174
2025-10-10 14:56:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:56:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:56:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:56:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2308MB allocated=2263MB
2025-10-10 14:56:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:56:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:56:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:56:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:56:34 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.198292, avg_loss=0.754957, seen=40, correct=20, accuracy=0.500000
2025-10-10 14:56:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:56:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:56:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:56:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2308MB allocated=2263MB
2025-10-10 14:56:36 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.600000, curr=0.500000
2025-10-10 14:56:47 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 14:56:47 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 14:56:47 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 14:56:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-10-10 14:56:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:56:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:56:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-10-10 14:56:48 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=46, loss_sum=29.886803, avg_loss=0.649713, seen=46, correct=26, accuracy=0.565217
2025-10-10 14:56:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:56:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:56:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:56:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2308MB allocated=2263MB
2025-10-10 14:56:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:56:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:56:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:56:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:56:52 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.146809, avg_loss=0.678670, seen=40, correct=24, accuracy=0.600000
2025-10-10 14:56:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:56:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:56:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:56:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2308MB allocated=2263MB
2025-10-10 14:56:54 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-10 14:57:04 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 14:57:04 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 14:57:04 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 14:57:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-10-10 14:57:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:57:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:57:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-10-10 14:57:05 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=46, loss_sum=30.429260, avg_loss=0.661506, seen=46, correct=26, accuracy=0.565217
2025-10-10 14:57:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:57:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:57:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:57:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2308MB allocated=2263MB
2025-10-10 14:57:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:57:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:57:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:57:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:57:08 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.544901, avg_loss=0.688623, seen=40, correct=24, accuracy=0.600000
2025-10-10 14:57:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:57:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:57:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:57:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2308MB allocated=2263MB
2025-10-10 14:57:10 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-10 14:57:18 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 14:57:18 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 14:57:18 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 14:57:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-10-10 14:57:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:57:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:57:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-10-10 14:57:20 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=46, loss_sum=29.467075, avg_loss=0.640589, seen=46, correct=30, accuracy=0.652174
2025-10-10 14:57:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:57:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:57:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:57:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2308MB allocated=2263MB
2025-10-10 14:57:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:57:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:57:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:57:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:57:25 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.044857, avg_loss=0.701121, seen=40, correct=24, accuracy=0.600000
2025-10-10 14:57:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:57:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:57:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:57:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2308MB allocated=2263MB
2025-10-10 14:57:26 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-10 14:57:37 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 14:57:37 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 14:57:37 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 14:57:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-10-10 14:57:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:57:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:57:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-10-10 14:57:38 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=46, loss_sum=30.223776, avg_loss=0.657039, seen=46, correct=26, accuracy=0.565217
2025-10-10 14:57:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:57:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:57:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:57:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2308MB allocated=2263MB
2025-10-10 14:57:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:57:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:57:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:57:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:57:42 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.093071, avg_loss=0.702327, seen=40, correct=25, accuracy=0.625000
2025-10-10 14:57:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:57:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:57:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:57:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2308MB allocated=2263MB
2025-10-10 14:57:45 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-10 14:57:53 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 14:57:53 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 14:57:53 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 14:57:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-10-10 14:57:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:57:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:57:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-10-10 14:57:54 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=46, loss_sum=30.903044, avg_loss=0.671805, seen=46, correct=29, accuracy=0.630435
2025-10-10 14:57:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:57:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:57:56 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:57:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2308MB allocated=2263MB
2025-10-10 14:57:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:57:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:57:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:57:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:57:58 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.388271, avg_loss=0.759707, seen=40, correct=22, accuracy=0.550000
2025-10-10 14:57:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:57:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:58:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:58:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2308MB allocated=2263MB
2025-10-10 14:58:01 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.625000, curr=0.550000
2025-10-10 14:58:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 14:58:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 14:58:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:58:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:58:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2308MB allocated=2263MB
2025-10-10 14:58:02 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #48', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 14:58:02 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #48', 'Round': 0, 'Results_raw': {}}
2025-10-10 14:58:03 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 14:58:03 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-10 14:58:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:58:04 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-10 14:58:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:58:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:58:06 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-10 14:58:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-10-10 14:58:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:58:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:58:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-10-10 14:58:08 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=100, loss_sum=70.667015, avg_loss=0.706670, seen=100, correct=50, accuracy=0.500000
2025-10-10 14:58:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:58:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:58:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:58:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2298MB allocated=2254MB
2025-10-10 14:58:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:58:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:58:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:58:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:58:13 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.389885, avg_loss=0.759747, seen=40, correct=16, accuracy=0.400000
2025-10-10 14:58:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:58:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:58:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:58:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2298MB allocated=2254MB
2025-10-10 14:58:15 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.400000
2025-10-10 14:58:15 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-10 14:58:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=476, total=1901)
2025-10-10 14:58:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:58:16 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-10 14:58:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:58:16 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-10-10 14:58:16 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=238, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-10-10 14:58:26 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-10 14:58:26 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-10 14:58:26 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-10 14:58:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-10-10 14:58:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:58:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:58:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-10-10 14:58:27 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=100, loss_sum=76.479858, avg_loss=0.764799, seen=100, correct=43, accuracy=0.430000
2025-10-10 14:58:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:58:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:58:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:58:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2322MB allocated=2271MB
2025-10-10 14:58:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:58:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:58:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:58:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:58:32 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=31.054735, avg_loss=0.776368, seen=40, correct=19, accuracy=0.475000
2025-10-10 14:58:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:58:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:58:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:58:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2322MB allocated=2271MB
2025-10-10 14:58:34 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.475000
2025-10-10 14:58:46 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-10 14:58:46 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-10 14:58:46 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-10 14:58:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-10-10 14:58:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:58:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:58:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-10-10 14:58:48 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=100, loss_sum=74.075241, avg_loss=0.740752, seen=100, correct=49, accuracy=0.490000
2025-10-10 14:58:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:58:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:58:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:58:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2322MB allocated=2271MB
2025-10-10 14:58:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:58:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:58:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:58:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:58:53 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=31.016827, avg_loss=0.775421, seen=40, correct=17, accuracy=0.425000
2025-10-10 14:58:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:58:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:58:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:58:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2322MB allocated=2271MB
2025-10-10 14:58:55 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.475000, curr=0.425000
2025-10-10 14:59:06 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-10 14:59:06 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-10 14:59:06 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-10 14:59:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-10-10 14:59:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:59:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:59:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-10-10 14:59:07 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=100, loss_sum=73.792328, avg_loss=0.737923, seen=100, correct=46, accuracy=0.460000
2025-10-10 14:59:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:59:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:59:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:59:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2322MB allocated=2271MB
2025-10-10 14:59:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:59:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:59:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:59:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:59:12 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.653534, avg_loss=0.766338, seen=40, correct=17, accuracy=0.425000
2025-10-10 14:59:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:59:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:59:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:59:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2322MB allocated=2271MB
2025-10-10 14:59:13 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.475000, curr=0.425000
2025-10-10 14:59:21 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-10 14:59:21 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-10 14:59:21 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-10 14:59:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-10-10 14:59:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:59:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:59:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-10-10 14:59:23 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=100, loss_sum=68.807022, avg_loss=0.688070, seen=100, correct=52, accuracy=0.520000
2025-10-10 14:59:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:59:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:59:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:59:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2322MB allocated=2271MB
2025-10-10 14:59:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:59:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:59:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:59:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:59:27 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.671114, avg_loss=0.766778, seen=40, correct=17, accuracy=0.425000
2025-10-10 14:59:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:59:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:59:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:59:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2322MB allocated=2271MB
2025-10-10 14:59:28 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/25), best=0.475000, curr=0.425000
2025-10-10 14:59:38 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-10 14:59:38 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-10 14:59:38 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-10 14:59:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-10-10 14:59:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:59:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:59:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-10-10 14:59:40 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=100, loss_sum=68.511086, avg_loss=0.685111, seen=100, correct=55, accuracy=0.550000
2025-10-10 14:59:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:59:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:59:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:59:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2322MB allocated=2271MB
2025-10-10 14:59:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 14:59:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:59:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:59:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 14:59:45 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.279537, avg_loss=0.756988, seen=40, correct=16, accuracy=0.400000
2025-10-10 14:59:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:59:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:59:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 14:59:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2322MB allocated=2271MB
2025-10-10 14:59:46 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/25), best=0.475000, curr=0.400000
2025-10-10 14:59:55 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-10 14:59:55 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-10 14:59:55 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-10 14:59:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-10-10 14:59:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 14:59:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 14:59:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-10-10 14:59:57 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=100, loss_sum=72.193741, avg_loss=0.721937, seen=100, correct=48, accuracy=0.480000
2025-10-10 14:59:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 14:59:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 15:00:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 15:00:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2322MB allocated=2271MB
2025-10-10 15:00:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 15:00:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 15:00:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 15:00:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 15:00:02 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.481827, avg_loss=0.762046, seen=40, correct=18, accuracy=0.450000
2025-10-10 15:00:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 15:00:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 15:00:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 15:00:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2322MB allocated=2271MB
2025-10-10 15:00:03 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/25), best=0.475000, curr=0.450000
2025-10-10 15:00:12 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-10 15:00:12 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-10 15:00:12 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-10 15:00:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-10-10 15:00:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 15:00:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 15:00:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-10-10 15:00:14 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=100, loss_sum=73.053604, avg_loss=0.730536, seen=100, correct=51, accuracy=0.510000
2025-10-10 15:00:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 15:00:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 15:00:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 15:00:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2322MB allocated=2271MB
2025-10-10 15:00:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 15:00:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 15:00:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 15:00:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 15:00:20 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.239410, avg_loss=0.755985, seen=40, correct=18, accuracy=0.450000
2025-10-10 15:00:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 15:00:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 15:00:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 15:00:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2322MB allocated=2271MB
2025-10-10 15:00:22 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/25), best=0.475000, curr=0.450000
2025-10-10 15:00:30 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-10 15:00:30 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-10 15:00:30 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-10 15:00:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-10-10 15:00:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 15:00:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 15:00:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-10-10 15:00:32 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=100, loss_sum=74.661575, avg_loss=0.746616, seen=100, correct=46, accuracy=0.460000
2025-10-10 15:00:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 15:00:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 15:00:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 15:00:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2322MB allocated=2271MB
2025-10-10 15:00:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 15:00:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 15:00:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 15:00:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 15:00:37 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.324026, avg_loss=0.758101, seen=40, correct=19, accuracy=0.475000
2025-10-10 15:00:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 15:00:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 15:00:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 15:00:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2322MB allocated=2271MB
2025-10-10 15:00:38 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.475000
2025-10-10 15:00:49 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-10 15:00:49 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-10 15:00:49 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-10 15:00:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-10-10 15:00:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 15:00:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 15:00:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-10-10 15:00:51 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=100, loss_sum=69.545883, avg_loss=0.695459, seen=100, correct=52, accuracy=0.520000
2025-10-10 15:00:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 15:00:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 15:00:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 15:00:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2322MB allocated=2271MB
2025-10-10 15:00:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 15:00:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 15:00:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 15:00:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 15:00:56 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.726219, avg_loss=0.743155, seen=40, correct=15, accuracy=0.375000
2025-10-10 15:00:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 15:00:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 15:00:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 15:00:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2322MB allocated=2271MB
2025-10-10 15:00:58 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/25), best=0.475000, curr=0.375000
2025-10-10 15:01:07 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-10 15:01:07 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-10 15:01:07 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-10 15:01:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-10-10 15:01:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 15:01:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 15:01:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-10-10 15:01:09 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=100, loss_sum=66.817993, avg_loss=0.668180, seen=100, correct=57, accuracy=0.570000
2025-10-10 15:01:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 15:01:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 15:01:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 15:01:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2322MB allocated=2271MB
2025-10-10 15:01:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-10 15:01:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 15:01:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-10 15:01:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-10 15:01:13 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.203732, avg_loss=0.755093, seen=40, correct=18, accuracy=0.450000
2025-10-10 15:01:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-10 15:01:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 15:01:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 15:01:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2322MB allocated=2271MB
2025-10-10 15:01:15 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/25), best=0.475000, curr=0.450000
2025-10-10 15:01:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-10 15:01:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-10 15:01:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-10 15:01:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-10 15:01:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2322MB allocated=2271MB
2025-10-10 15:01:17 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #45', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-10 15:01:17 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #45', 'Round': 0, 'Results_raw': {}}
2025-10-10 15:01:18 (federatedscope.core.workers.server:493) INFO: Server: Training is finished! (skip final evaluation)
2025-10-10 15:01:18 (federatedscope.core.monitors.monitor:268) INFO: In worker #0, the system-related metrics are: {'id': 0, 'fl_end_time_minutes': 169.060791, 'total_model_size': 0, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 8752, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 15:01:18 (federatedscope.core.workers.client:842) INFO: ================= client 1 received finish message =================
2025-10-10 15:01:18 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 15:01:18 (federatedscope.core.monitors.monitor:268) INFO: In worker #1, the system-related metrics are: {'id': 1, 'fl_end_time_minutes': 169.0657984166667, 'total_model_size': 537763968, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1221568, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 15:01:18 (federatedscope.core.workers.client:842) INFO: ================= client 2 received finish message =================
2025-10-10 15:01:18 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 15:01:18 (federatedscope.core.monitors.monitor:268) INFO: In worker #2, the system-related metrics are: {'id': 2, 'fl_end_time_minutes': 168.98735646666668, 'total_model_size': 537763968, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1221560, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 15:01:18 (federatedscope.core.workers.client:842) INFO: ================= client 3 received finish message =================
2025-10-10 15:01:18 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 15:01:18 (federatedscope.core.monitors.monitor:268) INFO: In worker #3, the system-related metrics are: {'id': 3, 'fl_end_time_minutes': 168.94654333333335, 'total_model_size': 537763968, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1221568, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 15:01:18 (federatedscope.core.workers.client:842) INFO: ================= client 4 received finish message =================
2025-10-10 15:01:18 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 15:01:18 (federatedscope.core.monitors.monitor:268) INFO: In worker #4, the system-related metrics are: {'id': 4, 'fl_end_time_minutes': 168.90108818333334, 'total_model_size': 537763968, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1221568, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 15:01:18 (federatedscope.core.workers.client:842) INFO: ================= client 5 received finish message =================
2025-10-10 15:01:19 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 15:01:19 (federatedscope.core.monitors.monitor:268) INFO: In worker #5, the system-related metrics are: {'id': 5, 'fl_end_time_minutes': 168.85958781666668, 'total_model_size': 537763968, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1221568, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 15:01:19 (federatedscope.core.workers.client:842) INFO: ================= client 6 received finish message =================
2025-10-10 15:01:19 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 15:01:19 (federatedscope.core.monitors.monitor:268) INFO: In worker #6, the system-related metrics are: {'id': 6, 'fl_end_time_minutes': 168.8160632, 'total_model_size': 537763968, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1221568, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 15:01:19 (federatedscope.core.workers.client:842) INFO: ================= client 7 received finish message =================
2025-10-10 15:01:19 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 15:01:19 (federatedscope.core.monitors.monitor:268) INFO: In worker #7, the system-related metrics are: {'id': 7, 'fl_end_time_minutes': 168.77181768333332, 'total_model_size': 537763968, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1221568, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 15:01:19 (federatedscope.core.workers.client:842) INFO: ================= client 8 received finish message =================
2025-10-10 15:01:19 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 15:01:19 (federatedscope.core.monitors.monitor:268) INFO: In worker #8, the system-related metrics are: {'id': 8, 'fl_end_time_minutes': 168.73027085, 'total_model_size': 537763968, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1221560, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 15:01:19 (federatedscope.core.workers.client:842) INFO: ================= client 9 received finish message =================
2025-10-10 15:01:19 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 15:01:19 (federatedscope.core.monitors.monitor:268) INFO: In worker #9, the system-related metrics are: {'id': 9, 'fl_end_time_minutes': 168.66962055000002, 'total_model_size': 537763968, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1221568, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 15:01:19 (federatedscope.core.workers.client:842) INFO: ================= client 10 received finish message =================
2025-10-10 15:01:20 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 15:01:20 (federatedscope.core.monitors.monitor:268) INFO: In worker #10, the system-related metrics are: {'id': 10, 'fl_end_time_minutes': 168.62437666666668, 'total_model_size': 537763968, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1221568, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 15:01:20 (federatedscope.core.workers.client:842) INFO: ================= client 11 received finish message =================
2025-10-10 15:01:20 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 15:01:20 (federatedscope.core.monitors.monitor:268) INFO: In worker #11, the system-related metrics are: {'id': 11, 'fl_end_time_minutes': 168.58215729999998, 'total_model_size': 537763968, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1221560, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 15:01:20 (federatedscope.core.workers.client:842) INFO: ================= client 12 received finish message =================
2025-10-10 15:01:20 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 15:01:20 (federatedscope.core.monitors.monitor:268) INFO: In worker #12, the system-related metrics are: {'id': 12, 'fl_end_time_minutes': 168.54162946666665, 'total_model_size': 537763968, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1221568, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 15:01:20 (federatedscope.core.workers.client:842) INFO: ================= client 13 received finish message =================
2025-10-10 15:01:20 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 15:01:20 (federatedscope.core.monitors.monitor:268) INFO: In worker #13, the system-related metrics are: {'id': 13, 'fl_end_time_minutes': 168.49523073333333, 'total_model_size': 537763968, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1221568, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 15:01:20 (federatedscope.core.workers.client:842) INFO: ================= client 14 received finish message =================
2025-10-10 15:01:20 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 15:01:20 (federatedscope.core.monitors.monitor:268) INFO: In worker #14, the system-related metrics are: {'id': 14, 'fl_end_time_minutes': 168.45164988333332, 'total_model_size': 537763968, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1221568, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 15:01:20 (federatedscope.core.workers.client:842) INFO: ================= client 15 received finish message =================
2025-10-10 15:01:20 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 15:01:20 (federatedscope.core.monitors.monitor:268) INFO: In worker #15, the system-related metrics are: {'id': 15, 'fl_end_time_minutes': 168.40918386666667, 'total_model_size': 537763968, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1221568, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 15:01:20 (federatedscope.core.workers.client:842) INFO: ================= client 16 received finish message =================
2025-10-10 15:01:21 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 15:01:21 (federatedscope.core.monitors.monitor:268) INFO: In worker #16, the system-related metrics are: {'id': 16, 'fl_end_time_minutes': 168.3679644, 'total_model_size': 537763968, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1221568, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 15:01:21 (federatedscope.core.workers.client:842) INFO: ================= client 17 received finish message =================
2025-10-10 15:01:21 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 15:01:21 (federatedscope.core.monitors.monitor:268) INFO: In worker #17, the system-related metrics are: {'id': 17, 'fl_end_time_minutes': 168.32697551666666, 'total_model_size': 537763968, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1221568, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 15:01:21 (federatedscope.core.workers.client:842) INFO: ================= client 18 received finish message =================
2025-10-10 15:01:21 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 15:01:21 (federatedscope.core.monitors.monitor:268) INFO: In worker #18, the system-related metrics are: {'id': 18, 'fl_end_time_minutes': 168.26606828333334, 'total_model_size': 537763968, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1221568, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 15:01:21 (federatedscope.core.workers.client:842) INFO: ================= client 19 received finish message =================
2025-10-10 15:01:21 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 15:01:21 (federatedscope.core.monitors.monitor:268) INFO: In worker #19, the system-related metrics are: {'id': 19, 'fl_end_time_minutes': 168.22445058333332, 'total_model_size': 537763968, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1221568, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 15:01:21 (federatedscope.core.workers.client:842) INFO: ================= client 20 received finish message =================
2025-10-10 15:01:21 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 15:01:21 (federatedscope.core.monitors.monitor:268) INFO: In worker #20, the system-related metrics are: {'id': 20, 'fl_end_time_minutes': 168.1822001, 'total_model_size': 537763968, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1221568, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 15:01:21 (federatedscope.core.workers.client:842) INFO: ================= client 21 received finish message =================
2025-10-10 15:01:21 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 15:01:21 (federatedscope.core.monitors.monitor:268) INFO: In worker #21, the system-related metrics are: {'id': 21, 'fl_end_time_minutes': 168.13452751666668, 'total_model_size': 537763968, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1221568, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 15:01:21 (federatedscope.core.workers.client:842) INFO: ================= client 22 received finish message =================
2025-10-10 15:01:22 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 15:01:22 (federatedscope.core.monitors.monitor:268) INFO: In worker #22, the system-related metrics are: {'id': 22, 'fl_end_time_minutes': 168.09160638333333, 'total_model_size': 537763968, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1221568, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 15:01:22 (federatedscope.core.workers.client:842) INFO: ================= client 23 received finish message =================
2025-10-10 15:01:22 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 15:01:22 (federatedscope.core.monitors.monitor:268) INFO: In worker #23, the system-related metrics are: {'id': 23, 'fl_end_time_minutes': 168.04351499999999, 'total_model_size': 537763968, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1221568, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 15:01:22 (federatedscope.core.workers.client:842) INFO: ================= client 24 received finish message =================
2025-10-10 15:01:22 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 15:01:22 (federatedscope.core.monitors.monitor:268) INFO: In worker #24, the system-related metrics are: {'id': 24, 'fl_end_time_minutes': 167.99800969999998, 'total_model_size': 537763968, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1221568, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 15:01:22 (federatedscope.core.workers.client:842) INFO: ================= client 25 received finish message =================
2025-10-10 15:01:22 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 15:01:22 (federatedscope.core.monitors.monitor:268) INFO: In worker #25, the system-related metrics are: {'id': 25, 'fl_end_time_minutes': 167.95077398333333, 'total_model_size': 537763968, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1221568, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 15:01:22 (federatedscope.core.workers.client:842) INFO: ================= client 26 received finish message =================
2025-10-10 15:01:22 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 15:01:22 (federatedscope.core.monitors.monitor:268) INFO: In worker #26, the system-related metrics are: {'id': 26, 'fl_end_time_minutes': 167.88936676666665, 'total_model_size': 537763968, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1221568, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 15:01:22 (federatedscope.core.workers.client:842) INFO: ================= client 27 received finish message =================
2025-10-10 15:01:22 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 15:01:22 (federatedscope.core.monitors.monitor:268) INFO: In worker #27, the system-related metrics are: {'id': 27, 'fl_end_time_minutes': 167.84250083333333, 'total_model_size': 537763968, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1221568, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 15:01:22 (federatedscope.core.workers.client:842) INFO: ================= client 28 received finish message =================
2025-10-10 15:01:23 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 15:01:23 (federatedscope.core.monitors.monitor:268) INFO: In worker #28, the system-related metrics are: {'id': 28, 'fl_end_time_minutes': 167.79935558333332, 'total_model_size': 537763968, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1221568, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 15:01:23 (federatedscope.core.workers.client:842) INFO: ================= client 29 received finish message =================
2025-10-10 15:01:23 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 15:01:23 (federatedscope.core.monitors.monitor:268) INFO: In worker #29, the system-related metrics are: {'id': 29, 'fl_end_time_minutes': 167.7571791, 'total_model_size': 537763968, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1221568, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 15:01:23 (federatedscope.core.workers.client:842) INFO: ================= client 30 received finish message =================
2025-10-10 15:01:23 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 15:01:23 (federatedscope.core.monitors.monitor:268) INFO: In worker #30, the system-related metrics are: {'id': 30, 'fl_end_time_minutes': 167.71490406666666, 'total_model_size': 537763968, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1221568, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 15:01:23 (federatedscope.core.workers.client:842) INFO: ================= client 31 received finish message =================
2025-10-10 15:01:23 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 15:01:23 (federatedscope.core.monitors.monitor:268) INFO: In worker #31, the system-related metrics are: {'id': 31, 'fl_end_time_minutes': 167.66785191666668, 'total_model_size': 537763968, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1221568, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 15:01:23 (federatedscope.core.workers.client:842) INFO: ================= client 32 received finish message =================
2025-10-10 15:01:23 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 15:01:23 (federatedscope.core.monitors.monitor:268) INFO: In worker #32, the system-related metrics are: {'id': 32, 'fl_end_time_minutes': 167.62509131666667, 'total_model_size': 537763968, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1221568, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 15:01:23 (federatedscope.core.workers.client:842) INFO: ================= client 33 received finish message =================
2025-10-10 15:01:23 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 15:01:23 (federatedscope.core.monitors.monitor:268) INFO: In worker #33, the system-related metrics are: {'id': 33, 'fl_end_time_minutes': 167.58280961666665, 'total_model_size': 537763968, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1221568, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 15:01:23 (federatedscope.core.workers.client:842) INFO: ================= client 34 received finish message =================
2025-10-10 15:01:24 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 15:01:24 (federatedscope.core.monitors.monitor:268) INFO: In worker #34, the system-related metrics are: {'id': 34, 'fl_end_time_minutes': 167.5250157, 'total_model_size': 537763968, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1221568, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 15:01:24 (federatedscope.core.workers.client:842) INFO: ================= client 35 received finish message =================
2025-10-10 15:01:24 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 15:01:24 (federatedscope.core.monitors.monitor:268) INFO: In worker #35, the system-related metrics are: {'id': 35, 'fl_end_time_minutes': 167.47702526666666, 'total_model_size': 537763968, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1221568, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 15:01:24 (federatedscope.core.workers.client:842) INFO: ================= client 36 received finish message =================
2025-10-10 15:01:24 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 15:01:24 (federatedscope.core.monitors.monitor:268) INFO: In worker #36, the system-related metrics are: {'id': 36, 'fl_end_time_minutes': 167.43457148333334, 'total_model_size': 537763968, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1221568, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 15:01:24 (federatedscope.core.workers.client:842) INFO: ================= client 37 received finish message =================
2025-10-10 15:01:24 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 15:01:24 (federatedscope.core.monitors.monitor:268) INFO: In worker #37, the system-related metrics are: {'id': 37, 'fl_end_time_minutes': 167.39116315, 'total_model_size': 537763968, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1221568, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 15:01:24 (federatedscope.core.workers.client:842) INFO: ================= client 38 received finish message =================
2025-10-10 15:01:24 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 15:01:24 (federatedscope.core.monitors.monitor:268) INFO: In worker #38, the system-related metrics are: {'id': 38, 'fl_end_time_minutes': 167.34935468333333, 'total_model_size': 537763968, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1221568, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 15:01:24 (federatedscope.core.workers.client:842) INFO: ================= client 39 received finish message =================
2025-10-10 15:01:25 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 15:01:25 (federatedscope.core.monitors.monitor:268) INFO: In worker #39, the system-related metrics are: {'id': 39, 'fl_end_time_minutes': 167.30782875000003, 'total_model_size': 537763968, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1221568, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 15:01:25 (federatedscope.core.workers.client:842) INFO: ================= client 40 received finish message =================
2025-10-10 15:01:25 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 15:01:25 (federatedscope.core.monitors.monitor:268) INFO: In worker #40, the system-related metrics are: {'id': 40, 'fl_end_time_minutes': 167.26084584999998, 'total_model_size': 537763968, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1221568, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 15:01:25 (federatedscope.core.workers.client:842) INFO: ================= client 41 received finish message =================
2025-10-10 15:01:25 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 15:01:25 (federatedscope.core.monitors.monitor:268) INFO: In worker #41, the system-related metrics are: {'id': 41, 'fl_end_time_minutes': 167.21931750000002, 'total_model_size': 537763968, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1221568, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 15:01:25 (federatedscope.core.workers.client:842) INFO: ================= client 42 received finish message =================
2025-10-10 15:01:25 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 15:01:25 (federatedscope.core.monitors.monitor:268) INFO: In worker #42, the system-related metrics are: {'id': 42, 'fl_end_time_minutes': 167.17673680000001, 'total_model_size': 537763968, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1221568, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 15:01:25 (federatedscope.core.workers.client:842) INFO: ================= client 43 received finish message =================
2025-10-10 15:01:25 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 15:01:25 (federatedscope.core.monitors.monitor:268) INFO: In worker #43, the system-related metrics are: {'id': 43, 'fl_end_time_minutes': 167.11853408333334, 'total_model_size': 537763968, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1221568, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 15:01:25 (federatedscope.core.workers.client:842) INFO: ================= client 44 received finish message =================
2025-10-10 15:01:25 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 15:01:25 (federatedscope.core.monitors.monitor:268) INFO: In worker #44, the system-related metrics are: {'id': 44, 'fl_end_time_minutes': 167.07646281666666, 'total_model_size': 537763968, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1221568, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 15:01:25 (federatedscope.core.workers.client:842) INFO: ================= client 45 received finish message =================
2025-10-10 15:01:26 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 15:01:26 (federatedscope.core.monitors.monitor:268) INFO: In worker #45, the system-related metrics are: {'id': 45, 'fl_end_time_minutes': 167.03538405, 'total_model_size': 537763968, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1221568, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 15:01:26 (federatedscope.core.workers.client:842) INFO: ================= client 46 received finish message =================
2025-10-10 15:01:26 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 15:01:26 (federatedscope.core.monitors.monitor:268) INFO: In worker #46, the system-related metrics are: {'id': 46, 'fl_end_time_minutes': 166.99018736666667, 'total_model_size': 537763968, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1221568, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 15:01:26 (federatedscope.core.workers.client:842) INFO: ================= client 47 received finish message =================
2025-10-10 15:01:26 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 15:01:26 (federatedscope.core.monitors.monitor:268) INFO: In worker #47, the system-related metrics are: {'id': 47, 'fl_end_time_minutes': 166.94628348333333, 'total_model_size': 537763968, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1221568, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 15:01:26 (federatedscope.core.workers.client:842) INFO: ================= client 48 received finish message =================
2025-10-10 15:01:26 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 15:01:26 (federatedscope.core.monitors.monitor:268) INFO: In worker #48, the system-related metrics are: {'id': 48, 'fl_end_time_minutes': 166.90399938333334, 'total_model_size': 537763968, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1221568, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 15:01:26 (federatedscope.core.workers.client:842) INFO: ================= client 49 received finish message =================
2025-10-10 15:01:26 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 15:01:26 (federatedscope.core.monitors.monitor:268) INFO: In worker #49, the system-related metrics are: {'id': 49, 'fl_end_time_minutes': 166.86118591666667, 'total_model_size': 537763968, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1221568, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 15:01:26 (federatedscope.core.workers.client:842) INFO: ================= client 50 received finish message =================
2025-10-10 15:01:26 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 15:01:26 (federatedscope.core.monitors.monitor:268) INFO: In worker #50, the system-related metrics are: {'id': 50, 'fl_end_time_minutes': 166.81818696666667, 'total_model_size': 537763968, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1221568, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 15:01:26 (federatedscope.core.workers.client:842) INFO: ================= client 51 received finish message =================
2025-10-10 15:01:27 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 15:01:27 (federatedscope.core.monitors.monitor:268) INFO: In worker #51, the system-related metrics are: {'id': 51, 'fl_end_time_minutes': 166.77572845, 'total_model_size': 537763968, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1221568, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 15:01:27 (federatedscope.core.workers.client:842) INFO: ================= client 52 received finish message =================
2025-10-10 15:01:27 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 15:01:27 (federatedscope.core.monitors.monitor:268) INFO: In worker #52, the system-related metrics are: {'id': 52, 'fl_end_time_minutes': 166.71865703333333, 'total_model_size': 537763968, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1221568, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 15:01:27 (federatedscope.core.workers.client:842) INFO: ================= client 53 received finish message =================
2025-10-10 15:01:27 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=3360 skipped=0 missing=291 unexpected=0
2025-10-10 15:01:27 (federatedscope.core.monitors.monitor:268) INFO: In worker #53, the system-related metrics are: {'id': 53, 'fl_end_time_minutes': 166.676751, 'total_model_size': 537763968, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1221568, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-10 15:01:27 (federatedscope.core.monitors.monitor:359) INFO: After merging the system metrics from all works, we got avg: defaultdict(None, {'id': 'sys_avg', 'sys_avg/fl_end_time_minutes': 167.87854695956793, 'sys_avg/total_model_size': '503.35M', 'sys_avg/total_flops': '0.0', 'sys_avg/total_upload_bytes': '0.0', 'sys_avg/total_download_bytes': '1.14M', 'sys_avg/global_convergence_round': 0.0, 'sys_avg/local_convergence_round': 0.0, 'sys_avg/global_convergence_time_minutes': 0.0, 'sys_avg/local_convergence_time_minutes': 0.0})
2025-10-10 15:01:27 (federatedscope.core.monitors.monitor:360) INFO: After merging the system metrics from all works, we got std: defaultdict(None, {'id': 'sys_std', 'sys_std/fl_end_time_minutes': 0.7102534476197077, 'sys_std/total_model_size': '69.14M', 'sys_std/total_flops': '0.0', 'sys_std/total_upload_bytes': '0.0', 'sys_std/total_download_bytes': '159.68K', 'sys_std/global_convergence_round': 0.0, 'sys_std/local_convergence_round': 0.0, 'sys_std/global_convergence_time_minutes': 0.0, 'sys_std/local_convergence_time_minutes': 0.0})
