2025-09-13 22:32:25 (root:426) INFO: [logger] file handler -> exp/tldr/choice_qwen/pfl/fedbiscuit_u3_1.0/plain_plain/exp_print.log
2025-09-13 22:32:25 (root:51) INFO: [main] outdir=exp/tldr/choice_qwen/pfl/fedbiscuit_u3_1.0/plain_plain
2025-09-13 22:32:48 (federatedscope.core.data.base_translator:234) INFO: Main process: Completion file found. Skipping generation.
2025-09-13 22:33:30 (federatedscope.core.data.base_translator:264) INFO: [Final Split Summary][loaded][server=0][rank=0/4] Train=92858, Val=33082, Test=50715, Total=176655
2025-09-13 22:33:30 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=1][rank=0/4] Train=2793, Val=146, Test=40, Total=2979
2025-09-13 22:33:30 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=2][rank=0/4] Train=214, Val=11, Test=40, Total=265
2025-09-13 22:33:30 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=3][rank=0/4] Train=691, Val=36, Test=40, Total=767
2025-09-13 22:33:30 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=4][rank=0/4] Train=213, Val=11, Test=40, Total=264
2025-09-13 22:33:30 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=5][rank=0/4] Train=285, Val=14, Test=40, Total=339
2025-09-13 22:33:30 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=6][rank=0/4] Train=2547, Val=134, Test=40, Total=2721
2025-09-13 22:33:30 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=7][rank=0/4] Train=1088, Val=57, Test=40, Total=1185
2025-09-13 22:33:30 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=8][rank=0/4] Train=1316, Val=69, Test=40, Total=1425
2025-09-13 22:33:30 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=9][rank=0/4] Train=3572, Val=188, Test=40, Total=3800
2025-09-13 22:33:30 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=10][rank=0/4] Train=1209, Val=63, Test=40, Total=1312
2025-09-13 22:33:30 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=11][rank=0/4] Train=621, Val=32, Test=40, Total=693
2025-09-13 22:33:30 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=12][rank=0/4] Train=2605, Val=137, Test=40, Total=2782
2025-09-13 22:33:30 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=13][rank=0/4] Train=1372, Val=72, Test=40, Total=1484
2025-09-13 22:33:30 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=14][rank=0/4] Train=3055, Val=160, Test=40, Total=3255
2025-09-13 22:33:30 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=15][rank=0/4] Train=14550, Val=200, Test=40, Total=14790
2025-09-13 22:33:30 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=16][rank=0/4] Train=2589, Val=136, Test=40, Total=2765
2025-09-13 22:33:30 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=17][rank=0/4] Train=5883, Val=200, Test=40, Total=6123
2025-09-13 22:33:30 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=18][rank=0/4] Train=2576, Val=135, Test=40, Total=2751
2025-09-13 22:33:30 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=19][rank=0/4] Train=2102, Val=110, Test=40, Total=2252
2025-09-13 22:33:30 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=20][rank=0/4] Train=2399, Val=126, Test=40, Total=2565
2025-09-13 22:33:30 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=21][rank=0/4] Train=2915, Val=153, Test=40, Total=3108
2025-09-13 22:33:30 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=22][rank=0/4] Train=224, Val=11, Test=40, Total=275
2025-09-13 22:33:30 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=23][rank=0/4] Train=583, Val=30, Test=40, Total=653
2025-09-13 22:33:30 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=24][rank=0/4] Train=4944, Val=200, Test=40, Total=5184
2025-09-13 22:33:30 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=25][rank=0/4] Train=4647, Val=200, Test=40, Total=4887
2025-09-13 22:33:30 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=26][rank=0/4] Train=3063, Val=161, Test=40, Total=3264
2025-09-13 22:33:30 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=27][rank=0/4] Train=2342, Val=123, Test=40, Total=2505
2025-09-13 22:33:30 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=28][rank=0/4] Train=1434, Val=75, Test=40, Total=1549
2025-09-13 22:33:30 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=29][rank=0/4] Train=6191, Val=200, Test=40, Total=6431
2025-09-13 22:33:30 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=30][rank=0/4] Train=3247, Val=170, Test=40, Total=3457
2025-09-13 22:33:30 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=31][rank=0/4] Train=3679, Val=193, Test=40, Total=3912
2025-09-13 22:33:30 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=32][rank=0/4] Train=2144, Val=112, Test=40, Total=2296
2025-09-13 22:33:30 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=33][rank=0/4] Train=1409, Val=74, Test=40, Total=1523
2025-09-13 22:33:30 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=34][rank=0/4] Train=4486, Val=200, Test=40, Total=4726
2025-09-13 22:33:30 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=35][rank=0/4] Train=4736, Val=200, Test=40, Total=4976
2025-09-13 22:33:30 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=36][rank=0/4] Train=1030, Val=54, Test=40, Total=1124
2025-09-13 22:33:30 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=37][rank=0/4] Train=4273, Val=200, Test=40, Total=4513
2025-09-13 22:33:30 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=38][rank=0/4] Train=6171, Val=200, Test=40, Total=6411
2025-09-13 22:33:30 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=39][rank=0/4] Train=1594, Val=83, Test=40, Total=1717
2025-09-13 22:33:30 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=40][rank=0/4] Train=4005, Val=200, Test=40, Total=4245
2025-09-13 22:33:30 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=41][rank=0/4] Train=2275, Val=119, Test=40, Total=2434
2025-09-13 22:33:30 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=42][rank=0/4] Train=5772, Val=200, Test=40, Total=6012
2025-09-13 22:33:30 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=43][rank=0/4] Train=1694, Val=89, Test=40, Total=1823
2025-09-13 22:33:30 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=44][rank=0/4] Train=7916, Val=200, Test=40, Total=8156
2025-09-13 22:33:30 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=45][rank=0/4] Train=1901, Val=100, Test=40, Total=2041
2025-09-13 22:33:30 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=46][rank=0/4] Train=2100, Val=110, Test=40, Total=2250
2025-09-13 22:33:30 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=47][rank=0/4] Train=2812, Val=147, Test=40, Total=2999
2025-09-13 22:33:30 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=48][rank=0/4] Train=880, Val=46, Test=40, Total=966
2025-09-13 22:33:30 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=49][rank=0/4] Train=2521, Val=132, Test=40, Total=2693
2025-09-13 22:33:30 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=50][rank=0/4] Train=2527, Val=133, Test=40, Total=2700
2025-09-13 22:33:30 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=51][rank=0/4] Train=1580, Val=83, Test=40, Total=1703
2025-09-13 22:33:30 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=52][rank=0/4] Train=3589, Val=188, Test=40, Total=3817
2025-09-13 22:33:30 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=53][rank=0/4] Train=6791, Val=200, Test=40, Total=7031
2025-09-13 22:33:30 (federatedscope.core.configs.config:256) INFO: the used configs are: 
adapter:
  use: False
aggregator:
  BFT_args:
    
  byzantine_node_num: 0
  inside_weight: 1.0
  num_agg_groups: 1
  num_agg_topk: []
  outside_weight: 0.0
  robust_rule: fedavg
asyn:
  use: False
attack:
  alpha_TV: 0.001
  alpha_prop_loss: 0
  attack_method: 
  attacker_id: -1
  classifier_PIA: randomforest
  edge_num: 100
  edge_path: edge_data/
  freq: 10
  info_diff_type: l2
  inject_round: 0
  insert_round: 100000
  label_type: dirty
  max_ite: 400
  mean: [0.9637]
  mia_is_simulate_in: False
  mia_simulate_in_round: 20
  pgd_eps: 2
  pgd_lr: 0.1
  pgd_poisoning: False
  poison_ratio: 0.5
  reconstruct_lr: 0.01
  reconstruct_optim: Adam
  scale_para: 1.0
  scale_poisoning: False
  self_epoch: 6
  self_lr: 0.05
  self_opt: False
  setting: fix
  std: [0.1592]
  target_label_ind: -1
  trigger_path: trigger/
  trigger_type: edge
backend: torch
cfg_file: 
check_completeness: False
criterion:
  type: CrossEntropyLoss
data:
  args: []
  batch_size: 64
  cSBM_phi: [0.5, 0.5, 0.5]
  cache_dir: 
  consistent_label_distribution: True
  drop_last: False
  file_path: 
  hetero_data_name: []
  hetero_synth_batch_size: 32
  hetero_synth_feat_dim: 128
  hetero_synth_prim_weight: 0.5
  is_debug: False
  load_splits: False
  loader: 
  max_query_len: 128
  max_seq_len: 384
  max_tgt_len: 128
  num_contrast: 0
  num_of_client_for_data: []
  num_steps: 30
  num_workers: 0
  pre_transform: []
  quadratic:
    dim: 1
    max_curv: 12.5
    min_curv: 0.02
  root: data/
  save_data: False
  save_splits: False
  server_holds_all: False
  shuffle: True
  sizes: [10, 5]
  splits: [0.9, 0.09, 0.01]
  splits_path: ./final_data_splits
  splitter: meta
  splitter_args: []
  subsample: 1.0
  target_transform: []
  test_pre_transform: []
  test_target_transform: []
  test_transform: []
  transform: []
  trunc_stride: 128
  type: reddit-tldr-comparison-choice@llm
  val_pre_transform: []
  val_target_transform: []
  val_transform: []
  walk_length: 2
dataloader:
  batch_size: 2
  drop_last: False
  num_steps: 30
  num_workers: 0
  pin_memory: False
  shuffle: True
  sizes: [10, 5]
  theta: -1
  type: base
  walk_length: 2
device: 0
distribute:
  use: False
early_stop:
  delta: 0.0
  improve_indicator_mode: best
  patience: 0
eval:
  baseline_before_ft: True
  best_res_update_round_wise_key: val_loss
  count_flops: False
  every_n_train_steps: 10
  freq: 1
  metrics: ['loss', 'acc']
  monitoring: []
  outdir: exp/tldr/choice_qwen/pfl/fedbiscuit_u3_1.0/plain_plain/raw
  report: ['weighted_avg', 'avg', 'fairness', 'raw']
  split: ['val', 'test']
expname: 
expname_tag: 
feat_engr:
  num_bins: 5
  scenario: hfl
  secure:
    dp:
      
    encrypt:
      type: dummy
    key_size: 3072
    type: encrypt
  selec_threshold: 0.05
  selec_woe_binning: quantile
  type: 
federate:
  atc_load_from: 
  atc_vanilla: False
  client_idx_for_local_train: 0
  client_num: 53
  data_weighted_aggr: False
  ignore_weight: True
  join_in_info: []
  make_global_eval: False
  master_addr: 127.0.0.1
  master_port: 29500
  merge_test_data: False
  merge_val_data: False
  method: FedAvg
  mode: standalone
  online_aggr: False
  process_num: 1
  resource_info_file: 
  restore_from: 
  sample_client_num: 53
  sample_client_rate: -1.0
  sampler: uniform
  save_client_model: False
  save_freq: -1
  save_to: 
  share_local_model: True
  total_round_num: 1
  unseen_clients_rate: 0.0
  use_diff: False
  use_ss: False
fedopt:
  use: False
fedprox:
  use: False
fedsageplus:
  a: 1.0
  b: 1.0
  c: 1.0
  fedgen_epoch: 200
  gen_hidden: 128
  hide_portion: 0.5
  loc_epoch: 1
  num_pred: 5
fedswa:
  use: False
finetune:
  batch_or_epoch: epoch
  before_eval: False
  epoch_linear: 10
  freeze_param: 
  local_param: []
  local_update_steps: 1
  lr_linear: 0.005
  optimizer:
    lr: 0.1
    type: SGD
  scheduler:
    type: 
    warmup_ratio: 0.0
  simple_tuning: False
  weight_decay: 0.0
flitplus:
  factor_ema: 0.8
  lambdavat: 0.5
  tmpFed: 0.5
  weightReg: 1.0
gcflplus:
  EPS_1: 0.05
  EPS_2: 0.1
  seq_length: 5
  standardize: False
grad:
  grad_accum_count: 1
  grad_clip: -1.0
hpo:
  fedex:
    cutoff: 0.0
    diff: False
    eta0: -1.0
    flatten_ss: True
    gamma: 0.0
    pi_lr: 0.01
    psn: False
    sched: auto
    ss: 
    use: False
  fts:
    M: 100
    M_target: 200
    allow_load_existing_info: True
    diff: False
    fed_bo_max_iter: 50
    g_var: 1e-06
    gp_opt_schedule: 1
    local_bo_epochs: 50
    local_bo_max_iter: 50
    ls: 1.0
    obs_noise: 1e-06
    ss: 
    target_clients: []
    use: False
    v_kernel: 1.0
    var: 0.1
  init_cand_num: 16
  larger_better: False
  metric: client_summarized_weighted_avg.val_loss
  num_workers: 0
  pbt:
    max_stage: 5
    perf_threshold: 0.1
  pfedhpo:
    discrete: False
    ss: 
    target_fl_total_round: 1000
    train_anchor: False
    train_fl: False
    use: False
  scheduler: rs
  sha:
    budgets: []
    elim_rate: 3
    iter: 0
  ss: 
  table:
    eps: 0.1
    idx: 0
    num: 27
  trial_index: 0
  working_folder: hpo
llm:
  accelerator:
    config: 
    use: True
  adapter:
    args: [{'adapter_package': 'peft', 'adapter_method': 'lora', 'r': 8, 'lora_alpha': 16, 'lora_dropout': 0.05, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']}]
    balance: False
    count: 3
    grouping:
      round: 0
      use: True
    local_only: False
    mv_to_cpu: False
    use: True
    warmup:
      round: 10
      use: False
  cache:
    model: 
  chat:
    max_history_len: 10
    max_len: 1024
  deepspeed:
    ds_config: 
    use: False
  fedrlhf:
    config_file: 
    frequency: 100
    pretrained: False
    train:
      batch_or_epoch: batch
      local_update_steps: 10
    use: False
  grad_accum_step: 2
  max_new_token: 60
  num_completions: 2
  offsite_tuning:
    emu_align:
      data:
        root: data
        splits: [0.8, 0.1, 0.1]
        type: alpaca@llm
      exit_after_align: False
      init_enable_ground_truth: False
      initial_only: True
      kl_divergence: raw
      layerwise_distill: False
      restore_from: 
      save_to: 
      sim_loss: l2
      train:
        batch_or_epoch: batch
        enable_ground_truth: False
        initial_update_rounds: 50
        kd_loss_weight: 0.9
        lm_loss_weight: 0.1
        local_update_steps: 10
        optimizer:
          lr: 0.01
          type: SGD
      use: False
    emu_l: 1
    emu_r: 10
    eval_type: emu
    kwargs: [{}]
    llm_generated:
      ratio: 0.1
      use: False
    save_full_model: False
    strategy: drop_layer
    use: False
  retry_on_nan_loss: False
  reward_coeff: 0.1
  rlhf: False
  tok_len: 1024
model:
  contrast_temp: 1.0
  contrast_topk: 100
  downstream_tasks: []
  dropout: 0.5
  embed_size: 8
  gamma: 0
  graph_pooling: mean
  hidden: 256
  in_channels: 0
  input_shape: ()
  label_smoothing: 0.1
  lambda_: 0.1
  layer: 2
  length_penalty: 2.0
  llm_kwargs: [{}]
  llm_type: CausalLM
  load_from_local_pretrained_fs_config: 
  load_from_local_pretrained_model_path: checkpoints_1.0/final_tldr_choice_qwen_fedbiscuit_u3_plain_round_175.ckpt
  max_answer_len: 30
  max_length: 200
  max_tree_depth: 3
  min_length: 1
  model_num_per_trainer: 1
  model_type: google/bert_uncased_L-2_H-128_A-2
  n_best_size: 20
  no_repeat_ngram_size: 3
  null_score_diff_threshold: 0.0
  num_beams: 5
  num_item: 0
  num_labels: 1
  num_of_trees: 10
  num_user: 0
  out_channels: 1
  pretrain_tasks: []
  stage: 
  task: node
  type: Qwen/Qwen2-0.5B@huggingface_llm
  use_bias: True
  use_contrastive_loss: False
nbafl:
  use: False
outdir: exp/tldr/choice_qwen/pfl/fedbiscuit_u3_1.0/plain_plain
personalization:
  K: 5
  beta: 1.0
  epoch_feature: 1
  epoch_linear: 2
  local_param: []
  local_update_steps: 100
  lr: 1e-05
  lr_feature: 0.1
  lr_linear: 0.1
  regular_weight: 0.1
  share_non_trainable_para: False
  weight_decay: 0.0
print_decimal_digits: 6
quantization:
  method: none
  nbits: 8
regularizer:
  mu: 0.0
  type: 
seed: 0
sgdmf:
  use: False
train:
  batch_or_epoch: batch
  data_para_dids: []
  is_enable_half: True
  local_update_steps: 100
  optimizer:
    betas: (0.9, 0.95)
    lr: 1e-05
    type: AdamW
  scheduler:
    type: 
    warmup_ratio: 0.0
trainer:
  choices: ['A', 'B']
  disp_freq: 50
  local_entropy:
    alpha: 0.75
    eps: 0.0001
    gamma: 0.03
    inc_factor: 1.0
  sam:
    adaptive: False
    eta: 0.0
    rho: 1.0
  type: llmrewardchoicetrainer
  val_freq: 100000000
use_gpu: True
verbose: 1
vertical:
  use: False
wandb:
  use: False
2025-09-13 22:33:32 (federatedscope.core.auxiliaries.utils:175) INFO: The device information file is not provided
2025-09-13 22:33:32 (federatedscope.core.auxiliaries.model_builder:139) WARNING: The input shape is None. Please specify the `data.input_shape`(a tuple) or give the representative data to `get_model` if necessary
2025-09-13 22:33:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-build][rank=0] tok_len=151643 | base=Qwen2ForCausalLM | in_emb=(Embedding) num=151646 ptr=139622024568896 | out_emb=(Linear) num=151646 ptr=139622024568896 | lora_ptr=None
2025-09-13 22:33:46 (federatedscope.llm.model.model_builder:187) INFO: [Warmup-Init] loaded from checkpoints_1.0/final_tldr_choice_qwen_fedbiscuit_u3_plain_round_175.ckpt (round=175) | missing=291 unexpected=0
2025-09-13 22:33:46 (federatedscope.core.fed_runner:211) INFO: Server has been set up ... 
2025-09-13 22:33:47 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-13 22:33:50 (federatedscope.core.fed_runner:275) INFO: Client 1 has been set up ... 
2025-09-13 22:33:50 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-13 22:33:52 (federatedscope.core.fed_runner:275) INFO: Client 2 has been set up ... 
2025-09-13 22:33:52 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-13 22:33:55 (federatedscope.core.fed_runner:275) INFO: Client 3 has been set up ... 
2025-09-13 22:33:55 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-13 22:33:57 (federatedscope.core.fed_runner:275) INFO: Client 4 has been set up ... 
2025-09-13 22:33:57 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-13 22:34:00 (federatedscope.core.fed_runner:275) INFO: Client 5 has been set up ... 
2025-09-13 22:34:00 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-13 22:34:02 (federatedscope.core.fed_runner:275) INFO: Client 6 has been set up ... 
2025-09-13 22:34:02 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-13 22:34:05 (federatedscope.core.fed_runner:275) INFO: Client 7 has been set up ... 
2025-09-13 22:34:05 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-13 22:34:07 (federatedscope.core.fed_runner:275) INFO: Client 8 has been set up ... 
2025-09-13 22:34:07 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-13 22:34:10 (federatedscope.core.fed_runner:275) INFO: Client 9 has been set up ... 
2025-09-13 22:34:10 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-13 22:34:12 (federatedscope.core.fed_runner:275) INFO: Client 10 has been set up ... 
2025-09-13 22:34:12 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-13 22:34:15 (federatedscope.core.fed_runner:275) INFO: Client 11 has been set up ... 
2025-09-13 22:34:15 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-13 22:34:17 (federatedscope.core.fed_runner:275) INFO: Client 12 has been set up ... 
2025-09-13 22:34:17 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-13 22:34:20 (federatedscope.core.fed_runner:275) INFO: Client 13 has been set up ... 
2025-09-13 22:34:20 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-13 22:34:22 (federatedscope.core.fed_runner:275) INFO: Client 14 has been set up ... 
2025-09-13 22:34:22 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-13 22:34:25 (federatedscope.core.fed_runner:275) INFO: Client 15 has been set up ... 
2025-09-13 22:34:25 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-13 22:34:27 (federatedscope.core.fed_runner:275) INFO: Client 16 has been set up ... 
2025-09-13 22:34:27 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-13 22:34:30 (federatedscope.core.fed_runner:275) INFO: Client 17 has been set up ... 
2025-09-13 22:34:30 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-13 22:34:32 (federatedscope.core.fed_runner:275) INFO: Client 18 has been set up ... 
2025-09-13 22:34:32 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-13 22:34:36 (federatedscope.core.fed_runner:275) INFO: Client 19 has been set up ... 
2025-09-13 22:34:36 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-13 22:34:38 (federatedscope.core.fed_runner:275) INFO: Client 20 has been set up ... 
2025-09-13 22:34:38 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-13 22:34:40 (federatedscope.core.fed_runner:275) INFO: Client 21 has been set up ... 
2025-09-13 22:34:41 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-13 22:34:43 (federatedscope.core.fed_runner:275) INFO: Client 22 has been set up ... 
2025-09-13 22:34:43 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-13 22:34:46 (federatedscope.core.fed_runner:275) INFO: Client 23 has been set up ... 
2025-09-13 22:34:46 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-13 22:34:48 (federatedscope.core.fed_runner:275) INFO: Client 24 has been set up ... 
2025-09-13 22:34:48 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-13 22:34:51 (federatedscope.core.fed_runner:275) INFO: Client 25 has been set up ... 
2025-09-13 22:34:51 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-13 22:34:53 (federatedscope.core.fed_runner:275) INFO: Client 26 has been set up ... 
2025-09-13 22:34:53 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-13 22:34:56 (federatedscope.core.fed_runner:275) INFO: Client 27 has been set up ... 
2025-09-13 22:34:56 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-13 22:34:58 (federatedscope.core.fed_runner:275) INFO: Client 28 has been set up ... 
2025-09-13 22:34:58 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-13 22:35:01 (federatedscope.core.fed_runner:275) INFO: Client 29 has been set up ... 
2025-09-13 22:35:01 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-13 22:35:03 (federatedscope.core.fed_runner:275) INFO: Client 30 has been set up ... 
2025-09-13 22:35:03 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-13 22:35:06 (federatedscope.core.fed_runner:275) INFO: Client 31 has been set up ... 
2025-09-13 22:35:06 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-13 22:35:08 (federatedscope.core.fed_runner:275) INFO: Client 32 has been set up ... 
2025-09-13 22:35:08 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-13 22:35:11 (federatedscope.core.fed_runner:275) INFO: Client 33 has been set up ... 
2025-09-13 22:35:11 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-13 22:35:13 (federatedscope.core.fed_runner:275) INFO: Client 34 has been set up ... 
2025-09-13 22:35:14 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-13 22:35:16 (federatedscope.core.fed_runner:275) INFO: Client 35 has been set up ... 
2025-09-13 22:35:16 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-13 22:35:19 (federatedscope.core.fed_runner:275) INFO: Client 36 has been set up ... 
2025-09-13 22:35:19 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-13 22:35:21 (federatedscope.core.fed_runner:275) INFO: Client 37 has been set up ... 
2025-09-13 22:35:21 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-13 22:35:24 (federatedscope.core.fed_runner:275) INFO: Client 38 has been set up ... 
2025-09-13 22:35:24 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-13 22:35:27 (federatedscope.core.fed_runner:275) INFO: Client 39 has been set up ... 
2025-09-13 22:35:27 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-13 22:35:30 (federatedscope.core.fed_runner:275) INFO: Client 40 has been set up ... 
2025-09-13 22:35:30 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-13 22:35:32 (federatedscope.core.fed_runner:275) INFO: Client 41 has been set up ... 
2025-09-13 22:35:32 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-13 22:35:35 (federatedscope.core.fed_runner:275) INFO: Client 42 has been set up ... 
2025-09-13 22:35:35 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-13 22:35:37 (federatedscope.core.fed_runner:275) INFO: Client 43 has been set up ... 
2025-09-13 22:35:37 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-13 22:35:40 (federatedscope.core.fed_runner:275) INFO: Client 44 has been set up ... 
2025-09-13 22:35:40 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-13 22:35:42 (federatedscope.core.fed_runner:275) INFO: Client 45 has been set up ... 
2025-09-13 22:35:43 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-13 22:35:45 (federatedscope.core.fed_runner:275) INFO: Client 46 has been set up ... 
2025-09-13 22:35:45 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-13 22:35:47 (federatedscope.core.fed_runner:275) INFO: Client 47 has been set up ... 
2025-09-13 22:35:48 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-13 22:35:50 (federatedscope.core.fed_runner:275) INFO: Client 48 has been set up ... 
2025-09-13 22:35:50 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-13 22:35:52 (federatedscope.core.fed_runner:275) INFO: Client 49 has been set up ... 
2025-09-13 22:35:53 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-13 22:35:55 (federatedscope.core.fed_runner:275) INFO: Client 50 has been set up ... 
2025-09-13 22:35:55 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-13 22:35:57 (federatedscope.core.fed_runner:275) INFO: Client 51 has been set up ... 
2025-09-13 22:35:58 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-13 22:36:00 (federatedscope.core.fed_runner:275) INFO: Client 52 has been set up ... 
2025-09-13 22:36:00 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-13 22:36:02 (federatedscope.core.fed_runner:275) INFO: Client 53 has been set up ... 
2025-09-13 22:36:02 (federatedscope.core.trainers.trainer:569) INFO: Model meta-info: <class 'federatedscope.llm.model.adapter_builder.AdapterModel'>.
2025-09-13 22:36:02 (federatedscope.core.trainers.trainer:584) INFO: Num of original para names: 1344.
2025-09-13 22:36:02 (federatedscope.core.trainers.trainer:585) INFO: Num of original trainable para names: 1634.
2025-09-13 22:36:02 (federatedscope.core.trainers.trainer:587) INFO: Num of preserved para names in local update: 1344. 
Preserved para names in local update: {'base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight'}.
2025-09-13 22:36:02 (federatedscope.core.trainers.trainer:591) INFO: Num of filtered para names in local update: 0. 
Filtered para names in local update: set().
2025-09-13 22:36:02 (federatedscope.core.trainers.trainer:599) INFO: After register default hooks,
	the hooks_in_train is:
	{
	  "on_fit_start": [
	    "_hook_on_fit_start_numerical_precision",
	    "_hook_on_data_parallel_init",
	    "_hook_on_fit_start_init",
	    "_hook_on_fit_start_calculate_model_size"
	  ],
	  "on_batch_start": [
	    "_hook_on_batch_start_init"
	  ],
	  "on_batch_forward": [
	    "_hook_on_batch_forward",
	    "_hook_on_batch_forward_regularizer",
	    "_hook_on_batch_forward_flop_count"
	  ],
	  "on_batch_backward": [
	    "_hook_on_batch_backward"
	  ],
	  "on_batch_end": [
	    "_hook_on_batch_end"
	  ],
	  "on_fit_end": [
	    "_hook_on_fit_end",
	    "_hook_on_fit_end_free_space"
	  ]
	};
	the hooks_in_eval is:
            t{
	  "on_fit_start": [
	    "_hook_on_fit_start_numerical_precision",
	    "_hook_on_data_parallel_init",
	    "_hook_on_fit_start_init"
	  ],
	  "on_batch_start": [
	    "_hook_on_batch_start_init"
	  ],
	  "on_batch_forward": [
	    "_hook_on_batch_forward"
	  ],
	  "on_batch_end": [
	    "_hook_on_batch_end"
	  ],
	  "on_fit_end": [
	    "_hook_on_fit_end",
	    "_hook_on_fit_end_free_space"
	  ]
	}
2025-09-13 22:36:02 (federatedscope.llm.llm_local.server:103) INFO: Waited all clients join, start now...
2025-09-13 22:36:03 (federatedscope.llm.llm_local.server:111) INFO: ----------- Starting training (Round #0) -------------
2025-09-13 22:36:03 (federatedscope.llm.llm_local.server:114) INFO: Server: Performing a grouping step...
2025-09-13 22:36:11 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-13 22:36:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-09-13 22:36:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139622024568896 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:36:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=73, num_train_batch_last_epoch=27, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:36:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-13 22:36:19 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=146, loss_sum=96.015724, avg_loss=0.657642, seen=146, correct=86, accuracy=0.589041
2025-09-13 22:36:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:36:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:36:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:36:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1768MB
2025-09-13 22:36:20 (federatedscope.llm.llm_local.client:480) INFO: Client 1 Adapter 0 with val results: {'val_total': 146, 'val_loss': 96.0157241821289, 'val_avg_loss': 0.6576419464529377, 'val_seen': 146, 'val_correct': 86, 'val_acc': 0.589041095890411}
2025-09-13 22:36:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:36:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:36:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:36:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:36:22 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.803827, avg_loss=0.595096, seen=40, correct=27, accuracy=0.675000
2025-09-13 22:36:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:36:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:36:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:36:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2052MB allocated=1768MB
2025-09-13 22:36:23 (federatedscope.llm.llm_local.client:501) INFO: Client 1 Adapter 0 with test results: {'test_total': 40, 'test_loss': 23.8038272857666, 'test_avg_loss': 0.5950956821441651, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-13 22:36:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-09-13 22:36:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:36:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=73, num_train_batch_last_epoch=27, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:36:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-13 22:36:28 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=146, loss_sum=99.669968, avg_loss=0.682671, seen=146, correct=87, accuracy=0.595890
2025-09-13 22:36:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:36:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:36:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:36:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1743MB
2025-09-13 22:36:30 (federatedscope.llm.llm_local.client:480) INFO: Client 1 Adapter 1 with val results: {'val_total': 146, 'val_loss': 99.66996765136719, 'val_avg_loss': 0.6826710113107342, 'val_seen': 146, 'val_correct': 87, 'val_acc': 0.5958904109589042}
2025-09-13 22:36:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:36:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:36:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:36:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:36:31 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.653584, avg_loss=0.641340, seen=40, correct=27, accuracy=0.675000
2025-09-13 22:36:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:36:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:36:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:36:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1743MB
2025-09-13 22:36:32 (federatedscope.llm.llm_local.client:501) INFO: Client 1 Adapter 1 with test results: {'test_total': 40, 'test_loss': 25.653583526611328, 'test_avg_loss': 0.6413395881652832, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-13 22:36:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-09-13 22:36:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:36:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=73, num_train_batch_last_epoch=27, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:36:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-13 22:36:37 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=146, loss_sum=110.366455, avg_loss=0.755935, seen=146, correct=72, accuracy=0.493151
2025-09-13 22:36:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:36:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:36:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:36:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1743MB
2025-09-13 22:36:39 (federatedscope.llm.llm_local.client:480) INFO: Client 1 Adapter 2 with val results: {'val_total': 146, 'val_loss': 110.366455078125, 'val_avg_loss': 0.755934623822774, 'val_seen': 146, 'val_correct': 72, 'val_acc': 0.4931506849315068}
2025-09-13 22:36:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:36:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:36:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:36:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:36:40 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.522890, avg_loss=0.688072, seen=40, correct=21, accuracy=0.525000
2025-09-13 22:36:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:36:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:36:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:36:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1743MB
2025-09-13 22:36:41 (federatedscope.llm.llm_local.client:501) INFO: Client 1 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.522890090942383, 'test_avg_loss': 0.6880722522735596, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-13 22:36:41 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-13 22:36:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-13 22:36:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:36:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=4, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:36:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-13 22:36:43 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=8.704920, avg_loss=0.791356, seen=11, correct=6, accuracy=0.545455
2025-09-13 22:36:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:36:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:36:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:36:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1777MB
2025-09-13 22:36:44 (federatedscope.llm.llm_local.client:480) INFO: Client 2 Adapter 0 with val results: {'val_total': 11, 'val_loss': 8.704919815063477, 'val_avg_loss': 0.7913563468239524, 'val_seen': 11, 'val_correct': 6, 'val_acc': 0.5454545454545454}
2025-09-13 22:36:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:36:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:36:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:36:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:36:45 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.692604, avg_loss=0.667315, seen=40, correct=21, accuracy=0.525000
2025-09-13 22:36:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:36:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:36:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:36:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1777MB
2025-09-13 22:36:46 (federatedscope.llm.llm_local.client:501) INFO: Client 2 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.692604064941406, 'test_avg_loss': 0.6673151016235351, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-13 22:36:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-13 22:36:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:36:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=4, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:36:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-13 22:36:47 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=9.110340, avg_loss=0.828213, seen=11, correct=4, accuracy=0.363636
2025-09-13 22:36:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:36:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:36:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:36:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1752MB
2025-09-13 22:36:48 (federatedscope.llm.llm_local.client:480) INFO: Client 2 Adapter 1 with val results: {'val_total': 11, 'val_loss': 9.110340118408203, 'val_avg_loss': 0.8282127380371094, 'val_seen': 11, 'val_correct': 4, 'val_acc': 0.36363636363636365}
2025-09-13 22:36:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:36:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:36:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:36:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:36:49 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.739876, avg_loss=0.768497, seen=40, correct=18, accuracy=0.450000
2025-09-13 22:36:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:36:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:36:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:36:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1752MB
2025-09-13 22:36:50 (federatedscope.llm.llm_local.client:501) INFO: Client 2 Adapter 1 with test results: {'test_total': 40, 'test_loss': 30.73987579345703, 'test_avg_loss': 0.7684968948364258, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-13 22:36:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-13 22:36:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:36:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=4, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:36:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-13 22:36:51 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=9.049733, avg_loss=0.822703, seen=11, correct=5, accuracy=0.454545
2025-09-13 22:36:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:36:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:36:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:36:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1752MB
2025-09-13 22:36:52 (federatedscope.llm.llm_local.client:480) INFO: Client 2 Adapter 2 with val results: {'val_total': 11, 'val_loss': 9.04973316192627, 'val_avg_loss': 0.82270301472057, 'val_seen': 11, 'val_correct': 5, 'val_acc': 0.45454545454545453}
2025-09-13 22:36:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:36:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:36:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:36:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:36:53 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=31.848263, avg_loss=0.796207, seen=40, correct=14, accuracy=0.350000
2025-09-13 22:36:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:36:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:36:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:36:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1752MB
2025-09-13 22:36:54 (federatedscope.llm.llm_local.client:501) INFO: Client 2 Adapter 2 with test results: {'test_total': 40, 'test_loss': 31.848262786865234, 'test_avg_loss': 0.7962065696716308, 'test_seen': 40, 'test_correct': 14, 'test_acc': 0.35}
2025-09-13 22:36:54 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-13 22:36:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-09-13 22:36:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:36:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=18, num_train_batch_last_epoch=10, num_train_epoch=6, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:36:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-09-13 22:36:56 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=36, loss_sum=30.553190, avg_loss=0.848700, seen=36, correct=16, accuracy=0.444444
2025-09-13 22:36:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:36:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:36:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:36:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1785MB
2025-09-13 22:36:57 (federatedscope.llm.llm_local.client:480) INFO: Client 3 Adapter 0 with val results: {'val_total': 36, 'val_loss': 30.553190231323242, 'val_avg_loss': 0.8486997286478678, 'val_seen': 36, 'val_correct': 16, 'val_acc': 0.4444444444444444}
2025-09-13 22:36:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:36:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:36:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:36:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:36:58 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.892632, avg_loss=0.747316, seen=40, correct=19, accuracy=0.475000
2025-09-13 22:36:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:36:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:36:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:36:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2052MB allocated=1785MB
2025-09-13 22:36:59 (federatedscope.llm.llm_local.client:501) INFO: Client 3 Adapter 0 with test results: {'test_total': 40, 'test_loss': 29.89263153076172, 'test_avg_loss': 0.747315788269043, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-13 22:37:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-09-13 22:37:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:37:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=18, num_train_batch_last_epoch=10, num_train_epoch=6, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:37:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-09-13 22:37:01 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=36, loss_sum=27.238159, avg_loss=0.756616, seen=36, correct=19, accuracy=0.527778
2025-09-13 22:37:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:37:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:37:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:37:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1760MB
2025-09-13 22:37:02 (federatedscope.llm.llm_local.client:480) INFO: Client 3 Adapter 1 with val results: {'val_total': 36, 'val_loss': 27.2381591796875, 'val_avg_loss': 0.7566155327690972, 'val_seen': 36, 'val_correct': 19, 'val_acc': 0.5277777777777778}
2025-09-13 22:37:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:37:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:37:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:37:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:37:04 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.169872, avg_loss=0.729247, seen=40, correct=21, accuracy=0.525000
2025-09-13 22:37:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:37:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:37:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:37:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1760MB
2025-09-13 22:37:05 (federatedscope.llm.llm_local.client:501) INFO: Client 3 Adapter 1 with test results: {'test_total': 40, 'test_loss': 29.169872283935547, 'test_avg_loss': 0.7292468070983886, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-13 22:37:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-09-13 22:37:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:37:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=18, num_train_batch_last_epoch=10, num_train_epoch=6, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:37:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-09-13 22:37:06 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=36, loss_sum=24.887300, avg_loss=0.691314, seen=36, correct=22, accuracy=0.611111
2025-09-13 22:37:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:37:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:37:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:37:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1760MB
2025-09-13 22:37:07 (federatedscope.llm.llm_local.client:480) INFO: Client 3 Adapter 2 with val results: {'val_total': 36, 'val_loss': 24.887300491333008, 'val_avg_loss': 0.691313902537028, 'val_seen': 36, 'val_correct': 22, 'val_acc': 0.6111111111111112}
2025-09-13 22:37:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:37:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:37:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:37:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:37:08 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.284481, avg_loss=0.682112, seen=40, correct=24, accuracy=0.600000
2025-09-13 22:37:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:37:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:37:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:37:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1760MB
2025-09-13 22:37:09 (federatedscope.llm.llm_local.client:501) INFO: Client 3 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.284481048583984, 'test_avg_loss': 0.6821120262145997, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-13 22:37:10 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-13 22:37:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-13 22:37:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:37:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=4, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:37:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-13 22:37:11 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=8.217402, avg_loss=0.747037, seen=11, correct=5, accuracy=0.454545
2025-09-13 22:37:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:37:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:37:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:37:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1794MB
2025-09-13 22:37:13 (federatedscope.llm.llm_local.client:480) INFO: Client 4 Adapter 0 with val results: {'val_total': 11, 'val_loss': 8.217401504516602, 'val_avg_loss': 0.7470365004106001, 'val_seen': 11, 'val_correct': 5, 'val_acc': 0.45454545454545453}
2025-09-13 22:37:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:37:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:37:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:37:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:37:14 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.739975, avg_loss=0.668499, seen=40, correct=25, accuracy=0.625000
2025-09-13 22:37:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:37:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:37:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:37:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1794MB
2025-09-13 22:37:15 (federatedscope.llm.llm_local.client:501) INFO: Client 4 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.739974975585938, 'test_avg_loss': 0.6684993743896485, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-13 22:37:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-13 22:37:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:37:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=4, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:37:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-13 22:37:16 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=6.469686, avg_loss=0.588153, seen=11, correct=9, accuracy=0.818182
2025-09-13 22:37:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:37:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:37:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:37:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1769MB
2025-09-13 22:37:17 (federatedscope.llm.llm_local.client:480) INFO: Client 4 Adapter 1 with val results: {'val_total': 11, 'val_loss': 6.4696855545043945, 'val_avg_loss': 0.5881532322276722, 'val_seen': 11, 'val_correct': 9, 'val_acc': 0.8181818181818182}
2025-09-13 22:37:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:37:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:37:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:37:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:37:19 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.768991, avg_loss=0.769225, seen=40, correct=18, accuracy=0.450000
2025-09-13 22:37:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:37:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:37:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:37:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1769MB
2025-09-13 22:37:20 (federatedscope.llm.llm_local.client:501) INFO: Client 4 Adapter 1 with test results: {'test_total': 40, 'test_loss': 30.768991470336914, 'test_avg_loss': 0.7692247867584229, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-13 22:37:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-13 22:37:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:37:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=4, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:37:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-13 22:37:21 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=7.838526, avg_loss=0.712593, seen=11, correct=6, accuracy=0.545455
2025-09-13 22:37:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:37:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:37:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:37:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1769MB
2025-09-13 22:37:22 (federatedscope.llm.llm_local.client:480) INFO: Client 4 Adapter 2 with val results: {'val_total': 11, 'val_loss': 7.838525772094727, 'val_avg_loss': 0.7125932520086115, 'val_seen': 11, 'val_correct': 6, 'val_acc': 0.5454545454545454}
2025-09-13 22:37:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:37:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:37:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:37:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:37:23 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.812405, avg_loss=0.745310, seen=40, correct=19, accuracy=0.475000
2025-09-13 22:37:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:37:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:37:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:37:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1769MB
2025-09-13 22:37:24 (federatedscope.llm.llm_local.client:501) INFO: Client 4 Adapter 2 with test results: {'test_total': 40, 'test_loss': 29.81240463256836, 'test_avg_loss': 0.745310115814209, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-13 22:37:24 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-13 22:37:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-09-13 22:37:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:37:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=7, num_train_batch_last_epoch=2, num_train_epoch=15, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:37:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-09-13 22:37:25 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=14, loss_sum=10.001717, avg_loss=0.714408, seen=14, correct=7, accuracy=0.500000
2025-09-13 22:37:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:37:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:37:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:37:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1802MB
2025-09-13 22:37:27 (federatedscope.llm.llm_local.client:480) INFO: Client 5 Adapter 0 with val results: {'val_total': 14, 'val_loss': 10.001716613769531, 'val_avg_loss': 0.7144083295549665, 'val_seen': 14, 'val_correct': 7, 'val_acc': 0.5}
2025-09-13 22:37:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:37:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:37:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:37:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:37:29 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.398792, avg_loss=0.684970, seen=40, correct=25, accuracy=0.625000
2025-09-13 22:37:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:37:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:37:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:37:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1802MB
2025-09-13 22:37:30 (federatedscope.llm.llm_local.client:501) INFO: Client 5 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.398792266845703, 'test_avg_loss': 0.6849698066711426, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-13 22:37:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-09-13 22:37:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:37:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=7, num_train_batch_last_epoch=2, num_train_epoch=15, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:37:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-09-13 22:37:31 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=14, loss_sum=9.137687, avg_loss=0.652692, seen=14, correct=9, accuracy=0.642857
2025-09-13 22:37:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:37:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:37:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:37:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1777MB
2025-09-13 22:37:32 (federatedscope.llm.llm_local.client:480) INFO: Client 5 Adapter 1 with val results: {'val_total': 14, 'val_loss': 9.137686729431152, 'val_avg_loss': 0.6526919092450824, 'val_seen': 14, 'val_correct': 9, 'val_acc': 0.6428571428571429}
2025-09-13 22:37:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:37:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:37:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:37:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:37:33 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.366022, avg_loss=0.609151, seen=40, correct=29, accuracy=0.725000
2025-09-13 22:37:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:37:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:37:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:37:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1777MB
2025-09-13 22:37:34 (federatedscope.llm.llm_local.client:501) INFO: Client 5 Adapter 1 with test results: {'test_total': 40, 'test_loss': 24.36602210998535, 'test_avg_loss': 0.6091505527496338, 'test_seen': 40, 'test_correct': 29, 'test_acc': 0.725}
2025-09-13 22:37:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-09-13 22:37:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:37:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=7, num_train_batch_last_epoch=2, num_train_epoch=15, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:37:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-09-13 22:37:36 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=14, loss_sum=9.728856, avg_loss=0.694918, seen=14, correct=8, accuracy=0.571429
2025-09-13 22:37:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:37:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:37:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:37:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1777MB
2025-09-13 22:37:37 (federatedscope.llm.llm_local.client:480) INFO: Client 5 Adapter 2 with val results: {'val_total': 14, 'val_loss': 9.728856086730957, 'val_avg_loss': 0.6949182919093541, 'val_seen': 14, 'val_correct': 8, 'val_acc': 0.5714285714285714}
2025-09-13 22:37:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:37:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:37:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:37:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:37:38 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.623112, avg_loss=0.665578, seen=40, correct=25, accuracy=0.625000
2025-09-13 22:37:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:37:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:37:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:37:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1777MB
2025-09-13 22:37:40 (federatedscope.llm.llm_local.client:501) INFO: Client 5 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.623111724853516, 'test_avg_loss': 0.6655777931213379, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-13 22:37:40 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-13 22:37:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-09-13 22:37:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:37:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=33, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:37:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-13 22:37:45 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=134, loss_sum=90.888481, avg_loss=0.678272, seen=134, correct=84, accuracy=0.626866
2025-09-13 22:37:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:37:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:37:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:37:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1811MB
2025-09-13 22:37:46 (federatedscope.llm.llm_local.client:480) INFO: Client 6 Adapter 0 with val results: {'val_total': 134, 'val_loss': 90.88848114013672, 'val_avg_loss': 0.6782722473144531, 'val_seen': 134, 'val_correct': 84, 'val_acc': 0.6268656716417911}
2025-09-13 22:37:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:37:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:37:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:37:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:37:47 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=32.059788, avg_loss=0.801495, seen=40, correct=18, accuracy=0.450000
2025-09-13 22:37:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:37:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:37:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:37:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1811MB
2025-09-13 22:37:49 (federatedscope.llm.llm_local.client:501) INFO: Client 6 Adapter 0 with test results: {'test_total': 40, 'test_loss': 32.05978775024414, 'test_avg_loss': 0.8014946937561035, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-13 22:37:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-09-13 22:37:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:37:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=33, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:37:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-13 22:37:53 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=134, loss_sum=86.744911, avg_loss=0.647350, seen=134, correct=83, accuracy=0.619403
2025-09-13 22:37:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:37:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:37:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:37:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1786MB
2025-09-13 22:37:54 (federatedscope.llm.llm_local.client:480) INFO: Client 6 Adapter 1 with val results: {'val_total': 134, 'val_loss': 86.74491119384766, 'val_avg_loss': 0.6473500835361765, 'val_seen': 134, 'val_correct': 83, 'val_acc': 0.6194029850746269}
2025-09-13 22:37:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:37:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:37:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:37:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:37:56 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=32.375751, avg_loss=0.809394, seen=40, correct=21, accuracy=0.525000
2025-09-13 22:37:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:37:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:37:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:37:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1786MB
2025-09-13 22:37:57 (federatedscope.llm.llm_local.client:501) INFO: Client 6 Adapter 1 with test results: {'test_total': 40, 'test_loss': 32.37575149536133, 'test_avg_loss': 0.8093937873840332, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-13 22:37:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-09-13 22:37:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:37:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=33, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:38:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-13 22:38:01 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=134, loss_sum=100.361679, avg_loss=0.748968, seen=134, correct=67, accuracy=0.500000
2025-09-13 22:38:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:38:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:38:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:38:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1786MB
2025-09-13 22:38:02 (federatedscope.llm.llm_local.client:480) INFO: Client 6 Adapter 2 with val results: {'val_total': 134, 'val_loss': 100.36167907714844, 'val_avg_loss': 0.7489677543070778, 'val_seen': 134, 'val_correct': 67, 'val_acc': 0.5}
2025-09-13 22:38:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:38:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:38:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:38:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:38:03 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.833590, avg_loss=0.770840, seen=40, correct=23, accuracy=0.575000
2025-09-13 22:38:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:38:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:38:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:38:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1786MB
2025-09-13 22:38:04 (federatedscope.llm.llm_local.client:501) INFO: Client 6 Adapter 2 with test results: {'test_total': 40, 'test_loss': 30.833589553833008, 'test_avg_loss': 0.7708397388458252, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-13 22:38:04 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-13 22:38:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-09-13 22:38:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:38:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=29, num_train_batch_last_epoch=13, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:38:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-09-13 22:38:07 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=57, loss_sum=37.233315, avg_loss=0.653216, seen=57, correct=34, accuracy=0.596491
2025-09-13 22:38:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:38:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:38:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:38:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1819MB
2025-09-13 22:38:08 (federatedscope.llm.llm_local.client:480) INFO: Client 7 Adapter 0 with val results: {'val_total': 57, 'val_loss': 37.233314514160156, 'val_avg_loss': 0.6532160441080729, 'val_seen': 57, 'val_correct': 34, 'val_acc': 0.5964912280701754}
2025-09-13 22:38:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:38:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:38:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:38:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:38:09 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.211777, avg_loss=0.580294, seen=40, correct=27, accuracy=0.675000
2025-09-13 22:38:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:38:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:38:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:38:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1819MB
2025-09-13 22:38:10 (federatedscope.llm.llm_local.client:501) INFO: Client 7 Adapter 0 with test results: {'test_total': 40, 'test_loss': 23.211776733398438, 'test_avg_loss': 0.580294418334961, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-13 22:38:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-09-13 22:38:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:38:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=29, num_train_batch_last_epoch=13, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:38:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-09-13 22:38:12 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=57, loss_sum=38.044731, avg_loss=0.667451, seen=57, correct=31, accuracy=0.543860
2025-09-13 22:38:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:38:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:38:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:38:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1794MB
2025-09-13 22:38:14 (federatedscope.llm.llm_local.client:480) INFO: Client 7 Adapter 1 with val results: {'val_total': 57, 'val_loss': 38.04473114013672, 'val_avg_loss': 0.6674514235111705, 'val_seen': 57, 'val_correct': 31, 'val_acc': 0.543859649122807}
2025-09-13 22:38:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:38:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:38:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:38:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:38:15 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.119394, avg_loss=0.652985, seen=40, correct=23, accuracy=0.575000
2025-09-13 22:38:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:38:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:38:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:38:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1794MB
2025-09-13 22:38:16 (federatedscope.llm.llm_local.client:501) INFO: Client 7 Adapter 1 with test results: {'test_total': 40, 'test_loss': 26.119394302368164, 'test_avg_loss': 0.6529848575592041, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-13 22:38:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-09-13 22:38:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:38:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=29, num_train_batch_last_epoch=13, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:38:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-09-13 22:38:18 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=57, loss_sum=39.093658, avg_loss=0.685854, seen=57, correct=26, accuracy=0.456140
2025-09-13 22:38:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:38:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:38:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:38:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1794MB
2025-09-13 22:38:20 (federatedscope.llm.llm_local.client:480) INFO: Client 7 Adapter 2 with val results: {'val_total': 57, 'val_loss': 39.093658447265625, 'val_avg_loss': 0.6858536569695723, 'val_seen': 57, 'val_correct': 26, 'val_acc': 0.45614035087719296}
2025-09-13 22:38:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:38:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:38:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:38:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:38:21 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.539955, avg_loss=0.688499, seen=40, correct=22, accuracy=0.550000
2025-09-13 22:38:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:38:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:38:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:38:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1794MB
2025-09-13 22:38:22 (federatedscope.llm.llm_local.client:501) INFO: Client 7 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.539955139160156, 'test_avg_loss': 0.6884988784790039, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-13 22:38:22 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-13 22:38:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-09-13 22:38:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:38:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=35, num_train_batch_last_epoch=30, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:38:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-13 22:38:25 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=69, loss_sum=45.205505, avg_loss=0.655152, seen=69, correct=42, accuracy=0.608696
2025-09-13 22:38:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:38:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:38:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:38:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1828MB
2025-09-13 22:38:26 (federatedscope.llm.llm_local.client:480) INFO: Client 8 Adapter 0 with val results: {'val_total': 69, 'val_loss': 45.20550537109375, 'val_avg_loss': 0.6551522517549819, 'val_seen': 69, 'val_correct': 42, 'val_acc': 0.6086956521739131}
2025-09-13 22:38:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:38:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:38:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:38:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:38:28 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.922138, avg_loss=0.773053, seen=40, correct=20, accuracy=0.500000
2025-09-13 22:38:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:38:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:38:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:38:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1828MB
2025-09-13 22:38:29 (federatedscope.llm.llm_local.client:501) INFO: Client 8 Adapter 0 with test results: {'test_total': 40, 'test_loss': 30.922138214111328, 'test_avg_loss': 0.7730534553527832, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-13 22:38:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-09-13 22:38:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:38:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=35, num_train_batch_last_epoch=30, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:38:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-13 22:38:31 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=69, loss_sum=50.879860, avg_loss=0.737389, seen=69, correct=35, accuracy=0.507246
2025-09-13 22:38:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:38:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:38:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:38:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1803MB
2025-09-13 22:38:32 (federatedscope.llm.llm_local.client:480) INFO: Client 8 Adapter 1 with val results: {'val_total': 69, 'val_loss': 50.879859924316406, 'val_avg_loss': 0.7373892742654552, 'val_seen': 69, 'val_correct': 35, 'val_acc': 0.5072463768115942}
2025-09-13 22:38:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:38:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:38:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:38:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:38:35 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.078693, avg_loss=0.751967, seen=40, correct=19, accuracy=0.475000
2025-09-13 22:38:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:38:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:38:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:38:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1803MB
2025-09-13 22:38:36 (federatedscope.llm.llm_local.client:501) INFO: Client 8 Adapter 1 with test results: {'test_total': 40, 'test_loss': 30.078693389892578, 'test_avg_loss': 0.7519673347473145, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-13 22:38:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-09-13 22:38:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:38:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=35, num_train_batch_last_epoch=30, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:38:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-13 22:38:40 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=69, loss_sum=49.024086, avg_loss=0.710494, seen=69, correct=40, accuracy=0.579710
2025-09-13 22:38:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:38:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:38:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:38:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1803MB
2025-09-13 22:38:41 (federatedscope.llm.llm_local.client:480) INFO: Client 8 Adapter 2 with val results: {'val_total': 69, 'val_loss': 49.024085998535156, 'val_avg_loss': 0.7104939999787704, 'val_seen': 69, 'val_correct': 40, 'val_acc': 0.5797101449275363}
2025-09-13 22:38:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:38:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:38:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:38:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:38:43 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.688993, avg_loss=0.742225, seen=40, correct=19, accuracy=0.475000
2025-09-13 22:38:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:38:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:38:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:38:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1803MB
2025-09-13 22:38:44 (federatedscope.llm.llm_local.client:501) INFO: Client 8 Adapter 2 with test results: {'test_total': 40, 'test_loss': 29.688993453979492, 'test_avg_loss': 0.7422248363494873, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-13 22:38:44 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-13 22:38:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-13 22:38:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:38:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=6, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:38:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-13 22:38:50 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=125.062744, avg_loss=0.665227, seen=188, correct=112, accuracy=0.595745
2025-09-13 22:38:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:38:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:38:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:38:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1836MB
2025-09-13 22:38:51 (federatedscope.llm.llm_local.client:480) INFO: Client 9 Adapter 0 with val results: {'val_total': 188, 'val_loss': 125.062744140625, 'val_avg_loss': 0.665227362450133, 'val_seen': 188, 'val_correct': 112, 'val_acc': 0.5957446808510638}
2025-09-13 22:38:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:38:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:38:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:38:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:38:53 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.478233, avg_loss=0.661956, seen=40, correct=26, accuracy=0.650000
2025-09-13 22:38:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:38:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:38:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:38:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1836MB
2025-09-13 22:38:54 (federatedscope.llm.llm_local.client:501) INFO: Client 9 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.478233337402344, 'test_avg_loss': 0.6619558334350586, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-13 22:38:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-13 22:38:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:38:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=6, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:39:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-13 22:39:00 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=128.250107, avg_loss=0.682181, seen=188, correct=111, accuracy=0.590426
2025-09-13 22:39:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:39:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:39:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:39:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1811MB
2025-09-13 22:39:01 (federatedscope.llm.llm_local.client:480) INFO: Client 9 Adapter 1 with val results: {'val_total': 188, 'val_loss': 128.25010681152344, 'val_avg_loss': 0.682181419210231, 'val_seen': 188, 'val_correct': 111, 'val_acc': 0.5904255319148937}
2025-09-13 22:39:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:39:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:39:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:39:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:39:02 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.879795, avg_loss=0.721995, seen=40, correct=18, accuracy=0.450000
2025-09-13 22:39:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:39:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:39:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:39:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1811MB
2025-09-13 22:39:03 (federatedscope.llm.llm_local.client:501) INFO: Client 9 Adapter 1 with test results: {'test_total': 40, 'test_loss': 28.87979507446289, 'test_avg_loss': 0.7219948768615723, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-13 22:39:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-13 22:39:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:39:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=6, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:39:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-13 22:39:09 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=136.963516, avg_loss=0.728529, seen=188, correct=97, accuracy=0.515957
2025-09-13 22:39:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:39:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:39:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:39:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1811MB
2025-09-13 22:39:11 (federatedscope.llm.llm_local.client:480) INFO: Client 9 Adapter 2 with val results: {'val_total': 188, 'val_loss': 136.96351623535156, 'val_avg_loss': 0.7285293416774019, 'val_seen': 188, 'val_correct': 97, 'val_acc': 0.5159574468085106}
2025-09-13 22:39:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:39:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:39:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:39:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:39:12 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.842932, avg_loss=0.746073, seen=40, correct=20, accuracy=0.500000
2025-09-13 22:39:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:39:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:39:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:39:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1811MB
2025-09-13 22:39:13 (federatedscope.llm.llm_local.client:501) INFO: Client 9 Adapter 2 with test results: {'test_total': 40, 'test_loss': 29.842931747436523, 'test_avg_loss': 0.7460732936859131, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-13 22:39:13 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-13 22:39:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-09-13 22:39:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:39:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=32, num_train_batch_last_epoch=4, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:39:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-09-13 22:39:15 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=63, loss_sum=39.871059, avg_loss=0.632874, seen=63, correct=41, accuracy=0.650794
2025-09-13 22:39:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:39:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:39:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:39:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1845MB
2025-09-13 22:39:16 (federatedscope.llm.llm_local.client:480) INFO: Client 10 Adapter 0 with val results: {'val_total': 63, 'val_loss': 39.87105941772461, 'val_avg_loss': 0.6328739590115018, 'val_seen': 63, 'val_correct': 41, 'val_acc': 0.6507936507936508}
2025-09-13 22:39:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:39:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:39:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:39:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:39:18 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=22.495314, avg_loss=0.562383, seen=40, correct=30, accuracy=0.750000
2025-09-13 22:39:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:39:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:39:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:39:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1845MB
2025-09-13 22:39:19 (federatedscope.llm.llm_local.client:501) INFO: Client 10 Adapter 0 with test results: {'test_total': 40, 'test_loss': 22.49531364440918, 'test_avg_loss': 0.5623828411102295, 'test_seen': 40, 'test_correct': 30, 'test_acc': 0.75}
2025-09-13 22:39:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-09-13 22:39:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:39:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=32, num_train_batch_last_epoch=4, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:39:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-09-13 22:39:21 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=63, loss_sum=46.093513, avg_loss=0.731643, seen=63, correct=35, accuracy=0.555556
2025-09-13 22:39:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:39:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:39:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:39:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1820MB
2025-09-13 22:39:22 (federatedscope.llm.llm_local.client:480) INFO: Client 10 Adapter 1 with val results: {'val_total': 63, 'val_loss': 46.09351348876953, 'val_avg_loss': 0.73164307125031, 'val_seen': 63, 'val_correct': 35, 'val_acc': 0.5555555555555556}
2025-09-13 22:39:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:39:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:39:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:39:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:39:24 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.734718, avg_loss=0.618368, seen=40, correct=25, accuracy=0.625000
2025-09-13 22:39:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:39:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:39:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:39:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1820MB
2025-09-13 22:39:25 (federatedscope.llm.llm_local.client:501) INFO: Client 10 Adapter 1 with test results: {'test_total': 40, 'test_loss': 24.734718322753906, 'test_avg_loss': 0.6183679580688477, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-13 22:39:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-09-13 22:39:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:39:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=32, num_train_batch_last_epoch=4, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:39:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-09-13 22:39:27 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=63, loss_sum=51.054489, avg_loss=0.810389, seen=63, correct=26, accuracy=0.412698
2025-09-13 22:39:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:39:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:39:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:39:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1820MB
2025-09-13 22:39:28 (federatedscope.llm.llm_local.client:480) INFO: Client 10 Adapter 2 with val results: {'val_total': 63, 'val_loss': 51.05448913574219, 'val_avg_loss': 0.8103887164403522, 'val_seen': 63, 'val_correct': 26, 'val_acc': 0.4126984126984127}
2025-09-13 22:39:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:39:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:39:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:39:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:39:30 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.726551, avg_loss=0.693164, seen=40, correct=21, accuracy=0.525000
2025-09-13 22:39:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:39:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:39:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:39:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1820MB
2025-09-13 22:39:31 (federatedscope.llm.llm_local.client:501) INFO: Client 10 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.726551055908203, 'test_avg_loss': 0.6931637763977051, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-13 22:39:31 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-13 22:39:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-09-13 22:39:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:39:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=16, num_train_batch_last_epoch=4, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:39:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-13 22:39:33 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=32, loss_sum=19.872738, avg_loss=0.621023, seen=32, correct=20, accuracy=0.625000
2025-09-13 22:39:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:39:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:39:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:39:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1853MB
2025-09-13 22:39:35 (federatedscope.llm.llm_local.client:480) INFO: Client 11 Adapter 0 with val results: {'val_total': 32, 'val_loss': 19.872737884521484, 'val_avg_loss': 0.6210230588912964, 'val_seen': 32, 'val_correct': 20, 'val_acc': 0.625}
2025-09-13 22:39:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:39:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:39:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:39:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:39:36 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.140957, avg_loss=0.628524, seen=40, correct=22, accuracy=0.550000
2025-09-13 22:39:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:39:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:39:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:39:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1853MB
2025-09-13 22:39:37 (federatedscope.llm.llm_local.client:501) INFO: Client 11 Adapter 0 with test results: {'test_total': 40, 'test_loss': 25.14095687866211, 'test_avg_loss': 0.6285239219665527, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-13 22:39:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-09-13 22:39:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:39:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=16, num_train_batch_last_epoch=4, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:39:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-13 22:39:39 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=32, loss_sum=21.075325, avg_loss=0.658604, seen=32, correct=17, accuracy=0.531250
2025-09-13 22:39:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:39:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:39:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:39:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1828MB
2025-09-13 22:39:40 (federatedscope.llm.llm_local.client:480) INFO: Client 11 Adapter 1 with val results: {'val_total': 32, 'val_loss': 21.07532501220703, 'val_avg_loss': 0.6586039066314697, 'val_seen': 32, 'val_correct': 17, 'val_acc': 0.53125}
2025-09-13 22:39:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:39:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:39:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:39:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:39:41 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.262573, avg_loss=0.756564, seen=40, correct=20, accuracy=0.500000
2025-09-13 22:39:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:39:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:39:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:39:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1828MB
2025-09-13 22:39:42 (federatedscope.llm.llm_local.client:501) INFO: Client 11 Adapter 1 with test results: {'test_total': 40, 'test_loss': 30.2625732421875, 'test_avg_loss': 0.7565643310546875, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-13 22:39:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-09-13 22:39:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:39:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=16, num_train_batch_last_epoch=4, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:39:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-13 22:39:43 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=32, loss_sum=22.981369, avg_loss=0.718168, seen=32, correct=17, accuracy=0.531250
2025-09-13 22:39:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:39:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:39:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:39:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1828MB
2025-09-13 22:39:45 (federatedscope.llm.llm_local.client:480) INFO: Client 11 Adapter 2 with val results: {'val_total': 32, 'val_loss': 22.981369018554688, 'val_avg_loss': 0.718167781829834, 'val_seen': 32, 'val_correct': 17, 'val_acc': 0.53125}
2025-09-13 22:39:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:39:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:39:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:39:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:39:47 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.284948, avg_loss=0.732124, seen=40, correct=17, accuracy=0.425000
2025-09-13 22:39:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:39:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:39:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:39:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1828MB
2025-09-13 22:39:48 (federatedscope.llm.llm_local.client:501) INFO: Client 11 Adapter 2 with test results: {'test_total': 40, 'test_loss': 29.284948348999023, 'test_avg_loss': 0.7321237087249756, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}
2025-09-13 22:39:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-13 22:39:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-09-13 22:39:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:39:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=69, num_train_batch_last_epoch=31, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:39:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-09-13 22:39:52 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=137, loss_sum=90.848152, avg_loss=0.663125, seen=137, correct=83, accuracy=0.605839
2025-09-13 22:39:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:39:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:39:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:39:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1862MB
2025-09-13 22:39:54 (federatedscope.llm.llm_local.client:480) INFO: Client 12 Adapter 0 with val results: {'val_total': 137, 'val_loss': 90.84815216064453, 'val_avg_loss': 0.6631251982528797, 'val_seen': 137, 'val_correct': 83, 'val_acc': 0.6058394160583942}
2025-09-13 22:39:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:39:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:39:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:39:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:39:55 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.165676, avg_loss=0.629142, seen=40, correct=26, accuracy=0.650000
2025-09-13 22:39:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:39:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:39:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:39:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1862MB
2025-09-13 22:39:56 (federatedscope.llm.llm_local.client:501) INFO: Client 12 Adapter 0 with test results: {'test_total': 40, 'test_loss': 25.16567611694336, 'test_avg_loss': 0.629141902923584, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-13 22:39:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-09-13 22:39:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:39:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=69, num_train_batch_last_epoch=31, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:40:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-09-13 22:40:00 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=137, loss_sum=91.258629, avg_loss=0.666121, seen=137, correct=84, accuracy=0.613139
2025-09-13 22:40:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:40:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:40:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:40:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1837MB
2025-09-13 22:40:01 (federatedscope.llm.llm_local.client:480) INFO: Client 12 Adapter 1 with val results: {'val_total': 137, 'val_loss': 91.25862884521484, 'val_avg_loss': 0.6661213784322252, 'val_seen': 137, 'val_correct': 84, 'val_acc': 0.6131386861313869}
2025-09-13 22:40:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:40:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:40:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:40:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:40:02 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.024437, avg_loss=0.650611, seen=40, correct=23, accuracy=0.575000
2025-09-13 22:40:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:40:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:40:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:40:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1837MB
2025-09-13 22:40:04 (federatedscope.llm.llm_local.client:501) INFO: Client 12 Adapter 1 with test results: {'test_total': 40, 'test_loss': 26.024436950683594, 'test_avg_loss': 0.6506109237670898, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-13 22:40:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-09-13 22:40:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:40:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=69, num_train_batch_last_epoch=31, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:40:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-09-13 22:40:07 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=137, loss_sum=96.488243, avg_loss=0.704294, seen=137, correct=76, accuracy=0.554745
2025-09-13 22:40:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:40:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:40:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:40:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2052MB allocated=1837MB
2025-09-13 22:40:09 (federatedscope.llm.llm_local.client:480) INFO: Client 12 Adapter 2 with val results: {'val_total': 137, 'val_loss': 96.48824310302734, 'val_avg_loss': 0.7042937452775718, 'val_seen': 137, 'val_correct': 76, 'val_acc': 0.5547445255474452}
2025-09-13 22:40:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:40:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:40:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:40:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:40:10 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.267950, avg_loss=0.706699, seen=40, correct=20, accuracy=0.500000
2025-09-13 22:40:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:40:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:40:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:40:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1837MB
2025-09-13 22:40:12 (federatedscope.llm.llm_local.client:501) INFO: Client 12 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.2679500579834, 'test_avg_loss': 0.7066987514495849, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-13 22:40:12 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-13 22:40:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-09-13 22:40:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:40:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=36, num_train_batch_last_epoch=28, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:40:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-13 22:40:14 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=72, loss_sum=48.211880, avg_loss=0.669609, seen=72, correct=40, accuracy=0.555556
2025-09-13 22:40:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:40:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:40:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:40:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1870MB
2025-09-13 22:40:15 (federatedscope.llm.llm_local.client:480) INFO: Client 13 Adapter 0 with val results: {'val_total': 72, 'val_loss': 48.21187973022461, 'val_avg_loss': 0.669609440697564, 'val_seen': 72, 'val_correct': 40, 'val_acc': 0.5555555555555556}
2025-09-13 22:40:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:40:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:40:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:40:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:40:17 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.244825, avg_loss=0.656121, seen=40, correct=23, accuracy=0.575000
2025-09-13 22:40:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:40:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:40:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:40:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1870MB
2025-09-13 22:40:18 (federatedscope.llm.llm_local.client:501) INFO: Client 13 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.24482536315918, 'test_avg_loss': 0.6561206340789795, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-13 22:40:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-09-13 22:40:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:40:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=36, num_train_batch_last_epoch=28, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:40:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-13 22:40:20 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=72, loss_sum=47.685097, avg_loss=0.662293, seen=72, correct=48, accuracy=0.666667
2025-09-13 22:40:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:40:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:40:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:40:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1845MB
2025-09-13 22:40:21 (federatedscope.llm.llm_local.client:480) INFO: Client 13 Adapter 1 with val results: {'val_total': 72, 'val_loss': 47.685096740722656, 'val_avg_loss': 0.6622930102878146, 'val_seen': 72, 'val_correct': 48, 'val_acc': 0.6666666666666666}
2025-09-13 22:40:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:40:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:40:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:40:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:40:23 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.354694, avg_loss=0.733867, seen=40, correct=25, accuracy=0.625000
2025-09-13 22:40:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:40:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:40:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:40:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1845MB
2025-09-13 22:40:24 (federatedscope.llm.llm_local.client:501) INFO: Client 13 Adapter 1 with test results: {'test_total': 40, 'test_loss': 29.354694366455078, 'test_avg_loss': 0.7338673591613769, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-13 22:40:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-09-13 22:40:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:40:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=36, num_train_batch_last_epoch=28, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:40:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-13 22:40:26 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=72, loss_sum=51.193703, avg_loss=0.711024, seen=72, correct=37, accuracy=0.513889
2025-09-13 22:40:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:40:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:40:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:40:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1845MB
2025-09-13 22:40:28 (federatedscope.llm.llm_local.client:480) INFO: Client 13 Adapter 2 with val results: {'val_total': 72, 'val_loss': 51.193702697753906, 'val_avg_loss': 0.7110236485799154, 'val_seen': 72, 'val_correct': 37, 'val_acc': 0.5138888888888888}
2025-09-13 22:40:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:40:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:40:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:40:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:40:29 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.138718, avg_loss=0.753468, seen=40, correct=20, accuracy=0.500000
2025-09-13 22:40:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:40:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:40:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:40:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1845MB
2025-09-13 22:40:30 (federatedscope.llm.llm_local.client:501) INFO: Client 13 Adapter 2 with test results: {'test_total': 40, 'test_loss': 30.138717651367188, 'test_avg_loss': 0.7534679412841797, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-13 22:40:30 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-13 22:40:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-09-13 22:40:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:40:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=80, num_train_batch_last_epoch=20, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:40:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-09-13 22:40:35 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=160, loss_sum=106.404076, avg_loss=0.665025, seen=160, correct=101, accuracy=0.631250
2025-09-13 22:40:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:40:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:40:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:40:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1879MB
2025-09-13 22:40:36 (federatedscope.llm.llm_local.client:480) INFO: Client 14 Adapter 0 with val results: {'val_total': 160, 'val_loss': 106.4040756225586, 'val_avg_loss': 0.6650254726409912, 'val_seen': 160, 'val_correct': 101, 'val_acc': 0.63125}
2025-09-13 22:40:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:40:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:40:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:40:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:40:38 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.075207, avg_loss=0.601880, seen=40, correct=29, accuracy=0.725000
2025-09-13 22:40:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:40:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:40:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:40:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1879MB
2025-09-13 22:40:39 (federatedscope.llm.llm_local.client:501) INFO: Client 14 Adapter 0 with test results: {'test_total': 40, 'test_loss': 24.075206756591797, 'test_avg_loss': 0.6018801689147949, 'test_seen': 40, 'test_correct': 29, 'test_acc': 0.725}
2025-09-13 22:40:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-09-13 22:40:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:40:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=80, num_train_batch_last_epoch=20, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:40:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-09-13 22:40:43 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=160, loss_sum=109.722717, avg_loss=0.685767, seen=160, correct=93, accuracy=0.581250
2025-09-13 22:40:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:40:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:40:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:40:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1853MB
2025-09-13 22:40:45 (federatedscope.llm.llm_local.client:480) INFO: Client 14 Adapter 1 with val results: {'val_total': 160, 'val_loss': 109.72271728515625, 'val_avg_loss': 0.6857669830322266, 'val_seen': 160, 'val_correct': 93, 'val_acc': 0.58125}
2025-09-13 22:40:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:40:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:40:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:40:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:40:46 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.529572, avg_loss=0.663239, seen=40, correct=24, accuracy=0.600000
2025-09-13 22:40:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:40:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:40:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:40:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1853MB
2025-09-13 22:40:48 (federatedscope.llm.llm_local.client:501) INFO: Client 14 Adapter 1 with test results: {'test_total': 40, 'test_loss': 26.529571533203125, 'test_avg_loss': 0.6632392883300782, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-13 22:40:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-09-13 22:40:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:40:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=80, num_train_batch_last_epoch=20, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:40:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-09-13 22:40:52 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=160, loss_sum=112.264809, avg_loss=0.701655, seen=160, correct=89, accuracy=0.556250
2025-09-13 22:40:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:40:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:40:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:40:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1853MB
2025-09-13 22:40:53 (federatedscope.llm.llm_local.client:480) INFO: Client 14 Adapter 2 with val results: {'val_total': 160, 'val_loss': 112.26480865478516, 'val_avg_loss': 0.7016550540924072, 'val_seen': 160, 'val_correct': 89, 'val_acc': 0.55625}
2025-09-13 22:40:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:40:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:40:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:40:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:40:55 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.020016, avg_loss=0.725500, seen=40, correct=21, accuracy=0.525000
2025-09-13 22:40:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:40:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:40:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:40:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1853MB
2025-09-13 22:40:56 (federatedscope.llm.llm_local.client:501) INFO: Client 14 Adapter 2 with test results: {'test_total': 40, 'test_loss': 29.020015716552734, 'test_avg_loss': 0.7255003929138184, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-13 22:40:56 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-13 22:40:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 22:40:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:40:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:41:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 22:41:02 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=128.530457, avg_loss=0.642652, seen=200, correct=119, accuracy=0.595000
2025-09-13 22:41:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:41:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:41:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:41:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1887MB
2025-09-13 22:41:04 (federatedscope.llm.llm_local.client:480) INFO: Client 15 Adapter 0 with val results: {'val_total': 200, 'val_loss': 128.53045654296875, 'val_avg_loss': 0.6426522827148438, 'val_seen': 200, 'val_correct': 119, 'val_acc': 0.595}
2025-09-13 22:41:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:41:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:41:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:41:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:41:06 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.782816, avg_loss=0.694570, seen=40, correct=24, accuracy=0.600000
2025-09-13 22:41:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:41:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:41:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:41:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1887MB
2025-09-13 22:41:07 (federatedscope.llm.llm_local.client:501) INFO: Client 15 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.78281593322754, 'test_avg_loss': 0.6945703983306885, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-13 22:41:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 22:41:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:41:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:41:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 22:41:13 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=143.589737, avg_loss=0.717949, seen=200, correct=111, accuracy=0.555000
2025-09-13 22:41:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:41:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:41:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:41:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1862MB
2025-09-13 22:41:15 (federatedscope.llm.llm_local.client:480) INFO: Client 15 Adapter 1 with val results: {'val_total': 200, 'val_loss': 143.58973693847656, 'val_avg_loss': 0.7179486846923828, 'val_seen': 200, 'val_correct': 111, 'val_acc': 0.555}
2025-09-13 22:41:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:41:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:41:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:41:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:41:16 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.799725, avg_loss=0.644993, seen=40, correct=27, accuracy=0.675000
2025-09-13 22:41:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:41:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:41:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:41:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1862MB
2025-09-13 22:41:17 (federatedscope.llm.llm_local.client:501) INFO: Client 15 Adapter 1 with test results: {'test_total': 40, 'test_loss': 25.799724578857422, 'test_avg_loss': 0.6449931144714356, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-13 22:41:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 22:41:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:41:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:41:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 22:41:23 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=151.443451, avg_loss=0.757217, seen=200, correct=97, accuracy=0.485000
2025-09-13 22:41:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:41:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:41:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:41:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1862MB
2025-09-13 22:41:25 (federatedscope.llm.llm_local.client:480) INFO: Client 15 Adapter 2 with val results: {'val_total': 200, 'val_loss': 151.44345092773438, 'val_avg_loss': 0.7572172546386718, 'val_seen': 200, 'val_correct': 97, 'val_acc': 0.485}
2025-09-13 22:41:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:41:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:41:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:41:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:41:26 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.751192, avg_loss=0.668780, seen=40, correct=24, accuracy=0.600000
2025-09-13 22:41:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:41:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:41:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:41:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1862MB
2025-09-13 22:41:28 (federatedscope.llm.llm_local.client:501) INFO: Client 15 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.751192092895508, 'test_avg_loss': 0.6687798023223877, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-13 22:41:28 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-13 22:41:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-09-13 22:41:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:41:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=32, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:41:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-13 22:41:32 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=136, loss_sum=87.945442, avg_loss=0.646658, seen=136, correct=91, accuracy=0.669118
2025-09-13 22:41:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:41:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:41:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:41:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1896MB
2025-09-13 22:41:33 (federatedscope.llm.llm_local.client:480) INFO: Client 16 Adapter 0 with val results: {'val_total': 136, 'val_loss': 87.94544219970703, 'val_avg_loss': 0.6466576632331399, 'val_seen': 136, 'val_correct': 91, 'val_acc': 0.6691176470588235}
2025-09-13 22:41:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:41:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:41:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:41:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:41:35 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.559889, avg_loss=0.588997, seen=40, correct=26, accuracy=0.650000
2025-09-13 22:41:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:41:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:41:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:41:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1896MB
2025-09-13 22:41:36 (federatedscope.llm.llm_local.client:501) INFO: Client 16 Adapter 0 with test results: {'test_total': 40, 'test_loss': 23.55988883972168, 'test_avg_loss': 0.588997220993042, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-13 22:41:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-09-13 22:41:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:41:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=32, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:41:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-13 22:41:40 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=136, loss_sum=88.685684, avg_loss=0.652101, seen=136, correct=87, accuracy=0.639706
2025-09-13 22:41:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:41:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:41:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:41:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1870MB
2025-09-13 22:41:41 (federatedscope.llm.llm_local.client:480) INFO: Client 16 Adapter 1 with val results: {'val_total': 136, 'val_loss': 88.68568420410156, 'val_avg_loss': 0.6521006191478056, 'val_seen': 136, 'val_correct': 87, 'val_acc': 0.6397058823529411}
2025-09-13 22:41:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:41:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:41:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:41:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:41:43 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.230038, avg_loss=0.630751, seen=40, correct=25, accuracy=0.625000
2025-09-13 22:41:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:41:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:41:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:41:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1870MB
2025-09-13 22:41:44 (federatedscope.llm.llm_local.client:501) INFO: Client 16 Adapter 1 with test results: {'test_total': 40, 'test_loss': 25.230037689208984, 'test_avg_loss': 0.6307509422302247, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-13 22:41:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-09-13 22:41:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:41:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=32, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:41:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-13 22:41:47 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=136, loss_sum=92.243744, avg_loss=0.678263, seen=136, correct=82, accuracy=0.602941
2025-09-13 22:41:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:41:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:41:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:41:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1870MB
2025-09-13 22:41:49 (federatedscope.llm.llm_local.client:480) INFO: Client 16 Adapter 2 with val results: {'val_total': 136, 'val_loss': 92.24374389648438, 'val_avg_loss': 0.6782628227682674, 'val_seen': 136, 'val_correct': 82, 'val_acc': 0.6029411764705882}
2025-09-13 22:41:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:41:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:41:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:41:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:41:51 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.364260, avg_loss=0.684106, seen=40, correct=25, accuracy=0.625000
2025-09-13 22:41:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:41:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:41:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:41:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1870MB
2025-09-13 22:41:52 (federatedscope.llm.llm_local.client:501) INFO: Client 16 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.364259719848633, 'test_avg_loss': 0.6841064929962158, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-13 22:41:52 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-13 22:41:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 22:41:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:41:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:41:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 22:41:58 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=124.932602, avg_loss=0.624663, seen=200, correct=134, accuracy=0.670000
2025-09-13 22:41:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:41:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:41:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:42:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1904MB
2025-09-13 22:42:00 (federatedscope.llm.llm_local.client:480) INFO: Client 17 Adapter 0 with val results: {'val_total': 200, 'val_loss': 124.93260192871094, 'val_avg_loss': 0.6246630096435547, 'val_seen': 200, 'val_correct': 134, 'val_acc': 0.67}
2025-09-13 22:42:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:42:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:42:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:42:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:42:01 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.569347, avg_loss=0.664234, seen=40, correct=27, accuracy=0.675000
2025-09-13 22:42:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:42:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:42:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:42:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1904MB
2025-09-13 22:42:02 (federatedscope.llm.llm_local.client:501) INFO: Client 17 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.569347381591797, 'test_avg_loss': 0.6642336845397949, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-13 22:42:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 22:42:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:42:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:42:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 22:42:09 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=129.551178, avg_loss=0.647756, seen=200, correct=124, accuracy=0.620000
2025-09-13 22:42:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:42:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:42:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:42:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1879MB
2025-09-13 22:42:10 (federatedscope.llm.llm_local.client:480) INFO: Client 17 Adapter 1 with val results: {'val_total': 200, 'val_loss': 129.55117797851562, 'val_avg_loss': 0.6477558898925782, 'val_seen': 200, 'val_correct': 124, 'val_acc': 0.62}
2025-09-13 22:42:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:42:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:42:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:42:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:42:12 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.256126, avg_loss=0.731403, seen=40, correct=24, accuracy=0.600000
2025-09-13 22:42:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:42:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:42:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:42:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1879MB
2025-09-13 22:42:13 (federatedscope.llm.llm_local.client:501) INFO: Client 17 Adapter 1 with test results: {'test_total': 40, 'test_loss': 29.256126403808594, 'test_avg_loss': 0.7314031600952149, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-13 22:42:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 22:42:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:42:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:42:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 22:42:19 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=138.408813, avg_loss=0.692044, seen=200, correct=116, accuracy=0.580000
2025-09-13 22:42:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:42:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:42:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:42:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1879MB
2025-09-13 22:42:20 (federatedscope.llm.llm_local.client:480) INFO: Client 17 Adapter 2 with val results: {'val_total': 200, 'val_loss': 138.4088134765625, 'val_avg_loss': 0.6920440673828125, 'val_seen': 200, 'val_correct': 116, 'val_acc': 0.58}
2025-09-13 22:42:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:42:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:42:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:42:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:42:22 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.864765, avg_loss=0.746619, seen=40, correct=20, accuracy=0.500000
2025-09-13 22:42:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:42:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:42:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:42:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1879MB
2025-09-13 22:42:23 (federatedscope.llm.llm_local.client:501) INFO: Client 17 Adapter 2 with test results: {'test_total': 40, 'test_loss': 29.864765167236328, 'test_avg_loss': 0.7466191291809082, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-13 22:42:23 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-13 22:42:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-09-13 22:42:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:42:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=32, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:42:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-13 22:42:27 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=135, loss_sum=92.230881, avg_loss=0.683192, seen=135, correct=87, accuracy=0.644444
2025-09-13 22:42:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:42:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:42:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:42:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1913MB
2025-09-13 22:42:29 (federatedscope.llm.llm_local.client:480) INFO: Client 18 Adapter 0 with val results: {'val_total': 135, 'val_loss': 92.23088073730469, 'val_avg_loss': 0.6831917091652199, 'val_seen': 135, 'val_correct': 87, 'val_acc': 0.6444444444444445}
2025-09-13 22:42:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:42:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:42:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:42:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:42:30 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.125307, avg_loss=0.578133, seen=40, correct=28, accuracy=0.700000
2025-09-13 22:42:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:42:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:42:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:42:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1913MB
2025-09-13 22:42:32 (federatedscope.llm.llm_local.client:501) INFO: Client 18 Adapter 0 with test results: {'test_total': 40, 'test_loss': 23.125307083129883, 'test_avg_loss': 0.5781326770782471, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}
2025-09-13 22:42:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-09-13 22:42:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:42:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=32, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:42:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-13 22:42:36 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=135, loss_sum=96.689186, avg_loss=0.716216, seen=135, correct=72, accuracy=0.533333
2025-09-13 22:42:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:42:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:42:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:42:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1888MB
2025-09-13 22:42:38 (federatedscope.llm.llm_local.client:480) INFO: Client 18 Adapter 1 with val results: {'val_total': 135, 'val_loss': 96.6891860961914, 'val_avg_loss': 0.7162161933051215, 'val_seen': 135, 'val_correct': 72, 'val_acc': 0.5333333333333333}
2025-09-13 22:42:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:42:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:42:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:42:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:42:39 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.537102, avg_loss=0.763428, seen=40, correct=16, accuracy=0.400000
2025-09-13 22:42:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:42:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:42:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:42:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1888MB
2025-09-13 22:42:40 (federatedscope.llm.llm_local.client:501) INFO: Client 18 Adapter 1 with test results: {'test_total': 40, 'test_loss': 30.53710174560547, 'test_avg_loss': 0.7634275436401368, 'test_seen': 40, 'test_correct': 16, 'test_acc': 0.4}
2025-09-13 22:42:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-09-13 22:42:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:42:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=32, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:42:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-13 22:42:44 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=135, loss_sum=102.948143, avg_loss=0.762579, seen=135, correct=61, accuracy=0.451852
2025-09-13 22:42:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:42:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:42:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:42:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1888MB
2025-09-13 22:42:46 (federatedscope.llm.llm_local.client:480) INFO: Client 18 Adapter 2 with val results: {'val_total': 135, 'val_loss': 102.9481430053711, 'val_avg_loss': 0.762578837076823, 'val_seen': 135, 'val_correct': 61, 'val_acc': 0.45185185185185184}
2025-09-13 22:42:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:42:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:42:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:42:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:42:48 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=31.856651, avg_loss=0.796416, seen=40, correct=17, accuracy=0.425000
2025-09-13 22:42:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:42:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:42:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:42:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1888MB
2025-09-13 22:42:49 (federatedscope.llm.llm_local.client:501) INFO: Client 18 Adapter 2 with test results: {'test_total': 40, 'test_loss': 31.856651306152344, 'test_avg_loss': 0.7964162826538086, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}
2025-09-13 22:42:49 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-13 22:42:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-13 22:42:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:42:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=45, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:42:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-13 22:42:52 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=73.416138, avg_loss=0.667419, seen=110, correct=60, accuracy=0.545455
2025-09-13 22:42:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:42:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:42:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:42:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1921MB
2025-09-13 22:42:53 (federatedscope.llm.llm_local.client:480) INFO: Client 19 Adapter 0 with val results: {'val_total': 110, 'val_loss': 73.4161376953125, 'val_avg_loss': 0.66741943359375, 'val_seen': 110, 'val_correct': 60, 'val_acc': 0.5454545454545454}
2025-09-13 22:42:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:42:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:42:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:42:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:42:54 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=31.344749, avg_loss=0.783619, seen=40, correct=17, accuracy=0.425000
2025-09-13 22:42:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:42:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:42:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:42:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1921MB
2025-09-13 22:42:56 (federatedscope.llm.llm_local.client:501) INFO: Client 19 Adapter 0 with test results: {'test_total': 40, 'test_loss': 31.344749450683594, 'test_avg_loss': 0.7836187362670899, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}
2025-09-13 22:42:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-13 22:42:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:42:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=45, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:42:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-13 22:42:59 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=76.571625, avg_loss=0.696106, seen=110, correct=64, accuracy=0.581818
2025-09-13 22:42:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:42:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:42:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:43:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1896MB
2025-09-13 22:43:01 (federatedscope.llm.llm_local.client:480) INFO: Client 19 Adapter 1 with val results: {'val_total': 110, 'val_loss': 76.57162475585938, 'val_avg_loss': 0.6961056795987216, 'val_seen': 110, 'val_correct': 64, 'val_acc': 0.5818181818181818}
2025-09-13 22:43:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:43:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:43:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:43:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:43:02 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.629848, avg_loss=0.740746, seen=40, correct=23, accuracy=0.575000
2025-09-13 22:43:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:43:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:43:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:43:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1896MB
2025-09-13 22:43:04 (federatedscope.llm.llm_local.client:501) INFO: Client 19 Adapter 1 with test results: {'test_total': 40, 'test_loss': 29.62984848022461, 'test_avg_loss': 0.7407462120056152, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-13 22:43:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-13 22:43:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:43:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=45, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:43:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-13 22:43:07 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=85.049057, avg_loss=0.773173, seen=110, correct=54, accuracy=0.490909
2025-09-13 22:43:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:43:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:43:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:43:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1896MB
2025-09-13 22:43:08 (federatedscope.llm.llm_local.client:480) INFO: Client 19 Adapter 2 with val results: {'val_total': 110, 'val_loss': 85.04905700683594, 'val_avg_loss': 0.7731732455166903, 'val_seen': 110, 'val_correct': 54, 'val_acc': 0.4909090909090909}
2025-09-13 22:43:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:43:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:43:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:43:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:43:09 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.299122, avg_loss=0.757478, seen=40, correct=22, accuracy=0.550000
2025-09-13 22:43:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:43:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:43:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:43:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1896MB
2025-09-13 22:43:10 (federatedscope.llm.llm_local.client:501) INFO: Client 19 Adapter 2 with test results: {'test_total': 40, 'test_loss': 30.299121856689453, 'test_avg_loss': 0.7574780464172364, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-13 22:43:10 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-13 22:43:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-09-13 22:43:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:43:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=63, num_train_batch_last_epoch=37, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:43:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-09-13 22:43:15 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=126, loss_sum=90.953918, avg_loss=0.721856, seen=126, correct=72, accuracy=0.571429
2025-09-13 22:43:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:43:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:43:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:43:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1929MB
2025-09-13 22:43:16 (federatedscope.llm.llm_local.client:480) INFO: Client 20 Adapter 0 with val results: {'val_total': 126, 'val_loss': 90.95391845703125, 'val_avg_loss': 0.7218564956907242, 'val_seen': 126, 'val_correct': 72, 'val_acc': 0.5714285714285714}
2025-09-13 22:43:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:43:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:43:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:43:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:43:18 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.698410, avg_loss=0.617460, seen=40, correct=26, accuracy=0.650000
2025-09-13 22:43:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:43:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:43:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:43:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1929MB
2025-09-13 22:43:19 (federatedscope.llm.llm_local.client:501) INFO: Client 20 Adapter 0 with test results: {'test_total': 40, 'test_loss': 24.698410034179688, 'test_avg_loss': 0.6174602508544922, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-13 22:43:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-09-13 22:43:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:43:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=63, num_train_batch_last_epoch=37, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:43:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-09-13 22:43:23 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=126, loss_sum=87.442879, avg_loss=0.693991, seen=126, correct=76, accuracy=0.603175
2025-09-13 22:43:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:43:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:43:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:43:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1904MB
2025-09-13 22:43:24 (federatedscope.llm.llm_local.client:480) INFO: Client 20 Adapter 1 with val results: {'val_total': 126, 'val_loss': 87.44287872314453, 'val_avg_loss': 0.6939911009773375, 'val_seen': 126, 'val_correct': 76, 'val_acc': 0.6031746031746031}
2025-09-13 22:43:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:43:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:43:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:43:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:43:25 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.069031, avg_loss=0.626726, seen=40, correct=25, accuracy=0.625000
2025-09-13 22:43:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:43:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:43:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:43:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1904MB
2025-09-13 22:43:26 (federatedscope.llm.llm_local.client:501) INFO: Client 20 Adapter 1 with test results: {'test_total': 40, 'test_loss': 25.06903076171875, 'test_avg_loss': 0.6267257690429687, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-13 22:43:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-09-13 22:43:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:43:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=63, num_train_batch_last_epoch=37, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:43:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-09-13 22:43:31 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=126, loss_sum=91.955482, avg_loss=0.729805, seen=126, correct=69, accuracy=0.547619
2025-09-13 22:43:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:43:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:43:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:43:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1904MB
2025-09-13 22:43:33 (federatedscope.llm.llm_local.client:480) INFO: Client 20 Adapter 2 with val results: {'val_total': 126, 'val_loss': 91.95548248291016, 'val_avg_loss': 0.729805416531033, 'val_seen': 126, 'val_correct': 69, 'val_acc': 0.5476190476190477}
2025-09-13 22:43:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:43:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:43:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:43:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:43:34 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.845039, avg_loss=0.646126, seen=40, correct=24, accuracy=0.600000
2025-09-13 22:43:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:43:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:43:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:43:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1904MB
2025-09-13 22:43:35 (federatedscope.llm.llm_local.client:501) INFO: Client 20 Adapter 2 with test results: {'test_total': 40, 'test_loss': 25.84503936767578, 'test_avg_loss': 0.6461259841918945, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-13 22:43:35 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-13 22:43:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-09-13 22:43:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:43:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=77, num_train_batch_last_epoch=23, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:43:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-09-13 22:43:40 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=153, loss_sum=102.213348, avg_loss=0.668061, seen=153, correct=91, accuracy=0.594771
2025-09-13 22:43:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:43:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:43:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:43:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1938MB
2025-09-13 22:43:41 (federatedscope.llm.llm_local.client:480) INFO: Client 21 Adapter 0 with val results: {'val_total': 153, 'val_loss': 102.21334838867188, 'val_avg_loss': 0.6680611005795547, 'val_seen': 153, 'val_correct': 91, 'val_acc': 0.5947712418300654}
2025-09-13 22:43:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:43:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:43:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:43:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:43:42 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=31.212996, avg_loss=0.780325, seen=40, correct=18, accuracy=0.450000
2025-09-13 22:43:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:43:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:43:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:43:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1938MB
2025-09-13 22:43:44 (federatedscope.llm.llm_local.client:501) INFO: Client 21 Adapter 0 with test results: {'test_total': 40, 'test_loss': 31.212995529174805, 'test_avg_loss': 0.7803248882293701, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-13 22:43:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-09-13 22:43:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:43:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=77, num_train_batch_last_epoch=23, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:43:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-09-13 22:43:49 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=153, loss_sum=109.619461, avg_loss=0.716467, seen=153, correct=87, accuracy=0.568627
2025-09-13 22:43:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:43:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:43:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:43:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1913MB
2025-09-13 22:43:50 (federatedscope.llm.llm_local.client:480) INFO: Client 21 Adapter 1 with val results: {'val_total': 153, 'val_loss': 109.61946105957031, 'val_avg_loss': 0.7164670657488256, 'val_seen': 153, 'val_correct': 87, 'val_acc': 0.5686274509803921}
2025-09-13 22:43:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:43:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:43:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:43:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:43:51 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.177250, avg_loss=0.654431, seen=40, correct=23, accuracy=0.575000
2025-09-13 22:43:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:43:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:43:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:43:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1913MB
2025-09-13 22:43:52 (federatedscope.llm.llm_local.client:501) INFO: Client 21 Adapter 1 with test results: {'test_total': 40, 'test_loss': 26.177249908447266, 'test_avg_loss': 0.6544312477111817, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-13 22:43:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-09-13 22:43:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:43:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=77, num_train_batch_last_epoch=23, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:43:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-09-13 22:43:57 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=153, loss_sum=111.719849, avg_loss=0.730195, seen=153, correct=74, accuracy=0.483660
2025-09-13 22:43:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:43:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:43:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:43:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1913MB
2025-09-13 22:43:58 (federatedscope.llm.llm_local.client:480) INFO: Client 21 Adapter 2 with val results: {'val_total': 153, 'val_loss': 111.7198486328125, 'val_avg_loss': 0.7301950891033496, 'val_seen': 153, 'val_correct': 74, 'val_acc': 0.48366013071895425}
2025-09-13 22:43:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:43:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:43:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:43:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:43:59 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.223175, avg_loss=0.655579, seen=40, correct=23, accuracy=0.575000
2025-09-13 22:43:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:43:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:44:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:44:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1913MB
2025-09-13 22:44:00 (federatedscope.llm.llm_local.client:501) INFO: Client 21 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.223175048828125, 'test_avg_loss': 0.6555793762207032, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-13 22:44:00 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-13 22:44:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-13 22:44:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:44:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=4, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:44:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-13 22:44:02 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=9.255430, avg_loss=0.841403, seen=11, correct=4, accuracy=0.363636
2025-09-13 22:44:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:44:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:44:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:44:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1947MB
2025-09-13 22:44:03 (federatedscope.llm.llm_local.client:480) INFO: Client 22 Adapter 0 with val results: {'val_total': 11, 'val_loss': 9.255430221557617, 'val_avg_loss': 0.8414027474143289, 'val_seen': 11, 'val_correct': 4, 'val_acc': 0.36363636363636365}
2025-09-13 22:44:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:44:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:44:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:44:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:44:05 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.272121, avg_loss=0.681803, seen=40, correct=24, accuracy=0.600000
2025-09-13 22:44:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:44:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:44:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:44:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1947MB
2025-09-13 22:44:06 (federatedscope.llm.llm_local.client:501) INFO: Client 22 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.27212142944336, 'test_avg_loss': 0.681803035736084, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-13 22:44:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-13 22:44:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:44:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=4, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:44:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-13 22:44:07 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=7.779748, avg_loss=0.707250, seen=11, correct=7, accuracy=0.636364
2025-09-13 22:44:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:44:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:44:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:44:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1921MB
2025-09-13 22:44:08 (federatedscope.llm.llm_local.client:480) INFO: Client 22 Adapter 1 with val results: {'val_total': 11, 'val_loss': 7.779748439788818, 'val_avg_loss': 0.7072498581626199, 'val_seen': 11, 'val_correct': 7, 'val_acc': 0.6363636363636364}
2025-09-13 22:44:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:44:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:44:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:44:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:44:09 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.529257, avg_loss=0.763231, seen=40, correct=20, accuracy=0.500000
2025-09-13 22:44:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:44:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:44:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:44:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1921MB
2025-09-13 22:44:10 (federatedscope.llm.llm_local.client:501) INFO: Client 22 Adapter 1 with test results: {'test_total': 40, 'test_loss': 30.52925682067871, 'test_avg_loss': 0.7632314205169678, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-13 22:44:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-13 22:44:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:44:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=4, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:44:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-13 22:44:11 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=6.726081, avg_loss=0.611462, seen=11, correct=8, accuracy=0.727273
2025-09-13 22:44:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:44:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:44:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:44:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1921MB
2025-09-13 22:44:12 (federatedscope.llm.llm_local.client:480) INFO: Client 22 Adapter 2 with val results: {'val_total': 11, 'val_loss': 6.726081371307373, 'val_avg_loss': 0.6114619428461249, 'val_seen': 11, 'val_correct': 8, 'val_acc': 0.7272727272727273}
2025-09-13 22:44:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:44:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:44:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:44:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:44:13 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.364103, avg_loss=0.759103, seen=40, correct=19, accuracy=0.475000
2025-09-13 22:44:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:44:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:44:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:44:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1921MB
2025-09-13 22:44:14 (federatedscope.llm.llm_local.client:501) INFO: Client 22 Adapter 2 with test results: {'test_total': 40, 'test_loss': 30.364103317260742, 'test_avg_loss': 0.7591025829315186, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-13 22:44:14 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-13 22:44:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-09-13 22:44:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:44:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=15, num_train_batch_last_epoch=10, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:44:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-13 22:44:15 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=30, loss_sum=18.240786, avg_loss=0.608026, seen=30, correct=19, accuracy=0.633333
2025-09-13 22:44:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:44:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:44:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:44:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1955MB
2025-09-13 22:44:17 (federatedscope.llm.llm_local.client:480) INFO: Client 23 Adapter 0 with val results: {'val_total': 30, 'val_loss': 18.240785598754883, 'val_avg_loss': 0.6080261866251627, 'val_seen': 30, 'val_correct': 19, 'val_acc': 0.6333333333333333}
2025-09-13 22:44:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:44:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:44:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:44:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:44:19 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.124409, avg_loss=0.628110, seen=40, correct=24, accuracy=0.600000
2025-09-13 22:44:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:44:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:44:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:44:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1955MB
2025-09-13 22:44:20 (federatedscope.llm.llm_local.client:501) INFO: Client 23 Adapter 0 with test results: {'test_total': 40, 'test_loss': 25.124408721923828, 'test_avg_loss': 0.6281102180480957, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-13 22:44:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-09-13 22:44:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:44:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=15, num_train_batch_last_epoch=10, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:44:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-13 22:44:21 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=30, loss_sum=18.315636, avg_loss=0.610521, seen=30, correct=19, accuracy=0.633333
2025-09-13 22:44:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:44:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:44:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:44:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1930MB
2025-09-13 22:44:22 (federatedscope.llm.llm_local.client:480) INFO: Client 23 Adapter 1 with val results: {'val_total': 30, 'val_loss': 18.315635681152344, 'val_avg_loss': 0.6105211893717448, 'val_seen': 30, 'val_correct': 19, 'val_acc': 0.6333333333333333}
2025-09-13 22:44:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:44:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:44:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:44:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:44:23 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.893356, avg_loss=0.647334, seen=40, correct=25, accuracy=0.625000
2025-09-13 22:44:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:44:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:44:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:44:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1930MB
2025-09-13 22:44:24 (federatedscope.llm.llm_local.client:501) INFO: Client 23 Adapter 1 with test results: {'test_total': 40, 'test_loss': 25.893356323242188, 'test_avg_loss': 0.6473339080810547, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-13 22:44:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-09-13 22:44:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:44:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=15, num_train_batch_last_epoch=10, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:44:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-13 22:44:26 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=30, loss_sum=20.730812, avg_loss=0.691027, seen=30, correct=15, accuracy=0.500000
2025-09-13 22:44:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:44:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:44:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:44:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1930MB
2025-09-13 22:44:27 (federatedscope.llm.llm_local.client:480) INFO: Client 23 Adapter 2 with val results: {'val_total': 30, 'val_loss': 20.730812072753906, 'val_avg_loss': 0.6910270690917969, 'val_seen': 30, 'val_correct': 15, 'val_acc': 0.5}
2025-09-13 22:44:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:44:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:44:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:44:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:44:29 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.225643, avg_loss=0.655641, seen=40, correct=24, accuracy=0.600000
2025-09-13 22:44:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:44:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:44:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:44:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1930MB
2025-09-13 22:44:30 (federatedscope.llm.llm_local.client:501) INFO: Client 23 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.225643157958984, 'test_avg_loss': 0.6556410789489746, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-13 22:44:30 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-13 22:44:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 22:44:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:44:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:44:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 22:44:36 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=133.821930, avg_loss=0.669110, seen=200, correct=114, accuracy=0.570000
2025-09-13 22:44:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:44:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:44:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:44:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1964MB
2025-09-13 22:44:38 (federatedscope.llm.llm_local.client:480) INFO: Client 24 Adapter 0 with val results: {'val_total': 200, 'val_loss': 133.82192993164062, 'val_avg_loss': 0.6691096496582031, 'val_seen': 200, 'val_correct': 114, 'val_acc': 0.57}
2025-09-13 22:44:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:44:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:44:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:44:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:44:39 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=31.407673, avg_loss=0.785192, seen=40, correct=20, accuracy=0.500000
2025-09-13 22:44:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:44:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:44:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:44:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1964MB
2025-09-13 22:44:40 (federatedscope.llm.llm_local.client:501) INFO: Client 24 Adapter 0 with test results: {'test_total': 40, 'test_loss': 31.407672882080078, 'test_avg_loss': 0.785191822052002, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-13 22:44:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 22:44:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:44:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:44:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 22:44:46 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=140.554825, avg_loss=0.702774, seen=200, correct=110, accuracy=0.550000
2025-09-13 22:44:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:44:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:44:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:44:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1938MB
2025-09-13 22:44:48 (federatedscope.llm.llm_local.client:480) INFO: Client 24 Adapter 1 with val results: {'val_total': 200, 'val_loss': 140.55482482910156, 'val_avg_loss': 0.7027741241455078, 'val_seen': 200, 'val_correct': 110, 'val_acc': 0.55}
2025-09-13 22:44:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:44:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:44:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:44:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:44:49 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.943581, avg_loss=0.773590, seen=40, correct=20, accuracy=0.500000
2025-09-13 22:44:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:44:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:44:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:44:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1938MB
2025-09-13 22:44:50 (federatedscope.llm.llm_local.client:501) INFO: Client 24 Adapter 1 with test results: {'test_total': 40, 'test_loss': 30.943580627441406, 'test_avg_loss': 0.7735895156860352, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-13 22:44:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 22:44:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:44:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:44:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 22:44:56 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=138.304810, avg_loss=0.691524, seen=200, correct=112, accuracy=0.560000
2025-09-13 22:44:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:44:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:44:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:44:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1938MB
2025-09-13 22:44:57 (federatedscope.llm.llm_local.client:480) INFO: Client 24 Adapter 2 with val results: {'val_total': 200, 'val_loss': 138.3048095703125, 'val_avg_loss': 0.6915240478515625, 'val_seen': 200, 'val_correct': 112, 'val_acc': 0.56}
2025-09-13 22:44:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:44:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:44:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:44:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:44:59 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=32.992676, avg_loss=0.824817, seen=40, correct=14, accuracy=0.350000
2025-09-13 22:44:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:44:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:44:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:45:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1938MB
2025-09-13 22:45:00 (federatedscope.llm.llm_local.client:501) INFO: Client 24 Adapter 2 with test results: {'test_total': 40, 'test_loss': 32.99267578125, 'test_avg_loss': 0.82481689453125, 'test_seen': 40, 'test_correct': 14, 'test_acc': 0.35}
2025-09-13 22:45:00 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-13 22:45:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 22:45:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:45:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:45:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 22:45:07 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=148.046722, avg_loss=0.740234, seen=200, correct=104, accuracy=0.520000
2025-09-13 22:45:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:45:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:45:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:45:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1972MB
2025-09-13 22:45:08 (federatedscope.llm.llm_local.client:480) INFO: Client 25 Adapter 0 with val results: {'val_total': 200, 'val_loss': 148.04672241210938, 'val_avg_loss': 0.7402336120605468, 'val_seen': 200, 'val_correct': 104, 'val_acc': 0.52}
2025-09-13 22:45:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:45:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:45:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:45:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:45:09 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.860157, avg_loss=0.771504, seen=40, correct=18, accuracy=0.450000
2025-09-13 22:45:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:45:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:45:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:45:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1972MB
2025-09-13 22:45:10 (federatedscope.llm.llm_local.client:501) INFO: Client 25 Adapter 0 with test results: {'test_total': 40, 'test_loss': 30.860157012939453, 'test_avg_loss': 0.7715039253234863, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-13 22:45:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 22:45:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:45:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:45:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 22:45:16 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=141.218323, avg_loss=0.706092, seen=200, correct=102, accuracy=0.510000
2025-09-13 22:45:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:45:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:45:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:45:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1947MB
2025-09-13 22:45:18 (federatedscope.llm.llm_local.client:480) INFO: Client 25 Adapter 1 with val results: {'val_total': 200, 'val_loss': 141.21832275390625, 'val_avg_loss': 0.7060916137695312, 'val_seen': 200, 'val_correct': 102, 'val_acc': 0.51}
2025-09-13 22:45:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:45:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:45:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:45:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:45:19 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=34.815933, avg_loss=0.870398, seen=40, correct=20, accuracy=0.500000
2025-09-13 22:45:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:45:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:45:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:45:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1947MB
2025-09-13 22:45:20 (federatedscope.llm.llm_local.client:501) INFO: Client 25 Adapter 1 with test results: {'test_total': 40, 'test_loss': 34.81593322753906, 'test_avg_loss': 0.8703983306884766, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-13 22:45:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 22:45:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:45:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:45:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 22:45:26 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=147.853546, avg_loss=0.739268, seen=200, correct=100, accuracy=0.500000
2025-09-13 22:45:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:45:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:45:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:45:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1947MB
2025-09-13 22:45:28 (federatedscope.llm.llm_local.client:480) INFO: Client 25 Adapter 2 with val results: {'val_total': 200, 'val_loss': 147.85354614257812, 'val_avg_loss': 0.7392677307128906, 'val_seen': 200, 'val_correct': 100, 'val_acc': 0.5}
2025-09-13 22:45:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:45:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:45:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:45:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:45:30 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=33.862278, avg_loss=0.846557, seen=40, correct=20, accuracy=0.500000
2025-09-13 22:45:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:45:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:45:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:45:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1947MB
2025-09-13 22:45:31 (federatedscope.llm.llm_local.client:501) INFO: Client 25 Adapter 2 with test results: {'test_total': 40, 'test_loss': 33.86227798461914, 'test_avg_loss': 0.8465569496154786, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-13 22:45:31 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-13 22:45:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-09-13 22:45:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:45:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=81, num_train_batch_last_epoch=19, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:45:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-09-13 22:45:37 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=161, loss_sum=102.109970, avg_loss=0.634223, seen=161, correct=105, accuracy=0.652174
2025-09-13 22:45:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:45:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:45:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:45:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1980MB
2025-09-13 22:45:38 (federatedscope.llm.llm_local.client:480) INFO: Client 26 Adapter 0 with val results: {'val_total': 161, 'val_loss': 102.10997009277344, 'val_avg_loss': 0.6342234167253008, 'val_seen': 161, 'val_correct': 105, 'val_acc': 0.6521739130434783}
2025-09-13 22:45:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:45:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:45:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:45:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:45:39 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.788471, avg_loss=0.619712, seen=40, correct=27, accuracy=0.675000
2025-09-13 22:45:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:45:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:45:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:45:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1980MB
2025-09-13 22:45:40 (federatedscope.llm.llm_local.client:501) INFO: Client 26 Adapter 0 with test results: {'test_total': 40, 'test_loss': 24.788471221923828, 'test_avg_loss': 0.6197117805480957, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-13 22:45:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-09-13 22:45:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:45:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=81, num_train_batch_last_epoch=19, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:45:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-09-13 22:45:46 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=161, loss_sum=105.144859, avg_loss=0.653074, seen=161, correct=101, accuracy=0.627329
2025-09-13 22:45:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:45:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:45:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:45:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1955MB
2025-09-13 22:45:47 (federatedscope.llm.llm_local.client:480) INFO: Client 26 Adapter 1 with val results: {'val_total': 161, 'val_loss': 105.14485931396484, 'val_avg_loss': 0.6530736603351853, 'val_seen': 161, 'val_correct': 101, 'val_acc': 0.6273291925465838}
2025-09-13 22:45:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:45:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:45:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:45:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:45:48 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.835804, avg_loss=0.620895, seen=40, correct=29, accuracy=0.725000
2025-09-13 22:45:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:45:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:45:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:45:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1955MB
2025-09-13 22:45:49 (federatedscope.llm.llm_local.client:501) INFO: Client 26 Adapter 1 with test results: {'test_total': 40, 'test_loss': 24.835803985595703, 'test_avg_loss': 0.6208950996398925, 'test_seen': 40, 'test_correct': 29, 'test_acc': 0.725}
2025-09-13 22:45:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-09-13 22:45:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:45:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=81, num_train_batch_last_epoch=19, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:45:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-09-13 22:45:55 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=161, loss_sum=109.041557, avg_loss=0.677277, seen=161, correct=91, accuracy=0.565217
2025-09-13 22:45:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:45:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:45:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:45:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1955MB
2025-09-13 22:45:56 (federatedscope.llm.llm_local.client:480) INFO: Client 26 Adapter 2 with val results: {'val_total': 161, 'val_loss': 109.04155731201172, 'val_avg_loss': 0.677276753490756, 'val_seen': 161, 'val_correct': 91, 'val_acc': 0.5652173913043478}
2025-09-13 22:45:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:45:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:45:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:45:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:45:57 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.872202, avg_loss=0.696805, seen=40, correct=24, accuracy=0.600000
2025-09-13 22:45:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:45:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:45:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:45:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1955MB
2025-09-13 22:45:59 (federatedscope.llm.llm_local.client:501) INFO: Client 26 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.872201919555664, 'test_avg_loss': 0.6968050479888916, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-13 22:45:59 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-13 22:45:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-09-13 22:45:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:46:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=62, num_train_batch_last_epoch=38, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:46:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-09-13 22:46:03 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=123, loss_sum=81.774597, avg_loss=0.664834, seen=123, correct=81, accuracy=0.658537
2025-09-13 22:46:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:46:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:46:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:46:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1989MB
2025-09-13 22:46:04 (federatedscope.llm.llm_local.client:480) INFO: Client 27 Adapter 0 with val results: {'val_total': 123, 'val_loss': 81.77459716796875, 'val_avg_loss': 0.6648341233168191, 'val_seen': 123, 'val_correct': 81, 'val_acc': 0.6585365853658537}
2025-09-13 22:46:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:46:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:46:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:46:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:46:06 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.703140, avg_loss=0.642579, seen=40, correct=28, accuracy=0.700000
2025-09-13 22:46:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:46:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:46:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:46:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1989MB
2025-09-13 22:46:07 (federatedscope.llm.llm_local.client:501) INFO: Client 27 Adapter 0 with test results: {'test_total': 40, 'test_loss': 25.703140258789062, 'test_avg_loss': 0.6425785064697266, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}
2025-09-13 22:46:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-09-13 22:46:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:46:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=62, num_train_batch_last_epoch=38, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:46:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-09-13 22:46:11 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=123, loss_sum=81.497147, avg_loss=0.662578, seen=123, correct=72, accuracy=0.585366
2025-09-13 22:46:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:46:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:46:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:46:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1964MB
2025-09-13 22:46:12 (federatedscope.llm.llm_local.client:480) INFO: Client 27 Adapter 1 with val results: {'val_total': 123, 'val_loss': 81.49714660644531, 'val_avg_loss': 0.6625784276946773, 'val_seen': 123, 'val_correct': 72, 'val_acc': 0.5853658536585366}
2025-09-13 22:46:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:46:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:46:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:46:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:46:14 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.136164, avg_loss=0.703404, seen=40, correct=19, accuracy=0.475000
2025-09-13 22:46:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:46:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:46:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:46:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1964MB
2025-09-13 22:46:15 (federatedscope.llm.llm_local.client:501) INFO: Client 27 Adapter 1 with test results: {'test_total': 40, 'test_loss': 28.13616371154785, 'test_avg_loss': 0.7034040927886963, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-13 22:46:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-09-13 22:46:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:46:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=62, num_train_batch_last_epoch=38, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:46:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-09-13 22:46:19 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=123, loss_sum=88.094162, avg_loss=0.716213, seen=123, correct=61, accuracy=0.495935
2025-09-13 22:46:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:46:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:46:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:46:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1964MB
2025-09-13 22:46:21 (federatedscope.llm.llm_local.client:480) INFO: Client 27 Adapter 2 with val results: {'val_total': 123, 'val_loss': 88.09416198730469, 'val_avg_loss': 0.7162126990837779, 'val_seen': 123, 'val_correct': 61, 'val_acc': 0.4959349593495935}
2025-09-13 22:46:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:46:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:46:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:46:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:46:22 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=34.007206, avg_loss=0.850180, seen=40, correct=13, accuracy=0.325000
2025-09-13 22:46:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:46:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:46:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:46:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1964MB
2025-09-13 22:46:23 (federatedscope.llm.llm_local.client:501) INFO: Client 27 Adapter 2 with test results: {'test_total': 40, 'test_loss': 34.007205963134766, 'test_avg_loss': 0.8501801490783691, 'test_seen': 40, 'test_correct': 13, 'test_acc': 0.325}
2025-09-13 22:46:23 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-13 22:46:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-09-13 22:46:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:46:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=38, num_train_batch_last_epoch=24, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:46:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-13 22:46:25 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=75, loss_sum=51.505608, avg_loss=0.686741, seen=75, correct=42, accuracy=0.560000
2025-09-13 22:46:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:46:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:46:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:46:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1997MB
2025-09-13 22:46:27 (federatedscope.llm.llm_local.client:480) INFO: Client 28 Adapter 0 with val results: {'val_total': 75, 'val_loss': 51.50560760498047, 'val_avg_loss': 0.6867414347330729, 'val_seen': 75, 'val_correct': 42, 'val_acc': 0.56}
2025-09-13 22:46:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:46:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:46:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:46:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:46:29 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.894167, avg_loss=0.747354, seen=40, correct=20, accuracy=0.500000
2025-09-13 22:46:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:46:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:46:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:46:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1997MB
2025-09-13 22:46:30 (federatedscope.llm.llm_local.client:501) INFO: Client 28 Adapter 0 with test results: {'test_total': 40, 'test_loss': 29.894166946411133, 'test_avg_loss': 0.7473541736602783, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-13 22:46:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-09-13 22:46:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:46:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=38, num_train_batch_last_epoch=24, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:46:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-13 22:46:32 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=75, loss_sum=51.544289, avg_loss=0.687257, seen=75, correct=41, accuracy=0.546667
2025-09-13 22:46:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:46:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:46:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:46:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1972MB
2025-09-13 22:46:33 (federatedscope.llm.llm_local.client:480) INFO: Client 28 Adapter 1 with val results: {'val_total': 75, 'val_loss': 51.544288635253906, 'val_avg_loss': 0.6872571818033855, 'val_seen': 75, 'val_correct': 41, 'val_acc': 0.5466666666666666}
2025-09-13 22:46:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:46:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:46:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:46:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:46:35 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.006386, avg_loss=0.675160, seen=40, correct=24, accuracy=0.600000
2025-09-13 22:46:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:46:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:46:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:46:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1972MB
2025-09-13 22:46:36 (federatedscope.llm.llm_local.client:501) INFO: Client 28 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.006385803222656, 'test_avg_loss': 0.6751596450805664, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-13 22:46:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-09-13 22:46:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:46:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=38, num_train_batch_last_epoch=24, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:46:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-13 22:46:38 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=75, loss_sum=54.180084, avg_loss=0.722401, seen=75, correct=38, accuracy=0.506667
2025-09-13 22:46:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:46:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:46:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:46:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1972MB
2025-09-13 22:46:40 (federatedscope.llm.llm_local.client:480) INFO: Client 28 Adapter 2 with val results: {'val_total': 75, 'val_loss': 54.180084228515625, 'val_avg_loss': 0.722401123046875, 'val_seen': 75, 'val_correct': 38, 'val_acc': 0.5066666666666667}
2025-09-13 22:46:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:46:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:46:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:46:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:46:41 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.129791, avg_loss=0.703245, seen=40, correct=22, accuracy=0.550000
2025-09-13 22:46:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:46:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:46:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:46:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1972MB
2025-09-13 22:46:42 (federatedscope.llm.llm_local.client:501) INFO: Client 28 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.129791259765625, 'test_avg_loss': 0.7032447814941406, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-13 22:46:42 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-13 22:46:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 22:46:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:46:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:46:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 22:46:49 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=140.576584, avg_loss=0.702883, seen=200, correct=118, accuracy=0.590000
2025-09-13 22:46:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:46:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:46:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:46:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2006MB
2025-09-13 22:46:50 (federatedscope.llm.llm_local.client:480) INFO: Client 29 Adapter 0 with val results: {'val_total': 200, 'val_loss': 140.5765838623047, 'val_avg_loss': 0.7028829193115235, 'val_seen': 200, 'val_correct': 118, 'val_acc': 0.59}
2025-09-13 22:46:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:46:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:46:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:46:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:46:51 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.146515, avg_loss=0.678663, seen=40, correct=22, accuracy=0.550000
2025-09-13 22:46:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:46:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:46:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:46:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2006MB
2025-09-13 22:46:52 (federatedscope.llm.llm_local.client:501) INFO: Client 29 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.146514892578125, 'test_avg_loss': 0.6786628723144531, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-13 22:46:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 22:46:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:46:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:46:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 22:46:58 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=138.284454, avg_loss=0.691422, seen=200, correct=113, accuracy=0.565000
2025-09-13 22:46:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:46:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:46:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:47:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1981MB
2025-09-13 22:47:00 (federatedscope.llm.llm_local.client:480) INFO: Client 29 Adapter 1 with val results: {'val_total': 200, 'val_loss': 138.28445434570312, 'val_avg_loss': 0.6914222717285157, 'val_seen': 200, 'val_correct': 113, 'val_acc': 0.565}
2025-09-13 22:47:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:47:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:47:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:47:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:47:01 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=22.510845, avg_loss=0.562771, seen=40, correct=26, accuracy=0.650000
2025-09-13 22:47:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:47:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:47:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:47:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1981MB
2025-09-13 22:47:02 (federatedscope.llm.llm_local.client:501) INFO: Client 29 Adapter 1 with test results: {'test_total': 40, 'test_loss': 22.510845184326172, 'test_avg_loss': 0.5627711296081543, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-13 22:47:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 22:47:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:47:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:47:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 22:47:08 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=144.732513, avg_loss=0.723663, seen=200, correct=97, accuracy=0.485000
2025-09-13 22:47:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:47:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:47:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:47:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1981MB
2025-09-13 22:47:10 (federatedscope.llm.llm_local.client:480) INFO: Client 29 Adapter 2 with val results: {'val_total': 200, 'val_loss': 144.73251342773438, 'val_avg_loss': 0.7236625671386718, 'val_seen': 200, 'val_correct': 97, 'val_acc': 0.485}
2025-09-13 22:47:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:47:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:47:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:47:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:47:11 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.718197, avg_loss=0.642955, seen=40, correct=24, accuracy=0.600000
2025-09-13 22:47:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:47:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:47:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:47:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1981MB
2025-09-13 22:47:12 (federatedscope.llm.llm_local.client:501) INFO: Client 29 Adapter 2 with test results: {'test_total': 40, 'test_loss': 25.718196868896484, 'test_avg_loss': 0.6429549217224121, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-13 22:47:12 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-13 22:47:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-09-13 22:47:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:47:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=85, num_train_batch_last_epoch=15, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:47:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-09-13 22:47:18 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=170, loss_sum=111.965759, avg_loss=0.658622, seen=170, correct=106, accuracy=0.623529
2025-09-13 22:47:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:47:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:47:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:47:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2014MB
2025-09-13 22:47:19 (federatedscope.llm.llm_local.client:480) INFO: Client 30 Adapter 0 with val results: {'val_total': 170, 'val_loss': 111.96575927734375, 'val_avg_loss': 0.6586221133961397, 'val_seen': 170, 'val_correct': 106, 'val_acc': 0.6235294117647059}
2025-09-13 22:47:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:47:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:47:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:47:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:47:20 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.583946, avg_loss=0.639599, seen=40, correct=20, accuracy=0.500000
2025-09-13 22:47:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:47:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:47:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:47:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2014MB
2025-09-13 22:47:21 (federatedscope.llm.llm_local.client:501) INFO: Client 30 Adapter 0 with test results: {'test_total': 40, 'test_loss': 25.583946228027344, 'test_avg_loss': 0.6395986557006836, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-13 22:47:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-09-13 22:47:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:47:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=85, num_train_batch_last_epoch=15, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:47:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-09-13 22:47:27 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=170, loss_sum=112.006348, avg_loss=0.658861, seen=170, correct=109, accuracy=0.641176
2025-09-13 22:47:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:47:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:47:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:47:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1989MB
2025-09-13 22:47:28 (federatedscope.llm.llm_local.client:480) INFO: Client 30 Adapter 1 with val results: {'val_total': 170, 'val_loss': 112.00634765625, 'val_avg_loss': 0.6588608685661764, 'val_seen': 170, 'val_correct': 109, 'val_acc': 0.6411764705882353}
2025-09-13 22:47:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:47:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:47:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:47:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:47:29 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.475475, avg_loss=0.761887, seen=40, correct=20, accuracy=0.500000
2025-09-13 22:47:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:47:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:47:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:47:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1989MB
2025-09-13 22:47:30 (federatedscope.llm.llm_local.client:501) INFO: Client 30 Adapter 1 with test results: {'test_total': 40, 'test_loss': 30.475475311279297, 'test_avg_loss': 0.7618868827819825, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-13 22:47:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-09-13 22:47:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:47:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=85, num_train_batch_last_epoch=15, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:47:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-09-13 22:47:36 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=170, loss_sum=121.082352, avg_loss=0.712249, seen=170, correct=86, accuracy=0.505882
2025-09-13 22:47:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:47:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:47:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:47:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1989MB
2025-09-13 22:47:37 (federatedscope.llm.llm_local.client:480) INFO: Client 30 Adapter 2 with val results: {'val_total': 170, 'val_loss': 121.08235168457031, 'val_avg_loss': 0.712249127556296, 'val_seen': 170, 'val_correct': 86, 'val_acc': 0.5058823529411764}
2025-09-13 22:47:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:47:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:47:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:47:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:47:38 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.156672, avg_loss=0.728917, seen=40, correct=22, accuracy=0.550000
2025-09-13 22:47:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:47:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:47:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:47:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1989MB
2025-09-13 22:47:39 (federatedscope.llm.llm_local.client:501) INFO: Client 30 Adapter 2 with test results: {'test_total': 40, 'test_loss': 29.15667152404785, 'test_avg_loss': 0.7289167881011963, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-13 22:47:39 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-13 22:47:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-09-13 22:47:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:47:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=97, num_train_batch_last_epoch=3, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:47:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-09-13 22:47:46 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=193, loss_sum=147.960785, avg_loss=0.766636, seen=193, correct=97, accuracy=0.502591
2025-09-13 22:47:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:47:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:47:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:47:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2023MB
2025-09-13 22:47:47 (federatedscope.llm.llm_local.client:480) INFO: Client 31 Adapter 0 with val results: {'val_total': 193, 'val_loss': 147.96078491210938, 'val_avg_loss': 0.7666361912544527, 'val_seen': 193, 'val_correct': 97, 'val_acc': 0.5025906735751295}
2025-09-13 22:47:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:47:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:47:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:47:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:47:49 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.269188, avg_loss=0.631730, seen=40, correct=24, accuracy=0.600000
2025-09-13 22:47:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:47:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:47:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:47:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2023MB
2025-09-13 22:47:50 (federatedscope.llm.llm_local.client:501) INFO: Client 31 Adapter 0 with test results: {'test_total': 40, 'test_loss': 25.269187927246094, 'test_avg_loss': 0.6317296981811523, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-13 22:47:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-09-13 22:47:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:47:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=97, num_train_batch_last_epoch=3, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:47:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-09-13 22:47:56 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=193, loss_sum=134.772720, avg_loss=0.698304, seen=193, correct=110, accuracy=0.569948
2025-09-13 22:47:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:47:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:47:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:47:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1998MB
2025-09-13 22:47:58 (federatedscope.llm.llm_local.client:480) INFO: Client 31 Adapter 1 with val results: {'val_total': 193, 'val_loss': 134.77272033691406, 'val_avg_loss': 0.698304250450332, 'val_seen': 193, 'val_correct': 110, 'val_acc': 0.5699481865284974}
2025-09-13 22:47:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:47:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:47:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:47:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:47:59 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.675898, avg_loss=0.666897, seen=40, correct=26, accuracy=0.650000
2025-09-13 22:47:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:47:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:48:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:48:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1998MB
2025-09-13 22:48:00 (federatedscope.llm.llm_local.client:501) INFO: Client 31 Adapter 1 with test results: {'test_total': 40, 'test_loss': 26.6758975982666, 'test_avg_loss': 0.6668974399566651, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-13 22:48:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-09-13 22:48:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:48:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=97, num_train_batch_last_epoch=3, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:48:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-09-13 22:48:07 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=193, loss_sum=136.920746, avg_loss=0.709434, seen=193, correct=114, accuracy=0.590674
2025-09-13 22:48:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:48:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:48:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:48:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1998MB
2025-09-13 22:48:08 (federatedscope.llm.llm_local.client:480) INFO: Client 31 Adapter 2 with val results: {'val_total': 193, 'val_loss': 136.92074584960938, 'val_avg_loss': 0.7094339163192196, 'val_seen': 193, 'val_correct': 114, 'val_acc': 0.5906735751295337}
2025-09-13 22:48:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:48:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:48:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:48:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:48:09 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.275404, avg_loss=0.706885, seen=40, correct=19, accuracy=0.475000
2025-09-13 22:48:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:48:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:48:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:48:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1998MB
2025-09-13 22:48:10 (federatedscope.llm.llm_local.client:501) INFO: Client 31 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.27540397644043, 'test_avg_loss': 0.7068850994110107, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-13 22:48:10 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-13 22:48:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-09-13 22:48:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:48:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=56, num_train_batch_last_epoch=44, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:48:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-13 22:48:13 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=112, loss_sum=65.987061, avg_loss=0.589170, seen=112, correct=77, accuracy=0.687500
2025-09-13 22:48:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:48:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:48:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:48:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2031MB
2025-09-13 22:48:14 (federatedscope.llm.llm_local.client:480) INFO: Client 32 Adapter 0 with val results: {'val_total': 112, 'val_loss': 65.987060546875, 'val_avg_loss': 0.5891701834542411, 'val_seen': 112, 'val_correct': 77, 'val_acc': 0.6875}
2025-09-13 22:48:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:48:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:48:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:48:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:48:15 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.106054, avg_loss=0.702651, seen=40, correct=23, accuracy=0.575000
2025-09-13 22:48:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:48:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:48:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:48:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2031MB
2025-09-13 22:48:17 (federatedscope.llm.llm_local.client:501) INFO: Client 32 Adapter 0 with test results: {'test_total': 40, 'test_loss': 28.106054306030273, 'test_avg_loss': 0.7026513576507568, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-13 22:48:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-09-13 22:48:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:48:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=56, num_train_batch_last_epoch=44, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:48:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-13 22:48:20 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=112, loss_sum=67.183563, avg_loss=0.599853, seen=112, correct=80, accuracy=0.714286
2025-09-13 22:48:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:48:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:48:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:48:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2006MB
2025-09-13 22:48:22 (federatedscope.llm.llm_local.client:480) INFO: Client 32 Adapter 1 with val results: {'val_total': 112, 'val_loss': 67.18356323242188, 'val_avg_loss': 0.5998532431466239, 'val_seen': 112, 'val_correct': 80, 'val_acc': 0.7142857142857143}
2025-09-13 22:48:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:48:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:48:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:48:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:48:23 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.895454, avg_loss=0.722386, seen=40, correct=21, accuracy=0.525000
2025-09-13 22:48:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:48:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:48:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:48:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2006MB
2025-09-13 22:48:24 (federatedscope.llm.llm_local.client:501) INFO: Client 32 Adapter 1 with test results: {'test_total': 40, 'test_loss': 28.89545440673828, 'test_avg_loss': 0.722386360168457, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-13 22:48:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-09-13 22:48:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:48:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=56, num_train_batch_last_epoch=44, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:48:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-13 22:48:28 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=112, loss_sum=75.240929, avg_loss=0.671794, seen=112, correct=69, accuracy=0.616071
2025-09-13 22:48:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:48:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:48:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:48:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2006MB
2025-09-13 22:48:29 (federatedscope.llm.llm_local.client:480) INFO: Client 32 Adapter 2 with val results: {'val_total': 112, 'val_loss': 75.24092864990234, 'val_avg_loss': 0.6717940058026995, 'val_seen': 112, 'val_correct': 69, 'val_acc': 0.6160714285714286}
2025-09-13 22:48:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:48:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:48:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:48:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:48:30 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.678396, avg_loss=0.691960, seen=40, correct=22, accuracy=0.550000
2025-09-13 22:48:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:48:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:48:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:48:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2006MB
2025-09-13 22:48:31 (federatedscope.llm.llm_local.client:501) INFO: Client 32 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.678396224975586, 'test_avg_loss': 0.6919599056243897, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-13 22:48:32 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-13 22:48:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-09-13 22:48:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:48:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=37, num_train_batch_last_epoch=26, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:48:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-13 22:48:34 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=74, loss_sum=57.565788, avg_loss=0.777916, seen=74, correct=33, accuracy=0.445946
2025-09-13 22:48:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:48:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:48:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:48:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2040MB
2025-09-13 22:48:35 (federatedscope.llm.llm_local.client:480) INFO: Client 33 Adapter 0 with val results: {'val_total': 74, 'val_loss': 57.56578826904297, 'val_avg_loss': 0.7779160576897699, 'val_seen': 74, 'val_correct': 33, 'val_acc': 0.44594594594594594}
2025-09-13 22:48:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:48:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:48:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:48:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:48:37 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.359749, avg_loss=0.633994, seen=40, correct=25, accuracy=0.625000
2025-09-13 22:48:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:48:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:48:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:48:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2040MB
2025-09-13 22:48:38 (federatedscope.llm.llm_local.client:501) INFO: Client 33 Adapter 0 with test results: {'test_total': 40, 'test_loss': 25.35974884033203, 'test_avg_loss': 0.6339937210083008, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-13 22:48:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-09-13 22:48:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:48:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=37, num_train_batch_last_epoch=26, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:48:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-13 22:48:40 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=74, loss_sum=49.657257, avg_loss=0.671044, seen=74, correct=48, accuracy=0.648649
2025-09-13 22:48:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:48:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:48:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:48:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2015MB
2025-09-13 22:48:41 (federatedscope.llm.llm_local.client:480) INFO: Client 33 Adapter 1 with val results: {'val_total': 74, 'val_loss': 49.657257080078125, 'val_avg_loss': 0.6710440145956503, 'val_seen': 74, 'val_correct': 48, 'val_acc': 0.6486486486486487}
2025-09-13 22:48:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:48:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:48:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:48:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:48:43 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.514992, avg_loss=0.687875, seen=40, correct=23, accuracy=0.575000
2025-09-13 22:48:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:48:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:48:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:48:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2015MB
2025-09-13 22:48:44 (federatedscope.llm.llm_local.client:501) INFO: Client 33 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.514991760253906, 'test_avg_loss': 0.6878747940063477, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-13 22:48:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-09-13 22:48:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:48:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=37, num_train_batch_last_epoch=26, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:48:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-13 22:48:46 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=74, loss_sum=52.740669, avg_loss=0.712712, seen=74, correct=43, accuracy=0.581081
2025-09-13 22:48:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:48:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:48:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:48:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2015MB
2025-09-13 22:48:47 (federatedscope.llm.llm_local.client:480) INFO: Client 33 Adapter 2 with val results: {'val_total': 74, 'val_loss': 52.74066925048828, 'val_avg_loss': 0.71271174662822, 'val_seen': 74, 'val_correct': 43, 'val_acc': 0.581081081081081}
2025-09-13 22:48:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:48:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:48:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:48:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:48:49 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.349676, avg_loss=0.708742, seen=40, correct=24, accuracy=0.600000
2025-09-13 22:48:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:48:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:48:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:48:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2015MB
2025-09-13 22:48:50 (federatedscope.llm.llm_local.client:501) INFO: Client 33 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.34967613220215, 'test_avg_loss': 0.7087419033050537, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-13 22:48:50 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-13 22:48:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 22:48:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:48:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:48:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 22:48:57 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=126.023315, avg_loss=0.630117, seen=200, correct=129, accuracy=0.645000
2025-09-13 22:48:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:48:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:48:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:48:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2048MB
2025-09-13 22:48:59 (federatedscope.llm.llm_local.client:480) INFO: Client 34 Adapter 0 with val results: {'val_total': 200, 'val_loss': 126.0233154296875, 'val_avg_loss': 0.6301165771484375, 'val_seen': 200, 'val_correct': 129, 'val_acc': 0.645}
2025-09-13 22:48:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:48:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:48:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:49:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:49:01 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.174274, avg_loss=0.629357, seen=40, correct=25, accuracy=0.625000
2025-09-13 22:49:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:49:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:49:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:49:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2048MB
2025-09-13 22:49:02 (federatedscope.llm.llm_local.client:501) INFO: Client 34 Adapter 0 with test results: {'test_total': 40, 'test_loss': 25.174274444580078, 'test_avg_loss': 0.629356861114502, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-13 22:49:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 22:49:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:49:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:49:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 22:49:09 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=132.505585, avg_loss=0.662528, seen=200, correct=122, accuracy=0.610000
2025-09-13 22:49:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:49:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:49:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:49:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2023MB
2025-09-13 22:49:10 (federatedscope.llm.llm_local.client:480) INFO: Client 34 Adapter 1 with val results: {'val_total': 200, 'val_loss': 132.50558471679688, 'val_avg_loss': 0.6625279235839844, 'val_seen': 200, 'val_correct': 122, 'val_acc': 0.61}
2025-09-13 22:49:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:49:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:49:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:49:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:49:11 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=22.709688, avg_loss=0.567742, seen=40, correct=27, accuracy=0.675000
2025-09-13 22:49:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:49:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:49:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:49:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2023MB
2025-09-13 22:49:12 (federatedscope.llm.llm_local.client:501) INFO: Client 34 Adapter 1 with test results: {'test_total': 40, 'test_loss': 22.709688186645508, 'test_avg_loss': 0.5677422046661377, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-13 22:49:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 22:49:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:49:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:49:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 22:49:19 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=140.558365, avg_loss=0.702792, seen=200, correct=110, accuracy=0.550000
2025-09-13 22:49:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:49:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:49:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:49:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2023MB
2025-09-13 22:49:20 (federatedscope.llm.llm_local.client:480) INFO: Client 34 Adapter 2 with val results: {'val_total': 200, 'val_loss': 140.55836486816406, 'val_avg_loss': 0.7027918243408203, 'val_seen': 200, 'val_correct': 110, 'val_acc': 0.55}
2025-09-13 22:49:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:49:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:49:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:49:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:49:21 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.558096, avg_loss=0.588952, seen=40, correct=26, accuracy=0.650000
2025-09-13 22:49:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:49:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:49:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:49:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2023MB
2025-09-13 22:49:22 (federatedscope.llm.llm_local.client:501) INFO: Client 34 Adapter 2 with test results: {'test_total': 40, 'test_loss': 23.558095932006836, 'test_avg_loss': 0.5889523983001709, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-13 22:49:22 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-13 22:49:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 22:49:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:49:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:49:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 22:49:28 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=124.718170, avg_loss=0.623591, seen=200, correct=128, accuracy=0.640000
2025-09-13 22:49:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:49:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:49:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:49:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2057MB
2025-09-13 22:49:30 (federatedscope.llm.llm_local.client:480) INFO: Client 35 Adapter 0 with val results: {'val_total': 200, 'val_loss': 124.71817016601562, 'val_avg_loss': 0.6235908508300781, 'val_seen': 200, 'val_correct': 128, 'val_acc': 0.64}
2025-09-13 22:49:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:49:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:49:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:49:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:49:32 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.481073, avg_loss=0.637027, seen=40, correct=27, accuracy=0.675000
2025-09-13 22:49:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:49:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:49:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:49:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2057MB
2025-09-13 22:49:33 (federatedscope.llm.llm_local.client:501) INFO: Client 35 Adapter 0 with test results: {'test_total': 40, 'test_loss': 25.4810733795166, 'test_avg_loss': 0.6370268344879151, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-13 22:49:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 22:49:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:49:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:49:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 22:49:39 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=145.178772, avg_loss=0.725894, seen=200, correct=99, accuracy=0.495000
2025-09-13 22:49:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:49:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:49:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:49:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2032MB
2025-09-13 22:49:40 (federatedscope.llm.llm_local.client:480) INFO: Client 35 Adapter 1 with val results: {'val_total': 200, 'val_loss': 145.17877197265625, 'val_avg_loss': 0.7258938598632813, 'val_seen': 200, 'val_correct': 99, 'val_acc': 0.495}
2025-09-13 22:49:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:49:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:49:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:49:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:49:41 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=31.017796, avg_loss=0.775445, seen=40, correct=19, accuracy=0.475000
2025-09-13 22:49:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:49:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:49:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:49:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2032MB
2025-09-13 22:49:42 (federatedscope.llm.llm_local.client:501) INFO: Client 35 Adapter 1 with test results: {'test_total': 40, 'test_loss': 31.01779556274414, 'test_avg_loss': 0.7754448890686035, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-13 22:49:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 22:49:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:49:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:49:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 22:49:48 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=154.205383, avg_loss=0.771027, seen=200, correct=90, accuracy=0.450000
2025-09-13 22:49:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:49:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:49:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:49:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2032MB
2025-09-13 22:49:49 (federatedscope.llm.llm_local.client:480) INFO: Client 35 Adapter 2 with val results: {'val_total': 200, 'val_loss': 154.20538330078125, 'val_avg_loss': 0.7710269165039062, 'val_seen': 200, 'val_correct': 90, 'val_acc': 0.45}
2025-09-13 22:49:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:49:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:49:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:49:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:49:50 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.766665, avg_loss=0.769167, seen=40, correct=16, accuracy=0.400000
2025-09-13 22:49:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:49:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:49:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:49:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2032MB
2025-09-13 22:49:51 (federatedscope.llm.llm_local.client:501) INFO: Client 35 Adapter 2 with test results: {'test_total': 40, 'test_loss': 30.766664505004883, 'test_avg_loss': 0.7691666126251221, 'test_seen': 40, 'test_correct': 16, 'test_acc': 0.4}
2025-09-13 22:49:51 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-13 22:49:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-09-13 22:49:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:49:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=27, num_train_batch_last_epoch=19, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:49:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-09-13 22:49:53 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=54, loss_sum=37.449348, avg_loss=0.693506, seen=54, correct=28, accuracy=0.518519
2025-09-13 22:49:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:49:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:49:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:49:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2065MB
2025-09-13 22:49:54 (federatedscope.llm.llm_local.client:480) INFO: Client 36 Adapter 0 with val results: {'val_total': 54, 'val_loss': 37.44934844970703, 'val_avg_loss': 0.6935064527723525, 'val_seen': 54, 'val_correct': 28, 'val_acc': 0.5185185185185185}
2025-09-13 22:49:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:49:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:49:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:49:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:49:55 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.716837, avg_loss=0.642921, seen=40, correct=26, accuracy=0.650000
2025-09-13 22:49:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:49:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:49:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:49:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2065MB
2025-09-13 22:49:56 (federatedscope.llm.llm_local.client:501) INFO: Client 36 Adapter 0 with test results: {'test_total': 40, 'test_loss': 25.71683692932129, 'test_avg_loss': 0.6429209232330322, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-13 22:49:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-09-13 22:49:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:49:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=27, num_train_batch_last_epoch=19, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:49:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-09-13 22:49:58 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=54, loss_sum=40.060081, avg_loss=0.741853, seen=54, correct=29, accuracy=0.537037
2025-09-13 22:49:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:49:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:49:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:49:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2040MB
2025-09-13 22:49:59 (federatedscope.llm.llm_local.client:480) INFO: Client 36 Adapter 1 with val results: {'val_total': 54, 'val_loss': 40.060081481933594, 'val_avg_loss': 0.7418533607765481, 'val_seen': 54, 'val_correct': 29, 'val_acc': 0.5370370370370371}
2025-09-13 22:50:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:50:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:50:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:50:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:50:01 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.374037, avg_loss=0.584351, seen=40, correct=30, accuracy=0.750000
2025-09-13 22:50:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:50:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:50:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:50:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2040MB
2025-09-13 22:50:02 (federatedscope.llm.llm_local.client:501) INFO: Client 36 Adapter 1 with test results: {'test_total': 40, 'test_loss': 23.37403678894043, 'test_avg_loss': 0.5843509197235107, 'test_seen': 40, 'test_correct': 30, 'test_acc': 0.75}
2025-09-13 22:50:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-09-13 22:50:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:50:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=27, num_train_batch_last_epoch=19, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:50:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-09-13 22:50:04 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=54, loss_sum=38.380138, avg_loss=0.710743, seen=54, correct=29, accuracy=0.537037
2025-09-13 22:50:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:50:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:50:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:50:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2040MB
2025-09-13 22:50:06 (federatedscope.llm.llm_local.client:480) INFO: Client 36 Adapter 2 with val results: {'val_total': 54, 'val_loss': 38.3801383972168, 'val_avg_loss': 0.7107433036521629, 'val_seen': 54, 'val_correct': 29, 'val_acc': 0.5370370370370371}
2025-09-13 22:50:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:50:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:50:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:50:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:50:07 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.975658, avg_loss=0.649391, seen=40, correct=24, accuracy=0.600000
2025-09-13 22:50:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:50:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:50:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:50:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2040MB
2025-09-13 22:50:08 (federatedscope.llm.llm_local.client:501) INFO: Client 36 Adapter 2 with test results: {'test_total': 40, 'test_loss': 25.975658416748047, 'test_avg_loss': 0.6493914604187012, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-13 22:50:08 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-13 22:50:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 22:50:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:50:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:50:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 22:50:15 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=139.085373, avg_loss=0.695427, seen=200, correct=116, accuracy=0.580000
2025-09-13 22:50:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:50:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:50:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:50:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2074MB
2025-09-13 22:50:17 (federatedscope.llm.llm_local.client:480) INFO: Client 37 Adapter 0 with val results: {'val_total': 200, 'val_loss': 139.0853729248047, 'val_avg_loss': 0.6954268646240235, 'val_seen': 200, 'val_correct': 116, 'val_acc': 0.58}
2025-09-13 22:50:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:50:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:50:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:50:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:50:19 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.264021, avg_loss=0.681601, seen=40, correct=23, accuracy=0.575000
2025-09-13 22:50:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:50:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:50:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:50:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2074MB
2025-09-13 22:50:20 (federatedscope.llm.llm_local.client:501) INFO: Client 37 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.264020919799805, 'test_avg_loss': 0.6816005229949951, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-13 22:50:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 22:50:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:50:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:50:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 22:50:26 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=138.589188, avg_loss=0.692946, seen=200, correct=121, accuracy=0.605000
2025-09-13 22:50:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:50:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:50:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:50:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2049MB
2025-09-13 22:50:28 (federatedscope.llm.llm_local.client:480) INFO: Client 37 Adapter 1 with val results: {'val_total': 200, 'val_loss': 138.5891876220703, 'val_avg_loss': 0.6929459381103515, 'val_seen': 200, 'val_correct': 121, 'val_acc': 0.605}
2025-09-13 22:50:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:50:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:50:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:50:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:50:30 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.346579, avg_loss=0.633664, seen=40, correct=27, accuracy=0.675000
2025-09-13 22:50:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:50:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:50:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:50:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2049MB
2025-09-13 22:50:31 (federatedscope.llm.llm_local.client:501) INFO: Client 37 Adapter 1 with test results: {'test_total': 40, 'test_loss': 25.34657859802246, 'test_avg_loss': 0.6336644649505615, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-13 22:50:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 22:50:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:50:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:50:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 22:50:36 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=142.914093, avg_loss=0.714570, seen=200, correct=112, accuracy=0.560000
2025-09-13 22:50:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:50:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:50:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:50:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2049MB
2025-09-13 22:50:37 (federatedscope.llm.llm_local.client:480) INFO: Client 37 Adapter 2 with val results: {'val_total': 200, 'val_loss': 142.91409301757812, 'val_avg_loss': 0.7145704650878906, 'val_seen': 200, 'val_correct': 112, 'val_acc': 0.56}
2025-09-13 22:50:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:50:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:50:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:50:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:50:39 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.743881, avg_loss=0.693597, seen=40, correct=22, accuracy=0.550000
2025-09-13 22:50:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:50:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:50:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:50:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2049MB
2025-09-13 22:50:40 (federatedscope.llm.llm_local.client:501) INFO: Client 37 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.743881225585938, 'test_avg_loss': 0.6935970306396484, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-13 22:50:40 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-13 22:50:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 22:50:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:50:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:50:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 22:50:46 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=128.175812, avg_loss=0.640879, seen=200, correct=128, accuracy=0.640000
2025-09-13 22:50:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:50:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:50:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:50:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2082MB
2025-09-13 22:50:48 (federatedscope.llm.llm_local.client:480) INFO: Client 38 Adapter 0 with val results: {'val_total': 200, 'val_loss': 128.17581176757812, 'val_avg_loss': 0.6408790588378906, 'val_seen': 200, 'val_correct': 128, 'val_acc': 0.64}
2025-09-13 22:50:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:50:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:50:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:50:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:50:50 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.130604, avg_loss=0.628265, seen=40, correct=23, accuracy=0.575000
2025-09-13 22:50:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:50:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:50:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:50:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2082MB
2025-09-13 22:50:51 (federatedscope.llm.llm_local.client:501) INFO: Client 38 Adapter 0 with test results: {'test_total': 40, 'test_loss': 25.130603790283203, 'test_avg_loss': 0.62826509475708, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-13 22:50:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 22:50:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:50:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:50:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 22:50:57 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=138.406738, avg_loss=0.692034, seen=200, correct=113, accuracy=0.565000
2025-09-13 22:50:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:50:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:50:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:50:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2057MB
2025-09-13 22:50:59 (federatedscope.llm.llm_local.client:480) INFO: Client 38 Adapter 1 with val results: {'val_total': 200, 'val_loss': 138.40673828125, 'val_avg_loss': 0.69203369140625, 'val_seen': 200, 'val_correct': 113, 'val_acc': 0.565}
2025-09-13 22:51:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:51:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:51:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:51:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:51:01 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.412380, avg_loss=0.710310, seen=40, correct=20, accuracy=0.500000
2025-09-13 22:51:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:51:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:51:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:51:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2057MB
2025-09-13 22:51:02 (federatedscope.llm.llm_local.client:501) INFO: Client 38 Adapter 1 with test results: {'test_total': 40, 'test_loss': 28.41238021850586, 'test_avg_loss': 0.7103095054626465, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-13 22:51:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 22:51:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:51:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:51:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 22:51:09 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=146.087158, avg_loss=0.730436, seen=200, correct=95, accuracy=0.475000
2025-09-13 22:51:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:51:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:51:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:51:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2057MB
2025-09-13 22:51:11 (federatedscope.llm.llm_local.client:480) INFO: Client 38 Adapter 2 with val results: {'val_total': 200, 'val_loss': 146.087158203125, 'val_avg_loss': 0.730435791015625, 'val_seen': 200, 'val_correct': 95, 'val_acc': 0.475}
2025-09-13 22:51:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:51:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:51:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:51:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:51:12 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.503036, avg_loss=0.737576, seen=40, correct=19, accuracy=0.475000
2025-09-13 22:51:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:51:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:51:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:51:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2057MB
2025-09-13 22:51:13 (federatedscope.llm.llm_local.client:501) INFO: Client 38 Adapter 2 with test results: {'test_total': 40, 'test_loss': 29.503036499023438, 'test_avg_loss': 0.737575912475586, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-13 22:51:13 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-13 22:51:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-13 22:51:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:51:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=16, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:51:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-13 22:51:16 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=61.271500, avg_loss=0.738211, seen=83, correct=44, accuracy=0.530120
2025-09-13 22:51:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:51:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:51:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:51:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2091MB
2025-09-13 22:51:18 (federatedscope.llm.llm_local.client:480) INFO: Client 39 Adapter 0 with val results: {'val_total': 83, 'val_loss': 61.27149963378906, 'val_avg_loss': 0.738210838961314, 'val_seen': 83, 'val_correct': 44, 'val_acc': 0.5301204819277109}
2025-09-13 22:51:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:51:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:51:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:51:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:51:19 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.728601, avg_loss=0.768215, seen=40, correct=20, accuracy=0.500000
2025-09-13 22:51:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:51:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:51:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:51:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2091MB
2025-09-13 22:51:20 (federatedscope.llm.llm_local.client:501) INFO: Client 39 Adapter 0 with test results: {'test_total': 40, 'test_loss': 30.728601455688477, 'test_avg_loss': 0.7682150363922119, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-13 22:51:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-13 22:51:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:51:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=16, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:51:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-13 22:51:23 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=62.755722, avg_loss=0.756093, seen=83, correct=45, accuracy=0.542169
2025-09-13 22:51:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:51:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:51:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:51:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2065MB
2025-09-13 22:51:24 (federatedscope.llm.llm_local.client:480) INFO: Client 39 Adapter 1 with val results: {'val_total': 83, 'val_loss': 62.75572204589844, 'val_avg_loss': 0.7560930366975716, 'val_seen': 83, 'val_correct': 45, 'val_acc': 0.5421686746987951}
2025-09-13 22:51:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:51:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:51:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:51:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:51:25 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=31.923229, avg_loss=0.798081, seen=40, correct=20, accuracy=0.500000
2025-09-13 22:51:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:51:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:51:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:51:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2065MB
2025-09-13 22:51:26 (federatedscope.llm.llm_local.client:501) INFO: Client 39 Adapter 1 with test results: {'test_total': 40, 'test_loss': 31.923229217529297, 'test_avg_loss': 0.7980807304382325, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-13 22:51:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-13 22:51:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:51:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=16, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:51:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-13 22:51:29 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=62.325687, avg_loss=0.750912, seen=83, correct=41, accuracy=0.493976
2025-09-13 22:51:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:51:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:51:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:51:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2065MB
2025-09-13 22:51:30 (federatedscope.llm.llm_local.client:480) INFO: Client 39 Adapter 2 with val results: {'val_total': 83, 'val_loss': 62.325687408447266, 'val_avg_loss': 0.7509118964873165, 'val_seen': 83, 'val_correct': 41, 'val_acc': 0.4939759036144578}
2025-09-13 22:51:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:51:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:51:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:51:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:51:31 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.138805, avg_loss=0.753470, seen=40, correct=20, accuracy=0.500000
2025-09-13 22:51:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:51:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:51:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:51:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2065MB
2025-09-13 22:51:32 (federatedscope.llm.llm_local.client:501) INFO: Client 39 Adapter 2 with test results: {'test_total': 40, 'test_loss': 30.138805389404297, 'test_avg_loss': 0.7534701347351074, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-13 22:51:33 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-13 22:51:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 22:51:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:51:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:51:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 22:51:39 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=133.678314, avg_loss=0.668392, seen=200, correct=113, accuracy=0.565000
2025-09-13 22:51:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:51:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:51:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:51:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2099MB
2025-09-13 22:51:40 (federatedscope.llm.llm_local.client:480) INFO: Client 40 Adapter 0 with val results: {'val_total': 200, 'val_loss': 133.67831420898438, 'val_avg_loss': 0.6683915710449219, 'val_seen': 200, 'val_correct': 113, 'val_acc': 0.565}
2025-09-13 22:51:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:51:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:51:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:51:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:51:41 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.109516, avg_loss=0.677738, seen=40, correct=23, accuracy=0.575000
2025-09-13 22:51:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:51:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:51:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:51:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2099MB
2025-09-13 22:51:42 (federatedscope.llm.llm_local.client:501) INFO: Client 40 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.109516143798828, 'test_avg_loss': 0.6777379035949707, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-13 22:51:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 22:51:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:51:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:51:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 22:51:48 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=126.999008, avg_loss=0.634995, seen=200, correct=125, accuracy=0.625000
2025-09-13 22:51:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:51:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:51:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:51:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2074MB
2025-09-13 22:51:50 (federatedscope.llm.llm_local.client:480) INFO: Client 40 Adapter 1 with val results: {'val_total': 200, 'val_loss': 126.99900817871094, 'val_avg_loss': 0.6349950408935547, 'val_seen': 200, 'val_correct': 125, 'val_acc': 0.625}
2025-09-13 22:51:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:51:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:51:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:51:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:51:52 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.684296, avg_loss=0.667107, seen=40, correct=24, accuracy=0.600000
2025-09-13 22:51:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:51:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:51:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:51:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2074MB
2025-09-13 22:51:53 (federatedscope.llm.llm_local.client:501) INFO: Client 40 Adapter 1 with test results: {'test_total': 40, 'test_loss': 26.684295654296875, 'test_avg_loss': 0.6671073913574219, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-13 22:51:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 22:51:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:51:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:51:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 22:51:59 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=135.811600, avg_loss=0.679058, seen=200, correct=117, accuracy=0.585000
2025-09-13 22:51:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:51:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:52:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:52:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2074MB
2025-09-13 22:52:00 (federatedscope.llm.llm_local.client:480) INFO: Client 40 Adapter 2 with val results: {'val_total': 200, 'val_loss': 135.8115997314453, 'val_avg_loss': 0.6790579986572266, 'val_seen': 200, 'val_correct': 117, 'val_acc': 0.585}
2025-09-13 22:52:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:52:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:52:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:52:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:52:02 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.915184, avg_loss=0.722880, seen=40, correct=23, accuracy=0.575000
2025-09-13 22:52:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:52:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:52:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:52:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2074MB
2025-09-13 22:52:03 (federatedscope.llm.llm_local.client:501) INFO: Client 40 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.915184020996094, 'test_avg_loss': 0.7228796005249023, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-13 22:52:03 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-13 22:52:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-09-13 22:52:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:52:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=60, num_train_batch_last_epoch=40, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:52:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-09-13 22:52:06 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=119, loss_sum=81.362404, avg_loss=0.683718, seen=119, correct=73, accuracy=0.613445
2025-09-13 22:52:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:52:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:52:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:52:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2108MB
2025-09-13 22:52:08 (federatedscope.llm.llm_local.client:480) INFO: Client 41 Adapter 0 with val results: {'val_total': 119, 'val_loss': 81.3624038696289, 'val_avg_loss': 0.6837176795767135, 'val_seen': 119, 'val_correct': 73, 'val_acc': 0.6134453781512605}
2025-09-13 22:52:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:52:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:52:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:52:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:52:10 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.895924, avg_loss=0.722398, seen=40, correct=23, accuracy=0.575000
2025-09-13 22:52:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:52:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:52:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:52:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2108MB
2025-09-13 22:52:11 (federatedscope.llm.llm_local.client:501) INFO: Client 41 Adapter 0 with test results: {'test_total': 40, 'test_loss': 28.895923614501953, 'test_avg_loss': 0.7223980903625489, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-13 22:52:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-09-13 22:52:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:52:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=60, num_train_batch_last_epoch=40, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:52:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-09-13 22:52:14 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=119, loss_sum=87.240211, avg_loss=0.733111, seen=119, correct=58, accuracy=0.487395
2025-09-13 22:52:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:52:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:52:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:52:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2083MB
2025-09-13 22:52:16 (federatedscope.llm.llm_local.client:480) INFO: Client 41 Adapter 1 with val results: {'val_total': 119, 'val_loss': 87.2402114868164, 'val_avg_loss': 0.7331110208976168, 'val_seen': 119, 'val_correct': 58, 'val_acc': 0.48739495798319327}
2025-09-13 22:52:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:52:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:52:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:52:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:52:18 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.245550, avg_loss=0.656139, seen=40, correct=26, accuracy=0.650000
2025-09-13 22:52:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:52:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:52:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:52:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2083MB
2025-09-13 22:52:19 (federatedscope.llm.llm_local.client:501) INFO: Client 41 Adapter 1 with test results: {'test_total': 40, 'test_loss': 26.24555015563965, 'test_avg_loss': 0.6561387538909912, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-13 22:52:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-09-13 22:52:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:52:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=60, num_train_batch_last_epoch=40, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:52:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-09-13 22:52:23 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=119, loss_sum=90.830536, avg_loss=0.763282, seen=119, correct=48, accuracy=0.403361
2025-09-13 22:52:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:52:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:52:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:52:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2083MB
2025-09-13 22:52:24 (federatedscope.llm.llm_local.client:480) INFO: Client 41 Adapter 2 with val results: {'val_total': 119, 'val_loss': 90.83053588867188, 'val_avg_loss': 0.76328181419052, 'val_seen': 119, 'val_correct': 48, 'val_acc': 0.40336134453781514}
2025-09-13 22:52:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:52:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:52:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:52:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:52:25 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.057684, avg_loss=0.701442, seen=40, correct=23, accuracy=0.575000
2025-09-13 22:52:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:52:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:52:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:52:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2083MB
2025-09-13 22:52:26 (federatedscope.llm.llm_local.client:501) INFO: Client 41 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.05768394470215, 'test_avg_loss': 0.7014420986175537, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-13 22:52:26 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-13 22:52:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 22:52:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:52:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:52:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 22:52:33 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=130.922058, avg_loss=0.654610, seen=200, correct=126, accuracy=0.630000
2025-09-13 22:52:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:52:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:52:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:52:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2116MB
2025-09-13 22:52:34 (federatedscope.llm.llm_local.client:480) INFO: Client 42 Adapter 0 with val results: {'val_total': 200, 'val_loss': 130.92205810546875, 'val_avg_loss': 0.6546102905273438, 'val_seen': 200, 'val_correct': 126, 'val_acc': 0.63}
2025-09-13 22:52:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:52:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:52:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:52:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:52:36 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.001026, avg_loss=0.675026, seen=40, correct=22, accuracy=0.550000
2025-09-13 22:52:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:52:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:52:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:52:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2116MB
2025-09-13 22:52:37 (federatedscope.llm.llm_local.client:501) INFO: Client 42 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.001026153564453, 'test_avg_loss': 0.6750256538391113, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-13 22:52:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 22:52:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:52:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:52:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 22:52:44 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=132.074326, avg_loss=0.660372, seen=200, correct=115, accuracy=0.575000
2025-09-13 22:52:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:52:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:52:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:52:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2091MB
2025-09-13 22:52:45 (federatedscope.llm.llm_local.client:480) INFO: Client 42 Adapter 1 with val results: {'val_total': 200, 'val_loss': 132.07432556152344, 'val_avg_loss': 0.6603716278076172, 'val_seen': 200, 'val_correct': 115, 'val_acc': 0.575}
2025-09-13 22:52:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:52:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:52:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:52:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:52:46 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.166790, avg_loss=0.654170, seen=40, correct=26, accuracy=0.650000
2025-09-13 22:52:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:52:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:52:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:52:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2091MB
2025-09-13 22:52:47 (federatedscope.llm.llm_local.client:501) INFO: Client 42 Adapter 1 with test results: {'test_total': 40, 'test_loss': 26.166790008544922, 'test_avg_loss': 0.654169750213623, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-13 22:52:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 22:52:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:52:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:52:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 22:52:54 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=140.606628, avg_loss=0.703033, seen=200, correct=104, accuracy=0.520000
2025-09-13 22:52:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:52:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:52:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:52:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2091MB
2025-09-13 22:52:56 (federatedscope.llm.llm_local.client:480) INFO: Client 42 Adapter 2 with val results: {'val_total': 200, 'val_loss': 140.60662841796875, 'val_avg_loss': 0.7030331420898438, 'val_seen': 200, 'val_correct': 104, 'val_acc': 0.52}
2025-09-13 22:52:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:52:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:52:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:52:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:52:57 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.982517, avg_loss=0.749563, seen=40, correct=19, accuracy=0.475000
2025-09-13 22:52:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:52:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:52:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:52:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2091MB
2025-09-13 22:52:58 (federatedscope.llm.llm_local.client:501) INFO: Client 42 Adapter 2 with test results: {'test_total': 40, 'test_loss': 29.98251724243164, 'test_avg_loss': 0.749562931060791, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-13 22:52:58 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-13 22:52:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-09-13 22:52:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:52:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=45, num_train_batch_last_epoch=10, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:53:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-09-13 22:53:01 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=89, loss_sum=62.287544, avg_loss=0.699860, seen=89, correct=47, accuracy=0.528090
2025-09-13 22:53:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:53:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:53:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:53:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2124MB
2025-09-13 22:53:02 (federatedscope.llm.llm_local.client:480) INFO: Client 43 Adapter 0 with val results: {'val_total': 89, 'val_loss': 62.28754425048828, 'val_avg_loss': 0.6998600477582952, 'val_seen': 89, 'val_correct': 47, 'val_acc': 0.5280898876404494}
2025-09-13 22:53:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:53:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:53:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:53:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:53:04 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.753326, avg_loss=0.643833, seen=40, correct=26, accuracy=0.650000
2025-09-13 22:53:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:53:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:53:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:53:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2124MB
2025-09-13 22:53:05 (federatedscope.llm.llm_local.client:501) INFO: Client 43 Adapter 0 with test results: {'test_total': 40, 'test_loss': 25.753326416015625, 'test_avg_loss': 0.6438331604003906, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-13 22:53:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-09-13 22:53:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:53:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=45, num_train_batch_last_epoch=10, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:53:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-09-13 22:53:08 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=89, loss_sum=63.345528, avg_loss=0.711748, seen=89, correct=46, accuracy=0.516854
2025-09-13 22:53:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:53:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:53:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:53:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2099MB
2025-09-13 22:53:09 (federatedscope.llm.llm_local.client:480) INFO: Client 43 Adapter 1 with val results: {'val_total': 89, 'val_loss': 63.34552764892578, 'val_avg_loss': 0.7117475016733233, 'val_seen': 89, 'val_correct': 46, 'val_acc': 0.5168539325842697}
2025-09-13 22:53:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:53:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:53:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:53:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:53:10 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.289692, avg_loss=0.632242, seen=40, correct=28, accuracy=0.700000
2025-09-13 22:53:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:53:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:53:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:53:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2099MB
2025-09-13 22:53:11 (federatedscope.llm.llm_local.client:501) INFO: Client 43 Adapter 1 with test results: {'test_total': 40, 'test_loss': 25.289691925048828, 'test_avg_loss': 0.6322422981262207, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}
2025-09-13 22:53:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-09-13 22:53:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:53:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=45, num_train_batch_last_epoch=10, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:53:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-09-13 22:53:14 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=89, loss_sum=66.690750, avg_loss=0.749334, seen=89, correct=43, accuracy=0.483146
2025-09-13 22:53:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:53:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:53:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:53:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2099MB
2025-09-13 22:53:15 (federatedscope.llm.llm_local.client:480) INFO: Client 43 Adapter 2 with val results: {'val_total': 89, 'val_loss': 66.69075012207031, 'val_avg_loss': 0.7493342710344979, 'val_seen': 89, 'val_correct': 43, 'val_acc': 0.48314606741573035}
2025-09-13 22:53:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:53:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:53:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:53:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:53:16 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.793308, avg_loss=0.669833, seen=40, correct=25, accuracy=0.625000
2025-09-13 22:53:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:53:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:53:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:53:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2099MB
2025-09-13 22:53:18 (federatedscope.llm.llm_local.client:501) INFO: Client 43 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.79330825805664, 'test_avg_loss': 0.669832706451416, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-13 22:53:18 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-13 22:53:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 22:53:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:53:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:53:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 22:53:24 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=141.849136, avg_loss=0.709246, seen=200, correct=104, accuracy=0.520000
2025-09-13 22:53:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:53:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:53:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:53:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2133MB
2025-09-13 22:53:26 (federatedscope.llm.llm_local.client:480) INFO: Client 44 Adapter 0 with val results: {'val_total': 200, 'val_loss': 141.84913635253906, 'val_avg_loss': 0.7092456817626953, 'val_seen': 200, 'val_correct': 104, 'val_acc': 0.52}
2025-09-13 22:53:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:53:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:53:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:53:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:53:28 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.672316, avg_loss=0.591808, seen=40, correct=26, accuracy=0.650000
2025-09-13 22:53:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:53:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:53:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:53:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2133MB
2025-09-13 22:53:29 (federatedscope.llm.llm_local.client:501) INFO: Client 44 Adapter 0 with test results: {'test_total': 40, 'test_loss': 23.67231559753418, 'test_avg_loss': 0.5918078899383545, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-13 22:53:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 22:53:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:53:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:53:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 22:53:35 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=148.699677, avg_loss=0.743498, seen=200, correct=100, accuracy=0.500000
2025-09-13 22:53:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:53:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:53:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:53:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2108MB
2025-09-13 22:53:36 (federatedscope.llm.llm_local.client:480) INFO: Client 44 Adapter 1 with val results: {'val_total': 200, 'val_loss': 148.69967651367188, 'val_avg_loss': 0.7434983825683594, 'val_seen': 200, 'val_correct': 100, 'val_acc': 0.5}
2025-09-13 22:53:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:53:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:53:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:53:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:53:38 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=31.847198, avg_loss=0.796180, seen=40, correct=17, accuracy=0.425000
2025-09-13 22:53:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:53:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:53:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:53:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2108MB
2025-09-13 22:53:39 (federatedscope.llm.llm_local.client:501) INFO: Client 44 Adapter 1 with test results: {'test_total': 40, 'test_loss': 31.847198486328125, 'test_avg_loss': 0.7961799621582031, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}
2025-09-13 22:53:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 22:53:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:53:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:53:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 22:53:45 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=152.218536, avg_loss=0.761093, seen=200, correct=97, accuracy=0.485000
2025-09-13 22:53:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:53:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:53:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:53:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2108MB
2025-09-13 22:53:46 (federatedscope.llm.llm_local.client:480) INFO: Client 44 Adapter 2 with val results: {'val_total': 200, 'val_loss': 152.21853637695312, 'val_avg_loss': 0.7610926818847656, 'val_seen': 200, 'val_correct': 97, 'val_acc': 0.485}
2025-09-13 22:53:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:53:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:53:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:53:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:53:47 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=32.276627, avg_loss=0.806916, seen=40, correct=16, accuracy=0.400000
2025-09-13 22:53:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:53:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:53:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:53:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2108MB
2025-09-13 22:53:48 (federatedscope.llm.llm_local.client:501) INFO: Client 44 Adapter 2 with test results: {'test_total': 40, 'test_loss': 32.27662658691406, 'test_avg_loss': 0.8069156646728516, 'test_seen': 40, 'test_correct': 16, 'test_acc': 0.4}
2025-09-13 22:53:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-13 22:53:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-09-13 22:53:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:53:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=50, num_train_batch_last_epoch=50, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:53:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-09-13 22:53:52 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=100, loss_sum=61.597645, avg_loss=0.615976, seen=100, correct=64, accuracy=0.640000
2025-09-13 22:53:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:53:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:53:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:53:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2141MB
2025-09-13 22:53:54 (federatedscope.llm.llm_local.client:480) INFO: Client 45 Adapter 0 with val results: {'val_total': 100, 'val_loss': 61.5976448059082, 'val_avg_loss': 0.615976448059082, 'val_seen': 100, 'val_correct': 64, 'val_acc': 0.64}
2025-09-13 22:53:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:53:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:53:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:53:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:53:55 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.346621, avg_loss=0.708666, seen=40, correct=21, accuracy=0.525000
2025-09-13 22:53:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:53:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:53:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:53:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2141MB
2025-09-13 22:53:56 (federatedscope.llm.llm_local.client:501) INFO: Client 45 Adapter 0 with test results: {'test_total': 40, 'test_loss': 28.346620559692383, 'test_avg_loss': 0.7086655139923096, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-13 22:53:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-09-13 22:53:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:53:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=50, num_train_batch_last_epoch=50, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:54:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-09-13 22:54:00 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=100, loss_sum=75.965248, avg_loss=0.759652, seen=100, correct=46, accuracy=0.460000
2025-09-13 22:54:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:54:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:54:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:54:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2116MB
2025-09-13 22:54:01 (federatedscope.llm.llm_local.client:480) INFO: Client 45 Adapter 1 with val results: {'val_total': 100, 'val_loss': 75.96524810791016, 'val_avg_loss': 0.7596524810791015, 'val_seen': 100, 'val_correct': 46, 'val_acc': 0.46}
2025-09-13 22:54:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:54:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:54:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:54:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:54:02 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.831476, avg_loss=0.720787, seen=40, correct=20, accuracy=0.500000
2025-09-13 22:54:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:54:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:54:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:54:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2116MB
2025-09-13 22:54:03 (federatedscope.llm.llm_local.client:501) INFO: Client 45 Adapter 1 with test results: {'test_total': 40, 'test_loss': 28.83147621154785, 'test_avg_loss': 0.7207869052886963, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-13 22:54:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-09-13 22:54:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:54:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=50, num_train_batch_last_epoch=50, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:54:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-09-13 22:54:06 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=100, loss_sum=79.352036, avg_loss=0.793520, seen=100, correct=38, accuracy=0.380000
2025-09-13 22:54:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:54:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:54:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:54:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2116MB
2025-09-13 22:54:07 (federatedscope.llm.llm_local.client:480) INFO: Client 45 Adapter 2 with val results: {'val_total': 100, 'val_loss': 79.35203552246094, 'val_avg_loss': 0.7935203552246094, 'val_seen': 100, 'val_correct': 38, 'val_acc': 0.38}
2025-09-13 22:54:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:54:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:54:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:54:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:54:08 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.698898, avg_loss=0.767472, seen=40, correct=20, accuracy=0.500000
2025-09-13 22:54:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:54:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:54:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:54:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2116MB
2025-09-13 22:54:09 (federatedscope.llm.llm_local.client:501) INFO: Client 45 Adapter 2 with test results: {'test_total': 40, 'test_loss': 30.698898315429688, 'test_avg_loss': 0.7674724578857421, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-13 22:54:09 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-13 22:54:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-13 22:54:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:54:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=45, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:54:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-13 22:54:13 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=74.554321, avg_loss=0.677767, seen=110, correct=67, accuracy=0.609091
2025-09-13 22:54:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:54:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:54:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:54:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2150MB
2025-09-13 22:54:15 (federatedscope.llm.llm_local.client:480) INFO: Client 46 Adapter 0 with val results: {'val_total': 110, 'val_loss': 74.5543212890625, 'val_avg_loss': 0.6777665571732955, 'val_seen': 110, 'val_correct': 67, 'val_acc': 0.6090909090909091}
2025-09-13 22:54:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:54:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:54:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:54:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:54:16 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.480263, avg_loss=0.587007, seen=40, correct=31, accuracy=0.775000
2025-09-13 22:54:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:54:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:54:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:54:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2150MB
2025-09-13 22:54:17 (federatedscope.llm.llm_local.client:501) INFO: Client 46 Adapter 0 with test results: {'test_total': 40, 'test_loss': 23.480262756347656, 'test_avg_loss': 0.5870065689086914, 'test_seen': 40, 'test_correct': 31, 'test_acc': 0.775}
2025-09-13 22:54:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-13 22:54:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:54:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=45, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:54:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-13 22:54:21 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=78.827789, avg_loss=0.716616, seen=110, correct=65, accuracy=0.590909
2025-09-13 22:54:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:54:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:54:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:54:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2124MB
2025-09-13 22:54:22 (federatedscope.llm.llm_local.client:480) INFO: Client 46 Adapter 1 with val results: {'val_total': 110, 'val_loss': 78.82778930664062, 'val_avg_loss': 0.7166162664240057, 'val_seen': 110, 'val_correct': 65, 'val_acc': 0.5909090909090909}
2025-09-13 22:54:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:54:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:54:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:54:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:54:23 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.884829, avg_loss=0.672121, seen=40, correct=19, accuracy=0.475000
2025-09-13 22:54:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:54:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:54:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:54:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2124MB
2025-09-13 22:54:24 (federatedscope.llm.llm_local.client:501) INFO: Client 46 Adapter 1 with test results: {'test_total': 40, 'test_loss': 26.884828567504883, 'test_avg_loss': 0.6721207141876221, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-13 22:54:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-13 22:54:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:54:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=45, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:54:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-13 22:54:27 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=78.533417, avg_loss=0.713940, seen=110, correct=60, accuracy=0.545455
2025-09-13 22:54:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:54:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:54:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:54:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2124MB
2025-09-13 22:54:29 (federatedscope.llm.llm_local.client:480) INFO: Client 46 Adapter 2 with val results: {'val_total': 110, 'val_loss': 78.53341674804688, 'val_avg_loss': 0.7139401522549715, 'val_seen': 110, 'val_correct': 60, 'val_acc': 0.5454545454545454}
2025-09-13 22:54:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:54:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:54:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:54:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:54:30 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.754070, avg_loss=0.743852, seen=40, correct=18, accuracy=0.450000
2025-09-13 22:54:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:54:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:54:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:54:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2124MB
2025-09-13 22:54:32 (federatedscope.llm.llm_local.client:501) INFO: Client 46 Adapter 2 with test results: {'test_total': 40, 'test_loss': 29.754070281982422, 'test_avg_loss': 0.7438517570495605, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-13 22:54:32 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-13 22:54:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-09-13 22:54:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:54:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=74, num_train_batch_last_epoch=26, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:54:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-13 22:54:37 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=147, loss_sum=104.704544, avg_loss=0.712276, seen=147, correct=75, accuracy=0.510204
2025-09-13 22:54:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:54:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:54:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:54:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2158MB
2025-09-13 22:54:38 (federatedscope.llm.llm_local.client:480) INFO: Client 47 Adapter 0 with val results: {'val_total': 147, 'val_loss': 104.70454406738281, 'val_avg_loss': 0.712275809982196, 'val_seen': 147, 'val_correct': 75, 'val_acc': 0.5102040816326531}
2025-09-13 22:54:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:54:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:54:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:54:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:54:40 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.475817, avg_loss=0.636895, seen=40, correct=23, accuracy=0.575000
2025-09-13 22:54:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:54:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:54:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:54:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2158MB
2025-09-13 22:54:41 (federatedscope.llm.llm_local.client:501) INFO: Client 47 Adapter 0 with test results: {'test_total': 40, 'test_loss': 25.47581672668457, 'test_avg_loss': 0.6368954181671143, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-13 22:54:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-09-13 22:54:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:54:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=74, num_train_batch_last_epoch=26, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:54:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-13 22:54:45 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=147, loss_sum=103.766289, avg_loss=0.705893, seen=147, correct=81, accuracy=0.551020
2025-09-13 22:54:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:54:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:54:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:54:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2133MB
2025-09-13 22:54:47 (federatedscope.llm.llm_local.client:480) INFO: Client 47 Adapter 1 with val results: {'val_total': 147, 'val_loss': 103.76628875732422, 'val_avg_loss': 0.7058931207981239, 'val_seen': 147, 'val_correct': 81, 'val_acc': 0.5510204081632653}
2025-09-13 22:54:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:54:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:54:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:54:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:54:48 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.400660, avg_loss=0.635016, seen=40, correct=28, accuracy=0.700000
2025-09-13 22:54:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:54:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:54:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:54:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2133MB
2025-09-13 22:54:49 (federatedscope.llm.llm_local.client:501) INFO: Client 47 Adapter 1 with test results: {'test_total': 40, 'test_loss': 25.400659561157227, 'test_avg_loss': 0.6350164890289307, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}
2025-09-13 22:54:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-09-13 22:54:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:54:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=74, num_train_batch_last_epoch=26, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:54:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-13 22:54:54 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=147, loss_sum=110.419334, avg_loss=0.751152, seen=147, correct=74, accuracy=0.503401
2025-09-13 22:54:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:54:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:54:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:54:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2133MB
2025-09-13 22:54:55 (federatedscope.llm.llm_local.client:480) INFO: Client 47 Adapter 2 with val results: {'val_total': 147, 'val_loss': 110.4193344116211, 'val_avg_loss': 0.7511519347729326, 'val_seen': 147, 'val_correct': 74, 'val_acc': 0.5034013605442177}
2025-09-13 22:54:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:54:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:54:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:54:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:54:57 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.441097, avg_loss=0.661027, seen=40, correct=23, accuracy=0.575000
2025-09-13 22:54:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:54:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:54:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:54:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2133MB
2025-09-13 22:54:58 (federatedscope.llm.llm_local.client:501) INFO: Client 47 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.441097259521484, 'test_avg_loss': 0.6610274314880371, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-13 22:54:58 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-13 22:54:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-09-13 22:54:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:54:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=23, num_train_batch_last_epoch=8, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:55:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-09-13 22:55:00 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=46, loss_sum=34.251175, avg_loss=0.744591, seen=46, correct=25, accuracy=0.543478
2025-09-13 22:55:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:55:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:55:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:55:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2166MB
2025-09-13 22:55:02 (federatedscope.llm.llm_local.client:480) INFO: Client 48 Adapter 0 with val results: {'val_total': 46, 'val_loss': 34.25117492675781, 'val_avg_loss': 0.7445907592773438, 'val_seen': 46, 'val_correct': 25, 'val_acc': 0.5434782608695652}
2025-09-13 22:55:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:55:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:55:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:55:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:55:03 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=32.091324, avg_loss=0.802283, seen=40, correct=19, accuracy=0.475000
2025-09-13 22:55:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:55:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:55:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:55:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2166MB
2025-09-13 22:55:04 (federatedscope.llm.llm_local.client:501) INFO: Client 48 Adapter 0 with test results: {'test_total': 40, 'test_loss': 32.09132385253906, 'test_avg_loss': 0.8022830963134766, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-13 22:55:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-09-13 22:55:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:55:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=23, num_train_batch_last_epoch=8, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:55:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-09-13 22:55:06 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=46, loss_sum=34.502502, avg_loss=0.750054, seen=46, correct=22, accuracy=0.478261
2025-09-13 22:55:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:55:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:55:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:55:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2141MB
2025-09-13 22:55:08 (federatedscope.llm.llm_local.client:480) INFO: Client 48 Adapter 1 with val results: {'val_total': 46, 'val_loss': 34.50250244140625, 'val_avg_loss': 0.7500544009001359, 'val_seen': 46, 'val_correct': 22, 'val_acc': 0.4782608695652174}
2025-09-13 22:55:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:55:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:55:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:55:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:55:09 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.312269, avg_loss=0.657807, seen=40, correct=27, accuracy=0.675000
2025-09-13 22:55:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:55:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:55:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:55:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2141MB
2025-09-13 22:55:11 (federatedscope.llm.llm_local.client:501) INFO: Client 48 Adapter 1 with test results: {'test_total': 40, 'test_loss': 26.31226921081543, 'test_avg_loss': 0.6578067302703857, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-13 22:55:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-09-13 22:55:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:55:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=23, num_train_batch_last_epoch=8, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:55:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-09-13 22:55:12 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=46, loss_sum=34.681763, avg_loss=0.753951, seen=46, correct=21, accuracy=0.456522
2025-09-13 22:55:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:55:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:55:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:55:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2141MB
2025-09-13 22:55:14 (federatedscope.llm.llm_local.client:480) INFO: Client 48 Adapter 2 with val results: {'val_total': 46, 'val_loss': 34.6817626953125, 'val_avg_loss': 0.753951362941576, 'val_seen': 46, 'val_correct': 21, 'val_acc': 0.45652173913043476}
2025-09-13 22:55:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:55:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:55:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:55:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:55:16 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.237648, avg_loss=0.630941, seen=40, correct=28, accuracy=0.700000
2025-09-13 22:55:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:55:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:55:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:55:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2141MB
2025-09-13 22:55:17 (federatedscope.llm.llm_local.client:501) INFO: Client 48 Adapter 2 with test results: {'test_total': 40, 'test_loss': 25.237648010253906, 'test_avg_loss': 0.6309412002563477, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}
2025-09-13 22:55:17 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-13 22:55:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-09-13 22:55:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:55:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=66, num_train_batch_last_epoch=34, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:55:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-09-13 22:55:21 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=132, loss_sum=85.622726, avg_loss=0.648657, seen=132, correct=88, accuracy=0.666667
2025-09-13 22:55:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:55:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:55:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:55:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2175MB
2025-09-13 22:55:23 (federatedscope.llm.llm_local.client:480) INFO: Client 49 Adapter 0 with val results: {'val_total': 132, 'val_loss': 85.62272644042969, 'val_avg_loss': 0.6486570184881036, 'val_seen': 132, 'val_correct': 88, 'val_acc': 0.6666666666666666}
2025-09-13 22:55:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:55:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:55:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:55:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:55:24 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=32.015907, avg_loss=0.800398, seen=40, correct=18, accuracy=0.450000
2025-09-13 22:55:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:55:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:55:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:55:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2175MB
2025-09-13 22:55:25 (federatedscope.llm.llm_local.client:501) INFO: Client 49 Adapter 0 with test results: {'test_total': 40, 'test_loss': 32.015907287597656, 'test_avg_loss': 0.8003976821899415, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-13 22:55:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-09-13 22:55:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:55:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=66, num_train_batch_last_epoch=34, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:55:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-09-13 22:55:30 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=132, loss_sum=89.094467, avg_loss=0.674958, seen=132, correct=76, accuracy=0.575758
2025-09-13 22:55:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:55:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:55:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:55:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2150MB
2025-09-13 22:55:32 (federatedscope.llm.llm_local.client:480) INFO: Client 49 Adapter 1 with val results: {'val_total': 132, 'val_loss': 89.09446716308594, 'val_avg_loss': 0.6749580845688329, 'val_seen': 132, 'val_correct': 76, 'val_acc': 0.5757575757575758}
2025-09-13 22:55:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:55:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:55:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:55:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:55:33 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.619446, avg_loss=0.765486, seen=40, correct=23, accuracy=0.575000
2025-09-13 22:55:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:55:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:55:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:55:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2150MB
2025-09-13 22:55:34 (federatedscope.llm.llm_local.client:501) INFO: Client 49 Adapter 1 with test results: {'test_total': 40, 'test_loss': 30.61944580078125, 'test_avg_loss': 0.7654861450195313, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-13 22:55:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-09-13 22:55:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:55:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=66, num_train_batch_last_epoch=34, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:55:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-09-13 22:55:39 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=132, loss_sum=88.833023, avg_loss=0.672977, seen=132, correct=74, accuracy=0.560606
2025-09-13 22:55:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:55:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:55:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:55:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2150MB
2025-09-13 22:55:40 (federatedscope.llm.llm_local.client:480) INFO: Client 49 Adapter 2 with val results: {'val_total': 132, 'val_loss': 88.83302307128906, 'val_avg_loss': 0.6729774475097656, 'val_seen': 132, 'val_correct': 74, 'val_acc': 0.5606060606060606}
2025-09-13 22:55:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:55:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:55:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:55:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:55:42 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=31.103800, avg_loss=0.777595, seen=40, correct=19, accuracy=0.475000
2025-09-13 22:55:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:55:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:55:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:55:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2150MB
2025-09-13 22:55:43 (federatedscope.llm.llm_local.client:501) INFO: Client 49 Adapter 2 with test results: {'test_total': 40, 'test_loss': 31.10379981994629, 'test_avg_loss': 0.7775949954986572, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-13 22:55:43 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-13 22:55:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-09-13 22:55:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:55:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=33, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:55:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-13 22:55:47 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=133, loss_sum=92.166473, avg_loss=0.692981, seen=133, correct=72, accuracy=0.541353
2025-09-13 22:55:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:55:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:55:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:55:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2183MB
2025-09-13 22:55:48 (federatedscope.llm.llm_local.client:480) INFO: Client 50 Adapter 0 with val results: {'val_total': 133, 'val_loss': 92.16647338867188, 'val_avg_loss': 0.6929810029223449, 'val_seen': 133, 'val_correct': 72, 'val_acc': 0.5413533834586466}
2025-09-13 22:55:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:55:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:55:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:55:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:55:49 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=31.003998, avg_loss=0.775100, seen=40, correct=19, accuracy=0.475000
2025-09-13 22:55:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:55:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:55:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:55:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2183MB
2025-09-13 22:55:50 (federatedscope.llm.llm_local.client:501) INFO: Client 50 Adapter 0 with test results: {'test_total': 40, 'test_loss': 31.003997802734375, 'test_avg_loss': 0.7750999450683593, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-13 22:55:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-09-13 22:55:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:55:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=33, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:55:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-13 22:55:54 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=133, loss_sum=92.835991, avg_loss=0.698015, seen=133, correct=71, accuracy=0.533835
2025-09-13 22:55:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:55:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:55:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:55:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2158MB
2025-09-13 22:55:56 (federatedscope.llm.llm_local.client:480) INFO: Client 50 Adapter 1 with val results: {'val_total': 133, 'val_loss': 92.83599090576172, 'val_avg_loss': 0.6980149692162535, 'val_seen': 133, 'val_correct': 71, 'val_acc': 0.5338345864661654}
2025-09-13 22:55:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:55:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:55:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:55:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:55:57 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.839626, avg_loss=0.720991, seen=40, correct=19, accuracy=0.475000
2025-09-13 22:55:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:55:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:55:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:55:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2158MB
2025-09-13 22:55:58 (federatedscope.llm.llm_local.client:501) INFO: Client 50 Adapter 1 with test results: {'test_total': 40, 'test_loss': 28.83962631225586, 'test_avg_loss': 0.7209906578063965, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-13 22:55:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-09-13 22:55:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:55:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=33, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:56:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-13 22:56:02 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=133, loss_sum=95.209595, avg_loss=0.715862, seen=133, correct=72, accuracy=0.541353
2025-09-13 22:56:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:56:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:56:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:56:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2158MB
2025-09-13 22:56:03 (federatedscope.llm.llm_local.client:480) INFO: Client 50 Adapter 2 with val results: {'val_total': 133, 'val_loss': 95.2095947265625, 'val_avg_loss': 0.7158616144854323, 'val_seen': 133, 'val_correct': 72, 'val_acc': 0.5413533834586466}
2025-09-13 22:56:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:56:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:56:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:56:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:56:05 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.969795, avg_loss=0.749245, seen=40, correct=22, accuracy=0.550000
2025-09-13 22:56:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:56:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:56:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:56:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2158MB
2025-09-13 22:56:06 (federatedscope.llm.llm_local.client:501) INFO: Client 50 Adapter 2 with test results: {'test_total': 40, 'test_loss': 29.96979522705078, 'test_avg_loss': 0.7492448806762695, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-13 22:56:06 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-13 22:56:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-13 22:56:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:56:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=16, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:56:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-13 22:56:09 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=58.059090, avg_loss=0.699507, seen=83, correct=48, accuracy=0.578313
2025-09-13 22:56:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:56:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:56:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:56:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2192MB
2025-09-13 22:56:11 (federatedscope.llm.llm_local.client:480) INFO: Client 51 Adapter 0 with val results: {'val_total': 83, 'val_loss': 58.05908966064453, 'val_avg_loss': 0.6995071043451149, 'val_seen': 83, 'val_correct': 48, 'val_acc': 0.5783132530120482}
2025-09-13 22:56:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:56:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:56:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:56:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:56:12 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.507074, avg_loss=0.712677, seen=40, correct=23, accuracy=0.575000
2025-09-13 22:56:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:56:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:56:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:56:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2192MB
2025-09-13 22:56:13 (federatedscope.llm.llm_local.client:501) INFO: Client 51 Adapter 0 with test results: {'test_total': 40, 'test_loss': 28.5070743560791, 'test_avg_loss': 0.7126768589019775, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-13 22:56:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-13 22:56:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:56:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=16, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:56:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-13 22:56:16 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=59.877991, avg_loss=0.721422, seen=83, correct=48, accuracy=0.578313
2025-09-13 22:56:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:56:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:56:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:56:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2166MB
2025-09-13 22:56:17 (federatedscope.llm.llm_local.client:480) INFO: Client 51 Adapter 1 with val results: {'val_total': 83, 'val_loss': 59.87799072265625, 'val_avg_loss': 0.7214215749717621, 'val_seen': 83, 'val_correct': 48, 'val_acc': 0.5783132530120482}
2025-09-13 22:56:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:56:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:56:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:56:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:56:18 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.501291, avg_loss=0.637532, seen=40, correct=26, accuracy=0.650000
2025-09-13 22:56:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:56:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:56:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:56:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2166MB
2025-09-13 22:56:19 (federatedscope.llm.llm_local.client:501) INFO: Client 51 Adapter 1 with test results: {'test_total': 40, 'test_loss': 25.501291275024414, 'test_avg_loss': 0.6375322818756104, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-13 22:56:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-13 22:56:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:56:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=16, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:56:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-13 22:56:22 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=63.463696, avg_loss=0.764623, seen=83, correct=37, accuracy=0.445783
2025-09-13 22:56:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:56:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:56:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:56:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2166MB
2025-09-13 22:56:23 (federatedscope.llm.llm_local.client:480) INFO: Client 51 Adapter 2 with val results: {'val_total': 83, 'val_loss': 63.46369552612305, 'val_avg_loss': 0.7646228376641331, 'val_seen': 83, 'val_correct': 37, 'val_acc': 0.4457831325301205}
2025-09-13 22:56:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:56:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:56:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:56:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:56:24 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.065525, avg_loss=0.701638, seen=40, correct=23, accuracy=0.575000
2025-09-13 22:56:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:56:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:56:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:56:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2166MB
2025-09-13 22:56:25 (federatedscope.llm.llm_local.client:501) INFO: Client 51 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.06552505493164, 'test_avg_loss': 0.701638126373291, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-13 22:56:25 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-13 22:56:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-13 22:56:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:56:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=6, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:56:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-13 22:56:32 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=128.542145, avg_loss=0.683735, seen=188, correct=108, accuracy=0.574468
2025-09-13 22:56:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:56:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:56:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:56:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2200MB
2025-09-13 22:56:33 (federatedscope.llm.llm_local.client:480) INFO: Client 52 Adapter 0 with val results: {'val_total': 188, 'val_loss': 128.54214477539062, 'val_avg_loss': 0.6837348126350565, 'val_seen': 188, 'val_correct': 108, 'val_acc': 0.574468085106383}
2025-09-13 22:56:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:56:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:56:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:56:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:56:35 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.185099, avg_loss=0.629627, seen=40, correct=26, accuracy=0.650000
2025-09-13 22:56:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:56:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:56:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:56:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2200MB
2025-09-13 22:56:36 (federatedscope.llm.llm_local.client:501) INFO: Client 52 Adapter 0 with test results: {'test_total': 40, 'test_loss': 25.18509864807129, 'test_avg_loss': 0.6296274662017822, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-13 22:56:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-13 22:56:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:56:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=6, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:56:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-13 22:56:42 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=136.906189, avg_loss=0.728224, seen=188, correct=102, accuracy=0.542553
2025-09-13 22:56:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:56:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:56:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:56:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2175MB
2025-09-13 22:56:43 (federatedscope.llm.llm_local.client:480) INFO: Client 52 Adapter 1 with val results: {'val_total': 188, 'val_loss': 136.90618896484375, 'val_avg_loss': 0.7282244093874668, 'val_seen': 188, 'val_correct': 102, 'val_acc': 0.5425531914893617}
2025-09-13 22:56:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:56:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:56:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:56:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:56:45 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.182064, avg_loss=0.654552, seen=40, correct=24, accuracy=0.600000
2025-09-13 22:56:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:56:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:56:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:56:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2175MB
2025-09-13 22:56:46 (federatedscope.llm.llm_local.client:501) INFO: Client 52 Adapter 1 with test results: {'test_total': 40, 'test_loss': 26.182064056396484, 'test_avg_loss': 0.6545516014099121, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-13 22:56:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-13 22:56:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:56:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=6, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:56:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-13 22:56:52 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=137.165924, avg_loss=0.729606, seen=188, correct=95, accuracy=0.505319
2025-09-13 22:56:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:56:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:56:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:56:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2175MB
2025-09-13 22:56:54 (federatedscope.llm.llm_local.client:480) INFO: Client 52 Adapter 2 with val results: {'val_total': 188, 'val_loss': 137.16592407226562, 'val_avg_loss': 0.7296059791077959, 'val_seen': 188, 'val_correct': 95, 'val_acc': 0.5053191489361702}
2025-09-13 22:56:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:56:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:56:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:56:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:56:55 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.846958, avg_loss=0.696174, seen=40, correct=21, accuracy=0.525000
2025-09-13 22:56:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:56:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:56:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:56:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2175MB
2025-09-13 22:56:56 (federatedscope.llm.llm_local.client:501) INFO: Client 52 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.84695816040039, 'test_avg_loss': 0.6961739540100098, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-13 22:56:57 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-13 22:56:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 22:56:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:56:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:57:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 22:57:02 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=136.735306, avg_loss=0.683677, seen=200, correct=119, accuracy=0.595000
2025-09-13 22:57:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:57:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:57:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:57:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2208MB
2025-09-13 22:57:04 (federatedscope.llm.llm_local.client:480) INFO: Client 53 Adapter 0 with val results: {'val_total': 200, 'val_loss': 136.7353057861328, 'val_avg_loss': 0.6836765289306641, 'val_seen': 200, 'val_correct': 119, 'val_acc': 0.595}
2025-09-13 22:57:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:57:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:57:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:57:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:57:05 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.795116, avg_loss=0.644878, seen=40, correct=27, accuracy=0.675000
2025-09-13 22:57:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:57:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:57:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:57:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2208MB
2025-09-13 22:57:06 (federatedscope.llm.llm_local.client:501) INFO: Client 53 Adapter 0 with test results: {'test_total': 40, 'test_loss': 25.795116424560547, 'test_avg_loss': 0.6448779106140137, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-13 22:57:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 22:57:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:57:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:57:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 22:57:12 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=138.213470, avg_loss=0.691067, seen=200, correct=114, accuracy=0.570000
2025-09-13 22:57:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:57:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:57:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:57:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2183MB
2025-09-13 22:57:14 (federatedscope.llm.llm_local.client:480) INFO: Client 53 Adapter 1 with val results: {'val_total': 200, 'val_loss': 138.21347045898438, 'val_avg_loss': 0.6910673522949219, 'val_seen': 200, 'val_correct': 114, 'val_acc': 0.57}
2025-09-13 22:57:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:57:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:57:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:57:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:57:16 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.105721, avg_loss=0.702643, seen=40, correct=23, accuracy=0.575000
2025-09-13 22:57:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:57:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:57:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:57:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2183MB
2025-09-13 22:57:17 (federatedscope.llm.llm_local.client:501) INFO: Client 53 Adapter 1 with test results: {'test_total': 40, 'test_loss': 28.10572052001953, 'test_avg_loss': 0.7026430130004883, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-13 22:57:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 22:57:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:57:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:57:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 22:57:24 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=138.793854, avg_loss=0.693969, seen=200, correct=113, accuracy=0.565000
2025-09-13 22:57:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:57:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:57:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:57:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2183MB
2025-09-13 22:57:25 (federatedscope.llm.llm_local.client:480) INFO: Client 53 Adapter 2 with val results: {'val_total': 200, 'val_loss': 138.79385375976562, 'val_avg_loss': 0.6939692687988281, 'val_seen': 200, 'val_correct': 113, 'val_acc': 0.565}
2025-09-13 22:57:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:57:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:57:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:57:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:57:26 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.511322, avg_loss=0.687783, seen=40, correct=21, accuracy=0.525000
2025-09-13 22:57:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:57:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:57:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:57:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2183MB
2025-09-13 22:57:27 (federatedscope.llm.llm_local.client:501) INFO: Client 53 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.511322021484375, 'test_avg_loss': 0.6877830505371094, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-13 22:57:28 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-13 22:57:28 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-13 22:57:29 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-13 22:57:29 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-13 22:57:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 22:57:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:57:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:57:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 22:57:34 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=141.849136, avg_loss=0.709246, seen=200, correct=104, accuracy=0.520000
2025-09-13 22:57:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:57:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:57:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:57:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2304MB allocated=2192MB
2025-09-13 22:57:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:57:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:57:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:57:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:57:37 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.672316, avg_loss=0.591808, seen=40, correct=26, accuracy=0.650000
2025-09-13 22:57:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:57:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:57:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:57:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2192MB
2025-09-13 22:57:38 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-13 22:57:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1979, total=7916)
2025-09-13 22:57:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:57:39 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-13 22:57:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:57:39 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=990, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-13 22:57:47 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-13 22:57:50 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-13 22:57:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 22:57:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:57:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:57:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 22:57:58 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=141.359955, avg_loss=0.706800, seen=200, correct=111, accuracy=0.555000
2025-09-13 22:57:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:57:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:58:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:58:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 22:58:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:58:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:58:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:58:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:58:02 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.427443, avg_loss=0.610686, seen=40, correct=23, accuracy=0.575000
2025-09-13 22:58:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:58:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:58:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:58:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 22:58:12 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-13 22:58:14 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-13 22:58:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 22:58:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:58:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:58:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 22:58:21 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=139.451920, avg_loss=0.697260, seen=200, correct=115, accuracy=0.575000
2025-09-13 22:58:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:58:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:58:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:58:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 22:58:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:58:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:58:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:58:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:58:25 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.438599, avg_loss=0.635965, seen=40, correct=23, accuracy=0.575000
2025-09-13 22:58:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:58:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:58:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:58:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 22:58:36 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-13 22:58:36 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-13 22:58:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 22:58:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:58:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:58:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 22:58:44 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=139.363876, avg_loss=0.696819, seen=200, correct=118, accuracy=0.590000
2025-09-13 22:58:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:58:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:58:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:58:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 22:58:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:58:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:58:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:58:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:58:47 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.610802, avg_loss=0.640270, seen=40, correct=22, accuracy=0.550000
2025-09-13 22:58:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:58:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:58:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:58:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 22:58:56 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-13 22:58:58 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-13 22:58:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 22:58:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:58:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:59:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 22:59:05 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=139.863449, avg_loss=0.699317, seen=200, correct=112, accuracy=0.560000
2025-09-13 22:59:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:59:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:59:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:59:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 22:59:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:59:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:59:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:59:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:59:08 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.200121, avg_loss=0.630003, seen=40, correct=22, accuracy=0.550000
2025-09-13 22:59:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:59:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:59:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:59:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 22:59:19 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-13 22:59:20 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-13 22:59:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 22:59:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:59:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:59:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 22:59:28 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=140.332886, avg_loss=0.701664, seen=200, correct=111, accuracy=0.555000
2025-09-13 22:59:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:59:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:59:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:59:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 22:59:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:59:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:59:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:59:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:59:33 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.212774, avg_loss=0.605319, seen=40, correct=23, accuracy=0.575000
2025-09-13 22:59:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:59:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:59:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:59:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 22:59:44 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-13 22:59:45 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-13 22:59:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 22:59:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:59:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:59:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 22:59:53 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=141.329559, avg_loss=0.706648, seen=200, correct=107, accuracy=0.535000
2025-09-13 22:59:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:59:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:59:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:59:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 22:59:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 22:59:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:59:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 22:59:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 22:59:57 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.306265, avg_loss=0.607657, seen=40, correct=25, accuracy=0.625000
2025-09-13 22:59:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 22:59:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 22:59:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 22:59:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:00:09 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-13 23:00:09 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-13 23:00:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 23:00:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:00:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:00:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 23:00:16 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=141.096771, avg_loss=0.705484, seen=200, correct=108, accuracy=0.540000
2025-09-13 23:00:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:00:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:00:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:00:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:00:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:00:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:00:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:00:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:00:20 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.973898, avg_loss=0.599347, seen=40, correct=28, accuracy=0.700000
2025-09-13 23:00:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:00:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:00:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:00:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:00:30 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-13 23:00:31 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-13 23:00:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 23:00:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:00:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:00:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 23:00:38 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=141.823959, avg_loss=0.709120, seen=200, correct=111, accuracy=0.555000
2025-09-13 23:00:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:00:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:00:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:00:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:00:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:00:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:00:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:00:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:00:42 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.313469, avg_loss=0.607837, seen=40, correct=26, accuracy=0.650000
2025-09-13 23:00:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:00:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:00:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:00:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:00:53 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-13 23:00:53 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-13 23:00:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 23:00:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:00:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:01:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 23:01:01 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=140.844772, avg_loss=0.704224, seen=200, correct=111, accuracy=0.555000
2025-09-13 23:01:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:01:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:01:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:01:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:01:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:01:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:01:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:01:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:01:05 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.086521, avg_loss=0.627163, seen=40, correct=23, accuracy=0.575000
2025-09-13 23:01:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:01:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:01:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:01:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:01:16 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-13 23:01:17 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-13 23:01:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 23:01:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:01:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:01:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 23:01:24 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=139.665039, avg_loss=0.698325, seen=200, correct=110, accuracy=0.550000
2025-09-13 23:01:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:01:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:01:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:01:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:01:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:01:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:01:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:01:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:01:28 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.681845, avg_loss=0.617046, seen=40, correct=23, accuracy=0.575000
2025-09-13 23:01:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:01:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:01:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:01:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:01:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-13 23:01:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-13 23:01:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:01:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:01:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:01:31 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #44', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-13 23:01:31 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #44', 'Round': 0, 'Results_raw': {}}
2025-09-13 23:01:31 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-13 23:01:31 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 1 for training...
2025-09-13 23:01:31 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-13 23:01:31 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-13 23:01:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-09-13 23:01:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:01:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=37, num_train_batch_last_epoch=26, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:01:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-13 23:01:33 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=74, loss_sum=49.657257, avg_loss=0.671044, seen=74, correct=48, accuracy=0.648649
2025-09-13 23:01:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:01:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:01:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:01:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2314MB allocated=2200MB
2025-09-13 23:01:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:01:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:01:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:01:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:01:37 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.514992, avg_loss=0.687875, seen=40, correct=23, accuracy=0.575000
2025-09-13 23:01:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:01:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:01:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:01:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2314MB allocated=2200MB
2025-09-13 23:01:38 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-13 23:01:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=353, total=1409)
2025-09-13 23:01:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:01:38 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-13 23:01:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:01:38 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=177, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-13 23:01:45 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-13 23:01:46 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-13 23:01:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-09-13 23:01:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:01:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=37, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:01:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-13 23:01:49 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=74, loss_sum=49.798958, avg_loss=0.672959, seen=74, correct=46, accuracy=0.621622
2025-09-13 23:01:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:01:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:01:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:01:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-13 23:01:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:01:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:01:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:01:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:01:52 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.796791, avg_loss=0.719920, seen=40, correct=25, accuracy=0.625000
2025-09-13 23:01:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:01:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:01:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:01:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-13 23:02:04 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-13 23:02:04 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-13 23:02:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-09-13 23:02:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:02:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=37, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:02:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-13 23:02:07 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=74, loss_sum=50.574600, avg_loss=0.683441, seen=74, correct=47, accuracy=0.635135
2025-09-13 23:02:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:02:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:02:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:02:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-13 23:02:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:02:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:02:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:02:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:02:09 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.664604, avg_loss=0.691615, seen=40, correct=21, accuracy=0.525000
2025-09-13 23:02:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:02:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:02:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:02:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-13 23:02:21 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-13 23:02:21 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-13 23:02:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-09-13 23:02:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:02:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=37, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:02:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-13 23:02:25 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=74, loss_sum=51.260326, avg_loss=0.692707, seen=74, correct=41, accuracy=0.554054
2025-09-13 23:02:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:02:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:02:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:02:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-13 23:02:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:02:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:02:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:02:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:02:28 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.428204, avg_loss=0.660705, seen=40, correct=22, accuracy=0.550000
2025-09-13 23:02:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:02:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:02:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:02:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-13 23:02:38 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-13 23:02:40 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-13 23:02:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-09-13 23:02:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:02:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=37, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:02:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-13 23:02:43 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=74, loss_sum=52.552597, avg_loss=0.710170, seen=74, correct=39, accuracy=0.527027
2025-09-13 23:02:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:02:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:02:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:02:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-13 23:02:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:02:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:02:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:02:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:02:46 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.914963, avg_loss=0.672874, seen=40, correct=24, accuracy=0.600000
2025-09-13 23:02:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:02:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:02:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:02:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-13 23:02:58 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-13 23:02:58 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-13 23:02:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-09-13 23:02:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:02:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=37, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:03:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-13 23:03:02 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=74, loss_sum=51.864357, avg_loss=0.700870, seen=74, correct=41, accuracy=0.554054
2025-09-13 23:03:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:03:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:03:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:03:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-13 23:03:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:03:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:03:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:03:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:03:05 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.290653, avg_loss=0.682266, seen=40, correct=24, accuracy=0.600000
2025-09-13 23:03:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:03:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:03:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:03:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-13 23:03:15 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-13 23:03:16 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-13 23:03:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-09-13 23:03:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:03:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=37, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:03:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-13 23:03:19 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=74, loss_sum=51.855934, avg_loss=0.700756, seen=74, correct=38, accuracy=0.513514
2025-09-13 23:03:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:03:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:03:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:03:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-13 23:03:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:03:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:03:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:03:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:03:22 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.085289, avg_loss=0.677132, seen=40, correct=19, accuracy=0.475000
2025-09-13 23:03:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:03:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:03:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:03:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-13 23:03:30 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-13 23:03:32 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-13 23:03:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-09-13 23:03:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:03:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=37, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:03:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-13 23:03:35 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=74, loss_sum=51.967003, avg_loss=0.702257, seen=74, correct=39, accuracy=0.527027
2025-09-13 23:03:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:03:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:03:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:03:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-13 23:03:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:03:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:03:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:03:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:03:38 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.868134, avg_loss=0.671703, seen=40, correct=25, accuracy=0.625000
2025-09-13 23:03:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:03:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:03:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:03:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-13 23:03:49 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-13 23:03:50 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-13 23:03:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-09-13 23:03:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:03:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=37, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:03:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-13 23:03:54 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=74, loss_sum=52.438492, avg_loss=0.708628, seen=74, correct=43, accuracy=0.581081
2025-09-13 23:03:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:03:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:03:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:03:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-13 23:03:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:03:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:03:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:03:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:03:58 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.571152, avg_loss=0.664279, seen=40, correct=22, accuracy=0.550000
2025-09-13 23:03:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:03:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:03:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:03:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-13 23:04:07 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-13 23:04:09 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-13 23:04:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-09-13 23:04:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:04:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=37, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:04:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-13 23:04:13 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=74, loss_sum=53.323895, avg_loss=0.720593, seen=74, correct=42, accuracy=0.567568
2025-09-13 23:04:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:04:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:04:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:04:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-13 23:04:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:04:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:04:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:04:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:04:17 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.816244, avg_loss=0.670406, seen=40, correct=21, accuracy=0.525000
2025-09-13 23:04:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:04:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:04:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:04:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-13 23:04:28 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-13 23:04:28 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-13 23:04:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-09-13 23:04:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:04:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=37, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:04:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-13 23:04:32 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=74, loss_sum=53.332802, avg_loss=0.720714, seen=74, correct=43, accuracy=0.581081
2025-09-13 23:04:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:04:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:04:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:04:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-13 23:04:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:04:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:04:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:04:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:04:36 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.337206, avg_loss=0.683430, seen=40, correct=22, accuracy=0.550000
2025-09-13 23:04:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:04:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:04:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:04:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-13 23:04:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-13 23:04:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-13 23:04:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:04:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:04:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-13 23:04:38 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #33', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-13 23:04:38 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #33', 'Round': 0, 'Results_raw': {}}
2025-09-13 23:04:38 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-13 23:04:38 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-13 23:04:39 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-13 23:04:39 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-13 23:04:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-13 23:04:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:04:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=16, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:04:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-13 23:04:41 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=61.271500, avg_loss=0.738211, seen=83, correct=44, accuracy=0.530120
2025-09-13 23:04:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:04:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:04:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:04:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2314MB allocated=2200MB
2025-09-13 23:04:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:04:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:04:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:04:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:04:44 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.728601, avg_loss=0.768215, seen=40, correct=20, accuracy=0.500000
2025-09-13 23:04:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:04:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:04:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:04:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2314MB allocated=2200MB
2025-09-13 23:04:45 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-13 23:04:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=399, total=1594)
2025-09-13 23:04:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:04:46 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-13 23:04:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:04:46 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=200, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-13 23:04:56 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-13 23:04:56 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-13 23:04:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-13 23:04:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:04:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:04:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-13 23:04:59 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=60.106464, avg_loss=0.724174, seen=83, correct=50, accuracy=0.602410
2025-09-13 23:04:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:04:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:05:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:05:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:05:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:05:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:05:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:05:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:05:03 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.448063, avg_loss=0.761202, seen=40, correct=22, accuracy=0.550000
2025-09-13 23:05:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:05:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:05:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:05:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:05:15 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-13 23:05:15 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-13 23:05:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-13 23:05:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:05:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:05:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-13 23:05:19 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=60.830681, avg_loss=0.732900, seen=83, correct=48, accuracy=0.578313
2025-09-13 23:05:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:05:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:05:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:05:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:05:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:05:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:05:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:05:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:05:23 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.953281, avg_loss=0.773832, seen=40, correct=20, accuracy=0.500000
2025-09-13 23:05:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:05:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:05:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:05:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:05:33 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-13 23:05:34 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-13 23:05:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-13 23:05:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:05:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:05:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-13 23:05:37 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=59.707008, avg_loss=0.719362, seen=83, correct=46, accuracy=0.554217
2025-09-13 23:05:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:05:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:05:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:05:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:05:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:05:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:05:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:05:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:05:41 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.606205, avg_loss=0.740155, seen=40, correct=22, accuracy=0.550000
2025-09-13 23:05:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:05:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:05:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:05:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:05:52 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-13 23:05:52 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-13 23:05:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-13 23:05:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:05:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:05:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-13 23:05:55 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=60.977024, avg_loss=0.734663, seen=83, correct=44, accuracy=0.530120
2025-09-13 23:05:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:05:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:05:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:05:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:05:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:05:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:05:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:05:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:05:58 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.285721, avg_loss=0.732143, seen=40, correct=21, accuracy=0.525000
2025-09-13 23:05:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:05:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:05:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:06:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:06:10 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-13 23:06:10 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-13 23:06:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-13 23:06:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:06:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:06:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-13 23:06:14 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=61.990631, avg_loss=0.746875, seen=83, correct=41, accuracy=0.493976
2025-09-13 23:06:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:06:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:06:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:06:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:06:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:06:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:06:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:06:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:06:17 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.438187, avg_loss=0.735955, seen=40, correct=22, accuracy=0.550000
2025-09-13 23:06:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:06:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:06:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:06:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:06:27 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-13 23:06:29 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-13 23:06:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-13 23:06:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:06:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:06:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-13 23:06:32 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=61.159378, avg_loss=0.736860, seen=83, correct=44, accuracy=0.530120
2025-09-13 23:06:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:06:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:06:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:06:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:06:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:06:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:06:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:06:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:06:36 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.698315, avg_loss=0.742458, seen=40, correct=23, accuracy=0.575000
2025-09-13 23:06:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:06:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:06:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:06:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:06:48 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-13 23:06:48 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-13 23:06:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-13 23:06:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:06:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:06:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-13 23:06:52 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=59.511501, avg_loss=0.717006, seen=83, correct=45, accuracy=0.542169
2025-09-13 23:06:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:06:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:06:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:06:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:06:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:06:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:06:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:06:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:06:55 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.055660, avg_loss=0.726392, seen=40, correct=22, accuracy=0.550000
2025-09-13 23:06:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:06:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:06:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:06:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:07:04 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-13 23:07:06 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-13 23:07:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-13 23:07:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:07:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:07:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-13 23:07:10 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=59.372528, avg_loss=0.715332, seen=83, correct=45, accuracy=0.542169
2025-09-13 23:07:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:07:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:07:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:07:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:07:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:07:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:07:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:07:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:07:13 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.672092, avg_loss=0.716802, seen=40, correct=21, accuracy=0.525000
2025-09-13 23:07:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:07:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:07:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:07:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:07:23 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-13 23:07:25 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-13 23:07:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-13 23:07:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:07:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:07:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-13 23:07:28 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=59.467358, avg_loss=0.716474, seen=83, correct=45, accuracy=0.542169
2025-09-13 23:07:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:07:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:07:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:07:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:07:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:07:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:07:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:07:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:07:32 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.864044, avg_loss=0.721601, seen=40, correct=20, accuracy=0.500000
2025-09-13 23:07:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:07:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:07:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:07:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:07:42 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-13 23:07:44 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-13 23:07:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-13 23:07:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:07:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:07:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-13 23:07:48 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=59.672802, avg_loss=0.718949, seen=83, correct=43, accuracy=0.518072
2025-09-13 23:07:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:07:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:07:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:07:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:07:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:07:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:07:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:07:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:07:52 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.769173, avg_loss=0.719229, seen=40, correct=23, accuracy=0.575000
2025-09-13 23:07:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:07:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:07:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:07:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:07:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-13 23:07:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-13 23:07:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:07:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:07:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:07:54 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #39', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-13 23:07:54 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #39', 'Round': 0, 'Results_raw': {}}
2025-09-13 23:07:54 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-13 23:07:54 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-13 23:07:55 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-13 23:07:55 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-13 23:07:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 23:07:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:07:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:08:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 23:08:01 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=126.023315, avg_loss=0.630117, seen=200, correct=129, accuracy=0.645000
2025-09-13 23:08:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:08:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:08:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:08:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2314MB allocated=2200MB
2025-09-13 23:08:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:08:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:08:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:08:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:08:04 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.174274, avg_loss=0.629357, seen=40, correct=25, accuracy=0.625000
2025-09-13 23:08:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:08:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:08:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:08:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2314MB allocated=2200MB
2025-09-13 23:08:05 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-13 23:08:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1122, total=4486)
2025-09-13 23:08:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:08:05 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-13 23:08:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:08:05 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=561, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-13 23:08:16 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-13 23:08:16 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-13 23:08:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 23:08:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:08:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:08:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 23:08:24 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=123.408501, avg_loss=0.617043, seen=200, correct=138, accuracy=0.690000
2025-09-13 23:08:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:08:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:08:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:08:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-13 23:08:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:08:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:08:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:08:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:08:27 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=22.726303, avg_loss=0.568158, seen=40, correct=30, accuracy=0.750000
2025-09-13 23:08:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:08:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:08:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:08:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-13 23:08:37 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-13 23:08:39 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-13 23:08:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 23:08:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:08:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:08:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 23:08:47 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=125.124649, avg_loss=0.625623, seen=200, correct=136, accuracy=0.680000
2025-09-13 23:08:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:08:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:08:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:08:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-13 23:08:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:08:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:08:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:08:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:08:52 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=22.554279, avg_loss=0.563857, seen=40, correct=28, accuracy=0.700000
2025-09-13 23:08:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:08:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:08:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:08:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-13 23:09:03 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-13 23:09:03 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-13 23:09:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 23:09:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:09:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:09:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 23:09:11 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=125.583809, avg_loss=0.627919, seen=200, correct=135, accuracy=0.675000
2025-09-13 23:09:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:09:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:09:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:09:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-13 23:09:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:09:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:09:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:09:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:09:15 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.196091, avg_loss=0.579902, seen=40, correct=30, accuracy=0.750000
2025-09-13 23:09:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:09:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:09:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:09:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-13 23:09:24 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-13 23:09:25 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-13 23:09:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 23:09:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:09:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:09:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 23:09:31 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=124.852890, avg_loss=0.624264, seen=200, correct=139, accuracy=0.695000
2025-09-13 23:09:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:09:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:09:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:09:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-13 23:09:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:09:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:09:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:09:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:09:35 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.470730, avg_loss=0.586768, seen=40, correct=31, accuracy=0.775000
2025-09-13 23:09:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:09:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:09:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:09:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-13 23:09:45 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-13 23:09:46 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-13 23:09:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 23:09:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:09:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:09:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 23:09:54 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=125.169334, avg_loss=0.625847, seen=200, correct=130, accuracy=0.650000
2025-09-13 23:09:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:09:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:09:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:09:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-13 23:09:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:09:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:09:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:09:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:09:57 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.930950, avg_loss=0.598274, seen=40, correct=28, accuracy=0.700000
2025-09-13 23:09:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:09:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:09:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:09:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-13 23:10:06 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-13 23:10:07 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-13 23:10:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 23:10:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:10:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:10:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 23:10:15 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=125.313004, avg_loss=0.626565, seen=200, correct=129, accuracy=0.645000
2025-09-13 23:10:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:10:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:10:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:10:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-13 23:10:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:10:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:10:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:10:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:10:18 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.405184, avg_loss=0.585130, seen=40, correct=29, accuracy=0.725000
2025-09-13 23:10:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:10:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:10:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:10:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-13 23:10:30 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-13 23:10:30 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-13 23:10:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 23:10:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:10:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:10:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 23:10:37 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=124.782501, avg_loss=0.623913, seen=200, correct=127, accuracy=0.635000
2025-09-13 23:10:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:10:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:10:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:10:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-13 23:10:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:10:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:10:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:10:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:10:40 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.221233, avg_loss=0.580531, seen=40, correct=26, accuracy=0.650000
2025-09-13 23:10:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:10:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:10:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:10:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-13 23:10:52 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-13 23:10:52 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-13 23:10:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 23:10:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:10:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:10:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 23:10:59 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=125.655067, avg_loss=0.628275, seen=200, correct=128, accuracy=0.640000
2025-09-13 23:10:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:10:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:11:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:11:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-13 23:11:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:11:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:11:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:11:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:11:03 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.603746, avg_loss=0.590094, seen=40, correct=25, accuracy=0.625000
2025-09-13 23:11:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:11:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:11:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:11:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-13 23:11:12 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-13 23:11:13 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-13 23:11:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 23:11:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:11:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:11:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 23:11:22 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=124.821510, avg_loss=0.624108, seen=200, correct=131, accuracy=0.655000
2025-09-13 23:11:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:11:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:11:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:11:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-13 23:11:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:11:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:11:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:11:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:11:26 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.108170, avg_loss=0.602704, seen=40, correct=27, accuracy=0.675000
2025-09-13 23:11:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:11:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:11:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:11:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-13 23:11:37 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-13 23:11:38 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-13 23:11:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 23:11:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:11:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:11:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 23:11:45 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=123.348015, avg_loss=0.616740, seen=200, correct=133, accuracy=0.665000
2025-09-13 23:11:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:11:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:11:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:11:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-13 23:11:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:11:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:11:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:11:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:11:49 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.205519, avg_loss=0.580138, seen=40, correct=32, accuracy=0.800000
2025-09-13 23:11:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:11:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:11:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:11:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-13 23:11:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-13 23:11:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-13 23:11:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:11:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:11:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-13 23:11:51 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #34', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-13 23:11:51 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #34', 'Round': 0, 'Results_raw': {}}
2025-09-13 23:11:51 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-13 23:11:51 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-13 23:11:52 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-13 23:11:52 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-13 23:11:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-09-13 23:11:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:11:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=69, num_train_batch_last_epoch=31, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:11:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-09-13 23:11:56 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=137, loss_sum=90.848152, avg_loss=0.663125, seen=137, correct=83, accuracy=0.605839
2025-09-13 23:11:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:11:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:11:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:11:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2316MB allocated=2200MB
2025-09-13 23:11:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:11:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:11:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:11:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:11:59 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.165676, avg_loss=0.629142, seen=40, correct=26, accuracy=0.650000
2025-09-13 23:11:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:11:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:12:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:12:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2316MB allocated=2200MB
2025-09-13 23:12:01 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-13 23:12:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=652, total=2605)
2025-09-13 23:12:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:12:01 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-13 23:12:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:12:01 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=326, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-13 23:12:11 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-13 23:12:11 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-13 23:12:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-09-13 23:12:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:12:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=69, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:12:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-09-13 23:12:17 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=137, loss_sum=86.744934, avg_loss=0.633175, seen=137, correct=91, accuracy=0.664234
2025-09-13 23:12:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:12:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:12:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:12:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2326MB allocated=2217MB
2025-09-13 23:12:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:12:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:12:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:12:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:12:20 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.129036, avg_loss=0.628226, seen=40, correct=25, accuracy=0.625000
2025-09-13 23:12:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:12:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:12:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:12:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2326MB allocated=2217MB
2025-09-13 23:12:29 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-13 23:12:31 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-13 23:12:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-09-13 23:12:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:12:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=69, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:12:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-09-13 23:12:36 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=137, loss_sum=86.282120, avg_loss=0.629796, seen=137, correct=92, accuracy=0.671533
2025-09-13 23:12:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:12:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:12:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:12:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2326MB allocated=2217MB
2025-09-13 23:12:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:12:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:12:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:12:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:12:39 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.684797, avg_loss=0.642120, seen=40, correct=27, accuracy=0.675000
2025-09-13 23:12:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:12:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:12:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:12:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2326MB allocated=2217MB
2025-09-13 23:12:51 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-13 23:12:51 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-13 23:12:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-09-13 23:12:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:12:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=69, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:12:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-09-13 23:12:57 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=137, loss_sum=86.145462, avg_loss=0.628799, seen=137, correct=90, accuracy=0.656934
2025-09-13 23:12:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:12:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:12:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:12:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2326MB allocated=2217MB
2025-09-13 23:12:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:12:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:12:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:13:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:13:00 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.308653, avg_loss=0.657716, seen=40, correct=25, accuracy=0.625000
2025-09-13 23:13:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:13:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:13:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:13:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2326MB allocated=2217MB
2025-09-13 23:13:10 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-13 23:13:10 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-13 23:13:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-09-13 23:13:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:13:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=69, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:13:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-09-13 23:13:16 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=137, loss_sum=85.831802, avg_loss=0.626510, seen=137, correct=90, accuracy=0.656934
2025-09-13 23:13:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:13:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:13:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:13:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2326MB allocated=2217MB
2025-09-13 23:13:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:13:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:13:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:13:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:13:18 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.395107, avg_loss=0.659878, seen=40, correct=26, accuracy=0.650000
2025-09-13 23:13:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:13:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:13:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:13:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2326MB allocated=2217MB
2025-09-13 23:13:29 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-13 23:13:30 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-13 23:13:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-09-13 23:13:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:13:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=69, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:13:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-09-13 23:13:35 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=137, loss_sum=87.345818, avg_loss=0.637561, seen=137, correct=89, accuracy=0.649635
2025-09-13 23:13:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:13:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:13:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:13:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2326MB allocated=2217MB
2025-09-13 23:13:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:13:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:13:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:13:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:13:38 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.508509, avg_loss=0.637713, seen=40, correct=25, accuracy=0.625000
2025-09-13 23:13:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:13:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:13:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:13:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2326MB allocated=2217MB
2025-09-13 23:13:51 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-13 23:13:51 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-13 23:13:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-09-13 23:13:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:13:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=69, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:13:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-09-13 23:13:57 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=137, loss_sum=89.032501, avg_loss=0.649872, seen=137, correct=86, accuracy=0.627737
2025-09-13 23:13:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:13:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:13:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:13:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2326MB allocated=2217MB
2025-09-13 23:13:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:13:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:13:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:14:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:14:00 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.301968, avg_loss=0.632549, seen=40, correct=25, accuracy=0.625000
2025-09-13 23:14:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:14:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:14:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:14:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2326MB allocated=2217MB
2025-09-13 23:14:09 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-13 23:14:11 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-13 23:14:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-09-13 23:14:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:14:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=69, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:14:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-09-13 23:14:16 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=137, loss_sum=88.181541, avg_loss=0.643661, seen=137, correct=92, accuracy=0.671533
2025-09-13 23:14:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:14:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:14:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:14:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2326MB allocated=2217MB
2025-09-13 23:14:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:14:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:14:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:14:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:14:19 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.033215, avg_loss=0.625830, seen=40, correct=26, accuracy=0.650000
2025-09-13 23:14:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:14:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:14:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:14:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2326MB allocated=2217MB
2025-09-13 23:14:29 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-13 23:14:29 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-13 23:14:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-09-13 23:14:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:14:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=69, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:14:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-09-13 23:14:35 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=137, loss_sum=87.186966, avg_loss=0.636401, seen=137, correct=92, accuracy=0.671533
2025-09-13 23:14:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:14:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:14:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:14:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2326MB allocated=2217MB
2025-09-13 23:14:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:14:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:14:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:14:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:14:38 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.022152, avg_loss=0.650554, seen=40, correct=24, accuracy=0.600000
2025-09-13 23:14:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:14:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:14:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:14:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2326MB allocated=2217MB
2025-09-13 23:14:49 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-13 23:14:50 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-13 23:14:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-09-13 23:14:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:14:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=69, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:14:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-09-13 23:14:56 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=137, loss_sum=86.056496, avg_loss=0.628150, seen=137, correct=89, accuracy=0.649635
2025-09-13 23:14:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:14:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:14:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:14:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2326MB allocated=2217MB
2025-09-13 23:14:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:14:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:14:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:14:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:14:59 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.754539, avg_loss=0.668863, seen=40, correct=24, accuracy=0.600000
2025-09-13 23:14:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:14:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:15:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:15:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2326MB allocated=2217MB
2025-09-13 23:15:09 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-13 23:15:11 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-13 23:15:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-09-13 23:15:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:15:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=69, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:15:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-09-13 23:15:16 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=137, loss_sum=85.133163, avg_loss=0.621410, seen=137, correct=92, accuracy=0.671533
2025-09-13 23:15:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:15:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:15:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:15:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2326MB allocated=2217MB
2025-09-13 23:15:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:15:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:15:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:15:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:15:18 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.553091, avg_loss=0.663827, seen=40, correct=25, accuracy=0.625000
2025-09-13 23:15:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:15:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:15:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:15:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2326MB allocated=2217MB
2025-09-13 23:15:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-13 23:15:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-13 23:15:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:15:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:15:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2326MB allocated=2217MB
2025-09-13 23:15:20 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #12', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-13 23:15:20 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #12', 'Round': 0, 'Results_raw': {}}
2025-09-13 23:15:20 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-13 23:15:20 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 2 for training...
2025-09-13 23:15:21 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-13 23:15:21 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-13 23:15:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-09-13 23:15:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:15:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=18, num_train_batch_last_epoch=10, num_train_epoch=6, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:15:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-09-13 23:15:22 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=36, loss_sum=24.887300, avg_loss=0.691314, seen=36, correct=22, accuracy=0.611111
2025-09-13 23:15:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:15:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:15:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:15:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2316MB allocated=2200MB
2025-09-13 23:15:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:15:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:15:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:15:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:15:24 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.284481, avg_loss=0.682112, seen=40, correct=24, accuracy=0.600000
2025-09-13 23:15:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:15:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:15:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:15:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2316MB allocated=2200MB
2025-09-13 23:15:25 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-13 23:15:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=173, total=691)
2025-09-13 23:15:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:15:25 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-13 23:15:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:15:25 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=87, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-13 23:15:36 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-13 23:15:36 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-13 23:15:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-09-13 23:15:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:15:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=18, num_train_batch_last_epoch=200, num_train_epoch=6, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:15:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-09-13 23:15:38 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=36, loss_sum=24.924736, avg_loss=0.692354, seen=36, correct=21, accuracy=0.583333
2025-09-13 23:15:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:15:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:15:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:15:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2370MB allocated=2217MB
2025-09-13 23:15:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:15:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:15:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:15:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:15:42 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.742153, avg_loss=0.643554, seen=40, correct=25, accuracy=0.625000
2025-09-13 23:15:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:15:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:15:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:15:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2370MB allocated=2217MB
2025-09-13 23:15:52 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-13 23:15:54 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-13 23:15:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-09-13 23:15:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:15:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=18, num_train_batch_last_epoch=200, num_train_epoch=6, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:15:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-09-13 23:15:56 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=36, loss_sum=25.322254, avg_loss=0.703396, seen=36, correct=21, accuracy=0.583333
2025-09-13 23:15:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:15:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:15:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:15:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2370MB allocated=2217MB
2025-09-13 23:15:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:15:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:15:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:16:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:16:00 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.312458, avg_loss=0.682811, seen=40, correct=23, accuracy=0.575000
2025-09-13 23:16:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:16:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:16:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:16:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2370MB allocated=2217MB
2025-09-13 23:16:12 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-13 23:16:12 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-13 23:16:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-09-13 23:16:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:16:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=18, num_train_batch_last_epoch=200, num_train_epoch=6, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:16:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-09-13 23:16:13 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=36, loss_sum=25.368641, avg_loss=0.704684, seen=36, correct=20, accuracy=0.555556
2025-09-13 23:16:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:16:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:16:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:16:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2370MB allocated=2217MB
2025-09-13 23:16:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:16:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:16:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:16:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:16:16 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.727901, avg_loss=0.693198, seen=40, correct=23, accuracy=0.575000
2025-09-13 23:16:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:16:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:16:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:16:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2370MB allocated=2217MB
2025-09-13 23:16:26 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-13 23:16:27 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-13 23:16:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-09-13 23:16:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:16:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=18, num_train_batch_last_epoch=200, num_train_epoch=6, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:16:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-09-13 23:16:28 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=36, loss_sum=25.011982, avg_loss=0.694777, seen=36, correct=21, accuracy=0.583333
2025-09-13 23:16:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:16:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:16:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:16:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2370MB allocated=2217MB
2025-09-13 23:16:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:16:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:16:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:16:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:16:31 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.868567, avg_loss=0.696714, seen=40, correct=20, accuracy=0.500000
2025-09-13 23:16:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:16:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:16:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:16:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2370MB allocated=2217MB
2025-09-13 23:16:41 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-13 23:16:41 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-13 23:16:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-09-13 23:16:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:16:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=18, num_train_batch_last_epoch=200, num_train_epoch=6, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:16:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-09-13 23:16:43 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=36, loss_sum=25.360046, avg_loss=0.704446, seen=36, correct=19, accuracy=0.527778
2025-09-13 23:16:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:16:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:16:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:16:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2370MB allocated=2217MB
2025-09-13 23:16:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:16:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:16:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:16:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:16:47 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.595322, avg_loss=0.689883, seen=40, correct=21, accuracy=0.525000
2025-09-13 23:16:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:16:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:16:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:16:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2370MB allocated=2217MB
2025-09-13 23:16:57 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-13 23:16:58 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-13 23:16:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-09-13 23:16:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:16:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=18, num_train_batch_last_epoch=200, num_train_epoch=6, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:17:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-09-13 23:17:00 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=36, loss_sum=25.535009, avg_loss=0.709306, seen=36, correct=19, accuracy=0.527778
2025-09-13 23:17:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:17:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:17:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:17:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2370MB allocated=2217MB
2025-09-13 23:17:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:17:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:17:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:17:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:17:04 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.183899, avg_loss=0.679597, seen=40, correct=23, accuracy=0.575000
2025-09-13 23:17:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:17:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:17:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:17:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2370MB allocated=2217MB
2025-09-13 23:17:14 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-13 23:17:16 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-13 23:17:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-09-13 23:17:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:17:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=18, num_train_batch_last_epoch=200, num_train_epoch=6, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:17:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-09-13 23:17:17 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=36, loss_sum=25.430138, avg_loss=0.706393, seen=36, correct=22, accuracy=0.611111
2025-09-13 23:17:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:17:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:17:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:17:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2370MB allocated=2217MB
2025-09-13 23:17:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:17:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:17:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:17:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:17:20 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.671207, avg_loss=0.691780, seen=40, correct=22, accuracy=0.550000
2025-09-13 23:17:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:17:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:17:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:17:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2370MB allocated=2217MB
2025-09-13 23:17:30 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-13 23:17:32 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-13 23:17:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-09-13 23:17:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:17:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=18, num_train_batch_last_epoch=200, num_train_epoch=6, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:17:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-09-13 23:17:33 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=36, loss_sum=24.820349, avg_loss=0.689454, seen=36, correct=21, accuracy=0.583333
2025-09-13 23:17:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:17:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:17:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:17:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2370MB allocated=2217MB
2025-09-13 23:17:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:17:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:17:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:17:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:17:36 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.660810, avg_loss=0.691520, seen=40, correct=23, accuracy=0.575000
2025-09-13 23:17:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:17:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:17:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:17:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2370MB allocated=2217MB
2025-09-13 23:17:46 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-13 23:17:46 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-13 23:17:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-09-13 23:17:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:17:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=18, num_train_batch_last_epoch=200, num_train_epoch=6, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:17:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-09-13 23:17:48 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=36, loss_sum=24.993122, avg_loss=0.694253, seen=36, correct=20, accuracy=0.555556
2025-09-13 23:17:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:17:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:17:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:17:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2370MB allocated=2217MB
2025-09-13 23:17:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:17:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:17:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:17:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:17:50 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.044693, avg_loss=0.701117, seen=40, correct=20, accuracy=0.500000
2025-09-13 23:17:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:17:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:17:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:17:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2370MB allocated=2217MB
2025-09-13 23:18:01 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-13 23:18:02 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-13 23:18:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-09-13 23:18:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:18:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=18, num_train_batch_last_epoch=200, num_train_epoch=6, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:18:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-09-13 23:18:03 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=36, loss_sum=25.374117, avg_loss=0.704837, seen=36, correct=20, accuracy=0.555556
2025-09-13 23:18:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:18:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:18:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:18:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2370MB allocated=2217MB
2025-09-13 23:18:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:18:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:18:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:18:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:18:06 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.167908, avg_loss=0.679198, seen=40, correct=23, accuracy=0.575000
2025-09-13 23:18:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:18:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:18:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:18:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2370MB allocated=2217MB
2025-09-13 23:18:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-13 23:18:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-13 23:18:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:18:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:18:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2370MB allocated=2217MB
2025-09-13 23:18:09 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #3', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-13 23:18:09 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #3', 'Round': 0, 'Results_raw': {}}
2025-09-13 23:18:09 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-13 23:18:09 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-13 23:18:09 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-13 23:18:09 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-13 23:18:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-09-13 23:18:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:18:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=56, num_train_batch_last_epoch=44, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:18:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-13 23:18:12 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=112, loss_sum=65.987061, avg_loss=0.589170, seen=112, correct=77, accuracy=0.687500
2025-09-13 23:18:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:18:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:18:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:18:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2316MB allocated=2200MB
2025-09-13 23:18:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:18:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:18:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:18:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:18:14 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.106054, avg_loss=0.702651, seen=40, correct=23, accuracy=0.575000
2025-09-13 23:18:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:18:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:18:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:18:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2316MB allocated=2200MB
2025-09-13 23:18:15 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-13 23:18:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=536, total=2144)
2025-09-13 23:18:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:18:15 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-13 23:18:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:18:15 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=268, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-13 23:18:24 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-13 23:18:25 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-13 23:18:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-09-13 23:18:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:18:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=56, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:18:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-13 23:18:29 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=112, loss_sum=64.183823, avg_loss=0.573070, seen=112, correct=83, accuracy=0.741071
2025-09-13 23:18:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:18:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:18:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:18:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:18:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:18:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:18:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:18:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:18:33 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.298265, avg_loss=0.682457, seen=40, correct=25, accuracy=0.625000
2025-09-13 23:18:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:18:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:18:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:18:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:18:43 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-13 23:18:44 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-13 23:18:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-09-13 23:18:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:18:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=56, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:18:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-13 23:18:48 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=112, loss_sum=64.040466, avg_loss=0.571790, seen=112, correct=85, accuracy=0.758929
2025-09-13 23:18:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:18:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:18:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:18:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:18:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:18:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:18:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:18:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:18:51 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.016369, avg_loss=0.675409, seen=40, correct=24, accuracy=0.600000
2025-09-13 23:18:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:18:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:18:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:18:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:19:00 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-13 23:19:00 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-13 23:19:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-09-13 23:19:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:19:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=56, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:19:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-13 23:19:05 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=112, loss_sum=64.535278, avg_loss=0.576208, seen=112, correct=82, accuracy=0.732143
2025-09-13 23:19:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:19:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:19:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:19:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2217MB
2025-09-13 23:19:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:19:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:19:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:19:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:19:09 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.421619, avg_loss=0.685540, seen=40, correct=25, accuracy=0.625000
2025-09-13 23:19:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:19:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:19:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:19:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:19:19 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-13 23:19:20 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-13 23:19:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-09-13 23:19:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:19:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=56, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:19:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-13 23:19:25 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=112, loss_sum=65.694191, avg_loss=0.586555, seen=112, correct=79, accuracy=0.705357
2025-09-13 23:19:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:19:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:19:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:19:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:19:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:19:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:19:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:19:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:19:28 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.628815, avg_loss=0.690720, seen=40, correct=26, accuracy=0.650000
2025-09-13 23:19:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:19:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:19:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:19:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:19:37 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-13 23:19:40 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-13 23:19:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-09-13 23:19:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:19:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=56, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:19:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-13 23:19:45 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=112, loss_sum=64.156609, avg_loss=0.572827, seen=112, correct=82, accuracy=0.732143
2025-09-13 23:19:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:19:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:19:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:19:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:19:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:19:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:19:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:19:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:19:48 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.829868, avg_loss=0.695747, seen=40, correct=24, accuracy=0.600000
2025-09-13 23:19:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:19:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:19:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:19:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:19:59 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-13 23:19:59 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-13 23:19:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-09-13 23:19:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:19:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=56, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:20:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-13 23:20:03 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=112, loss_sum=64.739548, avg_loss=0.578032, seen=112, correct=79, accuracy=0.705357
2025-09-13 23:20:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:20:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:20:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:20:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:20:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:20:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:20:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:20:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:20:07 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.666222, avg_loss=0.691656, seen=40, correct=24, accuracy=0.600000
2025-09-13 23:20:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:20:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:20:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:20:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:20:17 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-13 23:20:18 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-13 23:20:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-09-13 23:20:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:20:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=56, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:20:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-13 23:20:22 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=112, loss_sum=64.849228, avg_loss=0.579011, seen=112, correct=81, accuracy=0.723214
2025-09-13 23:20:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:20:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:20:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:20:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:20:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:20:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:20:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:20:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:20:26 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.621872, avg_loss=0.690547, seen=40, correct=23, accuracy=0.575000
2025-09-13 23:20:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:20:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:20:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:20:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:20:35 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-13 23:20:36 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-13 23:20:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-09-13 23:20:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:20:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=56, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:20:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-13 23:20:40 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=112, loss_sum=63.656143, avg_loss=0.568358, seen=112, correct=82, accuracy=0.732143
2025-09-13 23:20:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:20:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:20:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:20:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:20:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:20:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:20:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:20:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:20:43 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.338879, avg_loss=0.683472, seen=40, correct=25, accuracy=0.625000
2025-09-13 23:20:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:20:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:20:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:20:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:20:54 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-13 23:20:54 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-13 23:20:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-09-13 23:20:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:20:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=56, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:20:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-13 23:20:58 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=112, loss_sum=63.229973, avg_loss=0.564553, seen=112, correct=83, accuracy=0.741071
2025-09-13 23:20:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:20:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:20:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:21:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:21:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:21:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:21:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:21:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:21:01 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.867558, avg_loss=0.696689, seen=40, correct=23, accuracy=0.575000
2025-09-13 23:21:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:21:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:21:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:21:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:21:12 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-13 23:21:12 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-13 23:21:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-09-13 23:21:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:21:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=56, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:21:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-13 23:21:15 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=112, loss_sum=65.638664, avg_loss=0.586060, seen=112, correct=81, accuracy=0.723214
2025-09-13 23:21:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:21:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:21:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:21:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:21:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:21:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:21:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:21:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:21:19 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.825218, avg_loss=0.695630, seen=40, correct=24, accuracy=0.600000
2025-09-13 23:21:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:21:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:21:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:21:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:21:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-13 23:21:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-13 23:21:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:21:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:21:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:21:21 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #32', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-13 23:21:21 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #32', 'Round': 0, 'Results_raw': {}}
2025-09-13 23:21:21 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-13 23:21:21 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-13 23:21:22 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-13 23:21:22 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-13 23:21:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 23:21:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:21:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:21:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 23:21:28 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=130.922058, avg_loss=0.654610, seen=200, correct=126, accuracy=0.630000
2025-09-13 23:21:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:21:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:21:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:21:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2316MB allocated=2200MB
2025-09-13 23:21:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:21:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:21:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:21:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:21:31 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.001026, avg_loss=0.675026, seen=40, correct=22, accuracy=0.550000
2025-09-13 23:21:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:21:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:21:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:21:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2316MB allocated=2200MB
2025-09-13 23:21:33 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-13 23:21:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1443, total=5772)
2025-09-13 23:21:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:21:33 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-13 23:21:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:21:33 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=722, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-13 23:21:42 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-13 23:21:42 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-13 23:21:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 23:21:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:21:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:21:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 23:21:50 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=125.340195, avg_loss=0.626701, seen=200, correct=125, accuracy=0.625000
2025-09-13 23:21:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:21:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:21:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:21:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2342MB allocated=2217MB
2025-09-13 23:21:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:21:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:21:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:21:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:21:54 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.033026, avg_loss=0.625826, seen=40, correct=30, accuracy=0.750000
2025-09-13 23:21:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:21:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:21:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:21:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2342MB allocated=2217MB
2025-09-13 23:22:04 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-13 23:22:06 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-13 23:22:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 23:22:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:22:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:22:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 23:22:13 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=126.698509, avg_loss=0.633493, seen=200, correct=121, accuracy=0.605000
2025-09-13 23:22:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:22:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:22:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:22:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2342MB allocated=2217MB
2025-09-13 23:22:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:22:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:22:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:22:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:22:17 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.203728, avg_loss=0.630093, seen=40, correct=28, accuracy=0.700000
2025-09-13 23:22:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:22:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:22:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:22:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2342MB allocated=2217MB
2025-09-13 23:22:27 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-13 23:22:28 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-13 23:22:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 23:22:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:22:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:22:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 23:22:36 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=125.317795, avg_loss=0.626589, seen=200, correct=123, accuracy=0.615000
2025-09-13 23:22:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:22:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:22:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:22:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2342MB allocated=2217MB
2025-09-13 23:22:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:22:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:22:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:22:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:22:39 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.932741, avg_loss=0.648319, seen=40, correct=29, accuracy=0.725000
2025-09-13 23:22:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:22:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:22:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:22:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2342MB allocated=2217MB
2025-09-13 23:22:49 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-13 23:22:51 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-13 23:22:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 23:22:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:22:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:22:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 23:22:58 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=127.484497, avg_loss=0.637422, seen=200, correct=125, accuracy=0.625000
2025-09-13 23:22:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:22:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:22:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:22:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2342MB allocated=2217MB
2025-09-13 23:23:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:23:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:23:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:23:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:23:00 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.284180, avg_loss=0.707104, seen=40, correct=23, accuracy=0.575000
2025-09-13 23:23:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:23:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:23:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:23:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2342MB allocated=2217MB
2025-09-13 23:23:12 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-13 23:23:12 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-13 23:23:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 23:23:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:23:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:23:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 23:23:20 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=131.348877, avg_loss=0.656744, seen=200, correct=123, accuracy=0.615000
2025-09-13 23:23:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:23:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:23:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:23:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2342MB allocated=2217MB
2025-09-13 23:23:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:23:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:23:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:23:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:23:24 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.660856, avg_loss=0.766521, seen=40, correct=23, accuracy=0.575000
2025-09-13 23:23:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:23:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:23:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:23:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2342MB allocated=2217MB
2025-09-13 23:23:35 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-13 23:23:35 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-13 23:23:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 23:23:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:23:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:23:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 23:23:43 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=130.222687, avg_loss=0.651113, seen=200, correct=120, accuracy=0.600000
2025-09-13 23:23:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:23:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:23:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:23:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2342MB allocated=2217MB
2025-09-13 23:23:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:23:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:23:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:23:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:23:47 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.926010, avg_loss=0.723150, seen=40, correct=24, accuracy=0.600000
2025-09-13 23:23:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:23:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:23:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:23:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2342MB allocated=2217MB
2025-09-13 23:23:59 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-13 23:23:59 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-13 23:23:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 23:23:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:23:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:24:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 23:24:07 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=127.532013, avg_loss=0.637660, seen=200, correct=128, accuracy=0.640000
2025-09-13 23:24:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:24:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:24:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:24:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2342MB allocated=2217MB
2025-09-13 23:24:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:24:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:24:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:24:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:24:12 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.188999, avg_loss=0.679725, seen=40, correct=26, accuracy=0.650000
2025-09-13 23:24:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:24:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:24:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:24:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2342MB allocated=2217MB
2025-09-13 23:24:22 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-13 23:24:24 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-13 23:24:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 23:24:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:24:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:24:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 23:24:31 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=126.739929, avg_loss=0.633700, seen=200, correct=131, accuracy=0.655000
2025-09-13 23:24:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:24:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:24:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:24:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2342MB allocated=2217MB
2025-09-13 23:24:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:24:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:24:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:24:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:24:35 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.124609, avg_loss=0.653115, seen=40, correct=28, accuracy=0.700000
2025-09-13 23:24:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:24:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:24:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:24:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2342MB allocated=2217MB
2025-09-13 23:24:47 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-13 23:24:48 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-13 23:24:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 23:24:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:24:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:24:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 23:24:56 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=125.336914, avg_loss=0.626685, seen=200, correct=130, accuracy=0.650000
2025-09-13 23:24:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:24:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:24:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:24:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2342MB allocated=2217MB
2025-09-13 23:24:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:24:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:24:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:24:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:24:59 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.354382, avg_loss=0.658860, seen=40, correct=27, accuracy=0.675000
2025-09-13 23:24:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:24:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:25:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:25:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2342MB allocated=2217MB
2025-09-13 23:25:09 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-13 23:25:10 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-13 23:25:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 23:25:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:25:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:25:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 23:25:19 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=125.331726, avg_loss=0.626659, seen=200, correct=129, accuracy=0.645000
2025-09-13 23:25:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:25:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:25:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:25:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2342MB allocated=2217MB
2025-09-13 23:25:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:25:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:25:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:25:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:25:23 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.965364, avg_loss=0.674134, seen=40, correct=25, accuracy=0.625000
2025-09-13 23:25:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:25:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:25:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:25:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2342MB allocated=2217MB
2025-09-13 23:25:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-13 23:25:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-13 23:25:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:25:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:25:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2342MB allocated=2217MB
2025-09-13 23:25:25 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #42', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-13 23:25:25 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #42', 'Round': 0, 'Results_raw': {}}
2025-09-13 23:25:25 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-13 23:25:25 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-13 23:25:26 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-13 23:25:26 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-13 23:25:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-09-13 23:25:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:25:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=85, num_train_batch_last_epoch=15, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:25:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-09-13 23:25:31 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=170, loss_sum=111.965759, avg_loss=0.658622, seen=170, correct=106, accuracy=0.623529
2025-09-13 23:25:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:25:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:25:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:25:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2316MB allocated=2200MB
2025-09-13 23:25:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:25:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:25:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:25:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:25:34 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.583946, avg_loss=0.639599, seen=40, correct=20, accuracy=0.500000
2025-09-13 23:25:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:25:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:25:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:25:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2316MB allocated=2200MB
2025-09-13 23:25:35 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-13 23:25:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=812, total=3247)
2025-09-13 23:25:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:25:35 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-13 23:25:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:25:35 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=406, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-13 23:25:45 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-13 23:25:46 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-13 23:25:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-09-13 23:25:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:25:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=85, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:25:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-09-13 23:25:52 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=170, loss_sum=111.989532, avg_loss=0.658762, seen=170, correct=105, accuracy=0.617647
2025-09-13 23:25:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:25:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:25:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:25:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:25:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:25:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:25:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:25:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:25:56 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.898540, avg_loss=0.672464, seen=40, correct=23, accuracy=0.575000
2025-09-13 23:25:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:25:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:25:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:25:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:26:04 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-13 23:26:06 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-13 23:26:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-09-13 23:26:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:26:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=85, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:26:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-09-13 23:26:13 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=170, loss_sum=111.467583, avg_loss=0.655692, seen=170, correct=106, accuracy=0.623529
2025-09-13 23:26:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:26:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:26:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:26:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:26:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:26:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:26:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:26:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:26:17 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.157883, avg_loss=0.678947, seen=40, correct=22, accuracy=0.550000
2025-09-13 23:26:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:26:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:26:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:26:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:26:29 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-13 23:26:29 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-13 23:26:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-09-13 23:26:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:26:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=85, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:26:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-09-13 23:26:34 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=170, loss_sum=110.881248, avg_loss=0.652243, seen=170, correct=110, accuracy=0.647059
2025-09-13 23:26:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:26:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:26:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:26:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:26:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:26:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:26:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:26:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:26:38 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.263611, avg_loss=0.656590, seen=40, correct=24, accuracy=0.600000
2025-09-13 23:26:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:26:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:26:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:26:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:26:47 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-13 23:26:49 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-13 23:26:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-09-13 23:26:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:26:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=85, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:26:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-09-13 23:26:56 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=170, loss_sum=111.657196, avg_loss=0.656807, seen=170, correct=105, accuracy=0.617647
2025-09-13 23:26:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:26:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:26:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:26:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:26:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:26:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:26:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:27:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:27:00 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.648312, avg_loss=0.641208, seen=40, correct=25, accuracy=0.625000
2025-09-13 23:27:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:27:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:27:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:27:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:27:10 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-13 23:27:10 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-13 23:27:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-09-13 23:27:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:27:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=85, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:27:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-09-13 23:27:16 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=170, loss_sum=110.144485, avg_loss=0.647909, seen=170, correct=107, accuracy=0.629412
2025-09-13 23:27:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:27:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:27:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:27:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:27:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:27:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:27:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:27:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:27:19 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.246536, avg_loss=0.656163, seen=40, correct=21, accuracy=0.525000
2025-09-13 23:27:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:27:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:27:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:27:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:27:30 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-13 23:27:30 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-13 23:27:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-09-13 23:27:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:27:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=85, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:27:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-09-13 23:27:37 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=170, loss_sum=110.256104, avg_loss=0.648565, seen=170, correct=109, accuracy=0.641176
2025-09-13 23:27:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:27:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:27:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:27:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:27:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:27:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:27:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:27:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:27:41 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.588614, avg_loss=0.664715, seen=40, correct=22, accuracy=0.550000
2025-09-13 23:27:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:27:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:27:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:27:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:27:52 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-13 23:27:53 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-13 23:27:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-09-13 23:27:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:27:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=85, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:28:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-09-13 23:28:00 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=170, loss_sum=110.530090, avg_loss=0.650177, seen=170, correct=106, accuracy=0.623529
2025-09-13 23:28:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:28:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:28:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:28:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:28:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:28:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:28:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:28:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:28:04 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.479088, avg_loss=0.661977, seen=40, correct=22, accuracy=0.550000
2025-09-13 23:28:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:28:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:28:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:28:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:28:14 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-13 23:28:16 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-13 23:28:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-09-13 23:28:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:28:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=85, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:28:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-09-13 23:28:22 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=170, loss_sum=109.490990, avg_loss=0.644065, seen=170, correct=110, accuracy=0.647059
2025-09-13 23:28:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:28:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:28:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:28:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:28:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:28:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:28:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:28:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:28:26 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.841391, avg_loss=0.646035, seen=40, correct=23, accuracy=0.575000
2025-09-13 23:28:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:28:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:28:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:28:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:28:36 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-13 23:28:37 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-13 23:28:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-09-13 23:28:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:28:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=85, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:28:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-09-13 23:28:44 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=170, loss_sum=109.781609, avg_loss=0.645774, seen=170, correct=104, accuracy=0.611765
2025-09-13 23:28:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:28:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:28:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:28:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:28:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:28:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:28:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:28:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:28:47 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.229689, avg_loss=0.655742, seen=40, correct=23, accuracy=0.575000
2025-09-13 23:28:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:28:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:28:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:28:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:28:56 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-13 23:28:56 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-13 23:28:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-09-13 23:28:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:28:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=85, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:29:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-09-13 23:29:02 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=170, loss_sum=109.908066, avg_loss=0.646518, seen=170, correct=100, accuracy=0.588235
2025-09-13 23:29:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:29:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:29:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:29:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:29:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:29:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:29:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:29:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:29:06 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.668194, avg_loss=0.666705, seen=40, correct=23, accuracy=0.575000
2025-09-13 23:29:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:29:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:29:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:29:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:29:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-13 23:29:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-13 23:29:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:29:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:29:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:29:08 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #30', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-13 23:29:08 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #30', 'Round': 0, 'Results_raw': {}}
2025-09-13 23:29:08 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-13 23:29:08 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 1 for training...
2025-09-13 23:29:09 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-13 23:29:09 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-13 23:29:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-09-13 23:29:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:29:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=62, num_train_batch_last_epoch=38, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:29:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-09-13 23:29:13 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=123, loss_sum=81.497147, avg_loss=0.662578, seen=123, correct=72, accuracy=0.585366
2025-09-13 23:29:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:29:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:29:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:29:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2316MB allocated=2200MB
2025-09-13 23:29:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:29:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:29:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:29:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:29:16 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.136164, avg_loss=0.703404, seen=40, correct=19, accuracy=0.475000
2025-09-13 23:29:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:29:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:29:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:29:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2316MB allocated=2200MB
2025-09-13 23:29:18 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-13 23:29:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=586, total=2342)
2025-09-13 23:29:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:29:18 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-13 23:29:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:29:18 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=293, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-13 23:29:28 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-13 23:29:28 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-13 23:29:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-09-13 23:29:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:29:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=62, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:29:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-09-13 23:29:34 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=123, loss_sum=80.892166, avg_loss=0.657660, seen=123, correct=74, accuracy=0.601626
2025-09-13 23:29:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:29:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:29:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:29:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2217MB
2025-09-13 23:29:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:29:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:29:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:29:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:29:37 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.569700, avg_loss=0.664243, seen=40, correct=23, accuracy=0.575000
2025-09-13 23:29:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:29:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:29:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:29:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2217MB
2025-09-13 23:29:48 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-13 23:29:49 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-13 23:29:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-09-13 23:29:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:29:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=62, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:29:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-09-13 23:29:54 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=123, loss_sum=79.931664, avg_loss=0.649851, seen=123, correct=71, accuracy=0.577236
2025-09-13 23:29:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:29:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:29:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:29:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2217MB
2025-09-13 23:29:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:29:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:29:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:29:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:29:58 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.495701, avg_loss=0.637393, seen=40, correct=24, accuracy=0.600000
2025-09-13 23:29:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:29:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:29:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:29:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2217MB
2025-09-13 23:30:07 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-13 23:30:08 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-13 23:30:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-09-13 23:30:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:30:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=62, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:30:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-09-13 23:30:13 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=123, loss_sum=80.639542, avg_loss=0.655606, seen=123, correct=76, accuracy=0.617886
2025-09-13 23:30:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:30:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:30:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:30:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2217MB
2025-09-13 23:30:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:30:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:30:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:30:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:30:17 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.163853, avg_loss=0.629096, seen=40, correct=24, accuracy=0.600000
2025-09-13 23:30:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:30:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:30:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:30:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2217MB
2025-09-13 23:30:27 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-13 23:30:28 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-13 23:30:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-09-13 23:30:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:30:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=62, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:30:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-09-13 23:30:33 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=123, loss_sum=80.360535, avg_loss=0.653338, seen=123, correct=77, accuracy=0.626016
2025-09-13 23:30:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:30:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:30:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:30:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2217MB
2025-09-13 23:30:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:30:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:30:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:30:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:30:36 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.553644, avg_loss=0.613841, seen=40, correct=25, accuracy=0.625000
2025-09-13 23:30:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:30:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:30:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:30:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2217MB
2025-09-13 23:30:47 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-13 23:30:48 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-13 23:30:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-09-13 23:30:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:30:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=62, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:30:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-09-13 23:30:52 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=123, loss_sum=79.840324, avg_loss=0.649108, seen=123, correct=77, accuracy=0.626016
2025-09-13 23:30:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:30:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:30:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:30:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2217MB
2025-09-13 23:30:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:30:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:30:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:30:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:30:56 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.716396, avg_loss=0.617910, seen=40, correct=24, accuracy=0.600000
2025-09-13 23:30:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:30:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:30:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:30:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2217MB
2025-09-13 23:31:07 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-13 23:31:07 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-13 23:31:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-09-13 23:31:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:31:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=62, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:31:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-09-13 23:31:12 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=123, loss_sum=79.823227, avg_loss=0.648969, seen=123, correct=77, accuracy=0.626016
2025-09-13 23:31:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:31:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:31:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:31:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2217MB
2025-09-13 23:31:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:31:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:31:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:31:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:31:15 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.804489, avg_loss=0.620112, seen=40, correct=27, accuracy=0.675000
2025-09-13 23:31:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:31:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:31:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:31:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2217MB
2025-09-13 23:31:26 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-13 23:31:28 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-13 23:31:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-09-13 23:31:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:31:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=62, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:31:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-09-13 23:31:33 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=123, loss_sum=79.468399, avg_loss=0.646085, seen=123, correct=75, accuracy=0.609756
2025-09-13 23:31:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:31:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:31:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:31:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2217MB
2025-09-13 23:31:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:31:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:31:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:31:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:31:37 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.208149, avg_loss=0.605204, seen=40, correct=27, accuracy=0.675000
2025-09-13 23:31:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:31:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:31:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:31:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2217MB
2025-09-13 23:31:48 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-13 23:31:49 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-13 23:31:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-09-13 23:31:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:31:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=62, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:31:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-09-13 23:31:53 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=123, loss_sum=79.758446, avg_loss=0.648443, seen=123, correct=81, accuracy=0.658537
2025-09-13 23:31:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:31:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:31:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:31:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2217MB
2025-09-13 23:31:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:31:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:31:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:31:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:31:56 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.354916, avg_loss=0.583873, seen=40, correct=29, accuracy=0.725000
2025-09-13 23:31:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:31:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:31:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:31:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2217MB
2025-09-13 23:32:07 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-13 23:32:07 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-13 23:32:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-09-13 23:32:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:32:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=62, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:32:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-09-13 23:32:12 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=123, loss_sum=80.339462, avg_loss=0.653166, seen=123, correct=81, accuracy=0.658537
2025-09-13 23:32:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:32:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:32:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:32:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2217MB
2025-09-13 23:32:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:32:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:32:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:32:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:32:17 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.783970, avg_loss=0.594599, seen=40, correct=26, accuracy=0.650000
2025-09-13 23:32:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:32:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:32:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:32:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2217MB
2025-09-13 23:32:27 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-13 23:32:27 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-13 23:32:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-09-13 23:32:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:32:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=62, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:32:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-09-13 23:32:32 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=123, loss_sum=81.214615, avg_loss=0.660281, seen=123, correct=72, accuracy=0.585366
2025-09-13 23:32:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:32:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:32:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:32:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2217MB
2025-09-13 23:32:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:32:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:32:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:32:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:32:36 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.805851, avg_loss=0.595146, seen=40, correct=28, accuracy=0.700000
2025-09-13 23:32:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:32:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:32:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:32:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2217MB
2025-09-13 23:32:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-13 23:32:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-13 23:32:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:32:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:32:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2217MB
2025-09-13 23:32:38 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #27', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-13 23:32:38 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #27', 'Round': 0, 'Results_raw': {}}
2025-09-13 23:32:38 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-13 23:32:38 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 1 for training...
2025-09-13 23:32:39 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-13 23:32:39 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-13 23:32:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-09-13 23:32:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:32:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=7, num_train_batch_last_epoch=2, num_train_epoch=15, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:32:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-09-13 23:32:39 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=14, loss_sum=9.137687, avg_loss=0.652692, seen=14, correct=9, accuracy=0.642857
2025-09-13 23:32:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:32:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:32:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:32:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2316MB allocated=2200MB
2025-09-13 23:32:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:32:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:32:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:32:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:32:42 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.366022, avg_loss=0.609151, seen=40, correct=29, accuracy=0.725000
2025-09-13 23:32:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:32:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:32:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:32:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2316MB allocated=2200MB
2025-09-13 23:32:43 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-13 23:32:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=72, total=285)
2025-09-13 23:32:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:32:43 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-13 23:32:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:32:43 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=36, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-13 23:32:53 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-13 23:32:53 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-13 23:32:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-09-13 23:32:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:32:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=7, num_train_batch_last_epoch=200, num_train_epoch=15, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:32:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-09-13 23:32:54 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=14, loss_sum=8.932515, avg_loss=0.638037, seen=14, correct=10, accuracy=0.714286
2025-09-13 23:32:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:32:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:32:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:32:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2384MB allocated=2217MB
2025-09-13 23:32:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:32:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:32:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:32:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:32:58 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.510183, avg_loss=0.662755, seen=40, correct=21, accuracy=0.525000
2025-09-13 23:32:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:32:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:32:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:32:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2384MB allocated=2217MB
2025-09-13 23:33:08 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-13 23:33:09 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-13 23:33:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-09-13 23:33:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:33:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=7, num_train_batch_last_epoch=200, num_train_epoch=15, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:33:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-09-13 23:33:09 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=14, loss_sum=8.725660, avg_loss=0.623261, seen=14, correct=9, accuracy=0.642857
2025-09-13 23:33:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:33:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:33:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:33:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2384MB allocated=2217MB
2025-09-13 23:33:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:33:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:33:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:33:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:33:13 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.008278, avg_loss=0.675207, seen=40, correct=21, accuracy=0.525000
2025-09-13 23:33:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:33:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:33:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:33:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2384MB allocated=2217MB
2025-09-13 23:33:24 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-13 23:33:26 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-13 23:33:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-09-13 23:33:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:33:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=7, num_train_batch_last_epoch=200, num_train_epoch=15, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:33:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-09-13 23:33:26 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=14, loss_sum=8.653791, avg_loss=0.618128, seen=14, correct=9, accuracy=0.642857
2025-09-13 23:33:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:33:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:33:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:33:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2384MB allocated=2217MB
2025-09-13 23:33:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:33:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:33:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:33:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:33:30 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.774458, avg_loss=0.669361, seen=40, correct=21, accuracy=0.525000
2025-09-13 23:33:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:33:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:33:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:33:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2384MB allocated=2217MB
2025-09-13 23:33:39 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-13 23:33:39 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-13 23:33:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-09-13 23:33:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:33:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=7, num_train_batch_last_epoch=200, num_train_epoch=15, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:33:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-09-13 23:33:40 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=14, loss_sum=8.711827, avg_loss=0.622273, seen=14, correct=9, accuracy=0.642857
2025-09-13 23:33:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:33:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:33:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:33:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2384MB allocated=2217MB
2025-09-13 23:33:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:33:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:33:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:33:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:33:43 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.715384, avg_loss=0.667885, seen=40, correct=22, accuracy=0.550000
2025-09-13 23:33:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:33:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:33:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:33:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2384MB allocated=2217MB
2025-09-13 23:33:54 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-13 23:33:55 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-13 23:33:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-09-13 23:33:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:33:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=7, num_train_batch_last_epoch=200, num_train_epoch=15, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:33:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-09-13 23:33:56 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=14, loss_sum=8.404154, avg_loss=0.600297, seen=14, correct=9, accuracy=0.642857
2025-09-13 23:33:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:33:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:33:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:33:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2384MB allocated=2217MB
2025-09-13 23:33:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:33:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:33:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:33:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:33:59 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.768480, avg_loss=0.669212, seen=40, correct=22, accuracy=0.550000
2025-09-13 23:33:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:33:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:34:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:34:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2384MB allocated=2217MB
2025-09-13 23:34:11 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-13 23:34:11 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-13 23:34:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-09-13 23:34:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:34:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=7, num_train_batch_last_epoch=200, num_train_epoch=15, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:34:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-09-13 23:34:12 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=14, loss_sum=8.455968, avg_loss=0.603998, seen=14, correct=9, accuracy=0.642857
2025-09-13 23:34:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:34:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:34:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:34:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2384MB allocated=2217MB
2025-09-13 23:34:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:34:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:34:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:34:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:34:14 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.778589, avg_loss=0.669465, seen=40, correct=22, accuracy=0.550000
2025-09-13 23:34:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:34:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:34:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:34:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2384MB allocated=2217MB
2025-09-13 23:34:25 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-13 23:34:25 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-13 23:34:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-09-13 23:34:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:34:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=7, num_train_batch_last_epoch=200, num_train_epoch=15, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:34:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-09-13 23:34:26 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=14, loss_sum=8.426113, avg_loss=0.601865, seen=14, correct=9, accuracy=0.642857
2025-09-13 23:34:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:34:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:34:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:34:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2384MB allocated=2217MB
2025-09-13 23:34:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:34:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:34:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:34:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:34:30 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.040382, avg_loss=0.676010, seen=40, correct=21, accuracy=0.525000
2025-09-13 23:34:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:34:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:34:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:34:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2384MB allocated=2217MB
2025-09-13 23:34:40 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-13 23:34:41 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-13 23:34:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-09-13 23:34:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:34:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=7, num_train_batch_last_epoch=200, num_train_epoch=15, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:34:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-09-13 23:34:42 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=14, loss_sum=8.398298, avg_loss=0.599878, seen=14, correct=9, accuracy=0.642857
2025-09-13 23:34:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:34:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:34:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:34:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2384MB allocated=2217MB
2025-09-13 23:34:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:34:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:34:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:34:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:34:45 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.922335, avg_loss=0.673058, seen=40, correct=21, accuracy=0.525000
2025-09-13 23:34:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:34:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:34:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:34:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2384MB allocated=2217MB
2025-09-13 23:34:55 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-13 23:34:55 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-13 23:34:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-09-13 23:34:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:34:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=7, num_train_batch_last_epoch=200, num_train_epoch=15, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:34:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-09-13 23:34:56 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=14, loss_sum=8.208369, avg_loss=0.586312, seen=14, correct=9, accuracy=0.642857
2025-09-13 23:34:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:34:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:34:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:34:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2384MB allocated=2217MB
2025-09-13 23:34:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:34:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:34:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:35:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:35:00 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.997383, avg_loss=0.674935, seen=40, correct=22, accuracy=0.550000
2025-09-13 23:35:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:35:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:35:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:35:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2384MB allocated=2217MB
2025-09-13 23:35:10 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-13 23:35:11 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-13 23:35:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-09-13 23:35:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:35:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=7, num_train_batch_last_epoch=200, num_train_epoch=15, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:35:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-09-13 23:35:12 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=14, loss_sum=8.054525, avg_loss=0.575323, seen=14, correct=10, accuracy=0.714286
2025-09-13 23:35:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:35:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:35:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:35:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2384MB allocated=2217MB
2025-09-13 23:35:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:35:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:35:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:35:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:35:16 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.184923, avg_loss=0.679623, seen=40, correct=21, accuracy=0.525000
2025-09-13 23:35:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:35:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:35:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:35:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2384MB allocated=2217MB
2025-09-13 23:35:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-13 23:35:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-13 23:35:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:35:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:35:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2384MB allocated=2217MB
2025-09-13 23:35:18 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #5', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-13 23:35:18 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #5', 'Round': 0, 'Results_raw': {}}
2025-09-13 23:35:18 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-13 23:35:18 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-13 23:35:19 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-13 23:35:19 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-13 23:35:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-09-13 23:35:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:35:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=16, num_train_batch_last_epoch=4, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:35:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-13 23:35:20 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=32, loss_sum=19.872738, avg_loss=0.621023, seen=32, correct=20, accuracy=0.625000
2025-09-13 23:35:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:35:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:35:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:35:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2316MB allocated=2200MB
2025-09-13 23:35:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:35:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:35:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:35:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:35:22 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.140957, avg_loss=0.628524, seen=40, correct=22, accuracy=0.550000
2025-09-13 23:35:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:35:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:35:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:35:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2316MB allocated=2200MB
2025-09-13 23:35:23 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-13 23:35:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=156, total=621)
2025-09-13 23:35:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:35:24 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-13 23:35:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:35:24 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=78, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-13 23:35:31 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-13 23:35:32 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-13 23:35:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-09-13 23:35:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:35:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=16, num_train_batch_last_epoch=200, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:35:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-13 23:35:35 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=32, loss_sum=18.975464, avg_loss=0.592983, seen=32, correct=23, accuracy=0.718750
2025-09-13 23:35:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:35:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:35:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:35:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2388MB allocated=2217MB
2025-09-13 23:35:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:35:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:35:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:35:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:35:39 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.814377, avg_loss=0.670359, seen=40, correct=24, accuracy=0.600000
2025-09-13 23:35:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:35:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:35:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:35:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2388MB allocated=2217MB
2025-09-13 23:35:49 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-13 23:35:50 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-13 23:35:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-09-13 23:35:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:35:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=16, num_train_batch_last_epoch=200, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:35:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-13 23:35:51 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=32, loss_sum=19.380800, avg_loss=0.605650, seen=32, correct=19, accuracy=0.593750
2025-09-13 23:35:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:35:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:35:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:35:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2388MB allocated=2217MB
2025-09-13 23:35:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:35:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:35:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:35:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:35:55 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.668129, avg_loss=0.691703, seen=40, correct=21, accuracy=0.525000
2025-09-13 23:35:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:35:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:35:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:35:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2388MB allocated=2217MB
2025-09-13 23:36:05 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-13 23:36:07 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-13 23:36:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-09-13 23:36:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:36:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=16, num_train_batch_last_epoch=200, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:36:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-13 23:36:08 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=32, loss_sum=20.131355, avg_loss=0.629105, seen=32, correct=20, accuracy=0.625000
2025-09-13 23:36:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:36:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:36:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:36:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2388MB allocated=2217MB
2025-09-13 23:36:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:36:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:36:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:36:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:36:11 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.900421, avg_loss=0.722511, seen=40, correct=17, accuracy=0.425000
2025-09-13 23:36:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:36:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:36:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:36:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2388MB allocated=2217MB
2025-09-13 23:36:21 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-13 23:36:23 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-13 23:36:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-09-13 23:36:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:36:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=16, num_train_batch_last_epoch=200, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:36:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-13 23:36:24 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=32, loss_sum=19.134449, avg_loss=0.597952, seen=32, correct=20, accuracy=0.625000
2025-09-13 23:36:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:36:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:36:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:36:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2388MB allocated=2217MB
2025-09-13 23:36:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:36:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:36:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:36:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:36:27 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.201815, avg_loss=0.705045, seen=40, correct=21, accuracy=0.525000
2025-09-13 23:36:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:36:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:36:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:36:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2388MB allocated=2217MB
2025-09-13 23:36:38 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-13 23:36:39 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-13 23:36:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-09-13 23:36:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:36:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=16, num_train_batch_last_epoch=200, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:36:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-13 23:36:40 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=32, loss_sum=18.724764, avg_loss=0.585149, seen=32, correct=23, accuracy=0.718750
2025-09-13 23:36:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:36:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:36:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:36:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2388MB allocated=2217MB
2025-09-13 23:36:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:36:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:36:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:36:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:36:44 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.947115, avg_loss=0.673678, seen=40, correct=24, accuracy=0.600000
2025-09-13 23:36:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:36:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:36:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:36:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2388MB allocated=2217MB
2025-09-13 23:36:55 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-13 23:36:55 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-13 23:36:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-09-13 23:36:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:36:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=16, num_train_batch_last_epoch=200, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:36:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-13 23:36:56 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=32, loss_sum=19.142916, avg_loss=0.598216, seen=32, correct=20, accuracy=0.625000
2025-09-13 23:36:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:36:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:36:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:36:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2388MB allocated=2217MB
2025-09-13 23:36:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:36:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:36:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:37:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:37:00 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.323177, avg_loss=0.658079, seen=40, correct=23, accuracy=0.575000
2025-09-13 23:37:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:37:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:37:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:37:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2388MB allocated=2217MB
2025-09-13 23:37:10 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-13 23:37:11 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-13 23:37:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-09-13 23:37:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:37:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=16, num_train_batch_last_epoch=200, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:37:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-13 23:37:12 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=32, loss_sum=19.401205, avg_loss=0.606288, seen=32, correct=18, accuracy=0.562500
2025-09-13 23:37:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:37:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:37:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:37:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2388MB allocated=2217MB
2025-09-13 23:37:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:37:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:37:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:37:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:37:16 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.333439, avg_loss=0.708336, seen=40, correct=20, accuracy=0.500000
2025-09-13 23:37:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:37:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:37:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:37:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2388MB allocated=2217MB
2025-09-13 23:37:26 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-13 23:37:26 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-13 23:37:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-09-13 23:37:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:37:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=16, num_train_batch_last_epoch=200, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:37:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-13 23:37:28 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=32, loss_sum=19.339642, avg_loss=0.604364, seen=32, correct=18, accuracy=0.562500
2025-09-13 23:37:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:37:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:37:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:37:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2388MB allocated=2217MB
2025-09-13 23:37:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:37:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:37:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:37:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:37:32 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.803154, avg_loss=0.695079, seen=40, correct=22, accuracy=0.550000
2025-09-13 23:37:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:37:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:37:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:37:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2388MB allocated=2217MB
2025-09-13 23:37:42 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-13 23:37:43 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-13 23:37:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-09-13 23:37:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:37:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=16, num_train_batch_last_epoch=200, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:37:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-13 23:37:45 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=32, loss_sum=18.775244, avg_loss=0.586726, seen=32, correct=23, accuracy=0.718750
2025-09-13 23:37:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:37:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:37:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:37:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2388MB allocated=2217MB
2025-09-13 23:37:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:37:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:37:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:37:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:37:47 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.191559, avg_loss=0.679789, seen=40, correct=21, accuracy=0.525000
2025-09-13 23:37:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:37:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:37:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:37:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2388MB allocated=2217MB
2025-09-13 23:37:57 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-13 23:37:57 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-13 23:37:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-09-13 23:37:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:37:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=16, num_train_batch_last_epoch=200, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:37:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-13 23:37:59 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=32, loss_sum=18.544025, avg_loss=0.579501, seen=32, correct=20, accuracy=0.625000
2025-09-13 23:37:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:37:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:38:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:38:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2388MB allocated=2217MB
2025-09-13 23:38:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:38:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:38:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:38:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:38:02 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.953030, avg_loss=0.673826, seen=40, correct=21, accuracy=0.525000
2025-09-13 23:38:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:38:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:38:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:38:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2388MB allocated=2217MB
2025-09-13 23:38:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-13 23:38:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-13 23:38:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:38:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:38:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2388MB allocated=2217MB
2025-09-13 23:38:05 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #11', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-13 23:38:05 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #11', 'Round': 0, 'Results_raw': {}}
2025-09-13 23:38:05 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-13 23:38:05 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-13 23:38:05 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-13 23:38:05 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-13 23:38:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-09-13 23:38:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:38:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=38, num_train_batch_last_epoch=24, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:38:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-13 23:38:07 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=75, loss_sum=51.505608, avg_loss=0.686741, seen=75, correct=42, accuracy=0.560000
2025-09-13 23:38:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:38:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:38:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:38:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2318MB allocated=2200MB
2025-09-13 23:38:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:38:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:38:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:38:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:38:10 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.894167, avg_loss=0.747354, seen=40, correct=20, accuracy=0.500000
2025-09-13 23:38:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:38:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:38:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:38:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2318MB allocated=2200MB
2025-09-13 23:38:11 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-13 23:38:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=359, total=1434)
2025-09-13 23:38:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:38:12 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-13 23:38:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:38:12 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=180, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-13 23:38:20 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-13 23:38:22 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-13 23:38:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-09-13 23:38:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:38:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=38, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:38:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-13 23:38:25 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=75, loss_sum=52.464760, avg_loss=0.699530, seen=75, correct=41, accuracy=0.546667
2025-09-13 23:38:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:38:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:38:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:38:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2217MB
2025-09-13 23:38:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:38:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:38:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:38:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:38:28 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.953306, avg_loss=0.698833, seen=40, correct=23, accuracy=0.575000
2025-09-13 23:38:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:38:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:38:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:38:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2217MB
2025-09-13 23:38:38 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-13 23:38:38 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-13 23:38:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-09-13 23:38:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:38:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=38, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:38:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-13 23:38:42 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=75, loss_sum=52.476887, avg_loss=0.699692, seen=75, correct=40, accuracy=0.533333
2025-09-13 23:38:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:38:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:38:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:38:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2217MB
2025-09-13 23:38:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:38:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:38:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:38:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:38:45 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.590530, avg_loss=0.689763, seen=40, correct=20, accuracy=0.500000
2025-09-13 23:38:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:38:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:38:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:38:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2217MB
2025-09-13 23:38:54 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-13 23:38:56 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-13 23:38:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-09-13 23:38:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:38:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=38, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:38:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-13 23:38:59 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=75, loss_sum=52.112671, avg_loss=0.694836, seen=75, correct=40, accuracy=0.533333
2025-09-13 23:38:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:38:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:38:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:39:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2217MB
2025-09-13 23:39:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:39:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:39:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:39:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:39:01 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.066797, avg_loss=0.701670, seen=40, correct=20, accuracy=0.500000
2025-09-13 23:39:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:39:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:39:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:39:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2217MB
2025-09-13 23:39:11 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-13 23:39:13 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-13 23:39:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-09-13 23:39:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:39:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=38, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:39:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-13 23:39:16 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=75, loss_sum=52.177376, avg_loss=0.695698, seen=75, correct=40, accuracy=0.533333
2025-09-13 23:39:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:39:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:39:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:39:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2217MB
2025-09-13 23:39:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:39:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:39:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:39:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:39:20 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.605595, avg_loss=0.690140, seen=40, correct=18, accuracy=0.450000
2025-09-13 23:39:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:39:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:39:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:39:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2217MB
2025-09-13 23:39:30 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-13 23:39:30 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-13 23:39:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-09-13 23:39:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:39:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=38, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:39:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-13 23:39:33 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=75, loss_sum=51.575058, avg_loss=0.687667, seen=75, correct=40, accuracy=0.533333
2025-09-13 23:39:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:39:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:39:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:39:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2217MB
2025-09-13 23:39:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:39:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:39:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:39:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:39:36 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.028095, avg_loss=0.700702, seen=40, correct=21, accuracy=0.525000
2025-09-13 23:39:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:39:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:39:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:39:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2217MB
2025-09-13 23:39:47 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-13 23:39:47 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-13 23:39:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-09-13 23:39:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:39:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=38, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:39:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-13 23:39:50 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=75, loss_sum=50.570076, avg_loss=0.674268, seen=75, correct=41, accuracy=0.546667
2025-09-13 23:39:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:39:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:39:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:39:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2217MB
2025-09-13 23:39:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:39:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:39:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:39:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:39:55 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.903763, avg_loss=0.722594, seen=40, correct=20, accuracy=0.500000
2025-09-13 23:39:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:39:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:39:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:39:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2217MB
2025-09-13 23:40:05 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-13 23:40:06 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-13 23:40:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-09-13 23:40:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:40:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=38, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:40:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-13 23:40:09 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=75, loss_sum=50.531548, avg_loss=0.673754, seen=75, correct=41, accuracy=0.546667
2025-09-13 23:40:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:40:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:40:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:40:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2217MB
2025-09-13 23:40:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:40:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:40:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:40:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:40:11 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.213974, avg_loss=0.705349, seen=40, correct=19, accuracy=0.475000
2025-09-13 23:40:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:40:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:40:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:40:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2217MB
2025-09-13 23:40:22 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-13 23:40:23 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-13 23:40:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-09-13 23:40:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:40:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=38, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:40:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-13 23:40:26 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=75, loss_sum=51.458450, avg_loss=0.686113, seen=75, correct=40, accuracy=0.533333
2025-09-13 23:40:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:40:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:40:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:40:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2217MB
2025-09-13 23:40:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:40:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:40:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:40:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:40:29 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.619061, avg_loss=0.690477, seen=40, correct=18, accuracy=0.450000
2025-09-13 23:40:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:40:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:40:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:40:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2217MB
2025-09-13 23:40:41 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-13 23:40:41 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-13 23:40:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-09-13 23:40:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:40:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=38, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:40:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-13 23:40:44 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=75, loss_sum=52.498066, avg_loss=0.699974, seen=75, correct=41, accuracy=0.546667
2025-09-13 23:40:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:40:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:40:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:40:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2217MB
2025-09-13 23:40:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:40:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:40:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:40:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:40:49 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.936872, avg_loss=0.673422, seen=40, correct=21, accuracy=0.525000
2025-09-13 23:40:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:40:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:40:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:40:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2217MB
2025-09-13 23:40:58 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-13 23:41:00 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-13 23:41:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-09-13 23:41:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:41:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=38, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:41:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-13 23:41:03 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=75, loss_sum=51.538109, avg_loss=0.687175, seen=75, correct=40, accuracy=0.533333
2025-09-13 23:41:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:41:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:41:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:41:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2217MB
2025-09-13 23:41:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:41:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:41:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:41:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:41:07 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.259701, avg_loss=0.681493, seen=40, correct=20, accuracy=0.500000
2025-09-13 23:41:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:41:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:41:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:41:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2217MB
2025-09-13 23:41:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-13 23:41:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-13 23:41:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:41:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:41:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2217MB
2025-09-13 23:41:10 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #28', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-13 23:41:10 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #28', 'Round': 0, 'Results_raw': {}}
2025-09-13 23:41:10 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-13 23:41:10 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-13 23:41:11 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-13 23:41:11 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-13 23:41:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 23:41:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:41:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:41:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 23:41:17 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=136.735306, avg_loss=0.683677, seen=200, correct=119, accuracy=0.595000
2025-09-13 23:41:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:41:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:41:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:41:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2318MB allocated=2200MB
2025-09-13 23:41:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:41:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:41:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:41:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:41:19 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.795116, avg_loss=0.644878, seen=40, correct=27, accuracy=0.675000
2025-09-13 23:41:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:41:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:41:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:41:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2318MB allocated=2200MB
2025-09-13 23:41:21 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-13 23:41:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1698, total=6791)
2025-09-13 23:41:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:41:21 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-13 23:41:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:41:21 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=849, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-13 23:41:30 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-13 23:41:30 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-13 23:41:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 23:41:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:41:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:41:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 23:41:38 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=137.447525, avg_loss=0.687238, seen=200, correct=126, accuracy=0.630000
2025-09-13 23:41:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:41:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:41:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:41:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-13 23:41:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:41:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:41:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:41:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:41:42 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.080063, avg_loss=0.652002, seen=40, correct=27, accuracy=0.675000
2025-09-13 23:41:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:41:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:41:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:41:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-13 23:41:52 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-13 23:41:54 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-13 23:41:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 23:41:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:41:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:42:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 23:42:02 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=133.890900, avg_loss=0.669454, seen=200, correct=109, accuracy=0.545000
2025-09-13 23:42:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:42:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:42:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:42:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-13 23:42:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:42:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:42:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:42:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:42:05 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.273277, avg_loss=0.656832, seen=40, correct=23, accuracy=0.575000
2025-09-13 23:42:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:42:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:42:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:42:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-13 23:42:16 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-13 23:42:17 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-13 23:42:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 23:42:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:42:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:42:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 23:42:25 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=135.233307, avg_loss=0.676167, seen=200, correct=120, accuracy=0.600000
2025-09-13 23:42:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:42:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:42:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:42:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-13 23:42:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:42:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:42:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:42:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:42:28 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.713503, avg_loss=0.667838, seen=40, correct=23, accuracy=0.575000
2025-09-13 23:42:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:42:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:42:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:42:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-13 23:42:38 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-13 23:42:39 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-13 23:42:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 23:42:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:42:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:42:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 23:42:48 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=134.116333, avg_loss=0.670582, seen=200, correct=119, accuracy=0.595000
2025-09-13 23:42:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:42:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:42:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:42:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-13 23:42:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:42:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:42:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:42:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:42:51 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.394985, avg_loss=0.659875, seen=40, correct=26, accuracy=0.650000
2025-09-13 23:42:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:42:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:42:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:42:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-13 23:43:02 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-13 23:43:02 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-13 23:43:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 23:43:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:43:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:43:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 23:43:11 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=134.726288, avg_loss=0.673631, seen=200, correct=119, accuracy=0.595000
2025-09-13 23:43:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:43:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:43:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:43:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2217MB
2025-09-13 23:43:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:43:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:43:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:43:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:43:15 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.594498, avg_loss=0.664862, seen=40, correct=24, accuracy=0.600000
2025-09-13 23:43:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:43:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:43:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:43:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-13 23:43:25 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-13 23:43:27 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-13 23:43:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 23:43:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:43:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:43:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 23:43:36 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=135.260422, avg_loss=0.676302, seen=200, correct=116, accuracy=0.580000
2025-09-13 23:43:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:43:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:43:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:43:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-13 23:43:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:43:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:43:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:43:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:43:40 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.151340, avg_loss=0.653784, seen=40, correct=27, accuracy=0.675000
2025-09-13 23:43:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:43:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:43:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:43:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-13 23:43:51 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-13 23:43:52 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-13 23:43:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 23:43:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:43:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:44:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 23:44:00 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=133.873184, avg_loss=0.669366, seen=200, correct=119, accuracy=0.595000
2025-09-13 23:44:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:44:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:44:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:44:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-13 23:44:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:44:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:44:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:44:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:44:04 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.176773, avg_loss=0.654419, seen=40, correct=26, accuracy=0.650000
2025-09-13 23:44:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:44:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:44:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:44:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-13 23:44:14 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-13 23:44:15 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-13 23:44:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 23:44:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:44:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:44:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 23:44:22 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=133.060760, avg_loss=0.665304, seen=200, correct=121, accuracy=0.605000
2025-09-13 23:44:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:44:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:44:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:44:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-13 23:44:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:44:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:44:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:44:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:44:27 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.044315, avg_loss=0.651108, seen=40, correct=26, accuracy=0.650000
2025-09-13 23:44:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:44:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:44:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:44:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-13 23:44:37 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-13 23:44:39 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-13 23:44:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 23:44:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:44:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:44:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 23:44:47 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=133.835754, avg_loss=0.669179, seen=200, correct=121, accuracy=0.605000
2025-09-13 23:44:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:44:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:44:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:44:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-13 23:44:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:44:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:44:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:44:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:44:51 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.363518, avg_loss=0.659088, seen=40, correct=24, accuracy=0.600000
2025-09-13 23:44:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:44:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:44:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:44:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-13 23:45:02 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-13 23:45:02 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-13 23:45:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 23:45:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:45:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:45:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 23:45:11 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=134.739227, avg_loss=0.673696, seen=200, correct=120, accuracy=0.600000
2025-09-13 23:45:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:45:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:45:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:45:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-13 23:45:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:45:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:45:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:45:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:45:15 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.393183, avg_loss=0.659830, seen=40, correct=25, accuracy=0.625000
2025-09-13 23:45:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:45:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:45:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:45:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-13 23:45:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-13 23:45:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-13 23:45:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:45:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:45:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-13 23:45:17 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #53', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-13 23:45:17 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #53', 'Round': 0, 'Results_raw': {}}
2025-09-13 23:45:17 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-13 23:45:17 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 1 for training...
2025-09-13 23:45:18 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-13 23:45:18 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-13 23:45:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-09-13 23:45:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:45:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=97, num_train_batch_last_epoch=3, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:45:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-09-13 23:45:23 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=193, loss_sum=134.772720, avg_loss=0.698304, seen=193, correct=110, accuracy=0.569948
2025-09-13 23:45:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:45:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:45:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:45:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2318MB allocated=2200MB
2025-09-13 23:45:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:45:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:45:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:45:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:45:25 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.675898, avg_loss=0.666897, seen=40, correct=26, accuracy=0.650000
2025-09-13 23:45:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:45:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:45:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:45:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2318MB allocated=2200MB
2025-09-13 23:45:26 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-13 23:45:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=920, total=3679)
2025-09-13 23:45:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:45:27 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-13 23:45:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:45:27 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=460, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-13 23:45:36 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-13 23:45:37 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-13 23:45:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-09-13 23:45:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:45:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=97, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:45:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-09-13 23:45:45 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=193, loss_sum=135.266815, avg_loss=0.700864, seen=193, correct=107, accuracy=0.554404
2025-09-13 23:45:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:45:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:45:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:45:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2342MB allocated=2217MB
2025-09-13 23:45:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:45:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:45:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:45:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:45:49 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.221731, avg_loss=0.630543, seen=40, correct=25, accuracy=0.625000
2025-09-13 23:45:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:45:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:45:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:45:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2342MB allocated=2217MB
2025-09-13 23:46:01 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-13 23:46:01 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-13 23:46:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-09-13 23:46:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:46:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=97, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:46:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-09-13 23:46:08 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=193, loss_sum=134.732407, avg_loss=0.698095, seen=193, correct=108, accuracy=0.559585
2025-09-13 23:46:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:46:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:46:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:46:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2342MB allocated=2217MB
2025-09-13 23:46:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:46:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:46:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:46:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:46:12 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.399837, avg_loss=0.659996, seen=40, correct=25, accuracy=0.625000
2025-09-13 23:46:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:46:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:46:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:46:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2342MB allocated=2217MB
2025-09-13 23:46:22 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-13 23:46:23 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-13 23:46:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-09-13 23:46:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:46:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=97, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:46:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-09-13 23:46:30 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=193, loss_sum=137.020966, avg_loss=0.709953, seen=193, correct=108, accuracy=0.559585
2025-09-13 23:46:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:46:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:46:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:46:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2342MB allocated=2217MB
2025-09-13 23:46:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:46:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:46:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:46:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:46:34 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.575123, avg_loss=0.639378, seen=40, correct=26, accuracy=0.650000
2025-09-13 23:46:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:46:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:46:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:46:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2342MB allocated=2217MB
2025-09-13 23:46:45 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-13 23:46:45 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-13 23:46:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-09-13 23:46:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:46:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=97, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:46:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-09-13 23:46:52 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=193, loss_sum=135.834167, avg_loss=0.703804, seen=193, correct=106, accuracy=0.549223
2025-09-13 23:46:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:46:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:46:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:46:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2342MB allocated=2217MB
2025-09-13 23:46:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:46:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:46:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:46:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:46:56 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.047358, avg_loss=0.651184, seen=40, correct=25, accuracy=0.625000
2025-09-13 23:46:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:46:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:46:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:46:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2342MB allocated=2217MB
2025-09-13 23:47:05 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-13 23:47:07 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-13 23:47:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-09-13 23:47:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:47:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=97, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:47:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-09-13 23:47:14 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=193, loss_sum=132.697388, avg_loss=0.687551, seen=193, correct=109, accuracy=0.564767
2025-09-13 23:47:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:47:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:47:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:47:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2342MB allocated=2217MB
2025-09-13 23:47:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:47:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:47:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:47:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:47:18 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.722359, avg_loss=0.668059, seen=40, correct=23, accuracy=0.575000
2025-09-13 23:47:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:47:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:47:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:47:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2342MB allocated=2217MB
2025-09-13 23:47:30 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-13 23:47:30 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-13 23:47:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-09-13 23:47:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:47:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=97, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:47:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-09-13 23:47:37 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=193, loss_sum=132.446213, avg_loss=0.686250, seen=193, correct=111, accuracy=0.575130
2025-09-13 23:47:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:47:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:47:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:47:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2342MB allocated=2217MB
2025-09-13 23:47:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:47:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:47:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:47:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:47:40 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.924473, avg_loss=0.648112, seen=40, correct=26, accuracy=0.650000
2025-09-13 23:47:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:47:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:47:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:47:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2342MB allocated=2217MB
2025-09-13 23:47:51 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-13 23:47:51 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-13 23:47:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-09-13 23:47:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:47:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=97, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:47:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-09-13 23:47:58 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=193, loss_sum=135.322556, avg_loss=0.701153, seen=193, correct=101, accuracy=0.523316
2025-09-13 23:47:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:47:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:47:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:48:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2342MB allocated=2217MB
2025-09-13 23:48:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:48:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:48:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:48:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:48:01 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.349634, avg_loss=0.633741, seen=40, correct=24, accuracy=0.600000
2025-09-13 23:48:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:48:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:48:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:48:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2342MB allocated=2217MB
2025-09-13 23:48:12 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-13 23:48:12 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-13 23:48:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-09-13 23:48:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:48:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=97, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:48:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-09-13 23:48:19 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=193, loss_sum=138.606049, avg_loss=0.718166, seen=193, correct=96, accuracy=0.497409
2025-09-13 23:48:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:48:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:48:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:48:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2342MB allocated=2217MB
2025-09-13 23:48:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:48:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:48:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:48:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:48:22 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.970615, avg_loss=0.624265, seen=40, correct=26, accuracy=0.650000
2025-09-13 23:48:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:48:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:48:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:48:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2342MB allocated=2217MB
2025-09-13 23:48:33 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-13 23:48:34 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-13 23:48:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-09-13 23:48:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:48:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=97, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:48:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-09-13 23:48:43 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=193, loss_sum=134.830109, avg_loss=0.698602, seen=193, correct=105, accuracy=0.544041
2025-09-13 23:48:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:48:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:48:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:48:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2342MB allocated=2217MB
2025-09-13 23:48:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:48:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:48:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:48:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:48:47 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.460876, avg_loss=0.661522, seen=40, correct=23, accuracy=0.575000
2025-09-13 23:48:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:48:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:48:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:48:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2342MB allocated=2217MB
2025-09-13 23:48:58 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-13 23:48:59 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-13 23:48:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-09-13 23:48:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:48:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=97, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:49:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-09-13 23:49:07 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=193, loss_sum=132.696014, avg_loss=0.687544, seen=193, correct=110, accuracy=0.569948
2025-09-13 23:49:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:49:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:49:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:49:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2342MB allocated=2217MB
2025-09-13 23:49:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:49:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:49:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:49:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:49:11 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.992706, avg_loss=0.649818, seen=40, correct=24, accuracy=0.600000
2025-09-13 23:49:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:49:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:49:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:49:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2342MB allocated=2217MB
2025-09-13 23:49:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-13 23:49:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-13 23:49:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:49:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:49:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2342MB allocated=2217MB
2025-09-13 23:49:13 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #31', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-13 23:49:13 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #31', 'Round': 0, 'Results_raw': {}}
2025-09-13 23:49:13 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-13 23:49:13 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-13 23:49:14 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-13 23:49:14 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-13 23:49:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 23:49:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:49:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:49:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 23:49:21 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=128.175812, avg_loss=0.640879, seen=200, correct=128, accuracy=0.640000
2025-09-13 23:49:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:49:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:49:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:49:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2318MB allocated=2200MB
2025-09-13 23:49:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:49:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:49:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:49:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:49:24 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.130604, avg_loss=0.628265, seen=40, correct=23, accuracy=0.575000
2025-09-13 23:49:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:49:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:49:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:49:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2318MB allocated=2200MB
2025-09-13 23:49:25 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-13 23:49:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1543, total=6171)
2025-09-13 23:49:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:49:26 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-13 23:49:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:49:26 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=772, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-13 23:49:34 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-13 23:49:36 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-13 23:49:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 23:49:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:49:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:49:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 23:49:43 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=130.065323, avg_loss=0.650327, seen=200, correct=134, accuracy=0.670000
2025-09-13 23:49:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:49:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:49:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:49:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2217MB
2025-09-13 23:49:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:49:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:49:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:49:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:49:47 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.202967, avg_loss=0.630074, seen=40, correct=23, accuracy=0.575000
2025-09-13 23:49:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:49:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:49:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:49:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2217MB
2025-09-13 23:49:59 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-13 23:49:59 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-13 23:49:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 23:49:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:49:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:50:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 23:50:07 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=130.664795, avg_loss=0.653324, seen=200, correct=125, accuracy=0.625000
2025-09-13 23:50:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:50:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:50:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:50:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2217MB
2025-09-13 23:50:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:50:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:50:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:50:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:50:11 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.074060, avg_loss=0.651852, seen=40, correct=23, accuracy=0.575000
2025-09-13 23:50:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:50:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:50:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:50:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2217MB
2025-09-13 23:50:21 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-13 23:50:22 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-13 23:50:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 23:50:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:50:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:50:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 23:50:29 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=129.593933, avg_loss=0.647970, seen=200, correct=128, accuracy=0.640000
2025-09-13 23:50:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:50:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:50:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:50:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2217MB
2025-09-13 23:50:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:50:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:50:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:50:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:50:33 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.343000, avg_loss=0.658575, seen=40, correct=21, accuracy=0.525000
2025-09-13 23:50:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:50:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:50:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:50:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2217MB
2025-09-13 23:50:43 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-13 23:50:44 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-13 23:50:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 23:50:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:50:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:50:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 23:50:51 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=129.077438, avg_loss=0.645387, seen=200, correct=132, accuracy=0.660000
2025-09-13 23:50:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:50:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:50:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:50:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2217MB
2025-09-13 23:50:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:50:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:50:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:50:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:50:55 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.580385, avg_loss=0.664510, seen=40, correct=21, accuracy=0.525000
2025-09-13 23:50:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:50:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:50:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:50:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2217MB
2025-09-13 23:51:06 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-13 23:51:06 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-13 23:51:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 23:51:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:51:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:51:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 23:51:14 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=130.145767, avg_loss=0.650729, seen=200, correct=127, accuracy=0.635000
2025-09-13 23:51:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:51:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:51:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:51:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2217MB
2025-09-13 23:51:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:51:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:51:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:51:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:51:17 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.875198, avg_loss=0.671880, seen=40, correct=19, accuracy=0.475000
2025-09-13 23:51:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:51:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:51:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:51:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2217MB
2025-09-13 23:51:28 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-13 23:51:29 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-13 23:51:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 23:51:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:51:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:51:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 23:51:36 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=133.438934, avg_loss=0.667195, seen=200, correct=125, accuracy=0.625000
2025-09-13 23:51:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:51:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:51:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:51:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2217MB
2025-09-13 23:51:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:51:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:51:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:51:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:51:40 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.078953, avg_loss=0.676974, seen=40, correct=21, accuracy=0.525000
2025-09-13 23:51:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:51:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:51:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:51:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2217MB
2025-09-13 23:51:51 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-13 23:51:52 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-13 23:51:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 23:51:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:51:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:52:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 23:52:00 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=137.828735, avg_loss=0.689144, seen=200, correct=114, accuracy=0.570000
2025-09-13 23:52:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:52:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:52:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:52:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2217MB
2025-09-13 23:52:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:52:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:52:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:52:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:52:04 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.002604, avg_loss=0.675065, seen=40, correct=25, accuracy=0.625000
2025-09-13 23:52:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:52:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:52:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:52:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2217MB
2025-09-13 23:52:14 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-13 23:52:14 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-13 23:52:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 23:52:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:52:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:52:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 23:52:22 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=132.897842, avg_loss=0.664489, seen=200, correct=123, accuracy=0.615000
2025-09-13 23:52:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:52:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:52:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:52:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2217MB
2025-09-13 23:52:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:52:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:52:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:52:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:52:25 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.107227, avg_loss=0.652681, seen=40, correct=25, accuracy=0.625000
2025-09-13 23:52:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:52:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:52:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:52:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2217MB
2025-09-13 23:52:34 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-13 23:52:35 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-13 23:52:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 23:52:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:52:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:52:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 23:52:43 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=128.277512, avg_loss=0.641388, seen=200, correct=131, accuracy=0.655000
2025-09-13 23:52:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:52:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:52:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:52:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2217MB
2025-09-13 23:52:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:52:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:52:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:52:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:52:47 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.048986, avg_loss=0.651225, seen=40, correct=22, accuracy=0.550000
2025-09-13 23:52:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:52:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:52:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:52:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2217MB
2025-09-13 23:52:57 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-13 23:52:59 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-13 23:52:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 23:52:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:52:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:53:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 23:53:06 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=127.410034, avg_loss=0.637050, seen=200, correct=129, accuracy=0.645000
2025-09-13 23:53:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:53:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:53:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:53:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2217MB
2025-09-13 23:53:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:53:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:53:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:53:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:53:10 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.160904, avg_loss=0.679023, seen=40, correct=21, accuracy=0.525000
2025-09-13 23:53:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:53:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:53:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:53:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2217MB
2025-09-13 23:53:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-13 23:53:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-13 23:53:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:53:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:53:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2217MB
2025-09-13 23:53:12 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #38', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-13 23:53:12 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #38', 'Round': 0, 'Results_raw': {}}
2025-09-13 23:53:12 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-13 23:53:12 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-13 23:53:13 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-13 23:53:13 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-13 23:53:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-09-13 23:53:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:53:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=15, num_train_batch_last_epoch=10, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:53:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-13 23:53:14 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=30, loss_sum=18.240786, avg_loss=0.608026, seen=30, correct=19, accuracy=0.633333
2025-09-13 23:53:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:53:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:53:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:53:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2200MB
2025-09-13 23:53:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:53:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:53:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:53:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:53:16 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.124409, avg_loss=0.628110, seen=40, correct=24, accuracy=0.600000
2025-09-13 23:53:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:53:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:53:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:53:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2200MB
2025-09-13 23:53:17 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-13 23:53:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=146, total=583)
2025-09-13 23:53:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:53:18 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-13 23:53:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:53:18 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=73, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-13 23:53:28 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-13 23:53:28 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-13 23:53:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-09-13 23:53:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:53:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=15, num_train_batch_last_epoch=200, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:53:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-13 23:53:30 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=30, loss_sum=18.179888, avg_loss=0.605996, seen=30, correct=21, accuracy=0.700000
2025-09-13 23:53:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:53:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:53:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:53:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:53:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:53:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:53:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:53:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:53:34 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.073929, avg_loss=0.626848, seen=40, correct=25, accuracy=0.625000
2025-09-13 23:53:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:53:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:53:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:53:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:53:44 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-13 23:53:46 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-13 23:53:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-09-13 23:53:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:53:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=15, num_train_batch_last_epoch=200, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:53:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-13 23:53:47 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=30, loss_sum=18.937126, avg_loss=0.631238, seen=30, correct=19, accuracy=0.633333
2025-09-13 23:53:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:53:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:53:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:53:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:53:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:53:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:53:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:53:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:53:50 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.716354, avg_loss=0.617909, seen=40, correct=26, accuracy=0.650000
2025-09-13 23:53:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:53:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:53:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:53:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:54:02 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-13 23:54:02 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-13 23:54:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-09-13 23:54:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:54:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=15, num_train_batch_last_epoch=200, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:54:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-13 23:54:04 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=30, loss_sum=18.297066, avg_loss=0.609902, seen=30, correct=20, accuracy=0.666667
2025-09-13 23:54:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:54:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:54:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:54:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:54:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:54:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:54:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:54:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:54:07 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.026482, avg_loss=0.600662, seen=40, correct=27, accuracy=0.675000
2025-09-13 23:54:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:54:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:54:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:54:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:54:18 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-13 23:54:18 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-13 23:54:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-09-13 23:54:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:54:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=15, num_train_batch_last_epoch=200, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:54:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-13 23:54:19 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=30, loss_sum=18.006458, avg_loss=0.600215, seen=30, correct=23, accuracy=0.766667
2025-09-13 23:54:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:54:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:54:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:54:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:54:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:54:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:54:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:54:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:54:23 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.479771, avg_loss=0.611994, seen=40, correct=26, accuracy=0.650000
2025-09-13 23:54:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:54:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:54:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:54:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:54:33 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-13 23:54:34 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-13 23:54:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-09-13 23:54:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:54:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=15, num_train_batch_last_epoch=200, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:54:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-13 23:54:35 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=30, loss_sum=17.969954, avg_loss=0.598998, seen=30, correct=21, accuracy=0.700000
2025-09-13 23:54:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:54:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:54:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:54:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:54:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:54:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:54:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:54:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:54:39 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.464336, avg_loss=0.611608, seen=40, correct=26, accuracy=0.650000
2025-09-13 23:54:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:54:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:54:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:54:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:54:50 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-13 23:54:50 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-13 23:54:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-09-13 23:54:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:54:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=15, num_train_batch_last_epoch=200, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:54:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-13 23:54:51 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=30, loss_sum=17.920191, avg_loss=0.597340, seen=30, correct=20, accuracy=0.666667
2025-09-13 23:54:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:54:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:54:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:54:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:54:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:54:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:54:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:54:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:54:55 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.789364, avg_loss=0.594734, seen=40, correct=27, accuracy=0.675000
2025-09-13 23:54:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:54:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:54:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:54:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:55:05 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-13 23:55:06 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-13 23:55:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-09-13 23:55:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:55:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=15, num_train_batch_last_epoch=200, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:55:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-13 23:55:07 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=30, loss_sum=18.160282, avg_loss=0.605343, seen=30, correct=20, accuracy=0.666667
2025-09-13 23:55:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:55:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:55:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:55:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:55:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:55:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:55:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:55:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:55:10 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.156237, avg_loss=0.603906, seen=40, correct=26, accuracy=0.650000
2025-09-13 23:55:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:55:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:55:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:55:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:55:21 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-13 23:55:21 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-13 23:55:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-09-13 23:55:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:55:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=15, num_train_batch_last_epoch=200, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:55:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-13 23:55:22 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=30, loss_sum=17.980408, avg_loss=0.599347, seen=30, correct=20, accuracy=0.666667
2025-09-13 23:55:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:55:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:55:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:55:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:55:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:55:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:55:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:55:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:55:26 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.056875, avg_loss=0.601422, seen=40, correct=26, accuracy=0.650000
2025-09-13 23:55:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:55:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:55:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:55:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:55:35 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-13 23:55:36 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-13 23:55:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-09-13 23:55:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:55:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=15, num_train_batch_last_epoch=200, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:55:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-13 23:55:37 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=30, loss_sum=17.809031, avg_loss=0.593634, seen=30, correct=21, accuracy=0.700000
2025-09-13 23:55:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:55:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:55:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:55:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:55:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:55:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:55:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:55:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:55:40 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.855200, avg_loss=0.596380, seen=40, correct=25, accuracy=0.625000
2025-09-13 23:55:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:55:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:55:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:55:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:55:51 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-13 23:55:53 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-13 23:55:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-09-13 23:55:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:55:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=15, num_train_batch_last_epoch=200, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:55:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-13 23:55:54 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=30, loss_sum=17.956667, avg_loss=0.598556, seen=30, correct=21, accuracy=0.700000
2025-09-13 23:55:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:55:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:55:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:55:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:55:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:55:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:55:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:55:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:55:58 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.503630, avg_loss=0.587591, seen=40, correct=29, accuracy=0.725000
2025-09-13 23:55:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:55:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:55:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:55:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:55:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-13 23:55:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-13 23:55:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:56:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:56:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:56:00 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #23', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-13 23:56:00 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #23', 'Round': 0, 'Results_raw': {}}
2025-09-13 23:56:00 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-13 23:56:00 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-13 23:56:01 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-13 23:56:01 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-13 23:56:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-09-13 23:56:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:56:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=35, num_train_batch_last_epoch=30, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:56:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-13 23:56:02 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=69, loss_sum=45.205505, avg_loss=0.655152, seen=69, correct=42, accuracy=0.608696
2025-09-13 23:56:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:56:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:56:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:56:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2200MB
2025-09-13 23:56:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:56:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:56:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:56:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:56:04 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.922138, avg_loss=0.773053, seen=40, correct=20, accuracy=0.500000
2025-09-13 23:56:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:56:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:56:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:56:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2200MB
2025-09-13 23:56:05 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-13 23:56:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=329, total=1316)
2025-09-13 23:56:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:56:05 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-13 23:56:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:56:05 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=165, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-13 23:56:15 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-13 23:56:16 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-13 23:56:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-09-13 23:56:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:56:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=35, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:56:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-13 23:56:19 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=69, loss_sum=49.096809, avg_loss=0.711548, seen=69, correct=38, accuracy=0.550725
2025-09-13 23:56:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:56:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:56:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:56:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-13 23:56:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:56:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:56:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:56:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:56:22 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.245655, avg_loss=0.731141, seen=40, correct=25, accuracy=0.625000
2025-09-13 23:56:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:56:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:56:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:56:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-13 23:56:33 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-13 23:56:34 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-13 23:56:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-09-13 23:56:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:56:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=35, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:56:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-13 23:56:37 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=69, loss_sum=48.109810, avg_loss=0.697244, seen=69, correct=37, accuracy=0.536232
2025-09-13 23:56:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:56:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:56:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:56:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-13 23:56:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:56:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:56:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:56:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:56:41 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.492662, avg_loss=0.737317, seen=40, correct=24, accuracy=0.600000
2025-09-13 23:56:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:56:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:56:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:56:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-13 23:56:50 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-13 23:56:50 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-13 23:56:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-09-13 23:56:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:56:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=35, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:56:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-13 23:56:53 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=69, loss_sum=45.711781, avg_loss=0.662490, seen=69, correct=40, accuracy=0.579710
2025-09-13 23:56:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:56:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:56:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:56:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-13 23:56:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:56:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:56:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:56:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:56:57 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.690636, avg_loss=0.717266, seen=40, correct=23, accuracy=0.575000
2025-09-13 23:56:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:56:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:56:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:56:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-13 23:57:06 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-13 23:57:08 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-13 23:57:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-09-13 23:57:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:57:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=35, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:57:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-13 23:57:11 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=69, loss_sum=44.475365, avg_loss=0.644571, seen=69, correct=41, accuracy=0.594203
2025-09-13 23:57:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:57:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:57:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:57:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-13 23:57:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:57:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:57:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:57:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:57:14 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.182238, avg_loss=0.729556, seen=40, correct=20, accuracy=0.500000
2025-09-13 23:57:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:57:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:57:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:57:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-13 23:57:24 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-13 23:57:25 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-13 23:57:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-09-13 23:57:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:57:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=35, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:57:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-13 23:57:28 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=69, loss_sum=43.658924, avg_loss=0.632738, seen=69, correct=45, accuracy=0.652174
2025-09-13 23:57:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:57:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:57:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:57:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-13 23:57:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:57:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:57:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:57:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:57:31 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.538057, avg_loss=0.738451, seen=40, correct=21, accuracy=0.525000
2025-09-13 23:57:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:57:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:57:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:57:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-13 23:57:43 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-13 23:57:43 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-13 23:57:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-09-13 23:57:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:57:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=35, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:57:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-13 23:57:45 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=69, loss_sum=45.503181, avg_loss=0.659466, seen=69, correct=43, accuracy=0.623188
2025-09-13 23:57:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:57:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:57:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:57:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-13 23:57:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:57:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:57:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:57:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:57:49 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.113842, avg_loss=0.752846, seen=40, correct=19, accuracy=0.475000
2025-09-13 23:57:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:57:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:57:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:57:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-13 23:57:59 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-13 23:58:00 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-13 23:58:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-09-13 23:58:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:58:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=35, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:58:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-13 23:58:03 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=69, loss_sum=46.600861, avg_loss=0.675375, seen=69, correct=43, accuracy=0.623188
2025-09-13 23:58:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:58:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:58:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:58:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-13 23:58:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:58:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:58:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:58:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:58:07 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.313395, avg_loss=0.757835, seen=40, correct=21, accuracy=0.525000
2025-09-13 23:58:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:58:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:58:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:58:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-13 23:58:19 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-13 23:58:19 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-13 23:58:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-09-13 23:58:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:58:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=35, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:58:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-13 23:58:22 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=69, loss_sum=46.155136, avg_loss=0.668915, seen=69, correct=43, accuracy=0.623188
2025-09-13 23:58:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:58:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:58:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:58:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-13 23:58:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:58:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:58:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:58:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:58:26 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.366758, avg_loss=0.734169, seen=40, correct=20, accuracy=0.500000
2025-09-13 23:58:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:58:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:58:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:58:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-13 23:58:37 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-13 23:58:37 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-13 23:58:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-09-13 23:58:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:58:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=35, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:58:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-13 23:58:40 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=69, loss_sum=45.654659, avg_loss=0.661662, seen=69, correct=44, accuracy=0.637681
2025-09-13 23:58:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:58:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:58:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:58:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-13 23:58:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:58:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:58:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:58:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:58:43 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.490484, avg_loss=0.712262, seen=40, correct=21, accuracy=0.525000
2025-09-13 23:58:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:58:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:58:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:58:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-13 23:58:53 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-13 23:58:54 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-13 23:58:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-09-13 23:58:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:58:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=35, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:58:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-13 23:58:56 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=69, loss_sum=46.578667, avg_loss=0.675053, seen=69, correct=38, accuracy=0.550725
2025-09-13 23:58:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:58:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:58:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:58:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-13 23:58:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:58:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:58:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:58:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:58:59 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.825005, avg_loss=0.745625, seen=40, correct=21, accuracy=0.525000
2025-09-13 23:58:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:58:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:59:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:59:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-13 23:59:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-13 23:59:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-13 23:59:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:59:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:59:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-13 23:59:01 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #8', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-13 23:59:01 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #8', 'Round': 0, 'Results_raw': {}}
2025-09-13 23:59:01 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-13 23:59:01 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-13 23:59:02 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-13 23:59:02 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-13 23:59:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 23:59:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:59:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:59:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 23:59:08 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=128.530457, avg_loss=0.642652, seen=200, correct=119, accuracy=0.595000
2025-09-13 23:59:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:59:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:59:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:59:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2200MB
2025-09-13 23:59:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:59:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:59:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:59:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:59:11 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.782816, avg_loss=0.694570, seen=40, correct=24, accuracy=0.600000
2025-09-13 23:59:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:59:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:59:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:59:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2200MB
2025-09-13 23:59:12 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-13 23:59:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3638, total=14550)
2025-09-13 23:59:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:59:12 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-13 23:59:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:59:12 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=1819, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-13 23:59:21 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-13 23:59:21 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-13 23:59:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 23:59:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:59:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:59:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 23:59:29 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=129.295868, avg_loss=0.646479, seen=200, correct=125, accuracy=0.625000
2025-09-13 23:59:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:59:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:59:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:59:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:59:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:59:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:59:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:59:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:59:33 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.639530, avg_loss=0.715988, seen=40, correct=24, accuracy=0.600000
2025-09-13 23:59:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:59:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:59:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:59:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:59:44 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-13 23:59:45 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-13 23:59:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-13 23:59:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:59:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:59:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-13 23:59:54 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=129.933151, avg_loss=0.649666, seen=200, correct=124, accuracy=0.620000
2025-09-13 23:59:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:59:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:59:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-13 23:59:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-13 23:59:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-13 23:59:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:59:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-13 23:59:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-13 23:59:58 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.606962, avg_loss=0.690174, seen=40, correct=25, accuracy=0.625000
2025-09-13 23:59:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-13 23:59:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-13 23:59:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:00:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-14 00:00:10 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 00:00:10 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 00:00:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 00:00:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:00:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:00:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 00:00:18 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=133.255508, avg_loss=0.666278, seen=200, correct=120, accuracy=0.600000
2025-09-14 00:00:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:00:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:00:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:00:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-14 00:00:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:00:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:00:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:00:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:00:23 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.700750, avg_loss=0.692519, seen=40, correct=24, accuracy=0.600000
2025-09-14 00:00:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:00:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:00:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:00:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-14 00:00:32 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 00:00:33 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 00:00:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 00:00:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:00:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:00:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 00:00:41 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=136.968796, avg_loss=0.684844, seen=200, correct=114, accuracy=0.570000
2025-09-14 00:00:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:00:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:00:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:00:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-14 00:00:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:00:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:00:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:00:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:00:45 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.511389, avg_loss=0.687785, seen=40, correct=23, accuracy=0.575000
2025-09-14 00:00:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:00:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:00:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:00:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-14 00:00:55 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 00:00:56 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 00:00:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 00:00:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:00:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:01:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 00:01:04 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=133.749115, avg_loss=0.668746, seen=200, correct=120, accuracy=0.600000
2025-09-14 00:01:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:01:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:01:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:01:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-14 00:01:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:01:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:01:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:01:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:01:08 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.727371, avg_loss=0.668184, seen=40, correct=24, accuracy=0.600000
2025-09-14 00:01:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:01:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:01:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:01:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-14 00:01:19 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 00:01:19 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 00:01:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 00:01:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:01:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:01:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 00:01:27 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=131.879913, avg_loss=0.659400, seen=200, correct=117, accuracy=0.585000
2025-09-14 00:01:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:01:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:01:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:01:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-14 00:01:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:01:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:01:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:01:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:01:30 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.092354, avg_loss=0.677309, seen=40, correct=24, accuracy=0.600000
2025-09-14 00:01:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:01:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:01:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:01:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-14 00:01:40 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 00:01:41 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 00:01:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 00:01:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:01:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:01:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 00:01:49 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=130.833710, avg_loss=0.654169, seen=200, correct=122, accuracy=0.610000
2025-09-14 00:01:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:01:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:01:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:01:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-14 00:01:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:01:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:01:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:01:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:01:53 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.201750, avg_loss=0.705044, seen=40, correct=24, accuracy=0.600000
2025-09-14 00:01:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:01:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:01:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:01:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-14 00:02:01 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 00:02:02 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 00:02:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 00:02:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:02:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:02:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 00:02:10 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=131.681168, avg_loss=0.658406, seen=200, correct=125, accuracy=0.625000
2025-09-14 00:02:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:02:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:02:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:02:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-14 00:02:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:02:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:02:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:02:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:02:14 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.617329, avg_loss=0.715433, seen=40, correct=23, accuracy=0.575000
2025-09-14 00:02:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:02:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:02:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:02:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-14 00:02:24 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 00:02:26 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 00:02:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 00:02:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:02:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:02:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 00:02:33 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=130.170578, avg_loss=0.650853, seen=200, correct=125, accuracy=0.625000
2025-09-14 00:02:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:02:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:02:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:02:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-14 00:02:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:02:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:02:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:02:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:02:37 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.378153, avg_loss=0.684454, seen=40, correct=24, accuracy=0.600000
2025-09-14 00:02:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:02:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:02:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:02:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-14 00:02:47 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 00:02:48 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 00:02:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 00:02:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:02:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:02:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 00:02:56 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=128.213455, avg_loss=0.641067, seen=200, correct=125, accuracy=0.625000
2025-09-14 00:02:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:02:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:02:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:02:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-14 00:02:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:02:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:02:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:03:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:03:00 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.759554, avg_loss=0.668989, seen=40, correct=25, accuracy=0.625000
2025-09-14 00:03:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:03:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:03:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:03:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-14 00:03:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 00:03:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 00:03:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:03:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:03:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-14 00:03:02 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #15', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 00:03:02 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #15', 'Round': 0, 'Results_raw': {}}
2025-09-14 00:03:02 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 00:03:02 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-14 00:03:02 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 00:03:02 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 00:03:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 00:03:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:03:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:03:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 00:03:08 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=124.718170, avg_loss=0.623591, seen=200, correct=128, accuracy=0.640000
2025-09-14 00:03:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:03:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:03:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:03:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2200MB
2025-09-14 00:03:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:03:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:03:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:03:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:03:11 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.481073, avg_loss=0.637027, seen=40, correct=27, accuracy=0.675000
2025-09-14 00:03:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:03:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:03:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:03:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2200MB
2025-09-14 00:03:12 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 00:03:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1184, total=4736)
2025-09-14 00:03:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:03:12 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 00:03:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:03:12 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=592, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 00:03:22 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 00:03:22 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 00:03:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 00:03:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:03:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:03:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 00:03:30 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=126.961456, avg_loss=0.634807, seen=200, correct=128, accuracy=0.640000
2025-09-14 00:03:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:03:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:03:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:03:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2380MB allocated=2217MB
2025-09-14 00:03:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:03:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:03:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:03:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:03:33 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.508053, avg_loss=0.637701, seen=40, correct=27, accuracy=0.675000
2025-09-14 00:03:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:03:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:03:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:03:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2380MB allocated=2217MB
2025-09-14 00:03:43 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 00:03:45 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 00:03:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 00:03:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:03:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:03:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 00:03:52 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=128.432831, avg_loss=0.642164, seen=200, correct=127, accuracy=0.635000
2025-09-14 00:03:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:03:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:03:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:03:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2380MB allocated=2217MB
2025-09-14 00:03:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:03:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:03:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:03:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:03:56 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.169903, avg_loss=0.654248, seen=40, correct=25, accuracy=0.625000
2025-09-14 00:03:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:03:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:03:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:03:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2380MB allocated=2217MB
2025-09-14 00:04:07 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 00:04:08 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 00:04:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 00:04:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:04:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:04:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 00:04:15 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=127.496117, avg_loss=0.637481, seen=200, correct=132, accuracy=0.660000
2025-09-14 00:04:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:04:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:04:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:04:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2380MB allocated=2217MB
2025-09-14 00:04:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:04:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:04:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:04:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:04:20 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.864277, avg_loss=0.646607, seen=40, correct=26, accuracy=0.650000
2025-09-14 00:04:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:04:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:04:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:04:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2380MB allocated=2217MB
2025-09-14 00:04:31 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 00:04:31 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 00:04:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 00:04:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:04:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:04:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 00:04:39 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=125.724670, avg_loss=0.628623, seen=200, correct=129, accuracy=0.645000
2025-09-14 00:04:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:04:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:04:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:04:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2380MB allocated=2217MB
2025-09-14 00:04:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:04:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:04:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:04:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:04:42 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.228922, avg_loss=0.655723, seen=40, correct=23, accuracy=0.575000
2025-09-14 00:04:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:04:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:04:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:04:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2380MB allocated=2217MB
2025-09-14 00:04:54 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 00:04:54 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 00:04:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 00:04:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:04:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:05:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 00:05:01 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=124.742012, avg_loss=0.623710, seen=200, correct=134, accuracy=0.670000
2025-09-14 00:05:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:05:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:05:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:05:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2380MB allocated=2217MB
2025-09-14 00:05:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:05:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:05:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:05:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:05:05 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.929287, avg_loss=0.648232, seen=40, correct=25, accuracy=0.625000
2025-09-14 00:05:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:05:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:05:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:05:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2380MB allocated=2217MB
2025-09-14 00:05:14 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 00:05:15 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 00:05:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 00:05:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:05:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:05:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 00:05:23 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=127.078033, avg_loss=0.635390, seen=200, correct=123, accuracy=0.615000
2025-09-14 00:05:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:05:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:05:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:05:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2380MB allocated=2217MB
2025-09-14 00:05:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:05:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:05:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:05:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:05:27 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.227394, avg_loss=0.630685, seen=40, correct=25, accuracy=0.625000
2025-09-14 00:05:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:05:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:05:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:05:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2380MB allocated=2217MB
2025-09-14 00:05:37 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 00:05:38 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 00:05:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 00:05:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:05:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:05:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 00:05:45 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=128.291443, avg_loss=0.641457, seen=200, correct=121, accuracy=0.605000
2025-09-14 00:05:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:05:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:05:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:05:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2380MB allocated=2217MB
2025-09-14 00:05:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:05:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:05:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:05:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:05:49 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.225660, avg_loss=0.655642, seen=40, correct=26, accuracy=0.650000
2025-09-14 00:05:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:05:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:05:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:05:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2380MB allocated=2217MB
2025-09-14 00:06:00 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 00:06:01 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 00:06:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 00:06:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:06:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:06:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 00:06:08 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=127.568039, avg_loss=0.637840, seen=200, correct=128, accuracy=0.640000
2025-09-14 00:06:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:06:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:06:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:06:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2380MB allocated=2217MB
2025-09-14 00:06:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:06:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:06:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:06:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:06:10 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.646526, avg_loss=0.666163, seen=40, correct=25, accuracy=0.625000
2025-09-14 00:06:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:06:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:06:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:06:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2380MB allocated=2217MB
2025-09-14 00:06:19 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 00:06:20 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 00:06:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 00:06:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:06:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:06:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 00:06:28 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=128.991013, avg_loss=0.644955, seen=200, correct=119, accuracy=0.595000
2025-09-14 00:06:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:06:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:06:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:06:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2380MB allocated=2217MB
2025-09-14 00:06:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:06:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:06:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:06:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:06:32 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.766630, avg_loss=0.669166, seen=40, correct=25, accuracy=0.625000
2025-09-14 00:06:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:06:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:06:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:06:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2380MB allocated=2217MB
2025-09-14 00:06:43 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 00:06:44 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 00:06:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 00:06:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:06:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:06:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 00:06:51 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=129.044144, avg_loss=0.645221, seen=200, correct=123, accuracy=0.615000
2025-09-14 00:06:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:06:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:06:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:06:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2380MB allocated=2217MB
2025-09-14 00:06:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:06:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:06:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:06:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:06:54 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.872673, avg_loss=0.671817, seen=40, correct=26, accuracy=0.650000
2025-09-14 00:06:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:06:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:06:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:06:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2380MB allocated=2217MB
2025-09-14 00:06:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 00:06:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 00:06:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:06:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:06:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2380MB allocated=2217MB
2025-09-14 00:06:56 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #35', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 00:06:56 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #35', 'Round': 0, 'Results_raw': {}}
2025-09-14 00:06:57 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 00:06:57 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-14 00:06:57 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 00:06:57 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 00:06:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-09-14 00:06:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:06:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=66, num_train_batch_last_epoch=34, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:07:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-09-14 00:07:01 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=132, loss_sum=85.622726, avg_loss=0.648657, seen=132, correct=88, accuracy=0.666667
2025-09-14 00:07:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:07:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:07:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:07:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2200MB
2025-09-14 00:07:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:07:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:07:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:07:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:07:03 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=32.015907, avg_loss=0.800398, seen=40, correct=18, accuracy=0.450000
2025-09-14 00:07:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:07:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:07:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:07:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2200MB
2025-09-14 00:07:05 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 00:07:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=631, total=2521)
2025-09-14 00:07:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:07:05 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 00:07:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:07:05 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=316, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 00:07:15 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 00:07:15 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 00:07:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-09-14 00:07:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:07:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=66, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:07:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-09-14 00:07:20 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=132, loss_sum=83.099991, avg_loss=0.629545, seen=132, correct=91, accuracy=0.689394
2025-09-14 00:07:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:07:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:07:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:07:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2372MB allocated=2217MB
2025-09-14 00:07:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:07:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:07:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:07:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:07:22 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=31.564125, avg_loss=0.789103, seen=40, correct=17, accuracy=0.425000
2025-09-14 00:07:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:07:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:07:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:07:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2372MB allocated=2217MB
2025-09-14 00:07:32 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 00:07:34 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 00:07:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-09-14 00:07:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:07:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=66, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:07:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-09-14 00:07:38 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=132, loss_sum=84.597641, avg_loss=0.640891, seen=132, correct=87, accuracy=0.659091
2025-09-14 00:07:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:07:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:07:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:07:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2372MB allocated=2217MB
2025-09-14 00:07:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:07:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:07:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:07:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:07:42 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=31.318951, avg_loss=0.782974, seen=40, correct=20, accuracy=0.500000
2025-09-14 00:07:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:07:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:07:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:07:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2372MB allocated=2217MB
2025-09-14 00:07:52 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 00:07:53 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 00:07:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-09-14 00:07:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:07:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=66, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:07:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-09-14 00:07:58 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=132, loss_sum=88.260956, avg_loss=0.668644, seen=132, correct=83, accuracy=0.628788
2025-09-14 00:07:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:07:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:07:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:08:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2372MB allocated=2217MB
2025-09-14 00:08:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:08:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:08:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:08:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:08:02 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=31.540337, avg_loss=0.788508, seen=40, correct=20, accuracy=0.500000
2025-09-14 00:08:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:08:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:08:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:08:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2372MB allocated=2217MB
2025-09-14 00:08:13 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 00:08:14 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 00:08:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-09-14 00:08:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:08:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=66, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:08:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-09-14 00:08:20 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=132, loss_sum=87.631012, avg_loss=0.663871, seen=132, correct=82, accuracy=0.621212
2025-09-14 00:08:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:08:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:08:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:08:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2372MB allocated=2217MB
2025-09-14 00:08:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:08:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:08:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:08:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:08:22 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.959921, avg_loss=0.773998, seen=40, correct=20, accuracy=0.500000
2025-09-14 00:08:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:08:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:08:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:08:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2372MB allocated=2217MB
2025-09-14 00:08:33 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 00:08:34 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 00:08:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-09-14 00:08:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:08:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=66, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:08:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-09-14 00:08:40 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=132, loss_sum=86.061249, avg_loss=0.651979, seen=132, correct=82, accuracy=0.621212
2025-09-14 00:08:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:08:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:08:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:08:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2372MB allocated=2217MB
2025-09-14 00:08:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:08:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:08:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:08:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:08:44 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.685841, avg_loss=0.767146, seen=40, correct=20, accuracy=0.500000
2025-09-14 00:08:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:08:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:08:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:08:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2372MB allocated=2217MB
2025-09-14 00:08:56 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 00:08:56 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 00:08:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-09-14 00:08:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:08:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=66, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:09:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-09-14 00:09:01 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=132, loss_sum=84.409370, avg_loss=0.639465, seen=132, correct=84, accuracy=0.636364
2025-09-14 00:09:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:09:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:09:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:09:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2372MB allocated=2217MB
2025-09-14 00:09:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:09:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:09:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:09:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:09:03 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.532873, avg_loss=0.763322, seen=40, correct=19, accuracy=0.475000
2025-09-14 00:09:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:09:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:09:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:09:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2372MB allocated=2217MB
2025-09-14 00:09:12 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 00:09:13 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 00:09:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-09-14 00:09:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:09:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=66, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:09:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-09-14 00:09:18 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=132, loss_sum=86.816124, avg_loss=0.657698, seen=132, correct=81, accuracy=0.613636
2025-09-14 00:09:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:09:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:09:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:09:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2372MB allocated=2217MB
2025-09-14 00:09:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:09:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:09:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:09:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:09:22 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.347137, avg_loss=0.758678, seen=40, correct=20, accuracy=0.500000
2025-09-14 00:09:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:09:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:09:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:09:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2372MB allocated=2217MB
2025-09-14 00:09:33 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 00:09:33 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 00:09:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-09-14 00:09:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:09:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=66, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:09:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-09-14 00:09:39 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=132, loss_sum=88.518417, avg_loss=0.670594, seen=132, correct=76, accuracy=0.575758
2025-09-14 00:09:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:09:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:09:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:09:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2372MB allocated=2217MB
2025-09-14 00:09:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:09:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:09:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:09:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:09:42 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.358622, avg_loss=0.758966, seen=40, correct=21, accuracy=0.525000
2025-09-14 00:09:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:09:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:09:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:09:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2372MB allocated=2217MB
2025-09-14 00:09:49 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 00:09:51 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 00:09:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-09-14 00:09:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:09:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=66, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:09:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-09-14 00:09:57 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=132, loss_sum=87.226418, avg_loss=0.660806, seen=132, correct=79, accuracy=0.598485
2025-09-14 00:09:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:09:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:09:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:09:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2372MB allocated=2217MB
2025-09-14 00:09:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:09:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:09:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:10:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:10:00 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.911428, avg_loss=0.747786, seen=40, correct=21, accuracy=0.525000
2025-09-14 00:10:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:10:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:10:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:10:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2372MB allocated=2217MB
2025-09-14 00:10:11 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 00:10:11 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 00:10:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-09-14 00:10:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:10:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=66, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:10:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-09-14 00:10:16 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=132, loss_sum=87.613701, avg_loss=0.663740, seen=132, correct=78, accuracy=0.590909
2025-09-14 00:10:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:10:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:10:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:10:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2374MB allocated=2217MB
2025-09-14 00:10:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:10:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:10:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:10:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:10:19 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.007458, avg_loss=0.750186, seen=40, correct=21, accuracy=0.525000
2025-09-14 00:10:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:10:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:10:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:10:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2372MB allocated=2217MB
2025-09-14 00:10:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 00:10:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 00:10:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:10:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:10:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2372MB allocated=2217MB
2025-09-14 00:10:21 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #49', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 00:10:21 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #49', 'Round': 0, 'Results_raw': {}}
2025-09-14 00:10:21 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 00:10:21 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-14 00:10:22 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 00:10:22 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 00:10:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-14 00:10:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:10:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=45, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:10:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 00:10:25 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=73.416138, avg_loss=0.667419, seen=110, correct=60, accuracy=0.545455
2025-09-14 00:10:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:10:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:10:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:10:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2200MB
2025-09-14 00:10:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:10:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:10:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:10:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:10:27 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=31.344749, avg_loss=0.783619, seen=40, correct=17, accuracy=0.425000
2025-09-14 00:10:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:10:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:10:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:10:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2200MB
2025-09-14 00:10:28 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 00:10:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=526, total=2102)
2025-09-14 00:10:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:10:28 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 00:10:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:10:28 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=263, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 00:10:36 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 00:10:38 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 00:10:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-14 00:10:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:10:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:10:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 00:10:42 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=72.034760, avg_loss=0.654861, seen=110, correct=73, accuracy=0.663636
2025-09-14 00:10:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:10:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:10:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:10:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2398MB allocated=2217MB
2025-09-14 00:10:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:10:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:10:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:10:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:10:45 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.426451, avg_loss=0.760661, seen=40, correct=18, accuracy=0.450000
2025-09-14 00:10:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:10:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:10:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:10:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2398MB allocated=2217MB
2025-09-14 00:10:56 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 00:10:57 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 00:10:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-14 00:10:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:10:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:11:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 00:11:00 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=72.163170, avg_loss=0.656029, seen=110, correct=75, accuracy=0.681818
2025-09-14 00:11:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:11:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:11:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:11:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2398MB allocated=2217MB
2025-09-14 00:11:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:11:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:11:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:11:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:11:04 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.647890, avg_loss=0.766197, seen=40, correct=18, accuracy=0.450000
2025-09-14 00:11:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:11:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:11:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:11:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2398MB allocated=2217MB
2025-09-14 00:11:16 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 00:11:16 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 00:11:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-14 00:11:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:11:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:11:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 00:11:19 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=74.408272, avg_loss=0.676439, seen=110, correct=65, accuracy=0.590909
2025-09-14 00:11:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:11:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:11:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:11:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2398MB allocated=2217MB
2025-09-14 00:11:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:11:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:11:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:11:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:11:23 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.623993, avg_loss=0.740600, seen=40, correct=22, accuracy=0.550000
2025-09-14 00:11:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:11:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:11:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:11:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2398MB allocated=2217MB
2025-09-14 00:11:32 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 00:11:32 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 00:11:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-14 00:11:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:11:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:11:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 00:11:36 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=73.448334, avg_loss=0.667712, seen=110, correct=67, accuracy=0.609091
2025-09-14 00:11:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:11:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:11:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:11:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2398MB allocated=2217MB
2025-09-14 00:11:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:11:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:11:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:11:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:11:40 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.975689, avg_loss=0.724392, seen=40, correct=21, accuracy=0.525000
2025-09-14 00:11:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:11:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:11:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:11:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2398MB allocated=2217MB
2025-09-14 00:11:50 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 00:11:51 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 00:11:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-14 00:11:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:11:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:11:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 00:11:55 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=71.156616, avg_loss=0.646878, seen=110, correct=72, accuracy=0.654545
2025-09-14 00:11:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:11:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:11:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:11:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2398MB allocated=2217MB
2025-09-14 00:11:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:11:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:11:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:11:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:11:59 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.632675, avg_loss=0.715817, seen=40, correct=21, accuracy=0.525000
2025-09-14 00:11:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:11:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:11:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:12:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2398MB allocated=2217MB
2025-09-14 00:12:08 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 00:12:08 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 00:12:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-14 00:12:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:12:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:12:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 00:12:12 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=71.379120, avg_loss=0.648901, seen=110, correct=71, accuracy=0.645455
2025-09-14 00:12:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:12:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:12:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:12:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2398MB allocated=2217MB
2025-09-14 00:12:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:12:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:12:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:12:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:12:15 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.772875, avg_loss=0.719322, seen=40, correct=22, accuracy=0.550000
2025-09-14 00:12:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:12:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:12:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:12:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2398MB allocated=2217MB
2025-09-14 00:12:27 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 00:12:27 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 00:12:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-14 00:12:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:12:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:12:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 00:12:31 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=73.486305, avg_loss=0.668057, seen=110, correct=65, accuracy=0.590909
2025-09-14 00:12:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:12:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:12:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:12:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2398MB allocated=2217MB
2025-09-14 00:12:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:12:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:12:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:12:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:12:35 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.923574, avg_loss=0.723089, seen=40, correct=22, accuracy=0.550000
2025-09-14 00:12:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:12:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:12:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:12:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2398MB allocated=2217MB
2025-09-14 00:12:46 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 00:12:46 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 00:12:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-14 00:12:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:12:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:12:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 00:12:51 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=72.510628, avg_loss=0.659188, seen=110, correct=69, accuracy=0.627273
2025-09-14 00:12:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:12:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:12:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:12:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2398MB allocated=2217MB
2025-09-14 00:12:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:12:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:12:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:12:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:12:54 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.353249, avg_loss=0.733831, seen=40, correct=21, accuracy=0.525000
2025-09-14 00:12:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:12:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:12:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:12:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2398MB allocated=2217MB
2025-09-14 00:13:04 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 00:13:05 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 00:13:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-14 00:13:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:13:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:13:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 00:13:09 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=71.809120, avg_loss=0.652810, seen=110, correct=72, accuracy=0.654545
2025-09-14 00:13:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:13:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:13:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:13:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2398MB allocated=2217MB
2025-09-14 00:13:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:13:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:13:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:13:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:13:12 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.023472, avg_loss=0.725587, seen=40, correct=22, accuracy=0.550000
2025-09-14 00:13:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:13:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:13:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:13:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2398MB allocated=2217MB
2025-09-14 00:13:23 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 00:13:23 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 00:13:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-14 00:13:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:13:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:13:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 00:13:28 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=70.424820, avg_loss=0.640226, seen=110, correct=76, accuracy=0.690909
2025-09-14 00:13:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:13:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:13:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:13:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2398MB allocated=2217MB
2025-09-14 00:13:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:13:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:13:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:13:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:13:32 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.013411, avg_loss=0.750335, seen=40, correct=22, accuracy=0.550000
2025-09-14 00:13:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:13:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:13:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:13:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2398MB allocated=2217MB
2025-09-14 00:13:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 00:13:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 00:13:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:13:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:13:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2398MB allocated=2217MB
2025-09-14 00:13:34 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #19', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 00:13:34 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #19', 'Round': 0, 'Results_raw': {}}
2025-09-14 00:13:34 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 00:13:34 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-14 00:13:35 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 00:13:35 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 00:13:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-14 00:13:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:13:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=16, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:13:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-14 00:13:37 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=58.059090, avg_loss=0.699507, seen=83, correct=48, accuracy=0.578313
2025-09-14 00:13:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:13:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:13:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:13:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2200MB
2025-09-14 00:13:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:13:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:13:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:13:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:13:40 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.507074, avg_loss=0.712677, seen=40, correct=23, accuracy=0.575000
2025-09-14 00:13:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:13:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:13:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:13:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2200MB
2025-09-14 00:13:42 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 00:13:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=395, total=1580)
2025-09-14 00:13:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:13:42 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 00:13:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:13:42 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=198, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 00:13:51 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 00:13:53 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 00:13:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-14 00:13:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:13:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:13:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-14 00:13:56 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=59.096920, avg_loss=0.712011, seen=83, correct=49, accuracy=0.590361
2025-09-14 00:13:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:13:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:13:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:13:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2376MB allocated=2217MB
2025-09-14 00:13:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:13:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:13:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:14:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:14:00 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.961733, avg_loss=0.724043, seen=40, correct=23, accuracy=0.575000
2025-09-14 00:14:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:14:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:14:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:14:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2376MB allocated=2217MB
2025-09-14 00:14:11 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 00:14:12 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 00:14:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-14 00:14:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:14:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:14:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-14 00:14:15 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=57.010811, avg_loss=0.686877, seen=83, correct=55, accuracy=0.662651
2025-09-14 00:14:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:14:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:14:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:14:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2376MB allocated=2217MB
2025-09-14 00:14:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:14:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:14:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:14:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:14:18 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.149063, avg_loss=0.678727, seen=40, correct=26, accuracy=0.650000
2025-09-14 00:14:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:14:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:14:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:14:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2376MB allocated=2217MB
2025-09-14 00:14:28 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 00:14:28 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 00:14:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-14 00:14:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:14:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:14:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-14 00:14:30 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=56.205811, avg_loss=0.677178, seen=83, correct=52, accuracy=0.626506
2025-09-14 00:14:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:14:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:14:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:14:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2376MB allocated=2217MB
2025-09-14 00:14:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:14:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:14:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:14:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:14:34 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.963217, avg_loss=0.649080, seen=40, correct=28, accuracy=0.700000
2025-09-14 00:14:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:14:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:14:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:14:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2376MB allocated=2217MB
2025-09-14 00:14:43 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 00:14:45 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 00:14:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-14 00:14:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:14:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:14:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-14 00:14:48 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=56.636303, avg_loss=0.682365, seen=83, correct=55, accuracy=0.662651
2025-09-14 00:14:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:14:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:14:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:14:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2376MB allocated=2217MB
2025-09-14 00:14:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:14:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:14:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:14:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:14:52 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.119267, avg_loss=0.652982, seen=40, correct=27, accuracy=0.675000
2025-09-14 00:14:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:14:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:14:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:14:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2376MB allocated=2217MB
2025-09-14 00:15:03 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 00:15:03 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 00:15:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-14 00:15:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:15:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:15:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-14 00:15:07 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=56.688148, avg_loss=0.682990, seen=83, correct=55, accuracy=0.662651
2025-09-14 00:15:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:15:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:15:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:15:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2376MB allocated=2217MB
2025-09-14 00:15:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:15:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:15:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:15:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:15:10 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.598381, avg_loss=0.639960, seen=40, correct=29, accuracy=0.725000
2025-09-14 00:15:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:15:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:15:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:15:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2376MB allocated=2217MB
2025-09-14 00:15:20 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 00:15:22 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 00:15:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-14 00:15:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:15:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:15:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-14 00:15:25 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=56.085915, avg_loss=0.675734, seen=83, correct=48, accuracy=0.578313
2025-09-14 00:15:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:15:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:15:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:15:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2376MB allocated=2217MB
2025-09-14 00:15:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:15:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:15:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:15:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:15:29 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.643017, avg_loss=0.641075, seen=40, correct=26, accuracy=0.650000
2025-09-14 00:15:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:15:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:15:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:15:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2376MB allocated=2217MB
2025-09-14 00:15:38 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 00:15:39 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 00:15:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-14 00:15:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:15:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:15:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-14 00:15:41 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=56.889126, avg_loss=0.685411, seen=83, correct=48, accuracy=0.578313
2025-09-14 00:15:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:15:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:15:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:15:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2376MB allocated=2217MB
2025-09-14 00:15:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:15:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:15:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:15:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:15:45 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.418335, avg_loss=0.660458, seen=40, correct=24, accuracy=0.600000
2025-09-14 00:15:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:15:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:15:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:15:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2376MB allocated=2217MB
2025-09-14 00:15:56 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 00:15:57 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 00:15:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-14 00:15:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:15:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:16:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-14 00:16:00 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=55.831593, avg_loss=0.672670, seen=83, correct=48, accuracy=0.578313
2025-09-14 00:16:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:16:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:16:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:16:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2376MB allocated=2217MB
2025-09-14 00:16:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:16:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:16:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:16:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:16:04 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.901381, avg_loss=0.647535, seen=40, correct=26, accuracy=0.650000
2025-09-14 00:16:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:16:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:16:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:16:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2376MB allocated=2217MB
2025-09-14 00:16:15 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 00:16:15 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 00:16:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-14 00:16:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:16:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:16:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-14 00:16:18 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=56.735153, avg_loss=0.683556, seen=83, correct=52, accuracy=0.626506
2025-09-14 00:16:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:16:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:16:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:16:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2376MB allocated=2217MB
2025-09-14 00:16:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:16:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:16:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:16:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:16:22 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.457811, avg_loss=0.661445, seen=40, correct=27, accuracy=0.675000
2025-09-14 00:16:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:16:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:16:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:16:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2376MB allocated=2217MB
2025-09-14 00:16:34 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 00:16:34 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 00:16:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-14 00:16:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:16:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:16:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-14 00:16:37 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=57.260174, avg_loss=0.689882, seen=83, correct=50, accuracy=0.602410
2025-09-14 00:16:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:16:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:16:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:16:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2376MB allocated=2217MB
2025-09-14 00:16:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:16:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:16:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:16:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:16:41 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.085232, avg_loss=0.652131, seen=40, correct=28, accuracy=0.700000
2025-09-14 00:16:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:16:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:16:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:16:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2376MB allocated=2217MB
2025-09-14 00:16:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 00:16:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 00:16:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:16:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:16:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2376MB allocated=2217MB
2025-09-14 00:16:44 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #51', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 00:16:44 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #51', 'Round': 0, 'Results_raw': {}}
2025-09-14 00:16:44 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 00:16:44 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-14 00:16:45 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 00:16:45 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 00:16:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-09-14 00:16:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:16:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=27, num_train_batch_last_epoch=19, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:16:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-09-14 00:16:46 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=54, loss_sum=37.449348, avg_loss=0.693506, seen=54, correct=28, accuracy=0.518519
2025-09-14 00:16:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:16:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:16:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:16:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2200MB
2025-09-14 00:16:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:16:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:16:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:16:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:16:48 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.716837, avg_loss=0.642921, seen=40, correct=26, accuracy=0.650000
2025-09-14 00:16:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:16:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:16:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:16:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2200MB
2025-09-14 00:16:49 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 00:16:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=258, total=1030)
2025-09-14 00:16:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:16:49 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 00:16:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:16:49 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=129, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 00:16:58 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 00:16:59 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 00:16:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-09-14 00:16:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:16:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=27, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:17:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-09-14 00:17:01 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=54, loss_sum=37.771362, avg_loss=0.699470, seen=54, correct=29, accuracy=0.537037
2025-09-14 00:17:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:17:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:17:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:17:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-14 00:17:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:17:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:17:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:17:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:17:05 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.943569, avg_loss=0.623589, seen=40, correct=23, accuracy=0.575000
2025-09-14 00:17:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:17:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:17:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:17:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-14 00:17:15 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 00:17:16 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 00:17:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-09-14 00:17:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:17:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=27, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:17:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-09-14 00:17:19 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=54, loss_sum=37.418930, avg_loss=0.692943, seen=54, correct=28, accuracy=0.518519
2025-09-14 00:17:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:17:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:17:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:17:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-14 00:17:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:17:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:17:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:17:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:17:23 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.129475, avg_loss=0.628237, seen=40, correct=22, accuracy=0.550000
2025-09-14 00:17:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:17:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:17:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:17:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-14 00:17:33 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 00:17:35 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 00:17:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-09-14 00:17:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:17:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=27, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:17:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-09-14 00:17:37 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=54, loss_sum=36.501102, avg_loss=0.675946, seen=54, correct=29, accuracy=0.537037
2025-09-14 00:17:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:17:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:17:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:17:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-14 00:17:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:17:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:17:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:17:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:17:40 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.928095, avg_loss=0.623202, seen=40, correct=26, accuracy=0.650000
2025-09-14 00:17:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:17:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:17:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:17:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-14 00:17:50 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 00:17:51 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 00:17:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-09-14 00:17:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:17:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=27, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:17:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-09-14 00:17:54 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=54, loss_sum=36.605797, avg_loss=0.677885, seen=54, correct=30, accuracy=0.555556
2025-09-14 00:17:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:17:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:17:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:17:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-14 00:17:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:17:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:17:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:17:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:17:56 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.697172, avg_loss=0.617429, seen=40, correct=26, accuracy=0.650000
2025-09-14 00:17:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:17:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:17:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:17:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-14 00:18:06 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 00:18:08 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 00:18:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-09-14 00:18:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:18:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=27, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:18:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-09-14 00:18:11 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=54, loss_sum=36.476562, avg_loss=0.675492, seen=54, correct=28, accuracy=0.518519
2025-09-14 00:18:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:18:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:18:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:18:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-14 00:18:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:18:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:18:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:18:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:18:15 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.651052, avg_loss=0.616276, seen=40, correct=24, accuracy=0.600000
2025-09-14 00:18:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:18:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:18:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:18:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-14 00:18:26 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 00:18:27 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 00:18:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-09-14 00:18:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:18:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=27, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:18:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-09-14 00:18:29 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=54, loss_sum=36.079159, avg_loss=0.668133, seen=54, correct=29, accuracy=0.537037
2025-09-14 00:18:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:18:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:18:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:18:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-14 00:18:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:18:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:18:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:18:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:18:32 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.752489, avg_loss=0.593812, seen=40, correct=27, accuracy=0.675000
2025-09-14 00:18:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:18:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:18:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:18:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-14 00:18:42 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 00:18:42 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 00:18:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-09-14 00:18:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:18:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=27, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:18:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-09-14 00:18:45 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=54, loss_sum=35.858688, avg_loss=0.664050, seen=54, correct=31, accuracy=0.574074
2025-09-14 00:18:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:18:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:18:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:18:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-14 00:18:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:18:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:18:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:18:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:18:49 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.343937, avg_loss=0.608598, seen=40, correct=24, accuracy=0.600000
2025-09-14 00:18:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:18:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:18:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:18:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-14 00:19:01 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 00:19:01 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 00:19:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-09-14 00:19:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:19:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=27, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:19:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-09-14 00:19:03 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=54, loss_sum=36.571014, avg_loss=0.677241, seen=54, correct=29, accuracy=0.537037
2025-09-14 00:19:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:19:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:19:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:19:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-14 00:19:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:19:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:19:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:19:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:19:06 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.295687, avg_loss=0.632392, seen=40, correct=23, accuracy=0.575000
2025-09-14 00:19:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:19:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:19:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:19:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-14 00:19:15 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 00:19:15 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 00:19:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-09-14 00:19:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:19:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=27, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:19:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-09-14 00:19:18 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=54, loss_sum=35.909607, avg_loss=0.664993, seen=54, correct=30, accuracy=0.555556
2025-09-14 00:19:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:19:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:19:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:19:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-14 00:19:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:19:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:19:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:19:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:19:22 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.225227, avg_loss=0.630631, seen=40, correct=21, accuracy=0.525000
2025-09-14 00:19:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:19:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:19:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:19:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-14 00:19:32 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 00:19:34 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 00:19:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-09-14 00:19:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:19:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=27, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:19:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-09-14 00:19:36 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=54, loss_sum=35.699486, avg_loss=0.661102, seen=54, correct=28, accuracy=0.518519
2025-09-14 00:19:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:19:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:19:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:19:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-14 00:19:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:19:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:19:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:19:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:19:39 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.580633, avg_loss=0.614516, seen=40, correct=25, accuracy=0.625000
2025-09-14 00:19:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:19:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:19:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:19:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-14 00:19:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 00:19:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 00:19:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:19:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:19:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-14 00:19:41 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #36', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 00:19:41 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #36', 'Round': 0, 'Results_raw': {}}
2025-09-14 00:19:41 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 00:19:41 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-14 00:19:42 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 00:19:42 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 00:19:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-09-14 00:19:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:19:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=32, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:19:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 00:19:46 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=136, loss_sum=87.945442, avg_loss=0.646658, seen=136, correct=91, accuracy=0.669118
2025-09-14 00:19:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:19:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:19:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:19:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2326MB allocated=2200MB
2025-09-14 00:19:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:19:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:19:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:19:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:19:49 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.559889, avg_loss=0.588997, seen=40, correct=26, accuracy=0.650000
2025-09-14 00:19:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:19:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:19:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:19:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2326MB allocated=2200MB
2025-09-14 00:19:50 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 00:19:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=648, total=2589)
2025-09-14 00:19:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:19:51 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 00:19:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:19:51 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=324, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 00:20:01 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 00:20:01 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 00:20:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-09-14 00:20:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:20:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:20:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 00:20:06 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=136, loss_sum=87.268600, avg_loss=0.641681, seen=136, correct=84, accuracy=0.617647
2025-09-14 00:20:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:20:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:20:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:20:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2372MB allocated=2217MB
2025-09-14 00:20:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:20:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:20:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:20:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:20:10 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.273426, avg_loss=0.606836, seen=40, correct=29, accuracy=0.725000
2025-09-14 00:20:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:20:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:20:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:20:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2372MB allocated=2217MB
2025-09-14 00:20:20 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 00:20:20 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 00:20:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-09-14 00:20:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:20:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:20:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 00:20:25 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=136, loss_sum=86.799973, avg_loss=0.638235, seen=136, correct=88, accuracy=0.647059
2025-09-14 00:20:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:20:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:20:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:20:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2372MB allocated=2217MB
2025-09-14 00:20:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:20:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:20:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:20:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:20:29 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.418442, avg_loss=0.610461, seen=40, correct=26, accuracy=0.650000
2025-09-14 00:20:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:20:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:20:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:20:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2372MB allocated=2217MB
2025-09-14 00:20:41 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 00:20:41 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 00:20:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-09-14 00:20:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:20:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:20:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 00:20:46 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=136, loss_sum=85.992233, avg_loss=0.632296, seen=136, correct=94, accuracy=0.691176
2025-09-14 00:20:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:20:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:20:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:20:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2372MB allocated=2217MB
2025-09-14 00:20:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:20:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:20:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:20:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:20:49 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.749481, avg_loss=0.593737, seen=40, correct=27, accuracy=0.675000
2025-09-14 00:20:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:20:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:20:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:20:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2372MB allocated=2217MB
2025-09-14 00:20:59 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 00:21:01 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 00:21:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-09-14 00:21:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:21:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:21:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 00:21:07 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=136, loss_sum=86.046089, avg_loss=0.632692, seen=136, correct=90, accuracy=0.661765
2025-09-14 00:21:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:21:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:21:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:21:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2372MB allocated=2217MB
2025-09-14 00:21:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:21:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:21:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:21:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:21:11 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.208754, avg_loss=0.605219, seen=40, correct=28, accuracy=0.700000
2025-09-14 00:21:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:21:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:21:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:21:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2372MB allocated=2217MB
2025-09-14 00:21:22 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 00:21:22 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 00:21:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-09-14 00:21:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:21:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:21:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 00:21:28 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=136, loss_sum=86.309395, avg_loss=0.634628, seen=136, correct=88, accuracy=0.647059
2025-09-14 00:21:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:21:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:21:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:21:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2372MB allocated=2217MB
2025-09-14 00:21:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:21:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:21:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:21:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:21:30 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.273726, avg_loss=0.606843, seen=40, correct=29, accuracy=0.725000
2025-09-14 00:21:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:21:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:21:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:21:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2372MB allocated=2217MB
2025-09-14 00:21:40 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 00:21:42 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 00:21:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-09-14 00:21:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:21:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:21:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 00:21:48 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=136, loss_sum=86.026764, avg_loss=0.632550, seen=136, correct=84, accuracy=0.617647
2025-09-14 00:21:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:21:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:21:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:21:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2372MB allocated=2217MB
2025-09-14 00:21:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:21:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:21:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:21:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:21:52 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=22.901142, avg_loss=0.572529, seen=40, correct=31, accuracy=0.775000
2025-09-14 00:21:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:21:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:21:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:21:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2372MB allocated=2217MB
2025-09-14 00:22:03 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 00:22:04 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 00:22:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-09-14 00:22:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:22:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:22:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 00:22:09 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=136, loss_sum=85.899529, avg_loss=0.631614, seen=136, correct=85, accuracy=0.625000
2025-09-14 00:22:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:22:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:22:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:22:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2372MB allocated=2217MB
2025-09-14 00:22:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:22:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:22:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:22:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:22:12 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.135723, avg_loss=0.578393, seen=40, correct=31, accuracy=0.775000
2025-09-14 00:22:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:22:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:22:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:22:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2372MB allocated=2217MB
2025-09-14 00:22:24 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 00:22:24 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 00:22:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-09-14 00:22:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:22:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:22:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 00:22:29 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=136, loss_sum=87.302437, avg_loss=0.641930, seen=136, correct=84, accuracy=0.617647
2025-09-14 00:22:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:22:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:22:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:22:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2372MB allocated=2217MB
2025-09-14 00:22:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:22:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:22:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:22:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:22:32 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.188303, avg_loss=0.579708, seen=40, correct=31, accuracy=0.775000
2025-09-14 00:22:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:22:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:22:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:22:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2372MB allocated=2217MB
2025-09-14 00:22:42 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 00:22:44 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 00:22:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-09-14 00:22:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:22:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:22:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 00:22:50 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=136, loss_sum=87.003029, avg_loss=0.639728, seen=136, correct=86, accuracy=0.632353
2025-09-14 00:22:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:22:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:22:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:22:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2372MB allocated=2217MB
2025-09-14 00:22:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:22:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:22:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:22:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:22:53 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.374084, avg_loss=0.584352, seen=40, correct=32, accuracy=0.800000
2025-09-14 00:22:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:22:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:22:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:22:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2372MB allocated=2217MB
2025-09-14 00:23:03 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 00:23:05 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 00:23:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-09-14 00:23:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:23:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:23:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 00:23:10 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=136, loss_sum=86.032181, avg_loss=0.632590, seen=136, correct=84, accuracy=0.617647
2025-09-14 00:23:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:23:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:23:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:23:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2372MB allocated=2217MB
2025-09-14 00:23:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:23:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:23:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:23:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:23:13 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.643856, avg_loss=0.591096, seen=40, correct=31, accuracy=0.775000
2025-09-14 00:23:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:23:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:23:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:23:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2372MB allocated=2217MB
2025-09-14 00:23:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 00:23:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 00:23:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:23:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:23:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2372MB allocated=2217MB
2025-09-14 00:23:15 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #16', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 00:23:15 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #16', 'Round': 0, 'Results_raw': {}}
2025-09-14 00:23:15 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 00:23:15 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 1 for training...
2025-09-14 00:23:16 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 00:23:16 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 00:23:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-09-14 00:23:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:23:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=33, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:23:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 00:23:20 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=134, loss_sum=86.744911, avg_loss=0.647350, seen=134, correct=83, accuracy=0.619403
2025-09-14 00:23:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:23:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:23:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:23:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2326MB allocated=2200MB
2025-09-14 00:23:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:23:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:23:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:23:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:23:22 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=32.375751, avg_loss=0.809394, seen=40, correct=21, accuracy=0.525000
2025-09-14 00:23:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:23:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:23:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:23:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2326MB allocated=2200MB
2025-09-14 00:23:23 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 00:23:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=637, total=2547)
2025-09-14 00:23:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:23:24 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 00:23:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:23:24 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=319, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 00:23:33 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 00:23:33 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 00:23:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-09-14 00:23:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:23:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:23:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 00:23:38 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=134, loss_sum=85.017693, avg_loss=0.634460, seen=134, correct=87, accuracy=0.649254
2025-09-14 00:23:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:23:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:23:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:23:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-14 00:23:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:23:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:23:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:23:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:23:42 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=31.679039, avg_loss=0.791976, seen=40, correct=20, accuracy=0.500000
2025-09-14 00:23:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:23:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:23:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:23:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-14 00:23:53 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 00:23:53 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 00:23:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-09-14 00:23:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:23:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:23:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 00:23:58 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=134, loss_sum=83.895187, avg_loss=0.626083, seen=134, correct=91, accuracy=0.679104
2025-09-14 00:23:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:23:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:23:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:24:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-14 00:24:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:24:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:24:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:24:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:24:01 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=31.064035, avg_loss=0.776601, seen=40, correct=21, accuracy=0.525000
2025-09-14 00:24:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:24:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:24:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:24:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-14 00:24:11 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 00:24:12 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 00:24:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-09-14 00:24:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:24:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:24:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 00:24:17 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=134, loss_sum=85.018013, avg_loss=0.634463, seen=134, correct=89, accuracy=0.664179
2025-09-14 00:24:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:24:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:24:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:24:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-14 00:24:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:24:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:24:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:24:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:24:21 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.920673, avg_loss=0.773017, seen=40, correct=20, accuracy=0.500000
2025-09-14 00:24:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:24:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:24:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:24:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-14 00:24:32 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 00:24:32 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 00:24:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-09-14 00:24:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:24:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:24:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 00:24:38 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=134, loss_sum=84.185379, avg_loss=0.628249, seen=134, correct=90, accuracy=0.671642
2025-09-14 00:24:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:24:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:24:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:24:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-14 00:24:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:24:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:24:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:24:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:24:42 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.945431, avg_loss=0.773636, seen=40, correct=20, accuracy=0.500000
2025-09-14 00:24:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:24:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:24:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:24:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-14 00:24:53 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 00:24:53 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 00:24:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-09-14 00:24:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:24:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:24:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 00:24:58 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=134, loss_sum=84.966980, avg_loss=0.634082, seen=134, correct=88, accuracy=0.656716
2025-09-14 00:24:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:24:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:25:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:25:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-14 00:25:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:25:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:25:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:25:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:25:01 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.636288, avg_loss=0.765907, seen=40, correct=21, accuracy=0.525000
2025-09-14 00:25:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:25:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:25:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:25:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-14 00:25:13 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 00:25:13 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 00:25:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-09-14 00:25:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:25:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:25:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 00:25:18 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=134, loss_sum=82.884644, avg_loss=0.618542, seen=134, correct=89, accuracy=0.664179
2025-09-14 00:25:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:25:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:25:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:25:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-14 00:25:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:25:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:25:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:25:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:25:21 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.694838, avg_loss=0.767371, seen=40, correct=17, accuracy=0.425000
2025-09-14 00:25:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:25:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:25:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:25:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-14 00:25:31 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 00:25:33 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 00:25:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-09-14 00:25:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:25:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:25:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 00:25:38 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=134, loss_sum=82.932526, avg_loss=0.618899, seen=134, correct=92, accuracy=0.686567
2025-09-14 00:25:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:25:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:25:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:25:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-14 00:25:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:25:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:25:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:25:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:25:40 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.744698, avg_loss=0.768617, seen=40, correct=22, accuracy=0.550000
2025-09-14 00:25:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:25:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:25:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:25:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-14 00:25:51 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 00:25:53 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 00:25:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-09-14 00:25:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:25:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:25:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 00:25:58 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=134, loss_sum=81.541794, avg_loss=0.608521, seen=134, correct=92, accuracy=0.686567
2025-09-14 00:25:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:25:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:25:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:25:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-14 00:25:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:25:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:26:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:26:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:26:01 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.780685, avg_loss=0.769517, seen=40, correct=18, accuracy=0.450000
2025-09-14 00:26:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:26:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:26:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:26:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-14 00:26:12 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 00:26:12 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 00:26:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-09-14 00:26:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:26:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:26:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 00:26:17 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=134, loss_sum=81.396790, avg_loss=0.607439, seen=134, correct=87, accuracy=0.649254
2025-09-14 00:26:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:26:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:26:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:26:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-14 00:26:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:26:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:26:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:26:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:26:20 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.702599, avg_loss=0.767565, seen=40, correct=15, accuracy=0.375000
2025-09-14 00:26:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:26:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:26:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:26:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-14 00:26:31 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 00:26:31 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 00:26:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-09-14 00:26:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:26:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:26:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 00:26:36 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=134, loss_sum=83.525139, avg_loss=0.623322, seen=134, correct=88, accuracy=0.656716
2025-09-14 00:26:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:26:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:26:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:26:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-14 00:26:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:26:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:26:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:26:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:26:41 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.017761, avg_loss=0.750444, seen=40, correct=19, accuracy=0.475000
2025-09-14 00:26:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:26:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:26:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:26:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-14 00:26:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 00:26:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 00:26:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:26:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:26:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-14 00:26:43 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #6', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 00:26:43 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #6', 'Round': 0, 'Results_raw': {}}
2025-09-14 00:26:43 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 00:26:43 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 1 for training...
2025-09-14 00:26:44 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 00:26:44 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 00:26:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 00:26:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:26:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:26:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 00:26:49 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=138.284454, avg_loss=0.691422, seen=200, correct=113, accuracy=0.565000
2025-09-14 00:26:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:26:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:26:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:26:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2326MB allocated=2200MB
2025-09-14 00:26:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:26:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:26:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:26:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:26:51 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=22.510845, avg_loss=0.562771, seen=40, correct=26, accuracy=0.650000
2025-09-14 00:26:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:26:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:26:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:26:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2326MB allocated=2200MB
2025-09-14 00:26:53 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 00:26:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1548, total=6191)
2025-09-14 00:26:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:26:53 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 00:26:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:26:53 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=774, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 00:27:02 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 00:27:02 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 00:27:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 00:27:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:27:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:27:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 00:27:10 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=137.154953, avg_loss=0.685775, seen=200, correct=111, accuracy=0.555000
2025-09-14 00:27:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:27:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:27:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:27:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2364MB allocated=2217MB
2025-09-14 00:27:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:27:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:27:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:27:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:27:14 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.879293, avg_loss=0.646982, seen=40, correct=27, accuracy=0.675000
2025-09-14 00:27:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:27:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:27:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:27:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2364MB allocated=2217MB
2025-09-14 00:27:26 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 00:27:26 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 00:27:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 00:27:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:27:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:27:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 00:27:35 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=135.531876, avg_loss=0.677659, seen=200, correct=114, accuracy=0.570000
2025-09-14 00:27:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:27:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:27:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:27:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2364MB allocated=2217MB
2025-09-14 00:27:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:27:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:27:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:27:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:27:39 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.271454, avg_loss=0.631786, seen=40, correct=26, accuracy=0.650000
2025-09-14 00:27:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:27:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:27:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:27:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2364MB allocated=2217MB
2025-09-14 00:27:51 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 00:27:51 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 00:27:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 00:27:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:27:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:27:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 00:27:59 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=134.981613, avg_loss=0.674908, seen=200, correct=115, accuracy=0.575000
2025-09-14 00:27:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:27:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:28:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:28:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2364MB allocated=2217MB
2025-09-14 00:28:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:28:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:28:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:28:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:28:03 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.966400, avg_loss=0.599160, seen=40, correct=27, accuracy=0.675000
2025-09-14 00:28:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:28:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:28:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:28:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2364MB allocated=2217MB
2025-09-14 00:28:13 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 00:28:14 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 00:28:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 00:28:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:28:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:28:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 00:28:22 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=135.695160, avg_loss=0.678476, seen=200, correct=115, accuracy=0.575000
2025-09-14 00:28:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:28:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:28:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:28:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2364MB allocated=2217MB
2025-09-14 00:28:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:28:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:28:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:28:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:28:25 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=22.349154, avg_loss=0.558729, seen=40, correct=30, accuracy=0.750000
2025-09-14 00:28:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:28:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:28:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:28:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2364MB allocated=2217MB
2025-09-14 00:28:36 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 00:28:36 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 00:28:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 00:28:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:28:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:28:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 00:28:44 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=135.134674, avg_loss=0.675673, seen=200, correct=119, accuracy=0.595000
2025-09-14 00:28:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:28:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:28:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:28:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2364MB allocated=2217MB
2025-09-14 00:28:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:28:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:28:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:28:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:28:48 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=22.532303, avg_loss=0.563308, seen=40, correct=29, accuracy=0.725000
2025-09-14 00:28:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:28:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:28:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:28:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2364MB allocated=2217MB
2025-09-14 00:28:59 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 00:29:00 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 00:29:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 00:29:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:29:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:29:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 00:29:07 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=135.052826, avg_loss=0.675264, seen=200, correct=116, accuracy=0.580000
2025-09-14 00:29:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:29:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:29:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:29:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2364MB allocated=2217MB
2025-09-14 00:29:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:29:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:29:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:29:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:29:11 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.001715, avg_loss=0.600043, seen=40, correct=26, accuracy=0.650000
2025-09-14 00:29:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:29:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:29:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:29:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2364MB allocated=2217MB
2025-09-14 00:29:21 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 00:29:23 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 00:29:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 00:29:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:29:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:29:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 00:29:32 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=136.011017, avg_loss=0.680055, seen=200, correct=116, accuracy=0.580000
2025-09-14 00:29:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:29:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:29:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:29:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2364MB allocated=2217MB
2025-09-14 00:29:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:29:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:29:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:29:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:29:36 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.832743, avg_loss=0.620819, seen=40, correct=26, accuracy=0.650000
2025-09-14 00:29:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:29:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:29:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:29:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2364MB allocated=2217MB
2025-09-14 00:29:47 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 00:29:48 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 00:29:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 00:29:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:29:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:29:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 00:29:56 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=133.793777, avg_loss=0.668969, seen=200, correct=117, accuracy=0.585000
2025-09-14 00:29:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:29:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:29:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:29:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2364MB allocated=2217MB
2025-09-14 00:29:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:29:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:29:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:30:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:30:00 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.252665, avg_loss=0.581317, seen=40, correct=28, accuracy=0.700000
2025-09-14 00:30:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:30:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:30:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:30:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2364MB allocated=2217MB
2025-09-14 00:30:11 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 00:30:11 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 00:30:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 00:30:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:30:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:30:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 00:30:19 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=133.823288, avg_loss=0.669116, seen=200, correct=121, accuracy=0.605000
2025-09-14 00:30:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:30:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:30:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:30:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2364MB allocated=2217MB
2025-09-14 00:30:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:30:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:30:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:30:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:30:22 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=22.807785, avg_loss=0.570195, seen=40, correct=29, accuracy=0.725000
2025-09-14 00:30:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:30:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:30:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:30:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2364MB allocated=2217MB
2025-09-14 00:30:32 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 00:30:33 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 00:30:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 00:30:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:30:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:30:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 00:30:41 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=135.251404, avg_loss=0.676257, seen=200, correct=119, accuracy=0.595000
2025-09-14 00:30:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:30:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:30:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:30:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2364MB allocated=2217MB
2025-09-14 00:30:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:30:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:30:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:30:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:30:45 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.905247, avg_loss=0.597631, seen=40, correct=28, accuracy=0.700000
2025-09-14 00:30:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:30:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:30:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:30:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2364MB allocated=2217MB
2025-09-14 00:30:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 00:30:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 00:30:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:30:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:30:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2364MB allocated=2217MB
2025-09-14 00:30:48 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #29', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 00:30:48 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #29', 'Round': 0, 'Results_raw': {}}
2025-09-14 00:30:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 00:30:48 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-14 00:30:48 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 00:30:48 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 00:30:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 00:30:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:30:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:30:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 00:30:55 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=124.932602, avg_loss=0.624663, seen=200, correct=134, accuracy=0.670000
2025-09-14 00:30:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:30:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:30:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:30:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2200MB
2025-09-14 00:30:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:30:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:30:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:30:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:30:58 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.569347, avg_loss=0.664234, seen=40, correct=27, accuracy=0.675000
2025-09-14 00:30:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:30:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:30:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:30:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2200MB
2025-09-14 00:30:59 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 00:30:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1471, total=5883)
2025-09-14 00:30:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:30:59 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 00:30:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:30:59 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=736, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 00:31:09 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 00:31:09 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 00:31:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 00:31:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:31:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:31:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 00:31:16 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=126.336617, avg_loss=0.631683, seen=200, correct=128, accuracy=0.640000
2025-09-14 00:31:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:31:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:31:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:31:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2366MB allocated=2217MB
2025-09-14 00:31:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:31:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:31:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:31:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:31:20 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.212936, avg_loss=0.655323, seen=40, correct=25, accuracy=0.625000
2025-09-14 00:31:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:31:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:31:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:31:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2366MB allocated=2217MB
2025-09-14 00:31:30 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 00:31:31 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 00:31:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 00:31:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:31:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:31:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 00:31:38 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=125.361877, avg_loss=0.626809, seen=200, correct=128, accuracy=0.640000
2025-09-14 00:31:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:31:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:31:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:31:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2366MB allocated=2217MB
2025-09-14 00:31:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:31:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:31:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:31:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:31:42 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.863106, avg_loss=0.671578, seen=40, correct=25, accuracy=0.625000
2025-09-14 00:31:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:31:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:31:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:31:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2366MB allocated=2217MB
2025-09-14 00:31:53 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 00:31:54 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 00:31:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 00:31:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:31:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:32:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 00:32:02 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=122.674515, avg_loss=0.613373, seen=200, correct=133, accuracy=0.665000
2025-09-14 00:32:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:32:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:32:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:32:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2366MB allocated=2217MB
2025-09-14 00:32:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:32:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:32:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:32:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:32:05 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.887104, avg_loss=0.672178, seen=40, correct=26, accuracy=0.650000
2025-09-14 00:32:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:32:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:32:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:32:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2366MB allocated=2217MB
2025-09-14 00:32:14 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 00:32:14 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 00:32:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 00:32:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:32:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:32:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 00:32:22 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=123.714104, avg_loss=0.618571, seen=200, correct=131, accuracy=0.655000
2025-09-14 00:32:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:32:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:32:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:32:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2366MB allocated=2217MB
2025-09-14 00:32:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:32:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:32:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:32:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:32:25 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.319706, avg_loss=0.657993, seen=40, correct=26, accuracy=0.650000
2025-09-14 00:32:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:32:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:32:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:32:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2366MB allocated=2217MB
2025-09-14 00:32:37 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 00:32:37 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 00:32:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 00:32:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:32:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:32:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 00:32:44 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=124.300369, avg_loss=0.621502, seen=200, correct=128, accuracy=0.640000
2025-09-14 00:32:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:32:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:32:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:32:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2366MB allocated=2217MB
2025-09-14 00:32:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:32:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:32:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:32:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:32:48 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.440731, avg_loss=0.661018, seen=40, correct=24, accuracy=0.600000
2025-09-14 00:32:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:32:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:32:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:32:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2366MB allocated=2217MB
2025-09-14 00:32:59 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 00:33:00 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 00:33:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 00:33:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:33:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:33:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 00:33:07 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=124.626831, avg_loss=0.623134, seen=200, correct=128, accuracy=0.640000
2025-09-14 00:33:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:33:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:33:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:33:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2366MB allocated=2217MB
2025-09-14 00:33:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:33:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:33:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:33:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:33:11 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.115482, avg_loss=0.677887, seen=40, correct=25, accuracy=0.625000
2025-09-14 00:33:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:33:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:33:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:33:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2366MB allocated=2217MB
2025-09-14 00:33:22 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 00:33:23 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 00:33:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 00:33:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:33:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:33:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 00:33:31 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=125.204147, avg_loss=0.626021, seen=200, correct=126, accuracy=0.630000
2025-09-14 00:33:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:33:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:33:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:33:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2366MB allocated=2217MB
2025-09-14 00:33:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:33:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:33:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:33:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:33:34 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.410042, avg_loss=0.685251, seen=40, correct=25, accuracy=0.625000
2025-09-14 00:33:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:33:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:33:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:33:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2366MB allocated=2217MB
2025-09-14 00:33:47 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 00:33:47 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 00:33:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 00:33:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:33:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:33:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 00:33:54 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=124.016884, avg_loss=0.620084, seen=200, correct=130, accuracy=0.650000
2025-09-14 00:33:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:33:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:33:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:33:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2366MB allocated=2217MB
2025-09-14 00:33:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:33:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:33:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:33:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:33:58 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.721867, avg_loss=0.668047, seen=40, correct=27, accuracy=0.675000
2025-09-14 00:33:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:33:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:33:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:33:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2366MB allocated=2217MB
2025-09-14 00:34:09 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 00:34:09 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 00:34:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 00:34:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:34:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:34:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 00:34:18 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=124.572250, avg_loss=0.622861, seen=200, correct=129, accuracy=0.645000
2025-09-14 00:34:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:34:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:34:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:34:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2366MB allocated=2217MB
2025-09-14 00:34:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:34:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:34:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:34:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:34:20 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.121063, avg_loss=0.653027, seen=40, correct=28, accuracy=0.700000
2025-09-14 00:34:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:34:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:34:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:34:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2366MB allocated=2217MB
2025-09-14 00:34:31 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 00:34:31 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 00:34:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 00:34:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:34:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:34:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 00:34:39 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=125.228508, avg_loss=0.626143, seen=200, correct=128, accuracy=0.640000
2025-09-14 00:34:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:34:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:34:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:34:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2366MB allocated=2217MB
2025-09-14 00:34:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:34:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:34:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:34:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:34:43 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.149998, avg_loss=0.653750, seen=40, correct=28, accuracy=0.700000
2025-09-14 00:34:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:34:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:34:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:34:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2366MB allocated=2217MB
2025-09-14 00:34:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 00:34:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 00:34:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:34:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:34:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2366MB allocated=2217MB
2025-09-14 00:34:45 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #17', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 00:34:45 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #17', 'Round': 0, 'Results_raw': {}}
2025-09-14 00:34:45 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 00:34:45 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-14 00:34:46 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 00:34:46 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 00:34:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-14 00:34:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:34:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=45, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:34:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 00:34:49 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=74.554321, avg_loss=0.677767, seen=110, correct=67, accuracy=0.609091
2025-09-14 00:34:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:34:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:34:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:34:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2200MB
2025-09-14 00:34:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:34:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:34:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:34:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:34:52 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.480263, avg_loss=0.587007, seen=40, correct=31, accuracy=0.775000
2025-09-14 00:34:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:34:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:34:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:34:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2200MB
2025-09-14 00:34:53 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 00:34:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=525, total=2100)
2025-09-14 00:34:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:34:53 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 00:34:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:34:53 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=263, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 00:35:02 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 00:35:03 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 00:35:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-14 00:35:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:35:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:35:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 00:35:07 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=75.736153, avg_loss=0.688510, seen=110, correct=64, accuracy=0.581818
2025-09-14 00:35:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:35:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:35:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:35:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 00:35:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:35:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:35:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:35:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:35:11 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=22.983410, avg_loss=0.574585, seen=40, correct=29, accuracy=0.725000
2025-09-14 00:35:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:35:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:35:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:35:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 00:35:23 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 00:35:23 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 00:35:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-14 00:35:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:35:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:35:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 00:35:28 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=72.906410, avg_loss=0.662786, seen=110, correct=63, accuracy=0.572727
2025-09-14 00:35:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:35:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:35:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:35:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 00:35:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:35:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:35:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:35:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:35:30 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.333860, avg_loss=0.583347, seen=40, correct=25, accuracy=0.625000
2025-09-14 00:35:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:35:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:35:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:35:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 00:35:41 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 00:35:41 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 00:35:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-14 00:35:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:35:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:35:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 00:35:44 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=74.241234, avg_loss=0.674920, seen=110, correct=63, accuracy=0.572727
2025-09-14 00:35:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:35:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:35:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:35:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 00:35:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:35:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:35:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:35:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:35:48 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.163797, avg_loss=0.579095, seen=40, correct=24, accuracy=0.600000
2025-09-14 00:35:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:35:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:35:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:35:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 00:36:00 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 00:36:00 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 00:36:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-14 00:36:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:36:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:36:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 00:36:05 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=73.160110, avg_loss=0.665092, seen=110, correct=63, accuracy=0.572727
2025-09-14 00:36:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:36:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:36:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:36:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 00:36:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:36:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:36:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:36:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:36:08 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.098492, avg_loss=0.577462, seen=40, correct=25, accuracy=0.625000
2025-09-14 00:36:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:36:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:36:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:36:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 00:36:20 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 00:36:20 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 00:36:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-14 00:36:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:36:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:36:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 00:36:24 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=73.640381, avg_loss=0.669458, seen=110, correct=63, accuracy=0.572727
2025-09-14 00:36:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:36:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:36:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:36:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 00:36:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:36:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:36:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:36:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:36:27 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=22.935286, avg_loss=0.573382, seen=40, correct=25, accuracy=0.625000
2025-09-14 00:36:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:36:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:36:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:36:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 00:36:38 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 00:36:38 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 00:36:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-14 00:36:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:36:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:36:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 00:36:42 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=73.889832, avg_loss=0.671726, seen=110, correct=64, accuracy=0.581818
2025-09-14 00:36:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:36:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:36:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:36:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 00:36:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:36:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:36:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:36:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:36:46 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=22.963226, avg_loss=0.574081, seen=40, correct=29, accuracy=0.725000
2025-09-14 00:36:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:36:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:36:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:36:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 00:36:55 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 00:36:56 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 00:36:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-14 00:36:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:36:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:37:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 00:37:00 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=74.659271, avg_loss=0.678721, seen=110, correct=63, accuracy=0.572727
2025-09-14 00:37:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:37:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:37:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:37:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 00:37:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:37:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:37:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:37:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:37:05 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=22.934383, avg_loss=0.573360, seen=40, correct=30, accuracy=0.750000
2025-09-14 00:37:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:37:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:37:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:37:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 00:37:15 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 00:37:16 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 00:37:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-14 00:37:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:37:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:37:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 00:37:20 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=75.171173, avg_loss=0.683374, seen=110, correct=64, accuracy=0.581818
2025-09-14 00:37:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:37:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:37:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:37:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 00:37:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:37:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:37:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:37:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:37:24 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=22.594122, avg_loss=0.564853, seen=40, correct=30, accuracy=0.750000
2025-09-14 00:37:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:37:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:37:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:37:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 00:37:35 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 00:37:36 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 00:37:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-14 00:37:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:37:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:37:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 00:37:40 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=75.014870, avg_loss=0.681953, seen=110, correct=62, accuracy=0.563636
2025-09-14 00:37:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:37:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:37:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:37:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 00:37:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:37:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:37:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:37:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:37:43 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=22.481979, avg_loss=0.562049, seen=40, correct=32, accuracy=0.800000
2025-09-14 00:37:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:37:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:37:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:37:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 00:37:51 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 00:37:54 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 00:37:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-14 00:37:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:37:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:37:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 00:37:58 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=74.370415, avg_loss=0.676095, seen=110, correct=62, accuracy=0.563636
2025-09-14 00:37:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:37:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:37:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:38:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 00:38:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:38:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:38:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:38:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:38:01 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=22.890221, avg_loss=0.572256, seen=40, correct=29, accuracy=0.725000
2025-09-14 00:38:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:38:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:38:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:38:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 00:38:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 00:38:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 00:38:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:38:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:38:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 00:38:03 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #46', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 00:38:03 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #46', 'Round': 0, 'Results_raw': {}}
2025-09-14 00:38:04 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 00:38:04 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-14 00:38:04 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 00:38:04 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 00:38:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-09-14 00:38:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:38:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=77, num_train_batch_last_epoch=23, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:38:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-09-14 00:38:09 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=153, loss_sum=102.213348, avg_loss=0.668061, seen=153, correct=91, accuracy=0.594771
2025-09-14 00:38:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:38:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:38:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:38:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2200MB
2025-09-14 00:38:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:38:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:38:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:38:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:38:11 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=31.212996, avg_loss=0.780325, seen=40, correct=18, accuracy=0.450000
2025-09-14 00:38:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:38:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:38:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:38:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2200MB
2025-09-14 00:38:12 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 00:38:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=729, total=2915)
2025-09-14 00:38:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:38:13 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 00:38:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:38:13 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=365, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 00:38:22 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 00:38:23 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 00:38:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-09-14 00:38:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:38:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=77, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:38:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-09-14 00:38:29 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=153, loss_sum=104.187782, avg_loss=0.680966, seen=153, correct=85, accuracy=0.555556
2025-09-14 00:38:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:38:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:38:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:38:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2370MB allocated=2217MB
2025-09-14 00:38:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:38:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:38:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:38:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:38:34 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.803642, avg_loss=0.770091, seen=40, correct=19, accuracy=0.475000
2025-09-14 00:38:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:38:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:38:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:38:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2370MB allocated=2217MB
2025-09-14 00:38:43 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 00:38:45 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 00:38:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-09-14 00:38:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:38:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=77, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:38:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-09-14 00:38:51 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=153, loss_sum=101.586807, avg_loss=0.663966, seen=153, correct=92, accuracy=0.601307
2025-09-14 00:38:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:38:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:38:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:38:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2370MB allocated=2217MB
2025-09-14 00:38:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:38:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:38:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:38:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:38:55 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.295998, avg_loss=0.732400, seen=40, correct=21, accuracy=0.525000
2025-09-14 00:38:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:38:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:38:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:38:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2370MB allocated=2217MB
2025-09-14 00:39:06 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 00:39:06 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 00:39:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-09-14 00:39:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:39:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=77, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:39:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-09-14 00:39:12 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=153, loss_sum=101.579193, avg_loss=0.663916, seen=153, correct=89, accuracy=0.581699
2025-09-14 00:39:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:39:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:39:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:39:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2370MB allocated=2217MB
2025-09-14 00:39:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:39:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:39:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:39:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:39:16 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.057896, avg_loss=0.751447, seen=40, correct=20, accuracy=0.500000
2025-09-14 00:39:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:39:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:39:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:39:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2370MB allocated=2217MB
2025-09-14 00:39:28 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 00:39:28 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 00:39:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-09-14 00:39:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:39:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=77, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:39:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-09-14 00:39:35 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=153, loss_sum=103.807732, avg_loss=0.678482, seen=153, correct=92, accuracy=0.601307
2025-09-14 00:39:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:39:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:39:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:39:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2370MB allocated=2217MB
2025-09-14 00:39:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:39:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:39:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:39:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:39:39 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.057777, avg_loss=0.726444, seen=40, correct=24, accuracy=0.600000
2025-09-14 00:39:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:39:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:39:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:39:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2370MB allocated=2217MB
2025-09-14 00:39:48 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 00:39:49 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 00:39:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-09-14 00:39:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:39:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=77, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:39:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-09-14 00:39:54 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=153, loss_sum=103.147690, avg_loss=0.674168, seen=153, correct=94, accuracy=0.614379
2025-09-14 00:39:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:39:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:39:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:39:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2370MB allocated=2217MB
2025-09-14 00:39:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:39:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:39:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:39:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:39:58 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.795658, avg_loss=0.719891, seen=40, correct=24, accuracy=0.600000
2025-09-14 00:39:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:39:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:39:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:39:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2370MB allocated=2217MB
2025-09-14 00:40:10 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 00:40:10 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 00:40:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-09-14 00:40:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:40:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=77, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:40:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-09-14 00:40:17 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=153, loss_sum=103.686920, avg_loss=0.677692, seen=153, correct=89, accuracy=0.581699
2025-09-14 00:40:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:40:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:40:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:40:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2370MB allocated=2217MB
2025-09-14 00:40:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:40:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:40:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:40:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:40:20 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.417282, avg_loss=0.735432, seen=40, correct=21, accuracy=0.525000
2025-09-14 00:40:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:40:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:40:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:40:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2370MB allocated=2217MB
2025-09-14 00:40:29 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 00:40:31 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 00:40:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-09-14 00:40:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:40:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=77, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:40:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-09-14 00:40:37 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=153, loss_sum=102.004333, avg_loss=0.666695, seen=153, correct=90, accuracy=0.588235
2025-09-14 00:40:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:40:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:40:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:40:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2370MB allocated=2217MB
2025-09-14 00:40:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:40:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:40:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:40:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:40:41 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.653748, avg_loss=0.716344, seen=40, correct=24, accuracy=0.600000
2025-09-14 00:40:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:40:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:40:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:40:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2370MB allocated=2217MB
2025-09-14 00:40:53 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 00:40:53 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 00:40:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-09-14 00:40:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:40:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=77, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:40:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-09-14 00:40:59 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=153, loss_sum=103.869118, avg_loss=0.678883, seen=153, correct=86, accuracy=0.562092
2025-09-14 00:40:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:40:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:41:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:41:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2370MB allocated=2217MB
2025-09-14 00:41:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:41:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:41:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:41:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:41:03 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.182261, avg_loss=0.704557, seen=40, correct=24, accuracy=0.600000
2025-09-14 00:41:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:41:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:41:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:41:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2370MB allocated=2217MB
2025-09-14 00:41:13 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 00:41:14 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 00:41:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-09-14 00:41:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:41:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=77, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:41:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-09-14 00:41:20 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=153, loss_sum=103.514282, avg_loss=0.676564, seen=153, correct=85, accuracy=0.555556
2025-09-14 00:41:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:41:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:41:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:41:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2370MB allocated=2217MB
2025-09-14 00:41:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:41:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:41:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:41:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:41:25 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.424686, avg_loss=0.710617, seen=40, correct=25, accuracy=0.625000
2025-09-14 00:41:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:41:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:41:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:41:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2370MB allocated=2217MB
2025-09-14 00:41:35 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 00:41:36 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 00:41:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-09-14 00:41:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:41:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=77, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:41:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-09-14 00:41:42 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=153, loss_sum=102.219208, avg_loss=0.668099, seen=153, correct=90, accuracy=0.588235
2025-09-14 00:41:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:41:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:41:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:41:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2370MB allocated=2217MB
2025-09-14 00:41:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:41:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:41:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:41:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:41:46 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.495234, avg_loss=0.737381, seen=40, correct=18, accuracy=0.450000
2025-09-14 00:41:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:41:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:41:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:41:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2370MB allocated=2217MB
2025-09-14 00:41:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 00:41:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 00:41:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:41:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:41:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2370MB allocated=2217MB
2025-09-14 00:41:48 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #21', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 00:41:48 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #21', 'Round': 0, 'Results_raw': {}}
2025-09-14 00:41:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 00:41:48 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 1 for training...
2025-09-14 00:41:49 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 00:41:49 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 00:41:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-09-14 00:41:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:41:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=74, num_train_batch_last_epoch=26, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:41:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-14 00:41:54 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=147, loss_sum=103.766289, avg_loss=0.705893, seen=147, correct=81, accuracy=0.551020
2025-09-14 00:41:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:41:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:41:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:41:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2200MB
2025-09-14 00:41:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:41:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:41:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:41:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:41:57 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.400660, avg_loss=0.635016, seen=40, correct=28, accuracy=0.700000
2025-09-14 00:41:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:41:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:41:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:41:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2200MB
2025-09-14 00:41:58 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 00:41:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=703, total=2812)
2025-09-14 00:41:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:41:58 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 00:41:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:41:58 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=352, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 00:42:08 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 00:42:09 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 00:42:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-09-14 00:42:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:42:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=74, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:42:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-14 00:42:14 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=147, loss_sum=103.309319, avg_loss=0.702784, seen=147, correct=81, accuracy=0.551020
2025-09-14 00:42:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:42:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:42:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:42:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-14 00:42:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:42:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:42:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:42:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:42:18 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.768841, avg_loss=0.669221, seen=40, correct=22, accuracy=0.550000
2025-09-14 00:42:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:42:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:42:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:42:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-14 00:42:30 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 00:42:31 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 00:42:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-09-14 00:42:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:42:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=74, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:42:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-14 00:42:36 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=147, loss_sum=103.177338, avg_loss=0.701887, seen=147, correct=75, accuracy=0.510204
2025-09-14 00:42:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:42:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:42:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:42:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-14 00:42:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:42:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:42:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:42:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:42:40 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.725960, avg_loss=0.618149, seen=40, correct=23, accuracy=0.575000
2025-09-14 00:42:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:42:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:42:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:42:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-14 00:42:50 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 00:42:52 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 00:42:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-09-14 00:42:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:42:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=74, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:42:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-14 00:42:58 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=147, loss_sum=103.106033, avg_loss=0.701402, seen=147, correct=80, accuracy=0.544218
2025-09-14 00:42:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:42:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:42:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:43:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-14 00:43:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:43:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:43:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:43:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:43:02 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.934820, avg_loss=0.598371, seen=40, correct=26, accuracy=0.650000
2025-09-14 00:43:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:43:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:43:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:43:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-14 00:43:13 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 00:43:13 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 00:43:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-09-14 00:43:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:43:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=74, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:43:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-14 00:43:19 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=147, loss_sum=102.012856, avg_loss=0.693965, seen=147, correct=77, accuracy=0.523810
2025-09-14 00:43:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:43:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:43:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:43:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-14 00:43:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:43:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:43:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:43:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:43:23 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.373177, avg_loss=0.609329, seen=40, correct=25, accuracy=0.625000
2025-09-14 00:43:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:43:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:43:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:43:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-14 00:43:34 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 00:43:34 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 00:43:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-09-14 00:43:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:43:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=74, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:43:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-14 00:43:40 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=147, loss_sum=102.155029, avg_loss=0.694932, seen=147, correct=78, accuracy=0.530612
2025-09-14 00:43:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:43:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:43:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:43:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-14 00:43:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:43:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:43:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:43:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:43:43 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.271996, avg_loss=0.606800, seen=40, correct=27, accuracy=0.675000
2025-09-14 00:43:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:43:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:43:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:43:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-14 00:43:53 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 00:43:55 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 00:43:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-09-14 00:43:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:43:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=74, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:44:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-14 00:44:01 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=147, loss_sum=102.125870, avg_loss=0.694734, seen=147, correct=83, accuracy=0.564626
2025-09-14 00:44:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:44:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:44:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:44:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-14 00:44:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:44:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:44:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:44:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:44:05 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.647955, avg_loss=0.616199, seen=40, correct=25, accuracy=0.625000
2025-09-14 00:44:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:44:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:44:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:44:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-14 00:44:17 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 00:44:17 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 00:44:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-09-14 00:44:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:44:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=74, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:44:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-14 00:44:24 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=147, loss_sum=101.401871, avg_loss=0.689809, seen=147, correct=82, accuracy=0.557823
2025-09-14 00:44:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:44:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:44:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:44:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-14 00:44:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:44:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:44:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:44:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:44:28 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.572315, avg_loss=0.639308, seen=40, correct=21, accuracy=0.525000
2025-09-14 00:44:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:44:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:44:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:44:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-14 00:44:38 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 00:44:39 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 00:44:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-09-14 00:44:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:44:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=74, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:44:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-14 00:44:46 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=147, loss_sum=101.091011, avg_loss=0.687694, seen=147, correct=85, accuracy=0.578231
2025-09-14 00:44:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:44:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:44:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:44:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2340MB allocated=2217MB
2025-09-14 00:44:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:44:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:44:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:44:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:44:49 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.283642, avg_loss=0.657091, seen=40, correct=21, accuracy=0.525000
2025-09-14 00:44:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:44:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:44:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:44:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-14 00:45:01 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 00:45:01 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 00:45:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-09-14 00:45:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:45:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=74, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:45:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-14 00:45:07 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=147, loss_sum=100.038368, avg_loss=0.680533, seen=147, correct=85, accuracy=0.578231
2025-09-14 00:45:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:45:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:45:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:45:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-14 00:45:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:45:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:45:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:45:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:45:12 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.031525, avg_loss=0.625788, seen=40, correct=23, accuracy=0.575000
2025-09-14 00:45:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:45:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:45:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:45:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-14 00:45:22 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 00:45:23 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 00:45:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-09-14 00:45:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:45:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=74, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:45:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-14 00:45:29 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=147, loss_sum=100.320206, avg_loss=0.682450, seen=147, correct=85, accuracy=0.578231
2025-09-14 00:45:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:45:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:45:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:45:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-14 00:45:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:45:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:45:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:45:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:45:32 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.311840, avg_loss=0.632796, seen=40, correct=23, accuracy=0.575000
2025-09-14 00:45:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:45:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:45:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:45:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-14 00:45:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 00:45:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 00:45:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:45:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:45:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-14 00:45:34 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #47', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 00:45:34 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #47', 'Round': 0, 'Results_raw': {}}
2025-09-14 00:45:34 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 00:45:34 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-14 00:45:35 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 00:45:35 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 00:45:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-14 00:45:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:45:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=6, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:45:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-14 00:45:40 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=125.062744, avg_loss=0.665227, seen=188, correct=112, accuracy=0.595745
2025-09-14 00:45:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:45:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:45:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:45:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2200MB
2025-09-14 00:45:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:45:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:45:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:45:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:45:43 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.478233, avg_loss=0.661956, seen=40, correct=26, accuracy=0.650000
2025-09-14 00:45:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:45:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:45:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:45:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2200MB
2025-09-14 00:45:44 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 00:45:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=893, total=3572)
2025-09-14 00:45:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:45:45 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 00:45:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:45:45 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=447, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 00:45:54 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 00:45:55 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 00:45:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-14 00:45:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:45:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:46:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-14 00:46:02 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=120.447556, avg_loss=0.640678, seen=188, correct=120, accuracy=0.638298
2025-09-14 00:46:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:46:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:46:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:46:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 00:46:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:46:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:46:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:46:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:46:06 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.854286, avg_loss=0.696357, seen=40, correct=24, accuracy=0.600000
2025-09-14 00:46:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:46:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:46:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:46:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 00:46:18 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 00:46:18 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 00:46:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-14 00:46:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:46:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:46:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-14 00:46:25 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=121.755554, avg_loss=0.647636, seen=188, correct=113, accuracy=0.601064
2025-09-14 00:46:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:46:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:46:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:46:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 00:46:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:46:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:46:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:46:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:46:28 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.050602, avg_loss=0.676265, seen=40, correct=23, accuracy=0.575000
2025-09-14 00:46:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:46:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:46:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:46:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 00:46:38 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 00:46:40 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 00:46:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-14 00:46:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:46:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:46:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-14 00:46:48 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=121.698395, avg_loss=0.647332, seen=188, correct=117, accuracy=0.622340
2025-09-14 00:46:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:46:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:46:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:46:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 00:46:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:46:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:46:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:46:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:46:52 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.956514, avg_loss=0.673913, seen=40, correct=24, accuracy=0.600000
2025-09-14 00:46:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:46:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:46:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:46:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 00:47:04 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 00:47:04 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 00:47:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-14 00:47:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:47:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:47:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-14 00:47:12 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=120.242676, avg_loss=0.639589, seen=188, correct=115, accuracy=0.611702
2025-09-14 00:47:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:47:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:47:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:47:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 00:47:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:47:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:47:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:47:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:47:16 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.210011, avg_loss=0.680250, seen=40, correct=24, accuracy=0.600000
2025-09-14 00:47:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:47:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:47:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:47:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 00:47:27 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 00:47:27 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 00:47:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-14 00:47:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:47:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:47:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-14 00:47:35 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=120.150253, avg_loss=0.639097, seen=188, correct=120, accuracy=0.638298
2025-09-14 00:47:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:47:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:47:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:47:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 00:47:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:47:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:47:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:47:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:47:39 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.397417, avg_loss=0.684935, seen=40, correct=27, accuracy=0.675000
2025-09-14 00:47:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:47:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:47:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:47:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 00:47:49 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 00:47:51 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 00:47:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-14 00:47:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:47:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:47:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-14 00:47:58 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=122.967491, avg_loss=0.654082, seen=188, correct=112, accuracy=0.595745
2025-09-14 00:47:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:47:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:48:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:48:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 00:48:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:48:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:48:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:48:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:48:02 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.034798, avg_loss=0.650870, seen=40, correct=26, accuracy=0.650000
2025-09-14 00:48:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:48:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:48:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:48:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 00:48:10 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 00:48:12 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 00:48:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-14 00:48:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:48:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:48:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-14 00:48:19 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=124.696594, avg_loss=0.663280, seen=188, correct=121, accuracy=0.643617
2025-09-14 00:48:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:48:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:48:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:48:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 00:48:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:48:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:48:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:48:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:48:23 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.910921, avg_loss=0.672773, seen=40, correct=23, accuracy=0.575000
2025-09-14 00:48:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:48:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:48:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:48:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 00:48:35 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 00:48:37 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 00:48:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-14 00:48:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:48:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:48:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-14 00:48:44 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=122.926422, avg_loss=0.653864, seen=188, correct=116, accuracy=0.617021
2025-09-14 00:48:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:48:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:48:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:48:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 00:48:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:48:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:48:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:48:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:48:48 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.827671, avg_loss=0.670692, seen=40, correct=22, accuracy=0.550000
2025-09-14 00:48:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:48:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:48:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:48:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 00:49:00 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 00:49:00 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 00:49:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-14 00:49:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:49:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:49:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-14 00:49:07 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=119.143448, avg_loss=0.633742, seen=188, correct=121, accuracy=0.643617
2025-09-14 00:49:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:49:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:49:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:49:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 00:49:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:49:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:49:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:49:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:49:10 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.456930, avg_loss=0.686423, seen=40, correct=26, accuracy=0.650000
2025-09-14 00:49:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:49:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:49:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:49:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 00:49:20 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 00:49:22 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 00:49:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-14 00:49:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:49:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:49:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-14 00:49:29 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=120.898338, avg_loss=0.643076, seen=188, correct=121, accuracy=0.643617
2025-09-14 00:49:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:49:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:49:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:49:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 00:49:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:49:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:49:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:49:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:49:33 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.763247, avg_loss=0.669081, seen=40, correct=25, accuracy=0.625000
2025-09-14 00:49:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:49:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:49:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:49:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 00:49:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 00:49:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 00:49:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:49:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:49:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 00:49:35 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #9', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 00:49:35 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #9', 'Round': 0, 'Results_raw': {}}
2025-09-14 00:49:35 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 00:49:35 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-14 00:49:36 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 00:49:36 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 00:49:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-09-14 00:49:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:49:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=80, num_train_batch_last_epoch=20, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:49:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-09-14 00:49:40 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=160, loss_sum=106.404076, avg_loss=0.665025, seen=160, correct=101, accuracy=0.631250
2025-09-14 00:49:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:49:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:49:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:49:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2200MB
2025-09-14 00:49:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:49:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:49:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:49:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:49:43 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.075207, avg_loss=0.601880, seen=40, correct=29, accuracy=0.725000
2025-09-14 00:49:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:49:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:49:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:49:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2200MB
2025-09-14 00:49:44 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 00:49:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=764, total=3055)
2025-09-14 00:49:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:49:45 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 00:49:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:49:45 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=382, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 00:49:53 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 00:49:55 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 00:49:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-09-14 00:49:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:49:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=80, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:50:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-09-14 00:50:01 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=160, loss_sum=108.120972, avg_loss=0.675756, seen=160, correct=100, accuracy=0.625000
2025-09-14 00:50:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:50:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:50:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:50:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2217MB
2025-09-14 00:50:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:50:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:50:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:50:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:50:05 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.476713, avg_loss=0.636918, seen=40, correct=25, accuracy=0.625000
2025-09-14 00:50:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:50:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:50:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:50:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-14 00:50:16 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 00:50:16 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 00:50:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-09-14 00:50:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:50:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=80, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:50:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-09-14 00:50:22 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=160, loss_sum=106.581741, avg_loss=0.666136, seen=160, correct=96, accuracy=0.600000
2025-09-14 00:50:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:50:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:50:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:50:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2217MB
2025-09-14 00:50:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:50:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:50:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:50:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:50:26 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.104563, avg_loss=0.602614, seen=40, correct=26, accuracy=0.650000
2025-09-14 00:50:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:50:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:50:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:50:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-14 00:50:38 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 00:50:38 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 00:50:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-09-14 00:50:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:50:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=80, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:50:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-09-14 00:50:44 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=160, loss_sum=109.099976, avg_loss=0.681875, seen=160, correct=100, accuracy=0.625000
2025-09-14 00:50:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:50:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:50:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:50:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2217MB
2025-09-14 00:50:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:50:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:50:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:50:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:50:48 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.847513, avg_loss=0.621188, seen=40, correct=30, accuracy=0.750000
2025-09-14 00:50:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:50:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:50:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:50:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-14 00:50:59 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 00:51:00 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 00:51:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-09-14 00:51:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:51:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=80, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:51:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-09-14 00:51:06 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=160, loss_sum=106.368073, avg_loss=0.664800, seen=160, correct=100, accuracy=0.625000
2025-09-14 00:51:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:51:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:51:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:51:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-14 00:51:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:51:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:51:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:51:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:51:10 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.646910, avg_loss=0.641173, seen=40, correct=28, accuracy=0.700000
2025-09-14 00:51:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:51:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:51:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:51:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-14 00:51:21 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 00:51:21 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 00:51:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-09-14 00:51:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:51:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=80, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:51:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-09-14 00:51:27 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=160, loss_sum=104.055054, avg_loss=0.650344, seen=160, correct=100, accuracy=0.625000
2025-09-14 00:51:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:51:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:51:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:51:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-14 00:51:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:51:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:51:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:51:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:51:31 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.469837, avg_loss=0.636746, seen=40, correct=26, accuracy=0.650000
2025-09-14 00:51:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:51:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:51:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:51:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-14 00:51:41 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 00:51:42 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 00:51:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-09-14 00:51:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:51:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=80, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:51:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-09-14 00:51:48 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=160, loss_sum=106.065887, avg_loss=0.662912, seen=160, correct=95, accuracy=0.593750
2025-09-14 00:51:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:51:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:51:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:51:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-14 00:51:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:51:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:51:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:51:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:51:52 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.760380, avg_loss=0.619009, seen=40, correct=25, accuracy=0.625000
2025-09-14 00:51:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:51:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:51:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:51:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-14 00:52:04 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 00:52:04 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 00:52:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-09-14 00:52:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:52:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=80, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:52:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-09-14 00:52:10 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=160, loss_sum=106.863235, avg_loss=0.667895, seen=160, correct=95, accuracy=0.593750
2025-09-14 00:52:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:52:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:52:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:52:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-14 00:52:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:52:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:52:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:52:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:52:12 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.529961, avg_loss=0.613249, seen=40, correct=24, accuracy=0.600000
2025-09-14 00:52:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:52:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:52:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:52:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-14 00:52:22 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 00:52:22 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 00:52:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-09-14 00:52:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:52:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=80, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:52:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-09-14 00:52:28 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=160, loss_sum=104.202232, avg_loss=0.651264, seen=160, correct=96, accuracy=0.600000
2025-09-14 00:52:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:52:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:52:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:52:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-14 00:52:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:52:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:52:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:52:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:52:32 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.316162, avg_loss=0.607904, seen=40, correct=27, accuracy=0.675000
2025-09-14 00:52:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:52:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:52:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:52:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-14 00:52:42 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 00:52:42 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 00:52:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-09-14 00:52:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:52:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=80, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:52:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-09-14 00:52:48 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=160, loss_sum=102.485870, avg_loss=0.640537, seen=160, correct=97, accuracy=0.606250
2025-09-14 00:52:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:52:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:52:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:52:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-14 00:52:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:52:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:52:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:52:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:52:51 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.303461, avg_loss=0.632587, seen=40, correct=26, accuracy=0.650000
2025-09-14 00:52:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:52:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:52:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:52:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-14 00:53:01 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 00:53:03 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 00:53:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-09-14 00:53:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:53:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=80, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:53:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-09-14 00:53:09 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=160, loss_sum=102.546616, avg_loss=0.640916, seen=160, correct=98, accuracy=0.612500
2025-09-14 00:53:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:53:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:53:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:53:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-14 00:53:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:53:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:53:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:53:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:53:14 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.562557, avg_loss=0.639064, seen=40, correct=28, accuracy=0.700000
2025-09-14 00:53:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:53:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:53:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:53:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-14 00:53:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 00:53:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 00:53:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:53:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:53:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-14 00:53:16 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #14', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 00:53:16 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #14', 'Round': 0, 'Results_raw': {}}
2025-09-14 00:53:16 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 00:53:16 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-14 00:53:16 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 00:53:16 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 00:53:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-09-14 00:53:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:53:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=81, num_train_batch_last_epoch=19, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:53:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-09-14 00:53:22 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=161, loss_sum=102.109970, avg_loss=0.634223, seen=161, correct=105, accuracy=0.652174
2025-09-14 00:53:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:53:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:53:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:53:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2200MB
2025-09-14 00:53:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:53:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:53:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:53:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:53:25 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.788471, avg_loss=0.619712, seen=40, correct=27, accuracy=0.675000
2025-09-14 00:53:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:53:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:53:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:53:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2200MB
2025-09-14 00:53:26 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 00:53:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=766, total=3063)
2025-09-14 00:53:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:53:26 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 00:53:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:53:26 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=383, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 00:53:36 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 00:53:37 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 00:53:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-09-14 00:53:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:53:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=81, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:53:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-09-14 00:53:44 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=161, loss_sum=101.401787, avg_loss=0.629825, seen=161, correct=112, accuracy=0.695652
2025-09-14 00:53:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:53:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:53:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:53:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2217MB
2025-09-14 00:53:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:53:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:53:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:53:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:53:48 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.214569, avg_loss=0.605364, seen=40, correct=29, accuracy=0.725000
2025-09-14 00:53:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:53:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:53:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:53:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2217MB
2025-09-14 00:53:59 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 00:53:59 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 00:53:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-09-14 00:54:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:54:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=81, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:54:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-09-14 00:54:06 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=161, loss_sum=100.364487, avg_loss=0.623382, seen=161, correct=106, accuracy=0.658385
2025-09-14 00:54:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:54:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:54:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:54:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2217MB
2025-09-14 00:54:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:54:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:54:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:54:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:54:10 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.965023, avg_loss=0.624126, seen=40, correct=28, accuracy=0.700000
2025-09-14 00:54:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:54:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:54:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:54:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2217MB
2025-09-14 00:54:19 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 00:54:21 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 00:54:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-09-14 00:54:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:54:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=81, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:54:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-09-14 00:54:28 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=161, loss_sum=98.350746, avg_loss=0.610874, seen=161, correct=111, accuracy=0.689441
2025-09-14 00:54:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:54:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:54:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:54:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 00:54:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:54:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:54:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:54:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:54:31 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.088619, avg_loss=0.627215, seen=40, correct=29, accuracy=0.725000
2025-09-14 00:54:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:54:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:54:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:54:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2217MB
2025-09-14 00:54:43 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 00:54:44 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 00:54:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-09-14 00:54:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:54:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=81, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:54:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-09-14 00:54:51 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=161, loss_sum=99.496368, avg_loss=0.617990, seen=161, correct=104, accuracy=0.645963
2025-09-14 00:54:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:54:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:54:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:54:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2217MB
2025-09-14 00:54:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:54:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:54:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:54:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:54:55 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.396378, avg_loss=0.634909, seen=40, correct=29, accuracy=0.725000
2025-09-14 00:54:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:54:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:54:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:54:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2217MB
2025-09-14 00:55:05 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 00:55:07 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 00:55:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-09-14 00:55:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:55:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=81, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:55:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-09-14 00:55:13 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=161, loss_sum=101.703613, avg_loss=0.631699, seen=161, correct=106, accuracy=0.658385
2025-09-14 00:55:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:55:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:55:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:55:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2217MB
2025-09-14 00:55:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:55:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:55:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:55:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:55:17 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.709732, avg_loss=0.642743, seen=40, correct=27, accuracy=0.675000
2025-09-14 00:55:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:55:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:55:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:55:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2217MB
2025-09-14 00:55:29 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 00:55:29 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 00:55:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-09-14 00:55:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:55:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=81, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:55:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-09-14 00:55:36 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=161, loss_sum=99.692368, avg_loss=0.619207, seen=161, correct=109, accuracy=0.677019
2025-09-14 00:55:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:55:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:55:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:55:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2217MB
2025-09-14 00:55:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:55:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:55:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:55:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:55:40 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.672918, avg_loss=0.616823, seen=40, correct=27, accuracy=0.675000
2025-09-14 00:55:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:55:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:55:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:55:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2217MB
2025-09-14 00:55:49 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 00:55:50 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 00:55:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-09-14 00:55:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:55:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=81, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:55:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-09-14 00:55:56 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=161, loss_sum=99.603951, avg_loss=0.618658, seen=161, correct=112, accuracy=0.695652
2025-09-14 00:55:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:55:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:55:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:55:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 00:55:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:55:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:55:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:55:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:55:59 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.303751, avg_loss=0.607594, seen=40, correct=27, accuracy=0.675000
2025-09-14 00:55:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:55:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:55:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:56:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2217MB
2025-09-14 00:56:09 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 00:56:09 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 00:56:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-09-14 00:56:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:56:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=81, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:56:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-09-14 00:56:16 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=161, loss_sum=101.012451, avg_loss=0.627407, seen=161, correct=109, accuracy=0.677019
2025-09-14 00:56:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:56:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:56:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:56:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 00:56:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:56:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:56:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:56:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:56:19 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.607761, avg_loss=0.615194, seen=40, correct=30, accuracy=0.750000
2025-09-14 00:56:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:56:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:56:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:56:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2217MB
2025-09-14 00:56:30 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 00:56:30 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 00:56:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-09-14 00:56:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:56:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=81, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:56:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-09-14 00:56:36 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=161, loss_sum=104.661041, avg_loss=0.650069, seen=161, correct=101, accuracy=0.627329
2025-09-14 00:56:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:56:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:56:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:56:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2217MB
2025-09-14 00:56:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:56:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:56:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:56:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:56:41 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.509024, avg_loss=0.637726, seen=40, correct=25, accuracy=0.625000
2025-09-14 00:56:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:56:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:56:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:56:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2217MB
2025-09-14 00:56:52 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 00:56:52 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 00:56:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-09-14 00:56:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:56:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=81, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:56:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-09-14 00:56:59 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=161, loss_sum=106.693077, avg_loss=0.662690, seen=161, correct=93, accuracy=0.577640
2025-09-14 00:56:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:56:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:57:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:57:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2217MB
2025-09-14 00:57:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:57:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:57:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:57:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:57:02 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.766258, avg_loss=0.644156, seen=40, correct=24, accuracy=0.600000
2025-09-14 00:57:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:57:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:57:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:57:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2217MB
2025-09-14 00:57:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 00:57:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 00:57:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:57:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:57:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2217MB
2025-09-14 00:57:05 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #26', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 00:57:05 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #26', 'Round': 0, 'Results_raw': {}}
2025-09-14 00:57:05 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 00:57:05 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-14 00:57:05 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 00:57:05 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 00:57:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-09-14 00:57:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:57:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=32, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:57:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 00:57:10 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=135, loss_sum=92.230881, avg_loss=0.683192, seen=135, correct=87, accuracy=0.644444
2025-09-14 00:57:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:57:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:57:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:57:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2200MB
2025-09-14 00:57:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:57:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:57:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:57:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:57:13 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.125307, avg_loss=0.578133, seen=40, correct=28, accuracy=0.700000
2025-09-14 00:57:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:57:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:57:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:57:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2200MB
2025-09-14 00:57:15 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 00:57:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=644, total=2576)
2025-09-14 00:57:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:57:15 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 00:57:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:57:15 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=322, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 00:57:25 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 00:57:25 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 00:57:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-09-14 00:57:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:57:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:57:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 00:57:30 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=135, loss_sum=92.849777, avg_loss=0.687776, seen=135, correct=81, accuracy=0.600000
2025-09-14 00:57:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:57:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:57:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:57:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2368MB allocated=2217MB
2025-09-14 00:57:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:57:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:57:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:57:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:57:34 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.302656, avg_loss=0.607566, seen=40, correct=27, accuracy=0.675000
2025-09-14 00:57:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:57:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:57:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:57:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2368MB allocated=2217MB
2025-09-14 00:57:44 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 00:57:45 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 00:57:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-09-14 00:57:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:57:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:57:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 00:57:50 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=135, loss_sum=94.518951, avg_loss=0.700140, seen=135, correct=77, accuracy=0.570370
2025-09-14 00:57:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:57:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:57:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:57:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2368MB allocated=2217MB
2025-09-14 00:57:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:57:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:57:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:57:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:57:53 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.437046, avg_loss=0.685926, seen=40, correct=24, accuracy=0.600000
2025-09-14 00:57:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:57:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:57:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:57:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2368MB allocated=2217MB
2025-09-14 00:58:05 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 00:58:06 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 00:58:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-09-14 00:58:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:58:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:58:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 00:58:11 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=135, loss_sum=94.632568, avg_loss=0.700982, seen=135, correct=71, accuracy=0.525926
2025-09-14 00:58:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:58:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:58:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:58:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2368MB allocated=2217MB
2025-09-14 00:58:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:58:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:58:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:58:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:58:15 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.236885, avg_loss=0.705922, seen=40, correct=20, accuracy=0.500000
2025-09-14 00:58:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:58:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:58:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:58:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2368MB allocated=2217MB
2025-09-14 00:58:26 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 00:58:26 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 00:58:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-09-14 00:58:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:58:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:58:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 00:58:31 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=135, loss_sum=94.451859, avg_loss=0.699643, seen=135, correct=71, accuracy=0.525926
2025-09-14 00:58:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:58:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:58:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:58:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2368MB allocated=2217MB
2025-09-14 00:58:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:58:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:58:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:58:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:58:35 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.531391, avg_loss=0.688285, seen=40, correct=22, accuracy=0.550000
2025-09-14 00:58:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:58:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:58:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:58:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2368MB allocated=2217MB
2025-09-14 00:58:45 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 00:58:46 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 00:58:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-09-14 00:58:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:58:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:58:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 00:58:51 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=135, loss_sum=91.983154, avg_loss=0.681357, seen=135, correct=78, accuracy=0.577778
2025-09-14 00:58:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:58:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:58:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:58:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2368MB allocated=2217MB
2025-09-14 00:58:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:58:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:58:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:58:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:58:54 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.381050, avg_loss=0.684526, seen=40, correct=22, accuracy=0.550000
2025-09-14 00:58:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:58:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:58:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:58:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2368MB allocated=2217MB
2025-09-14 00:59:05 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 00:59:06 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 00:59:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-09-14 00:59:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:59:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:59:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 00:59:11 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=135, loss_sum=91.654053, avg_loss=0.678919, seen=135, correct=81, accuracy=0.600000
2025-09-14 00:59:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:59:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:59:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:59:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2368MB allocated=2217MB
2025-09-14 00:59:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:59:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:59:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:59:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:59:15 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.376179, avg_loss=0.659404, seen=40, correct=25, accuracy=0.625000
2025-09-14 00:59:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:59:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:59:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:59:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2368MB allocated=2217MB
2025-09-14 00:59:24 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 00:59:26 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 00:59:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-09-14 00:59:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:59:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:59:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 00:59:32 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=135, loss_sum=91.715454, avg_loss=0.679374, seen=135, correct=86, accuracy=0.637037
2025-09-14 00:59:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:59:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:59:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:59:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2368MB allocated=2217MB
2025-09-14 00:59:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:59:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:59:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:59:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:59:35 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.365589, avg_loss=0.634140, seen=40, correct=27, accuracy=0.675000
2025-09-14 00:59:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:59:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:59:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:59:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2368MB allocated=2217MB
2025-09-14 00:59:47 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 00:59:47 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 00:59:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-09-14 00:59:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:59:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:59:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 00:59:52 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=135, loss_sum=91.843369, avg_loss=0.680321, seen=135, correct=88, accuracy=0.651852
2025-09-14 00:59:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:59:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:59:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:59:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2368MB allocated=2217MB
2025-09-14 00:59:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 00:59:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:59:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 00:59:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 00:59:55 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.483376, avg_loss=0.637084, seen=40, correct=26, accuracy=0.650000
2025-09-14 00:59:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 00:59:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 00:59:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 00:59:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2368MB allocated=2217MB
2025-09-14 01:00:07 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 01:00:07 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 01:00:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-09-14 01:00:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:00:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:00:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 01:00:12 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=135, loss_sum=92.023338, avg_loss=0.681654, seen=135, correct=79, accuracy=0.585185
2025-09-14 01:00:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:00:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:00:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:00:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2368MB allocated=2217MB
2025-09-14 01:00:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:00:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:00:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:00:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:00:15 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.590384, avg_loss=0.639760, seen=40, correct=24, accuracy=0.600000
2025-09-14 01:00:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:00:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:00:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:00:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2368MB allocated=2217MB
2025-09-14 01:00:25 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 01:00:27 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 01:00:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-09-14 01:00:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:00:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:00:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 01:00:33 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=135, loss_sum=91.727585, avg_loss=0.679464, seen=135, correct=78, accuracy=0.577778
2025-09-14 01:00:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:00:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:00:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:00:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2368MB allocated=2217MB
2025-09-14 01:00:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:00:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:00:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:00:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:00:37 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.509308, avg_loss=0.637733, seen=40, correct=23, accuracy=0.575000
2025-09-14 01:00:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:00:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:00:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:00:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2368MB allocated=2217MB
2025-09-14 01:00:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 01:00:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 01:00:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:00:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:00:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2368MB allocated=2217MB
2025-09-14 01:00:39 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #18', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 01:00:39 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #18', 'Round': 0, 'Results_raw': {}}
2025-09-14 01:00:39 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 01:00:39 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-14 01:00:40 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 01:00:40 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 01:00:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-14 01:00:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:00:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=6, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:00:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-14 01:00:46 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=128.542145, avg_loss=0.683735, seen=188, correct=108, accuracy=0.574468
2025-09-14 01:00:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:00:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:00:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:00:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2200MB
2025-09-14 01:00:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:00:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:00:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:00:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:00:49 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.185099, avg_loss=0.629627, seen=40, correct=26, accuracy=0.650000
2025-09-14 01:00:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:00:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:00:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:00:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2200MB
2025-09-14 01:00:50 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 01:00:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=898, total=3589)
2025-09-14 01:00:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:00:50 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 01:00:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:00:50 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=449, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 01:00:59 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 01:01:01 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 01:01:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-14 01:01:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:01:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:01:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-14 01:01:06 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=128.470917, avg_loss=0.683356, seen=188, correct=104, accuracy=0.553191
2025-09-14 01:01:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:01:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:01:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:01:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2217MB
2025-09-14 01:01:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:01:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:01:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:01:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:01:10 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.865925, avg_loss=0.621648, seen=40, correct=27, accuracy=0.675000
2025-09-14 01:01:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:01:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:01:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:01:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2217MB
2025-09-14 01:01:21 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 01:01:21 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 01:01:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-14 01:01:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:01:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:01:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-14 01:01:29 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=129.089508, avg_loss=0.686646, seen=188, correct=107, accuracy=0.569149
2025-09-14 01:01:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:01:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:01:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:01:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2217MB
2025-09-14 01:01:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:01:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:01:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:01:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:01:32 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.390739, avg_loss=0.609768, seen=40, correct=26, accuracy=0.650000
2025-09-14 01:01:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:01:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:01:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:01:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2217MB
2025-09-14 01:01:43 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 01:01:43 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 01:01:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-14 01:01:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:01:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:01:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-14 01:01:51 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=129.510010, avg_loss=0.688883, seen=188, correct=106, accuracy=0.563830
2025-09-14 01:01:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:01:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:01:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:01:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2217MB
2025-09-14 01:01:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:01:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:01:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:01:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:01:55 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.854275, avg_loss=0.621357, seen=40, correct=27, accuracy=0.675000
2025-09-14 01:01:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:01:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:01:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:01:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2217MB
2025-09-14 01:02:06 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 01:02:06 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 01:02:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-14 01:02:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:02:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:02:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-14 01:02:15 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=130.398727, avg_loss=0.693610, seen=188, correct=109, accuracy=0.579787
2025-09-14 01:02:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:02:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:02:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:02:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2217MB
2025-09-14 01:02:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:02:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:02:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:02:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:02:19 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.134016, avg_loss=0.628350, seen=40, correct=29, accuracy=0.725000
2025-09-14 01:02:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:02:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:02:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:02:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2217MB
2025-09-14 01:02:29 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 01:02:31 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 01:02:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-14 01:02:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:02:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:02:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-14 01:02:37 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=128.135666, avg_loss=0.681573, seen=188, correct=105, accuracy=0.558511
2025-09-14 01:02:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:02:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:02:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:02:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2217MB
2025-09-14 01:02:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:02:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:02:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:02:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:02:41 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.363359, avg_loss=0.634084, seen=40, correct=26, accuracy=0.650000
2025-09-14 01:02:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:02:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:02:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:02:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2217MB
2025-09-14 01:02:52 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 01:02:53 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 01:02:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-14 01:02:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:02:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:03:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-14 01:03:02 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=126.937737, avg_loss=0.675201, seen=188, correct=108, accuracy=0.574468
2025-09-14 01:03:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:03:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:03:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:03:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2217MB
2025-09-14 01:03:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:03:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:03:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:03:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:03:06 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.160152, avg_loss=0.629004, seen=40, correct=26, accuracy=0.650000
2025-09-14 01:03:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:03:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:03:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:03:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2217MB
2025-09-14 01:03:16 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 01:03:18 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 01:03:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-14 01:03:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:03:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:03:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-14 01:03:25 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=127.737160, avg_loss=0.679453, seen=188, correct=107, accuracy=0.569149
2025-09-14 01:03:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:03:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:03:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:03:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2217MB
2025-09-14 01:03:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:03:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:03:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:03:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:03:29 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.861732, avg_loss=0.621543, seen=40, correct=27, accuracy=0.675000
2025-09-14 01:03:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:03:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:03:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:03:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2217MB
2025-09-14 01:03:40 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 01:03:40 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 01:03:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-14 01:03:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:03:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:03:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-14 01:03:48 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=126.603951, avg_loss=0.673425, seen=188, correct=109, accuracy=0.579787
2025-09-14 01:03:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:03:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:03:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:03:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2217MB
2025-09-14 01:03:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:03:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:03:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:03:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:03:53 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.199919, avg_loss=0.629998, seen=40, correct=26, accuracy=0.650000
2025-09-14 01:03:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:03:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:03:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:03:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2217MB
2025-09-14 01:04:04 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 01:04:04 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 01:04:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-14 01:04:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:04:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:04:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-14 01:04:11 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=126.923859, avg_loss=0.675127, seen=188, correct=114, accuracy=0.606383
2025-09-14 01:04:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:04:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:04:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:04:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2217MB
2025-09-14 01:04:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:04:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:04:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:04:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:04:15 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.396160, avg_loss=0.609904, seen=40, correct=27, accuracy=0.675000
2025-09-14 01:04:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:04:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:04:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:04:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2217MB
2025-09-14 01:04:26 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 01:04:27 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 01:04:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-14 01:04:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:04:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:04:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-14 01:04:34 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=125.008392, avg_loss=0.664938, seen=188, correct=111, accuracy=0.590426
2025-09-14 01:04:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:04:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:04:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:04:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2217MB
2025-09-14 01:04:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:04:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:04:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:04:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:04:38 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.917635, avg_loss=0.622941, seen=40, correct=26, accuracy=0.650000
2025-09-14 01:04:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:04:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:04:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:04:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2217MB
2025-09-14 01:04:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 01:04:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 01:04:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:04:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:04:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2217MB
2025-09-14 01:04:40 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #52', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 01:04:40 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #52', 'Round': 0, 'Results_raw': {}}
2025-09-14 01:04:40 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 01:04:40 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-14 01:04:41 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 01:04:41 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 01:04:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-09-14 01:04:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:04:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=45, num_train_batch_last_epoch=10, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:04:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-09-14 01:04:43 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=89, loss_sum=62.287544, avg_loss=0.699860, seen=89, correct=47, accuracy=0.528090
2025-09-14 01:04:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:04:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:04:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:04:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2200MB
2025-09-14 01:04:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:04:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:04:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:04:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:04:46 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.753326, avg_loss=0.643833, seen=40, correct=26, accuracy=0.650000
2025-09-14 01:04:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:04:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:04:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:04:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2200MB
2025-09-14 01:04:47 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 01:04:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=424, total=1694)
2025-09-14 01:04:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:04:47 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 01:04:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:04:47 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=212, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 01:04:56 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 01:04:58 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 01:04:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-09-14 01:04:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:04:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=45, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:05:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-09-14 01:05:01 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=89, loss_sum=61.704979, avg_loss=0.693314, seen=89, correct=50, accuracy=0.561798
2025-09-14 01:05:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:05:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:05:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:05:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2402MB allocated=2217MB
2025-09-14 01:05:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:05:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:05:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:05:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:05:06 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.330460, avg_loss=0.608261, seen=40, correct=28, accuracy=0.700000
2025-09-14 01:05:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:05:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:05:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:05:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2402MB allocated=2217MB
2025-09-14 01:05:17 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 01:05:17 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 01:05:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-09-14 01:05:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:05:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=45, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:05:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-09-14 01:05:21 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=89, loss_sum=61.567398, avg_loss=0.691769, seen=89, correct=47, accuracy=0.528090
2025-09-14 01:05:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:05:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:05:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:05:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2402MB allocated=2217MB
2025-09-14 01:05:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:05:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:05:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:05:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:05:24 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.339481, avg_loss=0.608487, seen=40, correct=28, accuracy=0.700000
2025-09-14 01:05:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:05:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:05:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:05:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2402MB allocated=2217MB
2025-09-14 01:05:35 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 01:05:36 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 01:05:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-09-14 01:05:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:05:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=45, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:05:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-09-14 01:05:39 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=89, loss_sum=61.838371, avg_loss=0.694813, seen=89, correct=49, accuracy=0.550562
2025-09-14 01:05:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:05:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:05:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:05:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2402MB allocated=2217MB
2025-09-14 01:05:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:05:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:05:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:05:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:05:42 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.379454, avg_loss=0.609486, seen=40, correct=28, accuracy=0.700000
2025-09-14 01:05:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:05:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:05:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:05:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2402MB allocated=2217MB
2025-09-14 01:05:53 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 01:05:53 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 01:05:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-09-14 01:05:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:05:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=45, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:05:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-09-14 01:05:57 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=89, loss_sum=62.196354, avg_loss=0.698835, seen=89, correct=51, accuracy=0.573034
2025-09-14 01:05:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:05:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:05:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:05:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2402MB allocated=2217MB
2025-09-14 01:05:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:05:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:05:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:06:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:06:00 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.073574, avg_loss=0.601839, seen=40, correct=29, accuracy=0.725000
2025-09-14 01:06:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:06:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:06:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:06:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2402MB allocated=2217MB
2025-09-14 01:06:12 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 01:06:13 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 01:06:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-09-14 01:06:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:06:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=45, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:06:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-09-14 01:06:16 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=89, loss_sum=63.112488, avg_loss=0.709129, seen=89, correct=52, accuracy=0.584270
2025-09-14 01:06:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:06:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:06:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:06:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2402MB allocated=2217MB
2025-09-14 01:06:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:06:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:06:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:06:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:06:20 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.637611, avg_loss=0.590940, seen=40, correct=30, accuracy=0.750000
2025-09-14 01:06:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:06:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:06:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:06:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2402MB allocated=2217MB
2025-09-14 01:06:30 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 01:06:32 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 01:06:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-09-14 01:06:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:06:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=45, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:06:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-09-14 01:06:35 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=89, loss_sum=61.963196, avg_loss=0.696216, seen=89, correct=54, accuracy=0.606742
2025-09-14 01:06:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:06:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:06:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:06:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2402MB allocated=2217MB
2025-09-14 01:06:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:06:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:06:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:06:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:06:39 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.300196, avg_loss=0.607505, seen=40, correct=26, accuracy=0.650000
2025-09-14 01:06:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:06:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:06:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:06:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2402MB allocated=2217MB
2025-09-14 01:06:51 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 01:06:51 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 01:06:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-09-14 01:06:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:06:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=45, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:06:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-09-14 01:06:54 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=89, loss_sum=61.707470, avg_loss=0.693342, seen=89, correct=51, accuracy=0.573034
2025-09-14 01:06:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:06:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:06:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:06:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2402MB allocated=2217MB
2025-09-14 01:06:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:06:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:06:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:06:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:06:58 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.257473, avg_loss=0.606437, seen=40, correct=31, accuracy=0.775000
2025-09-14 01:06:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:06:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:06:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:06:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2402MB allocated=2217MB
2025-09-14 01:07:08 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 01:07:09 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 01:07:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-09-14 01:07:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:07:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=45, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:07:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-09-14 01:07:13 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=89, loss_sum=61.522305, avg_loss=0.691262, seen=89, correct=53, accuracy=0.595506
2025-09-14 01:07:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:07:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:07:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:07:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2402MB allocated=2217MB
2025-09-14 01:07:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:07:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:07:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:07:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:07:17 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.035852, avg_loss=0.625896, seen=40, correct=28, accuracy=0.700000
2025-09-14 01:07:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:07:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:07:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:07:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2402MB allocated=2217MB
2025-09-14 01:07:27 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 01:07:28 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 01:07:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-09-14 01:07:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:07:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=45, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:07:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-09-14 01:07:32 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=89, loss_sum=62.486984, avg_loss=0.702101, seen=89, correct=56, accuracy=0.629213
2025-09-14 01:07:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:07:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:07:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:07:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2402MB allocated=2217MB
2025-09-14 01:07:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:07:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:07:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:07:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:07:36 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.208838, avg_loss=0.655221, seen=40, correct=24, accuracy=0.600000
2025-09-14 01:07:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:07:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:07:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:07:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2402MB allocated=2217MB
2025-09-14 01:07:46 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 01:07:48 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 01:07:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-09-14 01:07:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:07:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=45, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:07:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-09-14 01:07:51 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=89, loss_sum=63.226189, avg_loss=0.710407, seen=89, correct=53, accuracy=0.595506
2025-09-14 01:07:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:07:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:07:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:07:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2402MB allocated=2217MB
2025-09-14 01:07:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:07:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:07:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:07:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:07:55 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.077141, avg_loss=0.651929, seen=40, correct=26, accuracy=0.650000
2025-09-14 01:07:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:07:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:07:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:07:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2402MB allocated=2217MB
2025-09-14 01:07:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 01:07:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 01:07:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:07:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:07:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2402MB allocated=2217MB
2025-09-14 01:07:58 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #43', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 01:07:58 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #43', 'Round': 0, 'Results_raw': {}}
2025-09-14 01:07:58 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 01:07:58 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-14 01:07:58 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 01:07:58 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 01:07:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 01:07:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:07:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=4, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:07:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 01:07:59 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=8.704920, avg_loss=0.791356, seen=11, correct=6, accuracy=0.545455
2025-09-14 01:07:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:07:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:08:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:08:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2200MB
2025-09-14 01:08:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:08:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:08:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:08:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:08:01 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.692604, avg_loss=0.667315, seen=40, correct=21, accuracy=0.525000
2025-09-14 01:08:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:08:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:08:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:08:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2200MB
2025-09-14 01:08:03 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 01:08:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=54, total=214)
2025-09-14 01:08:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:08:03 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 01:08:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:08:03 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=27, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 01:08:13 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 01:08:13 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 01:08:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 01:08:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:08:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:08:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 01:08:14 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=10.047501, avg_loss=0.913409, seen=11, correct=4, accuracy=0.363636
2025-09-14 01:08:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:08:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:08:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:08:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2390MB allocated=2217MB
2025-09-14 01:08:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:08:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:08:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:08:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:08:17 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.088604, avg_loss=0.727215, seen=40, correct=22, accuracy=0.550000
2025-09-14 01:08:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:08:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:08:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:08:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2390MB allocated=2217MB
2025-09-14 01:08:27 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 01:08:29 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 01:08:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 01:08:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:08:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:08:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 01:08:30 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=11.485095, avg_loss=1.044100, seen=11, correct=4, accuracy=0.363636
2025-09-14 01:08:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:08:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:08:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:08:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2390MB allocated=2217MB
2025-09-14 01:08:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:08:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:08:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:08:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:08:33 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.827644, avg_loss=0.770691, seen=40, correct=21, accuracy=0.525000
2025-09-14 01:08:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:08:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:08:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:08:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2390MB allocated=2217MB
2025-09-14 01:08:44 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 01:08:44 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 01:08:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 01:08:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:08:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:08:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 01:08:45 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=12.251964, avg_loss=1.113815, seen=11, correct=4, accuracy=0.363636
2025-09-14 01:08:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:08:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:08:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:08:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2390MB allocated=2217MB
2025-09-14 01:08:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:08:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:08:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:08:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:08:49 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=31.562073, avg_loss=0.789052, seen=40, correct=23, accuracy=0.575000
2025-09-14 01:08:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:08:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:08:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:08:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2390MB allocated=2217MB
2025-09-14 01:08:57 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 01:08:57 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 01:08:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 01:08:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:08:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:08:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 01:08:58 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=12.037601, avg_loss=1.094327, seen=11, correct=4, accuracy=0.363636
2025-09-14 01:08:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:08:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:08:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:09:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2390MB allocated=2217MB
2025-09-14 01:09:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:09:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:09:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:09:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:09:01 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=31.466532, avg_loss=0.786663, seen=40, correct=22, accuracy=0.550000
2025-09-14 01:09:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:09:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:09:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:09:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2390MB allocated=2217MB
2025-09-14 01:09:12 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 01:09:13 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 01:09:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 01:09:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:09:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:09:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 01:09:14 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=12.068976, avg_loss=1.097180, seen=11, correct=4, accuracy=0.363636
2025-09-14 01:09:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:09:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:09:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:09:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2390MB allocated=2217MB
2025-09-14 01:09:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:09:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:09:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:09:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:09:17 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=31.259117, avg_loss=0.781478, seen=40, correct=21, accuracy=0.525000
2025-09-14 01:09:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:09:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:09:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:09:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2390MB allocated=2217MB
2025-09-14 01:09:26 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 01:09:27 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 01:09:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 01:09:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:09:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:09:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 01:09:28 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=12.526810, avg_loss=1.138801, seen=11, correct=5, accuracy=0.454545
2025-09-14 01:09:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:09:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:09:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:09:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2390MB allocated=2217MB
2025-09-14 01:09:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:09:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:09:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:09:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:09:31 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.965185, avg_loss=0.774130, seen=40, correct=21, accuracy=0.525000
2025-09-14 01:09:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:09:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:09:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:09:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2390MB allocated=2217MB
2025-09-14 01:09:41 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 01:09:42 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 01:09:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 01:09:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:09:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:09:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 01:09:43 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=12.892002, avg_loss=1.172000, seen=11, correct=5, accuracy=0.454545
2025-09-14 01:09:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:09:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:09:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:09:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2390MB allocated=2217MB
2025-09-14 01:09:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:09:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:09:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:09:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:09:46 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=32.020802, avg_loss=0.800520, seen=40, correct=21, accuracy=0.525000
2025-09-14 01:09:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:09:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:09:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:09:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2390MB allocated=2217MB
2025-09-14 01:09:56 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 01:09:58 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 01:09:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 01:09:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:09:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:09:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 01:09:59 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=13.318434, avg_loss=1.210767, seen=11, correct=5, accuracy=0.454545
2025-09-14 01:09:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:09:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:10:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:10:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2390MB allocated=2217MB
2025-09-14 01:10:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:10:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:10:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:10:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:10:02 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=32.263359, avg_loss=0.806584, seen=40, correct=21, accuracy=0.525000
2025-09-14 01:10:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:10:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:10:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:10:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2390MB allocated=2217MB
2025-09-14 01:10:13 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 01:10:13 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 01:10:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 01:10:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:10:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:10:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 01:10:14 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=13.354795, avg_loss=1.214072, seen=11, correct=5, accuracy=0.454545
2025-09-14 01:10:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:10:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:10:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:10:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2390MB allocated=2217MB
2025-09-14 01:10:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:10:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:10:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:10:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:10:16 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=32.927269, avg_loss=0.823182, seen=40, correct=20, accuracy=0.500000
2025-09-14 01:10:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:10:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:10:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:10:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2390MB allocated=2217MB
2025-09-14 01:10:26 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 01:10:27 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 01:10:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 01:10:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:10:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:10:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 01:10:28 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=14.088760, avg_loss=1.280796, seen=11, correct=5, accuracy=0.454545
2025-09-14 01:10:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:10:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:10:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:10:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2390MB allocated=2217MB
2025-09-14 01:10:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:10:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:10:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:10:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:10:30 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=34.075733, avg_loss=0.851893, seen=40, correct=21, accuracy=0.525000
2025-09-14 01:10:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:10:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:10:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:10:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2390MB allocated=2217MB
2025-09-14 01:10:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 01:10:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 01:10:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:10:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:10:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2390MB allocated=2217MB
2025-09-14 01:10:32 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #2', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 01:10:32 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #2', 'Round': 0, 'Results_raw': {}}
2025-09-14 01:10:32 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 01:10:32 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 1 for training...
2025-09-14 01:10:33 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 01:10:33 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 01:10:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-09-14 01:10:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:10:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=36, num_train_batch_last_epoch=28, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:10:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-14 01:10:35 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=72, loss_sum=47.685097, avg_loss=0.662293, seen=72, correct=48, accuracy=0.666667
2025-09-14 01:10:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:10:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:10:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:10:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2200MB
2025-09-14 01:10:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:10:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:10:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:10:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:10:38 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.354694, avg_loss=0.733867, seen=40, correct=25, accuracy=0.625000
2025-09-14 01:10:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:10:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:10:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:10:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2200MB
2025-09-14 01:10:39 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 01:10:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=343, total=1372)
2025-09-14 01:10:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:10:40 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 01:10:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:10:40 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=172, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 01:10:49 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 01:10:51 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 01:10:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-09-14 01:10:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:10:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=36, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:10:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-14 01:10:54 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=72, loss_sum=47.441429, avg_loss=0.658909, seen=72, correct=44, accuracy=0.611111
2025-09-14 01:10:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:10:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:10:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:10:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2340MB allocated=2217MB
2025-09-14 01:10:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:10:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:10:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:10:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:10:57 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.846338, avg_loss=0.721158, seen=40, correct=25, accuracy=0.625000
2025-09-14 01:10:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:10:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:10:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:10:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2340MB allocated=2217MB
2025-09-14 01:11:08 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 01:11:09 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 01:11:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-09-14 01:11:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:11:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=36, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:11:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-14 01:11:11 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=72, loss_sum=48.235329, avg_loss=0.669935, seen=72, correct=43, accuracy=0.597222
2025-09-14 01:11:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:11:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:11:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:11:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2340MB allocated=2217MB
2025-09-14 01:11:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:11:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:11:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:11:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:11:14 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.928934, avg_loss=0.723223, seen=40, correct=22, accuracy=0.550000
2025-09-14 01:11:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:11:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:11:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:11:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2340MB allocated=2217MB
2025-09-14 01:11:24 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 01:11:25 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 01:11:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-09-14 01:11:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:11:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=36, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:11:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-14 01:11:28 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=72, loss_sum=49.179314, avg_loss=0.683046, seen=72, correct=42, accuracy=0.583333
2025-09-14 01:11:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:11:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:11:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:11:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2340MB allocated=2217MB
2025-09-14 01:11:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:11:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:11:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:11:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:11:32 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.969271, avg_loss=0.699232, seen=40, correct=23, accuracy=0.575000
2025-09-14 01:11:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:11:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:11:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:11:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2340MB allocated=2217MB
2025-09-14 01:11:43 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 01:11:43 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 01:11:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-09-14 01:11:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:11:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=36, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:11:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-14 01:11:46 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=72, loss_sum=48.270702, avg_loss=0.670426, seen=72, correct=42, accuracy=0.583333
2025-09-14 01:11:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:11:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:11:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:11:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2340MB allocated=2217MB
2025-09-14 01:11:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:11:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:11:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:11:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:11:49 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.718563, avg_loss=0.667964, seen=40, correct=25, accuracy=0.625000
2025-09-14 01:11:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:11:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:11:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:11:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2340MB allocated=2217MB
2025-09-14 01:12:00 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 01:12:00 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 01:12:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-09-14 01:12:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:12:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=36, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:12:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-14 01:12:03 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=72, loss_sum=47.712242, avg_loss=0.662670, seen=72, correct=42, accuracy=0.583333
2025-09-14 01:12:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:12:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:12:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:12:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2342MB allocated=2217MB
2025-09-14 01:12:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:12:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:12:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:12:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:12:06 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.097408, avg_loss=0.652435, seen=40, correct=26, accuracy=0.650000
2025-09-14 01:12:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:12:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:12:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:12:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2340MB allocated=2217MB
2025-09-14 01:12:16 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 01:12:18 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 01:12:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-09-14 01:12:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:12:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=36, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:12:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-14 01:12:20 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=72, loss_sum=46.817753, avg_loss=0.650247, seen=72, correct=44, accuracy=0.611111
2025-09-14 01:12:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:12:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:12:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:12:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2340MB allocated=2217MB
2025-09-14 01:12:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:12:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:12:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:12:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:12:22 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.371067, avg_loss=0.659277, seen=40, correct=27, accuracy=0.675000
2025-09-14 01:12:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:12:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:12:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:12:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2340MB allocated=2217MB
2025-09-14 01:12:34 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 01:12:34 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 01:12:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-09-14 01:12:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:12:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=36, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:12:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-14 01:12:37 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=72, loss_sum=45.953438, avg_loss=0.638242, seen=72, correct=45, accuracy=0.625000
2025-09-14 01:12:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:12:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:12:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:12:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2340MB allocated=2217MB
2025-09-14 01:12:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:12:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:12:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:12:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:12:41 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.181797, avg_loss=0.654545, seen=40, correct=27, accuracy=0.675000
2025-09-14 01:12:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:12:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:12:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:12:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2340MB allocated=2217MB
2025-09-14 01:12:53 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 01:12:53 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 01:12:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-09-14 01:12:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:12:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=36, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:12:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-14 01:12:56 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=72, loss_sum=46.770203, avg_loss=0.649586, seen=72, correct=43, accuracy=0.597222
2025-09-14 01:12:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:12:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:12:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:12:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2340MB allocated=2217MB
2025-09-14 01:12:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:12:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:12:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:12:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:12:58 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.854240, avg_loss=0.646356, seen=40, correct=27, accuracy=0.675000
2025-09-14 01:12:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:12:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:12:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:13:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2340MB allocated=2217MB
2025-09-14 01:13:09 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 01:13:10 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 01:13:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-09-14 01:13:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:13:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=36, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:13:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-14 01:13:13 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=72, loss_sum=46.579815, avg_loss=0.646942, seen=72, correct=42, accuracy=0.583333
2025-09-14 01:13:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:13:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:13:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:13:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2340MB allocated=2217MB
2025-09-14 01:13:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:13:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:13:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:13:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:13:16 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.193008, avg_loss=0.654825, seen=40, correct=27, accuracy=0.675000
2025-09-14 01:13:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:13:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:13:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:13:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2340MB allocated=2217MB
2025-09-14 01:13:27 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 01:13:28 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 01:13:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-09-14 01:13:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:13:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=36, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:13:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-14 01:13:30 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=72, loss_sum=47.045963, avg_loss=0.653416, seen=72, correct=43, accuracy=0.597222
2025-09-14 01:13:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:13:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:13:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:13:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2340MB allocated=2217MB
2025-09-14 01:13:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:13:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:13:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:13:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:13:33 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.521244, avg_loss=0.688031, seen=40, correct=24, accuracy=0.600000
2025-09-14 01:13:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:13:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:13:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:13:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2340MB allocated=2217MB
2025-09-14 01:13:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 01:13:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 01:13:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:13:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:13:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2340MB allocated=2217MB
2025-09-14 01:13:35 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #13', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 01:13:35 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #13', 'Round': 0, 'Results_raw': {}}
2025-09-14 01:13:35 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 01:13:35 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-14 01:13:36 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 01:13:36 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 01:13:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-09-14 01:13:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:13:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=60, num_train_batch_last_epoch=40, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:13:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-09-14 01:13:39 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=119, loss_sum=81.362404, avg_loss=0.683718, seen=119, correct=73, accuracy=0.613445
2025-09-14 01:13:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:13:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:13:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:13:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2200MB
2025-09-14 01:13:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:13:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:13:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:13:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:13:42 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.895924, avg_loss=0.722398, seen=40, correct=23, accuracy=0.575000
2025-09-14 01:13:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:13:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:13:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:13:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2200MB
2025-09-14 01:13:43 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 01:13:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=569, total=2275)
2025-09-14 01:13:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:13:43 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 01:13:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:13:43 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=285, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 01:13:52 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 01:13:54 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 01:13:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-09-14 01:13:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:13:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=60, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:13:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-09-14 01:13:59 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=119, loss_sum=81.931923, avg_loss=0.688504, seen=119, correct=70, accuracy=0.588235
2025-09-14 01:13:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:14:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:14:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:14:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:14:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:14:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:14:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:14:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:14:03 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.532402, avg_loss=0.738310, seen=40, correct=24, accuracy=0.600000
2025-09-14 01:14:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:14:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:14:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:14:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:14:12 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 01:14:13 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 01:14:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-09-14 01:14:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:14:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=60, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:14:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-09-14 01:14:18 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=119, loss_sum=79.409874, avg_loss=0.667310, seen=119, correct=70, accuracy=0.588235
2025-09-14 01:14:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:14:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:14:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:14:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:14:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:14:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:14:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:14:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:14:22 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.043440, avg_loss=0.701086, seen=40, correct=23, accuracy=0.575000
2025-09-14 01:14:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:14:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:14:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:14:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:14:32 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 01:14:33 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 01:14:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-09-14 01:14:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:14:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=60, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:14:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-09-14 01:14:38 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=119, loss_sum=80.620056, avg_loss=0.677479, seen=119, correct=67, accuracy=0.563025
2025-09-14 01:14:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:14:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:14:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:14:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:14:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:14:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:14:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:14:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:14:41 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.542686, avg_loss=0.688567, seen=40, correct=21, accuracy=0.525000
2025-09-14 01:14:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:14:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:14:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:14:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:14:49 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 01:14:49 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 01:14:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-09-14 01:14:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:14:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=60, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:14:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-09-14 01:14:54 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=119, loss_sum=80.889641, avg_loss=0.679745, seen=119, correct=67, accuracy=0.563025
2025-09-14 01:14:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:14:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:14:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:14:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:14:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:14:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:14:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:14:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:14:58 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.461563, avg_loss=0.711539, seen=40, correct=20, accuracy=0.500000
2025-09-14 01:14:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:14:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:14:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:14:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:15:09 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 01:15:09 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 01:15:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-09-14 01:15:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:15:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=60, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:15:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-09-14 01:15:13 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=119, loss_sum=80.559898, avg_loss=0.676974, seen=119, correct=66, accuracy=0.554622
2025-09-14 01:15:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:15:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:15:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:15:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:15:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:15:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:15:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:15:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:15:17 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.041103, avg_loss=0.701028, seen=40, correct=21, accuracy=0.525000
2025-09-14 01:15:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:15:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:15:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:15:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:15:27 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 01:15:28 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 01:15:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-09-14 01:15:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:15:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=60, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:15:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-09-14 01:15:33 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=119, loss_sum=81.512466, avg_loss=0.684979, seen=119, correct=63, accuracy=0.529412
2025-09-14 01:15:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:15:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:15:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:15:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:15:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:15:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:15:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:15:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:15:37 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.040480, avg_loss=0.701012, seen=40, correct=22, accuracy=0.550000
2025-09-14 01:15:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:15:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:15:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:15:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:15:48 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 01:15:48 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 01:15:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-09-14 01:15:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:15:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=60, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:15:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-09-14 01:15:53 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=119, loss_sum=81.430557, avg_loss=0.684290, seen=119, correct=64, accuracy=0.537815
2025-09-14 01:15:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:15:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:15:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:15:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:15:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:15:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:15:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:15:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:15:55 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.665506, avg_loss=0.691638, seen=40, correct=21, accuracy=0.525000
2025-09-14 01:15:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:15:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:15:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:15:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:16:06 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 01:16:07 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 01:16:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-09-14 01:16:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:16:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=60, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:16:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-09-14 01:16:12 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=119, loss_sum=81.166321, avg_loss=0.682070, seen=119, correct=69, accuracy=0.579832
2025-09-14 01:16:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:16:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:16:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:16:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:16:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:16:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:16:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:16:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:16:16 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.992813, avg_loss=0.699820, seen=40, correct=24, accuracy=0.600000
2025-09-14 01:16:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:16:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:16:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:16:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:16:25 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 01:16:26 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 01:16:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-09-14 01:16:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:16:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=60, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:16:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-09-14 01:16:30 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=119, loss_sum=80.266975, avg_loss=0.674512, seen=119, correct=72, accuracy=0.605042
2025-09-14 01:16:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:16:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:16:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:16:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:16:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:16:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:16:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:16:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:16:34 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.490320, avg_loss=0.712258, seen=40, correct=24, accuracy=0.600000
2025-09-14 01:16:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:16:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:16:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:16:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:16:46 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 01:16:46 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 01:16:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-09-14 01:16:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:16:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=60, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:16:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-09-14 01:16:51 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=119, loss_sum=79.806602, avg_loss=0.670644, seen=119, correct=72, accuracy=0.605042
2025-09-14 01:16:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:16:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:16:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:16:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:16:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:16:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:16:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:16:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:16:54 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.290577, avg_loss=0.707264, seen=40, correct=22, accuracy=0.550000
2025-09-14 01:16:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:16:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:16:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:16:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:16:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 01:16:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 01:16:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:16:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:16:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:16:56 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #41', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 01:16:56 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #41', 'Round': 0, 'Results_raw': {}}
2025-09-14 01:16:56 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 01:16:56 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 1 for training...
2025-09-14 01:16:57 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 01:16:57 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 01:16:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 01:16:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:16:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:17:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 01:17:03 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=141.218323, avg_loss=0.706092, seen=200, correct=102, accuracy=0.510000
2025-09-14 01:17:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:17:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:17:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:17:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2200MB
2025-09-14 01:17:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:17:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:17:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:17:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:17:05 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=34.815933, avg_loss=0.870398, seen=40, correct=20, accuracy=0.500000
2025-09-14 01:17:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:17:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:17:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:17:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2200MB
2025-09-14 01:17:06 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 01:17:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1162, total=4647)
2025-09-14 01:17:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:17:07 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 01:17:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:17:07 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=581, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 01:17:15 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 01:17:16 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 01:17:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 01:17:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:17:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:17:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 01:17:24 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=140.049957, avg_loss=0.700250, seen=200, correct=99, accuracy=0.495000
2025-09-14 01:17:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:17:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:17:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:17:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:17:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:17:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:17:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:17:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:17:28 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=32.782993, avg_loss=0.819575, seen=40, correct=18, accuracy=0.450000
2025-09-14 01:17:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:17:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:17:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:17:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:17:37 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 01:17:37 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 01:17:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 01:17:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:17:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:17:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 01:17:45 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=142.685715, avg_loss=0.713429, seen=200, correct=108, accuracy=0.540000
2025-09-14 01:17:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:17:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:17:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:17:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:17:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:17:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:17:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:17:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:17:49 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.516901, avg_loss=0.737923, seen=40, correct=19, accuracy=0.475000
2025-09-14 01:17:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:17:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:17:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:17:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:18:00 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 01:18:01 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 01:18:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 01:18:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:18:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:18:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 01:18:08 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=143.365097, avg_loss=0.716825, seen=200, correct=108, accuracy=0.540000
2025-09-14 01:18:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:18:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:18:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:18:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:18:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:18:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:18:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:18:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:18:11 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.292065, avg_loss=0.732302, seen=40, correct=17, accuracy=0.425000
2025-09-14 01:18:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:18:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:18:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:18:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:18:21 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 01:18:24 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 01:18:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 01:18:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:18:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:18:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 01:18:31 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=141.904984, avg_loss=0.709525, seen=200, correct=114, accuracy=0.570000
2025-09-14 01:18:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:18:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:18:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:18:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:18:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:18:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:18:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:18:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:18:35 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.606413, avg_loss=0.740160, seen=40, correct=16, accuracy=0.400000
2025-09-14 01:18:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:18:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:18:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:18:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:18:47 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 01:18:47 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 01:18:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 01:18:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:18:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:18:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 01:18:54 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=143.177414, avg_loss=0.715887, seen=200, correct=110, accuracy=0.550000
2025-09-14 01:18:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:18:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:18:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:18:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:18:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:18:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:18:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:18:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:18:58 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.499578, avg_loss=0.762489, seen=40, correct=15, accuracy=0.375000
2025-09-14 01:18:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:18:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:18:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:19:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:19:09 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 01:19:11 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 01:19:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 01:19:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:19:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:19:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 01:19:18 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=143.071655, avg_loss=0.715358, seen=200, correct=112, accuracy=0.560000
2025-09-14 01:19:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:19:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:19:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:19:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:19:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:19:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:19:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:19:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:19:22 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.052898, avg_loss=0.751322, seen=40, correct=17, accuracy=0.425000
2025-09-14 01:19:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:19:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:19:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:19:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:19:33 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 01:19:34 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 01:19:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 01:19:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:19:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:19:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 01:19:41 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=141.115509, avg_loss=0.705578, seen=200, correct=111, accuracy=0.555000
2025-09-14 01:19:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:19:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:19:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:19:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:19:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:19:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:19:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:19:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:19:45 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.830950, avg_loss=0.745774, seen=40, correct=16, accuracy=0.400000
2025-09-14 01:19:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:19:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:19:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:19:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:19:56 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 01:19:57 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 01:19:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 01:19:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:19:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:20:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 01:20:04 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=140.235107, avg_loss=0.701176, seen=200, correct=111, accuracy=0.555000
2025-09-14 01:20:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:20:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:20:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:20:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:20:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:20:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:20:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:20:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:20:08 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.822880, avg_loss=0.745572, seen=40, correct=20, accuracy=0.500000
2025-09-14 01:20:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:20:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:20:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:20:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:20:19 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 01:20:19 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 01:20:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 01:20:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:20:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:20:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 01:20:26 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=141.852249, avg_loss=0.709261, seen=200, correct=107, accuracy=0.535000
2025-09-14 01:20:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:20:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:20:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:20:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:20:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:20:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:20:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:20:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:20:30 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.556522, avg_loss=0.763913, seen=40, correct=18, accuracy=0.450000
2025-09-14 01:20:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:20:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:20:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:20:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:20:40 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 01:20:40 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 01:20:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 01:20:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:20:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:20:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 01:20:48 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=140.731857, avg_loss=0.703659, seen=200, correct=106, accuracy=0.530000
2025-09-14 01:20:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:20:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:20:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:20:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:20:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:20:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:20:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:20:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:20:52 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.446152, avg_loss=0.761154, seen=40, correct=18, accuracy=0.450000
2025-09-14 01:20:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:20:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:20:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:20:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:20:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 01:20:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 01:20:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:20:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:20:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:20:54 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #25', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 01:20:54 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #25', 'Round': 0, 'Results_raw': {}}
2025-09-14 01:20:54 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 01:20:54 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-14 01:20:55 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 01:20:55 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 01:20:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-09-14 01:20:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:20:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=29, num_train_batch_last_epoch=13, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:20:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-09-14 01:20:56 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=57, loss_sum=37.233315, avg_loss=0.653216, seen=57, correct=34, accuracy=0.596491
2025-09-14 01:20:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:20:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:20:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:20:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2200MB
2025-09-14 01:20:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:20:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:20:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:20:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:20:59 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.211777, avg_loss=0.580294, seen=40, correct=27, accuracy=0.675000
2025-09-14 01:20:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:20:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:20:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:21:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2200MB
2025-09-14 01:21:00 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 01:21:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=272, total=1088)
2025-09-14 01:21:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:21:00 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 01:21:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:21:00 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=136, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 01:21:10 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 01:21:11 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 01:21:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-09-14 01:21:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:21:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=29, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:21:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-09-14 01:21:14 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=57, loss_sum=39.358627, avg_loss=0.690502, seen=57, correct=34, accuracy=0.596491
2025-09-14 01:21:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:21:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:21:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:21:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2354MB allocated=2217MB
2025-09-14 01:21:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:21:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:21:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:21:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:21:17 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.145582, avg_loss=0.578640, seen=40, correct=30, accuracy=0.750000
2025-09-14 01:21:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:21:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:21:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:21:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2354MB allocated=2217MB
2025-09-14 01:21:27 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 01:21:28 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 01:21:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-09-14 01:21:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:21:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=29, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:21:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-09-14 01:21:31 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=57, loss_sum=37.220695, avg_loss=0.652995, seen=57, correct=33, accuracy=0.578947
2025-09-14 01:21:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:21:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:21:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:21:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2354MB allocated=2217MB
2025-09-14 01:21:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:21:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:21:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:21:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:21:35 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=22.984100, avg_loss=0.574603, seen=40, correct=30, accuracy=0.750000
2025-09-14 01:21:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:21:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:21:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:21:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2354MB allocated=2217MB
2025-09-14 01:21:47 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 01:21:47 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 01:21:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-09-14 01:21:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:21:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=29, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:21:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-09-14 01:21:49 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=57, loss_sum=36.865730, avg_loss=0.646767, seen=57, correct=35, accuracy=0.614035
2025-09-14 01:21:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:21:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:21:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:21:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2354MB allocated=2217MB
2025-09-14 01:21:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:21:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:21:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:21:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:21:53 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.505173, avg_loss=0.587629, seen=40, correct=31, accuracy=0.775000
2025-09-14 01:21:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:21:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:21:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:21:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2354MB allocated=2217MB
2025-09-14 01:22:03 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 01:22:04 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 01:22:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-09-14 01:22:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:22:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=29, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:22:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-09-14 01:22:07 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=57, loss_sum=35.707138, avg_loss=0.626441, seen=57, correct=35, accuracy=0.614035
2025-09-14 01:22:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:22:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:22:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:22:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2354MB allocated=2217MB
2025-09-14 01:22:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:22:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:22:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:22:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:22:09 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.223988, avg_loss=0.580600, seen=40, correct=28, accuracy=0.700000
2025-09-14 01:22:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:22:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:22:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:22:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2354MB allocated=2217MB
2025-09-14 01:22:20 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 01:22:20 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 01:22:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-09-14 01:22:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:22:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=29, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:22:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-09-14 01:22:23 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=57, loss_sum=36.060715, avg_loss=0.632644, seen=57, correct=35, accuracy=0.614035
2025-09-14 01:22:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:22:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:22:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:22:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2354MB allocated=2217MB
2025-09-14 01:22:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:22:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:22:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:22:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:22:26 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=22.980789, avg_loss=0.574520, seen=40, correct=27, accuracy=0.675000
2025-09-14 01:22:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:22:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:22:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:22:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2354MB allocated=2217MB
2025-09-14 01:22:36 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 01:22:37 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 01:22:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-09-14 01:22:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:22:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=29, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:22:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-09-14 01:22:40 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=57, loss_sum=38.147961, avg_loss=0.669262, seen=57, correct=31, accuracy=0.543860
2025-09-14 01:22:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:22:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:22:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:22:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2354MB allocated=2217MB
2025-09-14 01:22:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:22:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:22:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:22:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:22:43 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.206427, avg_loss=0.580161, seen=40, correct=29, accuracy=0.725000
2025-09-14 01:22:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:22:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:22:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:22:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2354MB allocated=2217MB
2025-09-14 01:22:52 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 01:22:54 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 01:22:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-09-14 01:22:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:22:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=29, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:22:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-09-14 01:22:56 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=57, loss_sum=39.405628, avg_loss=0.691327, seen=57, correct=33, accuracy=0.578947
2025-09-14 01:22:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:22:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:22:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:22:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2354MB allocated=2217MB
2025-09-14 01:22:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:22:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:22:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:22:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:22:59 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.017082, avg_loss=0.600427, seen=40, correct=28, accuracy=0.700000
2025-09-14 01:22:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:22:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:23:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:23:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2354MB allocated=2217MB
2025-09-14 01:23:11 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 01:23:11 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 01:23:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-09-14 01:23:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:23:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=29, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:23:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-09-14 01:23:13 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=57, loss_sum=38.752228, avg_loss=0.679864, seen=57, correct=36, accuracy=0.631579
2025-09-14 01:23:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:23:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:23:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:23:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2354MB allocated=2217MB
2025-09-14 01:23:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:23:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:23:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:23:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:23:17 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.850410, avg_loss=0.596260, seen=40, correct=29, accuracy=0.725000
2025-09-14 01:23:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:23:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:23:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:23:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2354MB allocated=2217MB
2025-09-14 01:23:28 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 01:23:30 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 01:23:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-09-14 01:23:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:23:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=29, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:23:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-09-14 01:23:33 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=57, loss_sum=37.510086, avg_loss=0.658072, seen=57, correct=35, accuracy=0.614035
2025-09-14 01:23:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:23:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:23:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:23:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2354MB allocated=2217MB
2025-09-14 01:23:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:23:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:23:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:23:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:23:36 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.424896, avg_loss=0.585622, seen=40, correct=30, accuracy=0.750000
2025-09-14 01:23:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:23:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:23:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:23:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2354MB allocated=2217MB
2025-09-14 01:23:47 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 01:23:48 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 01:23:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-09-14 01:23:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:23:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=29, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:23:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-09-14 01:23:50 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=57, loss_sum=36.187679, avg_loss=0.634872, seen=57, correct=35, accuracy=0.614035
2025-09-14 01:23:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:23:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:23:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:23:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2354MB allocated=2217MB
2025-09-14 01:23:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:23:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:23:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:23:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:23:53 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.667870, avg_loss=0.591697, seen=40, correct=30, accuracy=0.750000
2025-09-14 01:23:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:23:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:23:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:23:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2354MB allocated=2217MB
2025-09-14 01:23:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 01:23:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 01:23:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:23:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:23:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2354MB allocated=2217MB
2025-09-14 01:23:55 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #7', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 01:23:55 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #7', 'Round': 0, 'Results_raw': {}}
2025-09-14 01:23:55 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 01:23:55 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-14 01:23:56 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 01:23:56 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 01:23:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 01:23:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:23:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:24:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 01:24:02 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=133.821930, avg_loss=0.669110, seen=200, correct=114, accuracy=0.570000
2025-09-14 01:24:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:24:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:24:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:24:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2200MB
2025-09-14 01:24:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:24:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:24:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:24:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:24:05 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=31.407673, avg_loss=0.785192, seen=40, correct=20, accuracy=0.500000
2025-09-14 01:24:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:24:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:24:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:24:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2200MB
2025-09-14 01:24:07 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 01:24:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1236, total=4944)
2025-09-14 01:24:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:24:07 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 01:24:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:24:07 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=618, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 01:24:17 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 01:24:17 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 01:24:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 01:24:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:24:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:24:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 01:24:24 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=131.606064, avg_loss=0.658030, seen=200, correct=117, accuracy=0.585000
2025-09-14 01:24:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:24:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:24:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:24:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2217MB
2025-09-14 01:24:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:24:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:24:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:24:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:24:28 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.870701, avg_loss=0.771768, seen=40, correct=19, accuracy=0.475000
2025-09-14 01:24:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:24:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:24:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:24:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:24:37 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 01:24:39 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 01:24:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 01:24:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:24:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:24:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 01:24:48 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=131.313171, avg_loss=0.656566, seen=200, correct=118, accuracy=0.590000
2025-09-14 01:24:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:24:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:24:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:24:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:24:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:24:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:24:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:24:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:24:51 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.900465, avg_loss=0.772512, seen=40, correct=21, accuracy=0.525000
2025-09-14 01:24:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:24:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:24:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:24:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:25:01 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 01:25:02 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 01:25:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 01:25:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:25:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:25:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 01:25:09 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=130.431580, avg_loss=0.652158, seen=200, correct=120, accuracy=0.600000
2025-09-14 01:25:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:25:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:25:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:25:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:25:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:25:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:25:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:25:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:25:13 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.593147, avg_loss=0.764829, seen=40, correct=21, accuracy=0.525000
2025-09-14 01:25:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:25:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:25:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:25:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:25:24 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 01:25:24 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 01:25:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 01:25:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:25:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:25:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 01:25:32 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=129.775925, avg_loss=0.648880, seen=200, correct=121, accuracy=0.605000
2025-09-14 01:25:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:25:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:25:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:25:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:25:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:25:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:25:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:25:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:25:36 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.586535, avg_loss=0.764663, seen=40, correct=20, accuracy=0.500000
2025-09-14 01:25:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:25:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:25:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:25:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:25:47 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 01:25:47 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 01:25:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 01:25:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:25:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:25:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 01:25:55 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=129.686722, avg_loss=0.648434, seen=200, correct=117, accuracy=0.585000
2025-09-14 01:25:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:25:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:25:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:25:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:25:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:25:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:25:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:25:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:25:59 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.093357, avg_loss=0.752334, seen=40, correct=20, accuracy=0.500000
2025-09-14 01:25:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:25:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:25:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:26:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:26:08 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 01:26:09 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 01:26:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 01:26:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:26:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:26:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 01:26:17 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=130.274445, avg_loss=0.651372, seen=200, correct=120, accuracy=0.600000
2025-09-14 01:26:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:26:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:26:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:26:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:26:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:26:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:26:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:26:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:26:20 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.384005, avg_loss=0.759600, seen=40, correct=19, accuracy=0.475000
2025-09-14 01:26:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:26:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:26:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:26:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:26:30 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 01:26:32 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 01:26:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 01:26:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:26:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:26:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 01:26:39 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=130.478699, avg_loss=0.652393, seen=200, correct=121, accuracy=0.605000
2025-09-14 01:26:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:26:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:26:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:26:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:26:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:26:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:26:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:26:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:26:43 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.945816, avg_loss=0.748645, seen=40, correct=20, accuracy=0.500000
2025-09-14 01:26:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:26:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:26:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:26:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:26:53 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 01:26:54 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 01:26:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 01:26:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:26:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:27:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 01:27:01 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=130.146637, avg_loss=0.650733, seen=200, correct=120, accuracy=0.600000
2025-09-14 01:27:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:27:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:27:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:27:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:27:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:27:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:27:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:27:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:27:05 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.384521, avg_loss=0.734613, seen=40, correct=20, accuracy=0.500000
2025-09-14 01:27:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:27:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:27:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:27:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:27:17 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 01:27:17 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 01:27:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 01:27:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:27:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:27:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 01:27:24 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=131.803192, avg_loss=0.659016, seen=200, correct=118, accuracy=0.590000
2025-09-14 01:27:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:27:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:27:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:27:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:27:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:27:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:27:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:27:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:27:27 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.277861, avg_loss=0.731947, seen=40, correct=21, accuracy=0.525000
2025-09-14 01:27:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:27:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:27:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:27:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:27:38 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 01:27:38 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 01:27:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 01:27:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:27:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:27:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 01:27:44 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=131.480698, avg_loss=0.657403, seen=200, correct=116, accuracy=0.580000
2025-09-14 01:27:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:27:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:27:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:27:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:27:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:27:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:27:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:27:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:27:47 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.962912, avg_loss=0.724073, seen=40, correct=22, accuracy=0.550000
2025-09-14 01:27:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:27:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:27:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:27:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:27:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 01:27:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 01:27:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:27:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:27:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:27:50 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #24', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 01:27:50 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #24', 'Round': 0, 'Results_raw': {}}
2025-09-14 01:27:50 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 01:27:50 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 1 for training...
2025-09-14 01:27:50 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 01:27:50 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 01:27:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 01:27:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:27:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:27:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 01:27:57 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=138.589188, avg_loss=0.692946, seen=200, correct=121, accuracy=0.605000
2025-09-14 01:27:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:27:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:27:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:27:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2200MB
2025-09-14 01:27:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:28:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:28:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:28:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:28:01 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.346579, avg_loss=0.633664, seen=40, correct=27, accuracy=0.675000
2025-09-14 01:28:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:28:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:28:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:28:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2200MB
2025-09-14 01:28:02 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 01:28:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1069, total=4273)
2025-09-14 01:28:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:28:02 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 01:28:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:28:02 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=535, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 01:28:12 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 01:28:13 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 01:28:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 01:28:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:28:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:28:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 01:28:21 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=135.851974, avg_loss=0.679260, seen=200, correct=118, accuracy=0.590000
2025-09-14 01:28:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:28:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:28:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:28:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:28:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:28:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:28:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:28:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:28:25 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.016762, avg_loss=0.625419, seen=40, correct=27, accuracy=0.675000
2025-09-14 01:28:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:28:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:28:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:28:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:28:36 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 01:28:37 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 01:28:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 01:28:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:28:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:28:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 01:28:45 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=138.439209, avg_loss=0.692196, seen=200, correct=118, accuracy=0.590000
2025-09-14 01:28:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:28:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:28:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:28:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:28:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:28:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:28:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:28:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:28:49 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.511126, avg_loss=0.687778, seen=40, correct=25, accuracy=0.625000
2025-09-14 01:28:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:28:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:28:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:28:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:29:00 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 01:29:00 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 01:29:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 01:29:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:29:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:29:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 01:29:09 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=139.346115, avg_loss=0.696731, seen=200, correct=117, accuracy=0.585000
2025-09-14 01:29:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:29:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:29:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:29:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:29:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:29:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:29:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:29:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:29:13 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.432796, avg_loss=0.685820, seen=40, correct=23, accuracy=0.575000
2025-09-14 01:29:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:29:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:29:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:29:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:29:24 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 01:29:25 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 01:29:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 01:29:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:29:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:29:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 01:29:34 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=137.054169, avg_loss=0.685271, seen=200, correct=115, accuracy=0.575000
2025-09-14 01:29:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:29:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:29:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:29:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:29:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:29:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:29:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:29:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:29:38 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.568501, avg_loss=0.664213, seen=40, correct=26, accuracy=0.650000
2025-09-14 01:29:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:29:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:29:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:29:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:29:49 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 01:29:49 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 01:29:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 01:29:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:29:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:29:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 01:29:58 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=135.763580, avg_loss=0.678818, seen=200, correct=114, accuracy=0.570000
2025-09-14 01:29:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:29:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:29:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:30:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:30:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:30:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:30:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:30:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:30:02 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.377039, avg_loss=0.659426, seen=40, correct=28, accuracy=0.700000
2025-09-14 01:30:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:30:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:30:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:30:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:30:13 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 01:30:13 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 01:30:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 01:30:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:30:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:30:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 01:30:22 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=134.929565, avg_loss=0.674648, seen=200, correct=113, accuracy=0.565000
2025-09-14 01:30:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:30:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:30:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:30:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:30:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:30:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:30:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:30:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:30:26 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.666092, avg_loss=0.641652, seen=40, correct=26, accuracy=0.650000
2025-09-14 01:30:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:30:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:30:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:30:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:30:38 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 01:30:38 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 01:30:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 01:30:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:30:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:30:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 01:30:45 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=135.216187, avg_loss=0.676081, seen=200, correct=118, accuracy=0.590000
2025-09-14 01:30:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:30:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:30:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:30:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:30:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:30:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:30:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:30:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:30:48 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.785793, avg_loss=0.619645, seen=40, correct=26, accuracy=0.650000
2025-09-14 01:30:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:30:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:30:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:30:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:31:00 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 01:31:00 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 01:31:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 01:31:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:31:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:31:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 01:31:08 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=134.854858, avg_loss=0.674274, seen=200, correct=112, accuracy=0.560000
2025-09-14 01:31:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:31:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:31:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:31:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:31:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:31:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:31:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:31:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:31:11 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.875336, avg_loss=0.621883, seen=40, correct=26, accuracy=0.650000
2025-09-14 01:31:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:31:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:31:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:31:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:31:23 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 01:31:23 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 01:31:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 01:31:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:31:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:31:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 01:31:31 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=133.214264, avg_loss=0.666071, seen=200, correct=123, accuracy=0.615000
2025-09-14 01:31:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:31:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:31:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:31:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:31:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:31:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:31:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:31:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:31:35 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.677687, avg_loss=0.641942, seen=40, correct=26, accuracy=0.650000
2025-09-14 01:31:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:31:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:31:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:31:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:31:46 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 01:31:46 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 01:31:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 01:31:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:31:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:31:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 01:31:54 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=133.697800, avg_loss=0.668489, seen=200, correct=118, accuracy=0.590000
2025-09-14 01:31:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:31:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:31:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:31:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:31:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:31:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:31:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:31:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:31:57 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.249016, avg_loss=0.656225, seen=40, correct=24, accuracy=0.600000
2025-09-14 01:31:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:31:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:31:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:31:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:31:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 01:31:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 01:31:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:31:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:32:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 01:32:00 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #37', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 01:32:00 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #37', 'Round': 0, 'Results_raw': {}}
2025-09-14 01:32:00 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 01:32:00 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 2 for training...
2025-09-14 01:32:01 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 01:32:01 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 01:32:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 01:32:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:32:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=4, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:32:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 01:32:01 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=6.726081, avg_loss=0.611462, seen=11, correct=8, accuracy=0.727273
2025-09-14 01:32:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:32:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:32:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:32:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2200MB
2025-09-14 01:32:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:32:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:32:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:32:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:32:04 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.364103, avg_loss=0.759103, seen=40, correct=19, accuracy=0.475000
2025-09-14 01:32:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:32:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:32:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:32:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2200MB
2025-09-14 01:32:05 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 01:32:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=56, total=224)
2025-09-14 01:32:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:32:05 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 01:32:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:32:05 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=28, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 01:32:15 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 01:32:15 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 01:32:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 01:32:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:32:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:32:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 01:32:15 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=6.496976, avg_loss=0.590634, seen=11, correct=8, accuracy=0.727273
2025-09-14 01:32:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:32:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:32:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:32:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-14 01:32:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:32:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:32:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:32:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:32:19 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.557831, avg_loss=0.738946, seen=40, correct=17, accuracy=0.425000
2025-09-14 01:32:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:32:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:32:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:32:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-14 01:32:29 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 01:32:29 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 01:32:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 01:32:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:32:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:32:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 01:32:30 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=7.267074, avg_loss=0.660643, seen=11, correct=7, accuracy=0.636364
2025-09-14 01:32:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:32:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:32:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:32:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-14 01:32:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:32:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:32:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:32:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:32:33 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.558716, avg_loss=0.713968, seen=40, correct=19, accuracy=0.475000
2025-09-14 01:32:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:32:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:32:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:32:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-14 01:32:45 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 01:32:45 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 01:32:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 01:32:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:32:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:32:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 01:32:46 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=7.595162, avg_loss=0.690469, seen=11, correct=7, accuracy=0.636364
2025-09-14 01:32:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:32:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:32:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:32:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-14 01:32:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:32:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:32:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:32:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:32:49 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.249424, avg_loss=0.706236, seen=40, correct=19, accuracy=0.475000
2025-09-14 01:32:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:32:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:32:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:32:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-14 01:32:59 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 01:32:59 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 01:32:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 01:32:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:32:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:33:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 01:33:00 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=7.290320, avg_loss=0.662756, seen=11, correct=7, accuracy=0.636364
2025-09-14 01:33:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:33:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:33:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:33:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-14 01:33:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:33:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:33:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:33:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:33:03 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.536486, avg_loss=0.713412, seen=40, correct=20, accuracy=0.500000
2025-09-14 01:33:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:33:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:33:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:33:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-14 01:33:14 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 01:33:14 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 01:33:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 01:33:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:33:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:33:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 01:33:15 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=6.911975, avg_loss=0.628361, seen=11, correct=7, accuracy=0.636364
2025-09-14 01:33:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:33:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:33:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:33:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-14 01:33:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:33:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:33:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:33:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:33:18 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.505556, avg_loss=0.712639, seen=40, correct=21, accuracy=0.525000
2025-09-14 01:33:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:33:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:33:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:33:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-14 01:33:30 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 01:33:30 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 01:33:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 01:33:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:33:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:33:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 01:33:31 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=6.880432, avg_loss=0.625494, seen=11, correct=7, accuracy=0.636364
2025-09-14 01:33:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:33:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:33:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:33:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-14 01:33:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:33:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:33:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:33:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:33:34 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.263960, avg_loss=0.706599, seen=40, correct=24, accuracy=0.600000
2025-09-14 01:33:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:33:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:33:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:33:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-14 01:33:45 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 01:33:46 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 01:33:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 01:33:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:33:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:33:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 01:33:46 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=6.777710, avg_loss=0.616155, seen=11, correct=8, accuracy=0.727273
2025-09-14 01:33:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:33:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:33:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:33:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-14 01:33:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:33:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:33:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:33:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:33:50 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.514729, avg_loss=0.712868, seen=40, correct=23, accuracy=0.575000
2025-09-14 01:33:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:33:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:33:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:33:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-14 01:34:02 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 01:34:02 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 01:34:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 01:34:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:34:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:34:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 01:34:03 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=6.713953, avg_loss=0.610359, seen=11, correct=8, accuracy=0.727273
2025-09-14 01:34:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:34:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:34:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:34:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-14 01:34:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:34:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:34:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:34:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:34:07 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.581068, avg_loss=0.714527, seen=40, correct=21, accuracy=0.525000
2025-09-14 01:34:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:34:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:34:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:34:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-14 01:34:18 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 01:34:19 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 01:34:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 01:34:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:34:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:34:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 01:34:19 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=6.806507, avg_loss=0.618773, seen=11, correct=7, accuracy=0.636364
2025-09-14 01:34:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:34:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:34:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:34:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-14 01:34:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:34:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:34:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:34:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:34:24 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.760305, avg_loss=0.719008, seen=40, correct=20, accuracy=0.500000
2025-09-14 01:34:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:34:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:34:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:34:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-14 01:34:36 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 01:34:36 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 01:34:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 01:34:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:34:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:34:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 01:34:36 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=6.585362, avg_loss=0.598669, seen=11, correct=8, accuracy=0.727273
2025-09-14 01:34:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:34:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:34:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:34:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-14 01:34:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:34:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:34:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:34:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:34:40 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.990307, avg_loss=0.724758, seen=40, correct=22, accuracy=0.550000
2025-09-14 01:34:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:34:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:34:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:34:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-14 01:34:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 01:34:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 01:34:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:34:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:34:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-14 01:34:42 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #22', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 01:34:42 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #22', 'Round': 0, 'Results_raw': {}}
2025-09-14 01:34:42 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 01:34:42 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 1 for training...
2025-09-14 01:34:43 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 01:34:43 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 01:34:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-09-14 01:34:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:34:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=63, num_train_batch_last_epoch=37, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:34:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-09-14 01:34:47 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=126, loss_sum=87.442879, avg_loss=0.693991, seen=126, correct=76, accuracy=0.603175
2025-09-14 01:34:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:34:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:34:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:34:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2200MB
2025-09-14 01:34:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:34:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:34:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:34:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:34:49 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.069031, avg_loss=0.626726, seen=40, correct=25, accuracy=0.625000
2025-09-14 01:34:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:34:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:34:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:34:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2200MB
2025-09-14 01:34:50 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 01:34:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=600, total=2399)
2025-09-14 01:34:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:34:51 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 01:34:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:34:51 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=300, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 01:35:00 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 01:35:00 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 01:35:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-09-14 01:35:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:35:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=63, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:35:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-09-14 01:35:05 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=126, loss_sum=85.191895, avg_loss=0.676126, seen=126, correct=78, accuracy=0.619048
2025-09-14 01:35:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:35:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:35:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:35:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2370MB allocated=2217MB
2025-09-14 01:35:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:35:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:35:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:35:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:35:09 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.762693, avg_loss=0.619067, seen=40, correct=28, accuracy=0.700000
2025-09-14 01:35:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:35:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:35:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:35:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2370MB allocated=2217MB
2025-09-14 01:35:19 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 01:35:21 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 01:35:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-09-14 01:35:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:35:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=63, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:35:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-09-14 01:35:27 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=126, loss_sum=87.146339, avg_loss=0.691638, seen=126, correct=78, accuracy=0.619048
2025-09-14 01:35:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:35:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:35:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:35:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2370MB allocated=2217MB
2025-09-14 01:35:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:35:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:35:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:35:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:35:31 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.144375, avg_loss=0.628609, seen=40, correct=29, accuracy=0.725000
2025-09-14 01:35:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:35:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:35:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:35:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2370MB allocated=2217MB
2025-09-14 01:35:42 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 01:35:43 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 01:35:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-09-14 01:35:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:35:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=63, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:35:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-09-14 01:35:49 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=126, loss_sum=85.884796, avg_loss=0.681625, seen=126, correct=74, accuracy=0.587302
2025-09-14 01:35:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:35:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:35:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:35:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2370MB allocated=2217MB
2025-09-14 01:35:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:35:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:35:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:35:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:35:51 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.690586, avg_loss=0.617265, seen=40, correct=30, accuracy=0.750000
2025-09-14 01:35:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:35:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:35:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:35:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2370MB allocated=2217MB
2025-09-14 01:36:02 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 01:36:02 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 01:36:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-09-14 01:36:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:36:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=63, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:36:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-09-14 01:36:08 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=126, loss_sum=84.097534, avg_loss=0.667441, seen=126, correct=78, accuracy=0.619048
2025-09-14 01:36:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:36:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:36:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:36:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2370MB allocated=2217MB
2025-09-14 01:36:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:36:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:36:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:36:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:36:12 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.812771, avg_loss=0.620319, seen=40, correct=28, accuracy=0.700000
2025-09-14 01:36:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:36:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:36:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:36:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2370MB allocated=2217MB
2025-09-14 01:36:24 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 01:36:24 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 01:36:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-09-14 01:36:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:36:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=63, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:36:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-09-14 01:36:29 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=126, loss_sum=86.380646, avg_loss=0.685561, seen=126, correct=74, accuracy=0.587302
2025-09-14 01:36:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:36:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:36:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:36:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2370MB allocated=2217MB
2025-09-14 01:36:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:36:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:36:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:36:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:36:32 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.249868, avg_loss=0.631247, seen=40, correct=29, accuracy=0.725000
2025-09-14 01:36:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:36:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:36:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:36:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2370MB allocated=2217MB
2025-09-14 01:36:44 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 01:36:44 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 01:36:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-09-14 01:36:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:36:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=63, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:36:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-09-14 01:36:48 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=126, loss_sum=87.535583, avg_loss=0.694727, seen=126, correct=70, accuracy=0.555556
2025-09-14 01:36:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:36:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:36:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:36:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2370MB allocated=2217MB
2025-09-14 01:36:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:36:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:36:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:36:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:36:52 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.516582, avg_loss=0.637915, seen=40, correct=26, accuracy=0.650000
2025-09-14 01:36:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:36:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:36:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:36:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2370MB allocated=2217MB
2025-09-14 01:37:02 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 01:37:03 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 01:37:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-09-14 01:37:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:37:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=63, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:37:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-09-14 01:37:08 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=126, loss_sum=88.121338, avg_loss=0.699376, seen=126, correct=69, accuracy=0.547619
2025-09-14 01:37:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:37:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:37:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:37:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2370MB allocated=2217MB
2025-09-14 01:37:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:37:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:37:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:37:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:37:12 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.294853, avg_loss=0.632371, seen=40, correct=27, accuracy=0.675000
2025-09-14 01:37:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:37:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:37:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:37:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2370MB allocated=2217MB
2025-09-14 01:37:23 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 01:37:23 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 01:37:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-09-14 01:37:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:37:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=63, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:37:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-09-14 01:37:28 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=126, loss_sum=88.494614, avg_loss=0.702338, seen=126, correct=70, accuracy=0.555556
2025-09-14 01:37:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:37:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:37:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:37:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2370MB allocated=2217MB
2025-09-14 01:37:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:37:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:37:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:37:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:37:32 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.269350, avg_loss=0.631734, seen=40, correct=25, accuracy=0.625000
2025-09-14 01:37:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:37:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:37:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:37:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2370MB allocated=2217MB
2025-09-14 01:37:42 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 01:37:44 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 01:37:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-09-14 01:37:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:37:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=63, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:37:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-09-14 01:37:48 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=126, loss_sum=87.748642, avg_loss=0.696418, seen=126, correct=72, accuracy=0.571429
2025-09-14 01:37:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:37:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:37:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:37:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2370MB allocated=2217MB
2025-09-14 01:37:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:37:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:37:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:37:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:37:52 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.601765, avg_loss=0.615044, seen=40, correct=28, accuracy=0.700000
2025-09-14 01:37:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:37:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:37:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:37:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2370MB allocated=2217MB
2025-09-14 01:38:03 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 01:38:04 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 01:38:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-09-14 01:38:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:38:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=63, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:38:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-09-14 01:38:09 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=126, loss_sum=86.184433, avg_loss=0.684003, seen=126, correct=75, accuracy=0.595238
2025-09-14 01:38:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:38:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:38:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:38:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2370MB allocated=2217MB
2025-09-14 01:38:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:38:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:38:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:38:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:38:13 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.194605, avg_loss=0.604865, seen=40, correct=30, accuracy=0.750000
2025-09-14 01:38:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:38:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:38:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:38:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2370MB allocated=2217MB
2025-09-14 01:38:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 01:38:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 01:38:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:38:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:38:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2370MB allocated=2217MB
2025-09-14 01:38:15 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #20', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 01:38:15 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #20', 'Round': 0, 'Results_raw': {}}
2025-09-14 01:38:15 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 01:38:15 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-14 01:38:16 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 01:38:16 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 01:38:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-09-14 01:38:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:38:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=32, num_train_batch_last_epoch=4, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:38:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-09-14 01:38:18 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=63, loss_sum=39.871059, avg_loss=0.632874, seen=63, correct=41, accuracy=0.650794
2025-09-14 01:38:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:38:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:38:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:38:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2200MB
2025-09-14 01:38:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:38:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:38:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:38:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:38:21 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=22.495314, avg_loss=0.562383, seen=40, correct=30, accuracy=0.750000
2025-09-14 01:38:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:38:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:38:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:38:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2200MB
2025-09-14 01:38:22 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 01:38:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=303, total=1209)
2025-09-14 01:38:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:38:22 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 01:38:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:38:22 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=152, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 01:38:32 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 01:38:33 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 01:38:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-09-14 01:38:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:38:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=32, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:38:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-09-14 01:38:36 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=63, loss_sum=42.386288, avg_loss=0.672798, seen=63, correct=41, accuracy=0.650794
2025-09-14 01:38:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:38:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:38:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:38:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2366MB allocated=2217MB
2025-09-14 01:38:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:38:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:38:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:38:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:38:40 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=22.094103, avg_loss=0.552353, seen=40, correct=32, accuracy=0.800000
2025-09-14 01:38:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:38:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:38:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:38:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2366MB allocated=2217MB
2025-09-14 01:38:51 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 01:38:51 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 01:38:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-09-14 01:38:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:38:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=32, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:38:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-09-14 01:38:53 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=63, loss_sum=43.196903, avg_loss=0.685665, seen=63, correct=37, accuracy=0.587302
2025-09-14 01:38:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:38:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:38:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:38:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2366MB allocated=2217MB
2025-09-14 01:38:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:38:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:38:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:38:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:38:56 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=22.811937, avg_loss=0.570298, seen=40, correct=30, accuracy=0.750000
2025-09-14 01:38:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:38:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:38:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:38:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2366MB allocated=2217MB
2025-09-14 01:39:07 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 01:39:07 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 01:39:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-09-14 01:39:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:39:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=32, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:39:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-09-14 01:39:09 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=63, loss_sum=43.157082, avg_loss=0.685033, seen=63, correct=39, accuracy=0.619048
2025-09-14 01:39:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:39:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:39:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:39:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2366MB allocated=2217MB
2025-09-14 01:39:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:39:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:39:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:39:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:39:13 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.081173, avg_loss=0.577029, seen=40, correct=31, accuracy=0.775000
2025-09-14 01:39:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:39:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:39:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:39:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2366MB allocated=2217MB
2025-09-14 01:39:23 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 01:39:24 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 01:39:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-09-14 01:39:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:39:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=32, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:39:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-09-14 01:39:26 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=63, loss_sum=43.542561, avg_loss=0.691152, seen=63, correct=41, accuracy=0.650794
2025-09-14 01:39:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:39:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:39:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:39:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2366MB allocated=2217MB
2025-09-14 01:39:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:39:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:39:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:39:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:39:29 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.207933, avg_loss=0.580198, seen=40, correct=29, accuracy=0.725000
2025-09-14 01:39:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:39:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:39:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:39:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2366MB allocated=2217MB
2025-09-14 01:39:37 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 01:39:38 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 01:39:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-09-14 01:39:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:39:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=32, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:39:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-09-14 01:39:41 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=63, loss_sum=43.286156, avg_loss=0.687082, seen=63, correct=38, accuracy=0.603175
2025-09-14 01:39:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:39:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:39:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:39:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2366MB allocated=2217MB
2025-09-14 01:39:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:39:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:39:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:39:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:39:44 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=22.716770, avg_loss=0.567919, seen=40, correct=32, accuracy=0.800000
2025-09-14 01:39:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:39:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:39:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:39:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2366MB allocated=2217MB
2025-09-14 01:39:55 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 01:39:55 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 01:39:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-09-14 01:39:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:39:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=32, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:39:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-09-14 01:39:58 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=63, loss_sum=41.873123, avg_loss=0.664653, seen=63, correct=42, accuracy=0.666667
2025-09-14 01:39:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:39:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:39:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:40:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2366MB allocated=2217MB
2025-09-14 01:40:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:40:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:40:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:40:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:40:01 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=22.027782, avg_loss=0.550695, seen=40, correct=31, accuracy=0.775000
2025-09-14 01:40:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:40:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:40:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:40:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2366MB allocated=2217MB
2025-09-14 01:40:10 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 01:40:12 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 01:40:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-09-14 01:40:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:40:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=32, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:40:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-09-14 01:40:14 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=63, loss_sum=40.527519, avg_loss=0.643294, seen=63, correct=39, accuracy=0.619048
2025-09-14 01:40:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:40:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:40:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:40:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2366MB allocated=2217MB
2025-09-14 01:40:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:40:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:40:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:40:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:40:17 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=21.572737, avg_loss=0.539318, seen=40, correct=30, accuracy=0.750000
2025-09-14 01:40:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:40:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:40:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:40:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2366MB allocated=2217MB
2025-09-14 01:40:28 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 01:40:28 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 01:40:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-09-14 01:40:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:40:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=32, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:40:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-09-14 01:40:31 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=63, loss_sum=40.271557, avg_loss=0.639231, seen=63, correct=40, accuracy=0.634921
2025-09-14 01:40:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:40:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:40:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:40:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2366MB allocated=2217MB
2025-09-14 01:40:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:40:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:40:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:40:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:40:35 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=21.037689, avg_loss=0.525942, seen=40, correct=31, accuracy=0.775000
2025-09-14 01:40:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:40:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:40:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:40:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2366MB allocated=2217MB
2025-09-14 01:40:44 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 01:40:45 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 01:40:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-09-14 01:40:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:40:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=32, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:40:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-09-14 01:40:47 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=63, loss_sum=40.873863, avg_loss=0.648791, seen=63, correct=41, accuracy=0.650794
2025-09-14 01:40:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:40:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:40:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:40:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2366MB allocated=2217MB
2025-09-14 01:40:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:40:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:40:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:40:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:40:50 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=20.789127, avg_loss=0.519728, seen=40, correct=33, accuracy=0.825000
2025-09-14 01:40:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:40:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:40:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:40:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2366MB allocated=2217MB
2025-09-14 01:40:59 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 01:41:00 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 01:41:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-09-14 01:41:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:41:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=32, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:41:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-09-14 01:41:02 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=63, loss_sum=42.891899, avg_loss=0.680824, seen=63, correct=41, accuracy=0.650794
2025-09-14 01:41:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:41:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:41:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:41:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2366MB allocated=2217MB
2025-09-14 01:41:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:41:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:41:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:41:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:41:05 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=22.114168, avg_loss=0.552854, seen=40, correct=31, accuracy=0.775000
2025-09-14 01:41:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:41:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:41:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:41:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2366MB allocated=2217MB
2025-09-14 01:41:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 01:41:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 01:41:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:41:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:41:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2366MB allocated=2217MB
2025-09-14 01:41:09 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #10', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 01:41:09 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #10', 'Round': 0, 'Results_raw': {}}
2025-09-14 01:41:09 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 01:41:09 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 1 for training...
2025-09-14 01:41:10 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 01:41:10 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 01:41:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 01:41:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:41:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:41:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 01:41:15 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=126.999008, avg_loss=0.634995, seen=200, correct=125, accuracy=0.625000
2025-09-14 01:41:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:41:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:41:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:41:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2200MB
2025-09-14 01:41:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:41:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:41:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:41:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:41:18 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.684296, avg_loss=0.667107, seen=40, correct=24, accuracy=0.600000
2025-09-14 01:41:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:41:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:41:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:41:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2200MB
2025-09-14 01:41:20 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 01:41:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1002, total=4005)
2025-09-14 01:41:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:41:20 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 01:41:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:41:20 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=501, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 01:41:31 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 01:41:31 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 01:41:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 01:41:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:41:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:41:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 01:41:39 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=126.135345, avg_loss=0.630677, seen=200, correct=126, accuracy=0.630000
2025-09-14 01:41:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:41:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:41:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:41:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-14 01:41:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:41:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:41:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:41:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:41:43 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.992508, avg_loss=0.649813, seen=40, correct=26, accuracy=0.650000
2025-09-14 01:41:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:41:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:41:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:41:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-14 01:41:54 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 01:41:56 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 01:41:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 01:41:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:41:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:42:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 01:42:03 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=127.149673, avg_loss=0.635748, seen=200, correct=127, accuracy=0.635000
2025-09-14 01:42:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:42:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:42:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:42:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-14 01:42:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:42:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:42:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:42:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:42:08 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.913145, avg_loss=0.647829, seen=40, correct=24, accuracy=0.600000
2025-09-14 01:42:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:42:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:42:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:42:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-14 01:42:17 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 01:42:19 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 01:42:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 01:42:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:42:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:42:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 01:42:26 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=127.431656, avg_loss=0.637158, seen=200, correct=124, accuracy=0.620000
2025-09-14 01:42:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:42:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:42:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:42:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-14 01:42:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:42:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:42:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:42:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:42:29 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.392035, avg_loss=0.634801, seen=40, correct=25, accuracy=0.625000
2025-09-14 01:42:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:42:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:42:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:42:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-14 01:42:40 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 01:42:40 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 01:42:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 01:42:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:42:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:42:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 01:42:48 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=128.338272, avg_loss=0.641691, seen=200, correct=127, accuracy=0.635000
2025-09-14 01:42:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:42:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:42:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:42:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-14 01:42:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:42:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:42:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:42:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:42:52 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.036730, avg_loss=0.650918, seen=40, correct=26, accuracy=0.650000
2025-09-14 01:42:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:42:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:42:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:42:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-14 01:43:03 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 01:43:03 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 01:43:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 01:43:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:43:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:43:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 01:43:10 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=127.386749, avg_loss=0.636934, seen=200, correct=125, accuracy=0.625000
2025-09-14 01:43:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:43:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:43:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:43:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-14 01:43:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:43:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:43:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:43:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:43:14 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.246677, avg_loss=0.656167, seen=40, correct=23, accuracy=0.575000
2025-09-14 01:43:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:43:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:43:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:43:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-14 01:43:23 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 01:43:25 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 01:43:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 01:43:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:43:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:43:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 01:43:33 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=125.402977, avg_loss=0.627015, seen=200, correct=128, accuracy=0.640000
2025-09-14 01:43:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:43:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:43:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:43:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-14 01:43:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:43:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:43:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:43:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:43:35 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.974390, avg_loss=0.649360, seen=40, correct=26, accuracy=0.650000
2025-09-14 01:43:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:43:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:43:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:43:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-14 01:43:43 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 01:43:46 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 01:43:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 01:43:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:43:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:43:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 01:43:53 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=124.916275, avg_loss=0.624581, seen=200, correct=128, accuracy=0.640000
2025-09-14 01:43:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:43:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:43:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:43:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-14 01:43:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:43:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:43:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:43:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:43:57 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.113781, avg_loss=0.652845, seen=40, correct=24, accuracy=0.600000
2025-09-14 01:43:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:43:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:43:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:43:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-14 01:44:08 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 01:44:08 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 01:44:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 01:44:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:44:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:44:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 01:44:17 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=126.020836, avg_loss=0.630104, seen=200, correct=123, accuracy=0.615000
2025-09-14 01:44:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:44:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:44:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:44:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-14 01:44:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:44:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:44:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:44:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:44:21 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.664249, avg_loss=0.666606, seen=40, correct=22, accuracy=0.550000
2025-09-14 01:44:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:44:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:44:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:44:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-14 01:44:31 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 01:44:32 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 01:44:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 01:44:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:44:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:44:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 01:44:41 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=126.408813, avg_loss=0.632044, seen=200, correct=126, accuracy=0.630000
2025-09-14 01:44:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:44:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:44:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:44:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-14 01:44:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:44:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:44:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:44:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:44:45 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.645578, avg_loss=0.666139, seen=40, correct=23, accuracy=0.575000
2025-09-14 01:44:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:44:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:44:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:44:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-14 01:44:53 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 01:44:55 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 01:44:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 01:44:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:44:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:45:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 01:45:02 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=128.729095, avg_loss=0.643645, seen=200, correct=122, accuracy=0.610000
2025-09-14 01:45:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:45:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:45:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:45:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-14 01:45:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:45:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:45:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:45:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:45:06 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.147858, avg_loss=0.678696, seen=40, correct=21, accuracy=0.525000
2025-09-14 01:45:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:45:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:45:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:45:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-14 01:45:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 01:45:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 01:45:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:45:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:45:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2338MB allocated=2217MB
2025-09-14 01:45:08 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #40', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 01:45:08 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #40', 'Round': 0, 'Results_raw': {}}
2025-09-14 01:45:08 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 01:45:08 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-14 01:45:09 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 01:45:09 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 01:45:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-09-14 01:45:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:45:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=33, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:45:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 01:45:13 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=133, loss_sum=92.166473, avg_loss=0.692981, seen=133, correct=72, accuracy=0.541353
2025-09-14 01:45:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:45:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:45:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:45:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2200MB
2025-09-14 01:45:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:45:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:45:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:45:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:45:15 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=31.003998, avg_loss=0.775100, seen=40, correct=19, accuracy=0.475000
2025-09-14 01:45:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:45:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:45:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:45:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2200MB
2025-09-14 01:45:16 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 01:45:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=632, total=2527)
2025-09-14 01:45:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:45:16 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 01:45:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:45:16 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=316, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 01:45:25 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 01:45:25 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 01:45:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-09-14 01:45:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:45:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:45:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 01:45:30 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=133, loss_sum=94.425064, avg_loss=0.709963, seen=133, correct=72, accuracy=0.541353
2025-09-14 01:45:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:45:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:45:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:45:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2217MB
2025-09-14 01:45:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:45:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:45:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:45:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:45:34 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.962200, avg_loss=0.774055, seen=40, correct=23, accuracy=0.575000
2025-09-14 01:45:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:45:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:45:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:45:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2217MB
2025-09-14 01:45:44 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 01:45:46 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 01:45:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-09-14 01:45:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:45:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:45:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 01:45:50 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=133, loss_sum=92.124939, avg_loss=0.692669, seen=133, correct=77, accuracy=0.578947
2025-09-14 01:45:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:45:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:45:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:45:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2217MB
2025-09-14 01:45:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:45:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:45:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:45:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:45:54 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.678917, avg_loss=0.766973, seen=40, correct=21, accuracy=0.525000
2025-09-14 01:45:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:45:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:45:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:45:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2217MB
2025-09-14 01:46:03 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 01:46:03 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 01:46:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-09-14 01:46:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:46:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:46:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 01:46:09 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=133, loss_sum=92.716682, avg_loss=0.697118, seen=133, correct=75, accuracy=0.563910
2025-09-14 01:46:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:46:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:46:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:46:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2217MB
2025-09-14 01:46:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:46:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:46:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:46:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:46:12 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.814157, avg_loss=0.745354, seen=40, correct=18, accuracy=0.450000
2025-09-14 01:46:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:46:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:46:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:46:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2217MB
2025-09-14 01:46:22 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 01:46:23 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 01:46:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-09-14 01:46:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:46:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:46:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 01:46:29 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=133, loss_sum=91.641678, avg_loss=0.689035, seen=133, correct=75, accuracy=0.563910
2025-09-14 01:46:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:46:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:46:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:46:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2217MB
2025-09-14 01:46:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:46:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:46:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:46:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:46:33 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.073130, avg_loss=0.726828, seen=40, correct=21, accuracy=0.525000
2025-09-14 01:46:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:46:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:46:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:46:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2217MB
2025-09-14 01:46:42 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 01:46:43 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 01:46:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-09-14 01:46:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:46:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:46:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 01:46:48 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=133, loss_sum=88.807526, avg_loss=0.667726, seen=133, correct=81, accuracy=0.609023
2025-09-14 01:46:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:46:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:46:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:46:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2217MB
2025-09-14 01:46:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:46:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:46:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:46:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:46:51 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.228428, avg_loss=0.755711, seen=40, correct=18, accuracy=0.450000
2025-09-14 01:46:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:46:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:46:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:46:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2217MB
2025-09-14 01:47:01 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 01:47:03 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 01:47:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-09-14 01:47:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:47:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:47:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 01:47:07 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=133, loss_sum=89.843819, avg_loss=0.675517, seen=133, correct=81, accuracy=0.609023
2025-09-14 01:47:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:47:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:47:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:47:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2217MB
2025-09-14 01:47:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:47:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:47:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:47:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:47:11 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.169283, avg_loss=0.729232, seen=40, correct=17, accuracy=0.425000
2025-09-14 01:47:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:47:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:47:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:47:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2217MB
2025-09-14 01:47:22 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 01:47:23 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 01:47:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-09-14 01:47:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:47:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:47:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 01:47:28 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=133, loss_sum=93.264786, avg_loss=0.701239, seen=133, correct=73, accuracy=0.548872
2025-09-14 01:47:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:47:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:47:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:47:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2217MB
2025-09-14 01:47:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:47:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:47:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:47:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:47:32 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.397884, avg_loss=0.709947, seen=40, correct=19, accuracy=0.475000
2025-09-14 01:47:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:47:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:47:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:47:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2217MB
2025-09-14 01:47:42 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 01:47:43 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 01:47:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-09-14 01:47:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:47:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:47:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 01:47:48 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=133, loss_sum=93.337738, avg_loss=0.701788, seen=133, correct=74, accuracy=0.556391
2025-09-14 01:47:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:47:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:47:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:47:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2217MB
2025-09-14 01:47:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:47:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:47:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:47:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:47:51 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.294361, avg_loss=0.707359, seen=40, correct=16, accuracy=0.400000
2025-09-14 01:47:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:47:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:47:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:47:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2217MB
2025-09-14 01:48:01 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 01:48:02 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 01:48:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-09-14 01:48:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:48:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:48:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 01:48:07 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=133, loss_sum=90.854736, avg_loss=0.683118, seen=133, correct=78, accuracy=0.586466
2025-09-14 01:48:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:48:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:48:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:48:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2217MB
2025-09-14 01:48:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:48:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:48:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:48:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:48:10 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.209015, avg_loss=0.755225, seen=40, correct=19, accuracy=0.475000
2025-09-14 01:48:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:48:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:48:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:48:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2217MB
2025-09-14 01:48:21 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 01:48:21 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 01:48:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-09-14 01:48:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:48:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:48:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 01:48:27 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=133, loss_sum=88.887955, avg_loss=0.668330, seen=133, correct=79, accuracy=0.593985
2025-09-14 01:48:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:48:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:48:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:48:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2217MB
2025-09-14 01:48:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:48:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:48:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:48:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:48:30 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=31.029591, avg_loss=0.775740, seen=40, correct=19, accuracy=0.475000
2025-09-14 01:48:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:48:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:48:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:48:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2217MB
2025-09-14 01:48:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 01:48:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 01:48:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:48:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:48:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2217MB
2025-09-14 01:48:32 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #50', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 01:48:32 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #50', 'Round': 0, 'Results_raw': {}}
2025-09-14 01:48:32 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 01:48:32 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 1 for training...
2025-09-14 01:48:33 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 01:48:33 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 01:48:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 01:48:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:48:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=4, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:48:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 01:48:34 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=6.469686, avg_loss=0.588153, seen=11, correct=9, accuracy=0.818182
2025-09-14 01:48:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:48:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:48:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:48:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2200MB
2025-09-14 01:48:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:48:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:48:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:48:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:48:36 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.768991, avg_loss=0.769225, seen=40, correct=18, accuracy=0.450000
2025-09-14 01:48:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:48:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:48:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:48:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2200MB
2025-09-14 01:48:37 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 01:48:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=54, total=213)
2025-09-14 01:48:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:48:37 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 01:48:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:48:37 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=27, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 01:48:47 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 01:48:48 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 01:48:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 01:48:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:48:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:48:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 01:48:48 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=7.110652, avg_loss=0.646423, seen=11, correct=7, accuracy=0.636364
2025-09-14 01:48:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:48:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:48:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:48:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2362MB allocated=2217MB
2025-09-14 01:48:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:48:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:48:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:48:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:48:52 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.845171, avg_loss=0.696129, seen=40, correct=22, accuracy=0.550000
2025-09-14 01:48:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:48:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:48:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:48:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2362MB allocated=2217MB
2025-09-14 01:49:01 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 01:49:01 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 01:49:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 01:49:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:49:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:49:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 01:49:01 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=7.357939, avg_loss=0.668904, seen=11, correct=8, accuracy=0.727273
2025-09-14 01:49:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:49:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:49:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:49:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2362MB allocated=2217MB
2025-09-14 01:49:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:49:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:49:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:49:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:49:05 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.733297, avg_loss=0.693332, seen=40, correct=25, accuracy=0.625000
2025-09-14 01:49:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:49:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:49:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:49:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2362MB allocated=2217MB
2025-09-14 01:49:16 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 01:49:17 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 01:49:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 01:49:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:49:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:49:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 01:49:18 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=7.149421, avg_loss=0.649947, seen=11, correct=7, accuracy=0.636364
2025-09-14 01:49:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:49:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:49:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:49:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2362MB allocated=2217MB
2025-09-14 01:49:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:49:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:49:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:49:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:49:21 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.811371, avg_loss=0.695284, seen=40, correct=24, accuracy=0.600000
2025-09-14 01:49:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:49:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:49:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:49:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2362MB allocated=2217MB
2025-09-14 01:49:31 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 01:49:33 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 01:49:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 01:49:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:49:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:49:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 01:49:33 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=7.131127, avg_loss=0.648284, seen=11, correct=6, accuracy=0.545455
2025-09-14 01:49:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:49:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:49:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:49:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2362MB allocated=2217MB
2025-09-14 01:49:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:49:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:49:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:49:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:49:37 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.139252, avg_loss=0.703481, seen=40, correct=23, accuracy=0.575000
2025-09-14 01:49:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:49:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:49:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:49:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2362MB allocated=2217MB
2025-09-14 01:49:47 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 01:49:48 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 01:49:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 01:49:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:49:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:49:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 01:49:49 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=7.009840, avg_loss=0.637258, seen=11, correct=6, accuracy=0.545455
2025-09-14 01:49:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:49:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:49:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:49:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2362MB allocated=2217MB
2025-09-14 01:49:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:49:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:49:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:49:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:49:52 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.469109, avg_loss=0.711728, seen=40, correct=23, accuracy=0.575000
2025-09-14 01:49:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:49:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:49:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:49:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2362MB allocated=2217MB
2025-09-14 01:50:03 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 01:50:03 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 01:50:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 01:50:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:50:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:50:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 01:50:04 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=6.880853, avg_loss=0.625532, seen=11, correct=5, accuracy=0.454545
2025-09-14 01:50:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:50:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:50:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:50:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2362MB allocated=2217MB
2025-09-14 01:50:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:50:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:50:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:50:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:50:06 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.235296, avg_loss=0.705882, seen=40, correct=25, accuracy=0.625000
2025-09-14 01:50:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:50:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:50:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:50:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2362MB allocated=2217MB
2025-09-14 01:50:17 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 01:50:18 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 01:50:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 01:50:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:50:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:50:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 01:50:18 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=6.914142, avg_loss=0.628558, seen=11, correct=5, accuracy=0.454545
2025-09-14 01:50:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:50:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:50:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:50:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2362MB allocated=2217MB
2025-09-14 01:50:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:50:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:50:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:50:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:50:22 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.345112, avg_loss=0.708628, seen=40, correct=25, accuracy=0.625000
2025-09-14 01:50:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:50:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:50:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:50:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2362MB allocated=2217MB
2025-09-14 01:50:32 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 01:50:34 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 01:50:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 01:50:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:50:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:50:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 01:50:34 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=7.029095, avg_loss=0.639009, seen=11, correct=4, accuracy=0.363636
2025-09-14 01:50:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:50:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:50:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:50:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2362MB allocated=2217MB
2025-09-14 01:50:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:50:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:50:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:50:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:50:38 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.911228, avg_loss=0.722781, seen=40, correct=25, accuracy=0.625000
2025-09-14 01:50:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:50:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:50:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:50:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2362MB allocated=2217MB
2025-09-14 01:50:49 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 01:50:49 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 01:50:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 01:50:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:50:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:50:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 01:50:50 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=6.918971, avg_loss=0.628997, seen=11, correct=4, accuracy=0.363636
2025-09-14 01:50:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:50:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:50:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:50:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2362MB allocated=2217MB
2025-09-14 01:50:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:50:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:50:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:50:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:50:53 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.537148, avg_loss=0.738429, seen=40, correct=24, accuracy=0.600000
2025-09-14 01:50:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:50:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:50:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:50:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2362MB allocated=2217MB
2025-09-14 01:51:04 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 01:51:04 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 01:51:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 01:51:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:51:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:51:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 01:51:05 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=7.017987, avg_loss=0.637999, seen=11, correct=4, accuracy=0.363636
2025-09-14 01:51:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:51:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:51:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:51:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2362MB allocated=2217MB
2025-09-14 01:51:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:51:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:51:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:51:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:51:07 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.007351, avg_loss=0.750184, seen=40, correct=25, accuracy=0.625000
2025-09-14 01:51:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:51:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:51:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:51:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2362MB allocated=2217MB
2025-09-14 01:51:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 01:51:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 01:51:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:51:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:51:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2362MB allocated=2217MB
2025-09-14 01:51:09 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #4', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 01:51:09 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #4', 'Round': 0, 'Results_raw': {}}
2025-09-14 01:51:09 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 01:51:09 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-14 01:51:10 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 01:51:10 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 01:51:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-09-14 01:51:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:51:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=73, num_train_batch_last_epoch=27, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:51:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-14 01:51:14 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=146, loss_sum=96.015724, avg_loss=0.657642, seen=146, correct=86, accuracy=0.589041
2025-09-14 01:51:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:51:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:51:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:51:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2200MB
2025-09-14 01:51:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:51:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:51:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:51:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:51:17 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.803827, avg_loss=0.595096, seen=40, correct=27, accuracy=0.675000
2025-09-14 01:51:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:51:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:51:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:51:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2200MB
2025-09-14 01:51:18 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 01:51:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=699, total=2793)
2025-09-14 01:51:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:51:18 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 01:51:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:51:18 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=350, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 01:51:28 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 01:51:28 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 01:51:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-09-14 01:51:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:51:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=73, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:51:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-14 01:51:33 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=146, loss_sum=93.957596, avg_loss=0.643545, seen=146, correct=91, accuracy=0.623288
2025-09-14 01:51:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:51:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:51:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:51:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2217MB
2025-09-14 01:51:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:51:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:51:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:51:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:51:37 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.132610, avg_loss=0.578315, seen=40, correct=28, accuracy=0.700000
2025-09-14 01:51:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:51:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:51:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:51:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2217MB
2025-09-14 01:51:48 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 01:51:48 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 01:51:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-09-14 01:51:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:51:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=73, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:51:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-14 01:51:54 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=146, loss_sum=94.963387, avg_loss=0.650434, seen=146, correct=90, accuracy=0.616438
2025-09-14 01:51:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:51:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:51:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:51:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2217MB
2025-09-14 01:51:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:51:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:51:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:51:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:51:58 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.916220, avg_loss=0.597905, seen=40, correct=27, accuracy=0.675000
2025-09-14 01:51:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:51:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:51:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:51:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2217MB
2025-09-14 01:52:09 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 01:52:09 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 01:52:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-09-14 01:52:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:52:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=73, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:52:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-14 01:52:14 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=146, loss_sum=95.957695, avg_loss=0.657244, seen=146, correct=88, accuracy=0.602740
2025-09-14 01:52:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:52:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:52:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:52:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2217MB
2025-09-14 01:52:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:52:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:52:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:52:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:52:18 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=22.927948, avg_loss=0.573199, seen=40, correct=28, accuracy=0.700000
2025-09-14 01:52:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:52:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:52:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:52:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2217MB
2025-09-14 01:52:28 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 01:52:29 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 01:52:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-09-14 01:52:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:52:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=73, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:52:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-14 01:52:35 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=146, loss_sum=95.371002, avg_loss=0.653226, seen=146, correct=91, accuracy=0.623288
2025-09-14 01:52:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:52:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:52:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:52:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2217MB
2025-09-14 01:52:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:52:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:52:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:52:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:52:38 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=22.808668, avg_loss=0.570217, seen=40, correct=27, accuracy=0.675000
2025-09-14 01:52:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:52:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:52:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:52:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2217MB
2025-09-14 01:52:50 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 01:52:50 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 01:52:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-09-14 01:52:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:52:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=73, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:52:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-14 01:52:55 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=146, loss_sum=94.602737, avg_loss=0.647964, seen=146, correct=92, accuracy=0.630137
2025-09-14 01:52:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:52:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:52:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:52:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2217MB
2025-09-14 01:52:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:52:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:52:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:52:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:52:59 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=22.823095, avg_loss=0.570577, seen=40, correct=29, accuracy=0.725000
2025-09-14 01:52:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:52:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:53:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:53:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2217MB
2025-09-14 01:53:09 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 01:53:09 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 01:53:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-09-14 01:53:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:53:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=73, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:53:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-14 01:53:14 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=146, loss_sum=92.707886, avg_loss=0.634986, seen=146, correct=94, accuracy=0.643836
2025-09-14 01:53:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:53:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:53:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:53:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2217MB
2025-09-14 01:53:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:53:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:53:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:53:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:53:18 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=22.825523, avg_loss=0.570638, seen=40, correct=31, accuracy=0.775000
2025-09-14 01:53:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:53:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:53:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:53:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2217MB
2025-09-14 01:53:28 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 01:53:29 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 01:53:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-09-14 01:53:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:53:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=73, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:53:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-14 01:53:35 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=146, loss_sum=91.391785, avg_loss=0.625971, seen=146, correct=96, accuracy=0.657534
2025-09-14 01:53:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:53:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:53:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:53:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2217MB
2025-09-14 01:53:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:53:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:53:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:53:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:53:38 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.508345, avg_loss=0.587709, seen=40, correct=29, accuracy=0.725000
2025-09-14 01:53:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:53:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:53:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:53:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2217MB
2025-09-14 01:53:48 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 01:53:49 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 01:53:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-09-14 01:53:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:53:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=73, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:53:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-14 01:53:55 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=146, loss_sum=92.513107, avg_loss=0.633651, seen=146, correct=93, accuracy=0.636986
2025-09-14 01:53:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:53:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:53:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:53:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2217MB
2025-09-14 01:53:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:53:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:53:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:53:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:53:59 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.453213, avg_loss=0.586330, seen=40, correct=28, accuracy=0.700000
2025-09-14 01:53:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:53:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:54:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:54:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2217MB
2025-09-14 01:54:09 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 01:54:11 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 01:54:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-09-14 01:54:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:54:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=73, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:54:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-14 01:54:16 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=146, loss_sum=92.219086, avg_loss=0.631638, seen=146, correct=90, accuracy=0.616438
2025-09-14 01:54:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:54:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:54:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:54:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2217MB
2025-09-14 01:54:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:54:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:54:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:54:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:54:19 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.436718, avg_loss=0.585918, seen=40, correct=29, accuracy=0.725000
2025-09-14 01:54:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:54:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:54:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:54:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2217MB
2025-09-14 01:54:26 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 01:54:30 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 01:54:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-09-14 01:54:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:54:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=73, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:54:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-14 01:54:36 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=146, loss_sum=92.342941, avg_loss=0.632486, seen=146, correct=89, accuracy=0.609589
2025-09-14 01:54:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:54:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:54:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:54:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2217MB
2025-09-14 01:54:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:54:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:54:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:54:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:54:40 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.980383, avg_loss=0.599510, seen=40, correct=26, accuracy=0.650000
2025-09-14 01:54:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:54:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:54:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:54:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2217MB
2025-09-14 01:54:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 01:54:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 01:54:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:54:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:54:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2217MB
2025-09-14 01:54:42 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #1', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 01:54:42 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #1', 'Round': 0, 'Results_raw': {}}
2025-09-14 01:54:43 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 01:54:43 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-14 01:54:43 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 01:54:43 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 01:54:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-09-14 01:54:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:54:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=23, num_train_batch_last_epoch=8, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:54:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-09-14 01:54:45 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=46, loss_sum=34.251175, avg_loss=0.744591, seen=46, correct=25, accuracy=0.543478
2025-09-14 01:54:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:54:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:54:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:54:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2200MB
2025-09-14 01:54:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:54:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:54:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:54:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:54:47 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=32.091324, avg_loss=0.802283, seen=40, correct=19, accuracy=0.475000
2025-09-14 01:54:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:54:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:54:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:54:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2200MB
2025-09-14 01:54:48 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 01:54:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=220, total=880)
2025-09-14 01:54:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:54:48 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 01:54:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:54:48 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=110, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 01:54:56 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 01:54:58 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 01:54:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-09-14 01:54:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:54:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=23, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:54:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-09-14 01:54:59 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=46, loss_sum=34.142704, avg_loss=0.742233, seen=46, correct=21, accuracy=0.456522
2025-09-14 01:54:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:54:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:55:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:55:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2348MB allocated=2217MB
2025-09-14 01:55:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:55:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:55:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:55:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:55:03 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.055014, avg_loss=0.751375, seen=40, correct=23, accuracy=0.575000
2025-09-14 01:55:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:55:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:55:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:55:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2348MB allocated=2217MB
2025-09-14 01:55:14 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 01:55:14 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 01:55:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-09-14 01:55:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:55:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=23, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:55:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-09-14 01:55:16 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=46, loss_sum=35.055367, avg_loss=0.762073, seen=46, correct=19, accuracy=0.413043
2025-09-14 01:55:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:55:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:55:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:55:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2348MB allocated=2217MB
2025-09-14 01:55:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:55:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:55:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:55:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:55:19 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.064753, avg_loss=0.751619, seen=40, correct=24, accuracy=0.600000
2025-09-14 01:55:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:55:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:55:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:55:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2348MB allocated=2217MB
2025-09-14 01:55:30 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 01:55:31 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 01:55:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-09-14 01:55:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:55:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=23, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:55:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-09-14 01:55:33 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=46, loss_sum=34.180950, avg_loss=0.743064, seen=46, correct=22, accuracy=0.478261
2025-09-14 01:55:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:55:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:55:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:55:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2348MB allocated=2217MB
2025-09-14 01:55:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:55:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:55:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:55:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:55:37 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.733406, avg_loss=0.718335, seen=40, correct=26, accuracy=0.650000
2025-09-14 01:55:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:55:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:55:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:55:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2348MB allocated=2217MB
2025-09-14 01:55:46 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 01:55:47 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 01:55:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-09-14 01:55:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:55:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=23, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:55:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-09-14 01:55:50 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=46, loss_sum=34.343422, avg_loss=0.746596, seen=46, correct=22, accuracy=0.478261
2025-09-14 01:55:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:55:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:55:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:55:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2348MB allocated=2217MB
2025-09-14 01:55:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:55:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:55:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:55:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:55:54 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.771685, avg_loss=0.744292, seen=40, correct=23, accuracy=0.575000
2025-09-14 01:55:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:55:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:55:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:55:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2348MB allocated=2217MB
2025-09-14 01:56:03 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 01:56:05 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 01:56:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-09-14 01:56:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:56:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=23, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:56:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-09-14 01:56:07 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=46, loss_sum=33.747871, avg_loss=0.733649, seen=46, correct=24, accuracy=0.521739
2025-09-14 01:56:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:56:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:56:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:56:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2348MB allocated=2217MB
2025-09-14 01:56:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:56:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:56:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:56:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:56:10 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.601725, avg_loss=0.765043, seen=40, correct=18, accuracy=0.450000
2025-09-14 01:56:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:56:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:56:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:56:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2348MB allocated=2217MB
2025-09-14 01:56:20 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 01:56:21 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 01:56:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-09-14 01:56:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:56:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=23, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:56:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-09-14 01:56:23 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=46, loss_sum=34.254982, avg_loss=0.744674, seen=46, correct=24, accuracy=0.521739
2025-09-14 01:56:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:56:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:56:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:56:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2348MB allocated=2217MB
2025-09-14 01:56:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:56:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:56:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:56:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:56:26 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.863426, avg_loss=0.771586, seen=40, correct=20, accuracy=0.500000
2025-09-14 01:56:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:56:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:56:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:56:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2348MB allocated=2217MB
2025-09-14 01:56:38 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 01:56:38 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 01:56:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-09-14 01:56:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:56:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=23, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:56:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-09-14 01:56:40 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=46, loss_sum=34.618492, avg_loss=0.752576, seen=46, correct=21, accuracy=0.456522
2025-09-14 01:56:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:56:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:56:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:56:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2348MB allocated=2217MB
2025-09-14 01:56:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:56:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:56:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:56:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:56:43 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.304665, avg_loss=0.757617, seen=40, correct=23, accuracy=0.575000
2025-09-14 01:56:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:56:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:56:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:56:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2348MB allocated=2217MB
2025-09-14 01:56:53 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 01:56:55 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 01:56:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-09-14 01:56:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:56:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=23, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:56:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-09-14 01:56:57 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=46, loss_sum=34.437542, avg_loss=0.748642, seen=46, correct=21, accuracy=0.456522
2025-09-14 01:56:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:56:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:56:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:56:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2348MB allocated=2217MB
2025-09-14 01:56:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:56:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:56:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:57:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:57:01 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.032740, avg_loss=0.725818, seen=40, correct=25, accuracy=0.625000
2025-09-14 01:57:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:57:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:57:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:57:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2348MB allocated=2217MB
2025-09-14 01:57:12 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 01:57:12 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 01:57:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-09-14 01:57:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:57:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=23, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:57:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-09-14 01:57:13 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=46, loss_sum=34.364464, avg_loss=0.747054, seen=46, correct=21, accuracy=0.456522
2025-09-14 01:57:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:57:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:57:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:57:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2348MB allocated=2217MB
2025-09-14 01:57:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:57:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:57:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:57:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:57:16 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.857922, avg_loss=0.721448, seen=40, correct=25, accuracy=0.625000
2025-09-14 01:57:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:57:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:57:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:57:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2348MB allocated=2217MB
2025-09-14 01:57:27 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 01:57:27 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 01:57:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-09-14 01:57:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:57:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=23, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:57:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-09-14 01:57:28 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=46, loss_sum=33.988411, avg_loss=0.738878, seen=46, correct=25, accuracy=0.543478
2025-09-14 01:57:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:57:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:57:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:57:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2348MB allocated=2217MB
2025-09-14 01:57:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:57:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:57:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:57:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:57:32 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.800446, avg_loss=0.745011, seen=40, correct=21, accuracy=0.525000
2025-09-14 01:57:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:57:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:57:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:57:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2348MB allocated=2217MB
2025-09-14 01:57:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 01:57:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 01:57:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:57:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:57:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2348MB allocated=2217MB
2025-09-14 01:57:34 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #48', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 01:57:34 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #48', 'Round': 0, 'Results_raw': {}}
2025-09-14 01:57:34 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 01:57:34 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-14 01:57:35 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 01:57:35 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 01:57:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-09-14 01:57:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:57:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=50, num_train_batch_last_epoch=50, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:57:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-09-14 01:57:37 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=100, loss_sum=61.597645, avg_loss=0.615976, seen=100, correct=64, accuracy=0.640000
2025-09-14 01:57:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:57:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:57:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:57:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2200MB
2025-09-14 01:57:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:57:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:57:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:57:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:57:40 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.346621, avg_loss=0.708666, seen=40, correct=21, accuracy=0.525000
2025-09-14 01:57:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:57:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:57:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:57:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2200MB
2025-09-14 01:57:41 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 01:57:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=476, total=1901)
2025-09-14 01:57:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:57:42 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 01:57:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:57:42 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=238, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 01:57:50 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 01:57:52 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 01:57:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-09-14 01:57:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:57:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=50, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:57:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-09-14 01:57:56 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=100, loss_sum=67.431290, avg_loss=0.674313, seen=100, correct=59, accuracy=0.590000
2025-09-14 01:57:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:57:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:57:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:57:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2217MB
2025-09-14 01:57:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:57:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:57:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:57:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:57:59 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.799618, avg_loss=0.719990, seen=40, correct=24, accuracy=0.600000
2025-09-14 01:57:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:57:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:58:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:58:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2217MB
2025-09-14 01:58:09 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 01:58:11 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 01:58:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-09-14 01:58:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:58:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=50, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:58:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-09-14 01:58:15 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=100, loss_sum=69.398460, avg_loss=0.693985, seen=100, correct=56, accuracy=0.560000
2025-09-14 01:58:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:58:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:58:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:58:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2217MB
2025-09-14 01:58:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:58:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:58:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:58:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:58:18 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.196430, avg_loss=0.704911, seen=40, correct=22, accuracy=0.550000
2025-09-14 01:58:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:58:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:58:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:58:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2217MB
2025-09-14 01:58:30 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 01:58:31 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 01:58:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-09-14 01:58:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:58:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=50, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:58:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-09-14 01:58:36 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=100, loss_sum=70.410919, avg_loss=0.704109, seen=100, correct=55, accuracy=0.550000
2025-09-14 01:58:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:58:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:58:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:58:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2217MB
2025-09-14 01:58:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:58:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:58:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:58:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:58:40 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.821400, avg_loss=0.720535, seen=40, correct=22, accuracy=0.550000
2025-09-14 01:58:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:58:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:58:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:58:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2217MB
2025-09-14 01:58:52 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 01:58:52 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 01:58:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-09-14 01:58:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:58:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=50, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:58:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-09-14 01:58:56 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=100, loss_sum=66.657303, avg_loss=0.666573, seen=100, correct=61, accuracy=0.610000
2025-09-14 01:58:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:58:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:58:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:58:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2217MB
2025-09-14 01:58:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:58:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:58:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:58:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:58:59 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.185188, avg_loss=0.729630, seen=40, correct=21, accuracy=0.525000
2025-09-14 01:58:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:58:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:59:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:59:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2217MB
2025-09-14 01:59:11 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 01:59:11 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 01:59:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-09-14 01:59:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:59:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=50, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:59:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-09-14 01:59:15 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=100, loss_sum=63.664307, avg_loss=0.636643, seen=100, correct=65, accuracy=0.650000
2025-09-14 01:59:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:59:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:59:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:59:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2217MB
2025-09-14 01:59:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:59:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:59:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:59:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:59:18 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.647301, avg_loss=0.716183, seen=40, correct=22, accuracy=0.550000
2025-09-14 01:59:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:59:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:59:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:59:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2217MB
2025-09-14 01:59:28 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 01:59:30 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 01:59:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-09-14 01:59:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:59:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=50, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:59:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-09-14 01:59:35 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=100, loss_sum=63.196541, avg_loss=0.631965, seen=100, correct=67, accuracy=0.670000
2025-09-14 01:59:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:59:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:59:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:59:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2217MB
2025-09-14 01:59:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:59:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:59:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:59:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:59:39 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.958935, avg_loss=0.698973, seen=40, correct=23, accuracy=0.575000
2025-09-14 01:59:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:59:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:59:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:59:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2217MB
2025-09-14 01:59:51 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 01:59:51 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 01:59:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-09-14 01:59:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:59:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=50, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:59:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-09-14 01:59:55 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=100, loss_sum=63.335533, avg_loss=0.633355, seen=100, correct=65, accuracy=0.650000
2025-09-14 01:59:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:59:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:59:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:59:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2217MB
2025-09-14 01:59:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 01:59:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:59:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 01:59:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 01:59:58 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.868332, avg_loss=0.696708, seen=40, correct=27, accuracy=0.675000
2025-09-14 01:59:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 01:59:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 01:59:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 01:59:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2217MB
2025-09-14 02:00:09 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 02:00:10 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 02:00:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-09-14 02:00:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:00:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=50, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:00:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-09-14 02:00:13 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=100, loss_sum=66.244179, avg_loss=0.662442, seen=100, correct=59, accuracy=0.590000
2025-09-14 02:00:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:00:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:00:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:00:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2217MB
2025-09-14 02:00:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:00:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:00:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:00:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:00:17 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.957323, avg_loss=0.698933, seen=40, correct=23, accuracy=0.575000
2025-09-14 02:00:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:00:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:00:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:00:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2217MB
2025-09-14 02:00:28 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 02:00:29 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 02:00:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-09-14 02:00:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:00:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=50, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:00:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-09-14 02:00:33 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=100, loss_sum=66.922554, avg_loss=0.669226, seen=100, correct=56, accuracy=0.560000
2025-09-14 02:00:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:00:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:00:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:00:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2217MB
2025-09-14 02:00:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:00:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:00:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:00:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:00:37 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.849392, avg_loss=0.721235, seen=40, correct=24, accuracy=0.600000
2025-09-14 02:00:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:00:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:00:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:00:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2217MB
2025-09-14 02:00:49 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 02:00:49 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 02:00:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-09-14 02:00:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:00:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=50, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:00:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-09-14 02:00:53 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=100, loss_sum=63.832306, avg_loss=0.638323, seen=100, correct=59, accuracy=0.590000
2025-09-14 02:00:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:00:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:00:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:00:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2217MB
2025-09-14 02:00:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:00:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:00:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:00:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:00:57 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.878376, avg_loss=0.721959, seen=40, correct=22, accuracy=0.550000
2025-09-14 02:00:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:00:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:00:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:00:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2217MB
2025-09-14 02:00:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 02:00:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 02:00:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:00:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:00:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2217MB
2025-09-14 02:00:59 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #45', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 02:00:59 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #45', 'Round': 0, 'Results_raw': {}}
2025-09-14 02:00:59 (federatedscope.core.workers.server:433) INFO: Server: Training is finished! Starting evaluation.
2025-09-14 02:01:00 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:01:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-09-14 02:01:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:01:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=73, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:01:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-14 02:01:04 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=146, loss_sum=97.288185, avg_loss=0.666357, seen=146, correct=90, accuracy=0.616438
2025-09-14 02:01:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:01:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:01:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:01:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2200MB
2025-09-14 02:01:06 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 146, 'val_loss': 97.2881851196289, 'val_avg_loss': 0.6663574323262254, 'val_seen': 146, 'val_correct': 90, 'val_acc': 0.6164383561643836}
2025-09-14 02:01:06 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 02:01:06 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 146, 'val_loss': 97.2881851196289, 'val_avg_loss': 0.6663574323262254, 'val_seen': 146, 'val_correct': 90, 'val_acc': 0.6164383561643836}
2025-09-14 02:01:06 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #1', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 37, 'val_loss': 21.26368659734726, 'val_avg_loss': 0.5746942323607367, 'val_seen': 37, 'val_correct': 27, 'val_acc': 0.7297297297297297}}
2025-09-14 02:01:06 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #1', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 146, 'val_loss': 97.2881851196289, 'val_avg_loss': 0.6663574323262254, 'val_seen': 146, 'val_correct': 90, 'val_acc': 0.6164383561643836}}
2025-09-14 02:01:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:01:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:01:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:01:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:01:08 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.495136, avg_loss=0.612378, seen=40, correct=31, accuracy=0.775000
2025-09-14 02:01:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:01:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:01:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:01:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2200MB
2025-09-14 02:01:09 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 24.495136260986328, 'test_avg_loss': 0.6123784065246582, 'test_seen': 40, 'test_correct': 31, 'test_acc': 0.775}
2025-09-14 02:01:09 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 146, 'val_loss': 97.2881851196289, 'val_avg_loss': 0.6663574323262254, 'val_seen': 146, 'val_correct': 90, 'val_acc': 0.6164383561643836}
2025-09-14 02:01:09 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 24.495136260986328, 'test_avg_loss': 0.6123784065246582, 'test_seen': 40, 'test_correct': 31, 'test_acc': 0.775}
2025-09-14 02:01:09 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #1', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 5.127327501773834, 'test_avg_loss': 0.5127327501773834, 'test_seen': 10, 'test_correct': 8, 'test_acc': 0.8}}
2025-09-14 02:01:09 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #1', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 24.495136260986328, 'test_avg_loss': 0.6123784065246582, 'test_seen': 40, 'test_correct': 31, 'test_acc': 0.775}}
2025-09-14 02:01:09 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 24.495136260986328, 'test_avg_loss': 0.6123784065246582, 'test_seen': 40, 'test_correct': 31, 'test_acc': 0.775}, metrics={'val_total': 146, 'val_loss': 97.2881851196289, 'val_avg_loss': 0.6663574323262254, 'val_seen': 146, 'val_correct': 90, 'val_acc': 0.6164383561643836, 'test_total': 40, 'test_loss': 24.495136260986328, 'test_avg_loss': 0.6123784065246582, 'test_seen': 40, 'test_correct': 31, 'test_acc': 0.775}
2025-09-14 02:01:09 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 02:01:09 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:01:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 02:01:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:01:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:01:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 02:01:10 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=10.239594, avg_loss=0.930872, seen=11, correct=4, accuracy=0.363636
2025-09-14 02:01:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:01:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:01:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:01:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2200MB
2025-09-14 02:01:11 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 11, 'val_loss': 10.239594459533691, 'val_avg_loss': 0.9308722235939719, 'val_seen': 11, 'val_correct': 4, 'val_acc': 0.36363636363636365}
2025-09-14 02:01:11 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 02:01:11 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 11, 'val_loss': 10.239594459533691, 'val_avg_loss': 0.9308722235939719, 'val_seen': 11, 'val_correct': 4, 'val_acc': 0.36363636363636365}
2025-09-14 02:01:11 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #2', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 3, 'val_loss': 2.8624539375305176, 'val_avg_loss': 0.9541513125101725, 'val_seen': 3, 'val_correct': 2, 'val_acc': 0.6666666666666666}}
2025-09-14 02:01:11 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #2', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 11, 'val_loss': 10.239594459533691, 'val_avg_loss': 0.9308722235939719, 'val_seen': 11, 'val_correct': 4, 'val_acc': 0.36363636363636365}}
2025-09-14 02:01:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:01:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:01:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:01:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:01:12 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.791149, avg_loss=0.694779, seen=40, correct=24, accuracy=0.600000
2025-09-14 02:01:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:01:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:01:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:01:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2200MB
2025-09-14 02:01:13 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 27.791149139404297, 'test_avg_loss': 0.6947787284851075, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-14 02:01:13 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 11, 'val_loss': 10.239594459533691, 'val_avg_loss': 0.9308722235939719, 'val_seen': 11, 'val_correct': 4, 'val_acc': 0.36363636363636365}
2025-09-14 02:01:13 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 27.791149139404297, 'test_avg_loss': 0.6947787284851075, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-14 02:01:13 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #2', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.082245051860809, 'test_avg_loss': 0.608224505186081, 'test_seen': 10, 'test_correct': 6, 'test_acc': 0.6}}
2025-09-14 02:01:13 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #2', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 27.791149139404297, 'test_avg_loss': 0.6947787284851075, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}}
2025-09-14 02:01:13 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 27.791149139404297, 'test_avg_loss': 0.6947787284851075, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}, metrics={'val_total': 11, 'val_loss': 10.239594459533691, 'val_avg_loss': 0.9308722235939719, 'val_seen': 11, 'val_correct': 4, 'val_acc': 0.36363636363636365, 'test_total': 40, 'test_loss': 27.791149139404297, 'test_avg_loss': 0.6947787284851075, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-14 02:01:13 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 02:01:13 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:01:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-09-14 02:01:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:01:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=18, num_train_batch_last_epoch=200, num_train_epoch=6, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:01:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-09-14 02:01:15 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=36, loss_sum=24.887300, avg_loss=0.691314, seen=36, correct=22, accuracy=0.611111
2025-09-14 02:01:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:01:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:01:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:01:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2200MB
2025-09-14 02:01:16 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 36, 'val_loss': 24.887300491333008, 'val_avg_loss': 0.691313902537028, 'val_seen': 36, 'val_correct': 22, 'val_acc': 0.6111111111111112}
2025-09-14 02:01:16 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 02:01:16 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 36, 'val_loss': 24.887300491333008, 'val_avg_loss': 0.691313902537028, 'val_seen': 36, 'val_correct': 22, 'val_acc': 0.6111111111111112}
2025-09-14 02:01:16 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #3', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 9, 'val_loss': 7.51884451508522, 'val_avg_loss': 0.8354271683428023, 'val_seen': 9, 'val_correct': 4, 'val_acc': 0.4444444444444444}}
2025-09-14 02:01:16 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #3', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 36, 'val_loss': 24.887300491333008, 'val_avg_loss': 0.691313902537028, 'val_seen': 36, 'val_correct': 22, 'val_acc': 0.6111111111111112}}
2025-09-14 02:01:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:01:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:01:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:01:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:01:18 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.284481, avg_loss=0.682112, seen=40, correct=24, accuracy=0.600000
2025-09-14 02:01:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:01:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:01:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:01:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2200MB
2025-09-14 02:01:19 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 27.284481048583984, 'test_avg_loss': 0.6821120262145997, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-14 02:01:19 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 36, 'val_loss': 24.887300491333008, 'val_avg_loss': 0.691313902537028, 'val_seen': 36, 'val_correct': 22, 'val_acc': 0.6111111111111112}
2025-09-14 02:01:19 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 27.284481048583984, 'test_avg_loss': 0.6821120262145997, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-14 02:01:19 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #3', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 7.937405467033386, 'test_avg_loss': 0.7937405467033386, 'test_seen': 10, 'test_correct': 4, 'test_acc': 0.4}}
2025-09-14 02:01:19 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #3', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 27.284481048583984, 'test_avg_loss': 0.6821120262145997, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}}
2025-09-14 02:01:19 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 27.284481048583984, 'test_avg_loss': 0.6821120262145997, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}, metrics={'val_total': 36, 'val_loss': 24.887300491333008, 'val_avg_loss': 0.691313902537028, 'val_seen': 36, 'val_correct': 22, 'val_acc': 0.6111111111111112, 'test_total': 40, 'test_loss': 27.284481048583984, 'test_avg_loss': 0.6821120262145997, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-14 02:01:19 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 02:01:19 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:01:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 02:01:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:01:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:01:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 02:01:20 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=6.469686, avg_loss=0.588153, seen=11, correct=9, accuracy=0.818182
2025-09-14 02:01:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:01:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:01:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:01:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2200MB
2025-09-14 02:01:21 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 11, 'val_loss': 6.4696855545043945, 'val_avg_loss': 0.5881532322276722, 'val_seen': 11, 'val_correct': 9, 'val_acc': 0.8181818181818182}
2025-09-14 02:01:21 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 02:01:21 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 11, 'val_loss': 6.4696855545043945, 'val_avg_loss': 0.5881532322276722, 'val_seen': 11, 'val_correct': 9, 'val_acc': 0.8181818181818182}
2025-09-14 02:01:21 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #4', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 3, 'val_loss': 2.2142206728458405, 'val_avg_loss': 0.7380735576152802, 'val_seen': 3, 'val_correct': 2, 'val_acc': 0.6666666666666666}}
2025-09-14 02:01:21 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #4', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 11, 'val_loss': 6.4696855545043945, 'val_avg_loss': 0.5881532322276722, 'val_seen': 11, 'val_correct': 9, 'val_acc': 0.8181818181818182}}
2025-09-14 02:01:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:01:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:01:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:01:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:01:22 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.768991, avg_loss=0.769225, seen=40, correct=18, accuracy=0.450000
2025-09-14 02:01:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:01:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:01:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:01:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2200MB
2025-09-14 02:01:23 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 30.768991470336914, 'test_avg_loss': 0.7692247867584229, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-14 02:01:23 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 11, 'val_loss': 6.4696855545043945, 'val_avg_loss': 0.5881532322276722, 'val_seen': 11, 'val_correct': 9, 'val_acc': 0.8181818181818182}
2025-09-14 02:01:23 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 30.768991470336914, 'test_avg_loss': 0.7692247867584229, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-14 02:01:23 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #4', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 9.145301342010498, 'test_avg_loss': 0.9145301342010498, 'test_seen': 10, 'test_correct': 4, 'test_acc': 0.4}}
2025-09-14 02:01:23 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #4', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 30.768991470336914, 'test_avg_loss': 0.7692247867584229, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}}
2025-09-14 02:01:23 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 30.768991470336914, 'test_avg_loss': 0.7692247867584229, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}, metrics={'val_total': 11, 'val_loss': 6.4696855545043945, 'val_avg_loss': 0.5881532322276722, 'val_seen': 11, 'val_correct': 9, 'val_acc': 0.8181818181818182, 'test_total': 40, 'test_loss': 30.768991470336914, 'test_avg_loss': 0.7692247867584229, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-14 02:01:23 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 02:01:23 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:01:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-09-14 02:01:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:01:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=7, num_train_batch_last_epoch=200, num_train_epoch=15, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:01:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-09-14 02:01:25 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=14, loss_sum=9.137687, avg_loss=0.652692, seen=14, correct=9, accuracy=0.642857
2025-09-14 02:01:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:01:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:01:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:01:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2200MB
2025-09-14 02:01:26 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 14, 'val_loss': 9.137686729431152, 'val_avg_loss': 0.6526919092450824, 'val_seen': 14, 'val_correct': 9, 'val_acc': 0.6428571428571429}
2025-09-14 02:01:26 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 02:01:26 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 14, 'val_loss': 9.137686729431152, 'val_avg_loss': 0.6526919092450824, 'val_seen': 14, 'val_correct': 9, 'val_acc': 0.6428571428571429}
2025-09-14 02:01:26 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #5', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 4, 'val_loss': 1.9400559067726135, 'val_avg_loss': 0.4850139766931534, 'val_seen': 4, 'val_correct': 3, 'val_acc': 0.75}}
2025-09-14 02:01:26 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #5', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 14, 'val_loss': 9.137686729431152, 'val_avg_loss': 0.6526919092450824, 'val_seen': 14, 'val_correct': 9, 'val_acc': 0.6428571428571429}}
2025-09-14 02:01:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:01:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:01:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:01:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:01:27 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.366022, avg_loss=0.609151, seen=40, correct=29, accuracy=0.725000
2025-09-14 02:01:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:01:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:01:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:01:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2200MB
2025-09-14 02:01:29 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 24.36602210998535, 'test_avg_loss': 0.6091505527496338, 'test_seen': 40, 'test_correct': 29, 'test_acc': 0.725}
2025-09-14 02:01:29 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 14, 'val_loss': 9.137686729431152, 'val_avg_loss': 0.6526919092450824, 'val_seen': 14, 'val_correct': 9, 'val_acc': 0.6428571428571429}
2025-09-14 02:01:29 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 24.36602210998535, 'test_avg_loss': 0.6091505527496338, 'test_seen': 40, 'test_correct': 29, 'test_acc': 0.725}
2025-09-14 02:01:29 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #5', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.698156833648682, 'test_avg_loss': 0.6698156833648682, 'test_seen': 10, 'test_correct': 6, 'test_acc': 0.6}}
2025-09-14 02:01:29 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #5', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 24.36602210998535, 'test_avg_loss': 0.6091505527496338, 'test_seen': 40, 'test_correct': 29, 'test_acc': 0.725}}
2025-09-14 02:01:29 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 24.36602210998535, 'test_avg_loss': 0.6091505527496338, 'test_seen': 40, 'test_correct': 29, 'test_acc': 0.725}, metrics={'val_total': 14, 'val_loss': 9.137686729431152, 'val_avg_loss': 0.6526919092450824, 'val_seen': 14, 'val_correct': 9, 'val_acc': 0.6428571428571429, 'test_total': 40, 'test_loss': 24.36602210998535, 'test_avg_loss': 0.6091505527496338, 'test_seen': 40, 'test_correct': 29, 'test_acc': 0.725}
2025-09-14 02:01:29 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 02:01:29 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:01:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-09-14 02:01:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:01:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:01:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 02:01:34 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=134, loss_sum=86.744911, avg_loss=0.647350, seen=134, correct=83, accuracy=0.619403
2025-09-14 02:01:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:01:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:01:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:01:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2200MB
2025-09-14 02:01:35 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 134, 'val_loss': 86.74491119384766, 'val_avg_loss': 0.6473500835361765, 'val_seen': 134, 'val_correct': 83, 'val_acc': 0.6194029850746269}
2025-09-14 02:01:35 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 02:01:35 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 134, 'val_loss': 86.74491119384766, 'val_avg_loss': 0.6473500835361765, 'val_seen': 134, 'val_correct': 83, 'val_acc': 0.6194029850746269}
2025-09-14 02:01:35 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #6', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 34, 'val_loss': 20.421720325946808, 'val_avg_loss': 0.6006388331160826, 'val_seen': 34, 'val_correct': 24, 'val_acc': 0.7058823529411765}}
2025-09-14 02:01:35 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #6', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 134, 'val_loss': 86.74491119384766, 'val_avg_loss': 0.6473500835361765, 'val_seen': 134, 'val_correct': 83, 'val_acc': 0.6194029850746269}}
2025-09-14 02:01:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:01:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:01:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:01:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:01:36 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=32.375751, avg_loss=0.809394, seen=40, correct=21, accuracy=0.525000
2025-09-14 02:01:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:01:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:01:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:01:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2200MB
2025-09-14 02:01:37 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 32.37575149536133, 'test_avg_loss': 0.8093937873840332, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-14 02:01:37 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 134, 'val_loss': 86.74491119384766, 'val_avg_loss': 0.6473500835361765, 'val_seen': 134, 'val_correct': 83, 'val_acc': 0.6194029850746269}
2025-09-14 02:01:37 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 32.37575149536133, 'test_avg_loss': 0.8093937873840332, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-14 02:01:37 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #6', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 8.231682896614075, 'test_avg_loss': 0.8231682896614074, 'test_seen': 10, 'test_correct': 5, 'test_acc': 0.5}}
2025-09-14 02:01:37 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #6', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 32.37575149536133, 'test_avg_loss': 0.8093937873840332, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}}
2025-09-14 02:01:37 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 32.37575149536133, 'test_avg_loss': 0.8093937873840332, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}, metrics={'val_total': 134, 'val_loss': 86.74491119384766, 'val_avg_loss': 0.6473500835361765, 'val_seen': 134, 'val_correct': 83, 'val_acc': 0.6194029850746269, 'test_total': 40, 'test_loss': 32.37575149536133, 'test_avg_loss': 0.8093937873840332, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-14 02:01:37 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 02:01:37 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:01:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-09-14 02:01:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:01:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=29, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:01:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-09-14 02:01:39 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=57, loss_sum=38.973934, avg_loss=0.683753, seen=57, correct=34, accuracy=0.596491
2025-09-14 02:01:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:01:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:01:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:01:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2200MB
2025-09-14 02:01:41 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 57, 'val_loss': 38.973934173583984, 'val_avg_loss': 0.6837532311155085, 'val_seen': 57, 'val_correct': 34, 'val_acc': 0.5964912280701754}
2025-09-14 02:01:41 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 02:01:41 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 57, 'val_loss': 38.973934173583984, 'val_avg_loss': 0.6837532311155085, 'val_seen': 57, 'val_correct': 34, 'val_acc': 0.5964912280701754}
2025-09-14 02:01:41 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #7', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 15, 'val_loss': 9.792174875736237, 'val_avg_loss': 0.6528116583824157, 'val_seen': 15, 'val_correct': 8, 'val_acc': 0.5333333333333333}}
2025-09-14 02:01:41 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #7', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 57, 'val_loss': 38.973934173583984, 'val_avg_loss': 0.6837532311155085, 'val_seen': 57, 'val_correct': 34, 'val_acc': 0.5964912280701754}}
2025-09-14 02:01:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:01:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:01:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:01:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:01:42 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.442902, avg_loss=0.611073, seen=40, correct=28, accuracy=0.700000
2025-09-14 02:01:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:01:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:01:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:01:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2200MB
2025-09-14 02:01:43 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 24.442901611328125, 'test_avg_loss': 0.6110725402832031, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}
2025-09-14 02:01:43 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 57, 'val_loss': 38.973934173583984, 'val_avg_loss': 0.6837532311155085, 'val_seen': 57, 'val_correct': 34, 'val_acc': 0.5964912280701754}
2025-09-14 02:01:43 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 24.442901611328125, 'test_avg_loss': 0.6110725402832031, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}
2025-09-14 02:01:43 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #7', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 4.272947072982788, 'test_avg_loss': 0.4272947072982788, 'test_seen': 10, 'test_correct': 10, 'test_acc': 1.0}}
2025-09-14 02:01:43 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #7', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 24.442901611328125, 'test_avg_loss': 0.6110725402832031, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}}
2025-09-14 02:01:43 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 24.442901611328125, 'test_avg_loss': 0.6110725402832031, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}, metrics={'val_total': 57, 'val_loss': 38.973934173583984, 'val_avg_loss': 0.6837532311155085, 'val_seen': 57, 'val_correct': 34, 'val_acc': 0.5964912280701754, 'test_total': 40, 'test_loss': 24.442901611328125, 'test_avg_loss': 0.6110725402832031, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}
2025-09-14 02:01:43 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 02:01:43 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:01:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-09-14 02:01:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:01:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=35, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:01:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-14 02:01:45 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=69, loss_sum=47.549088, avg_loss=0.689117, seen=69, correct=43, accuracy=0.623188
2025-09-14 02:01:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:01:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:01:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:01:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2200MB
2025-09-14 02:01:47 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 69, 'val_loss': 47.54908752441406, 'val_avg_loss': 0.6891172104987545, 'val_seen': 69, 'val_correct': 43, 'val_acc': 0.6231884057971014}
2025-09-14 02:01:47 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 02:01:47 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 69, 'val_loss': 47.54908752441406, 'val_avg_loss': 0.6891172104987545, 'val_seen': 69, 'val_correct': 43, 'val_acc': 0.6231884057971014}
2025-09-14 02:01:47 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #8', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 18, 'val_loss': 11.652391016483307, 'val_avg_loss': 0.6473550564712949, 'val_seen': 18, 'val_correct': 13, 'val_acc': 0.7222222222222222}}
2025-09-14 02:01:47 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #8', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 69, 'val_loss': 47.54908752441406, 'val_avg_loss': 0.6891172104987545, 'val_seen': 69, 'val_correct': 43, 'val_acc': 0.6231884057971014}}
2025-09-14 02:01:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:01:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:01:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:01:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:01:48 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.405933, avg_loss=0.735148, seen=40, correct=22, accuracy=0.550000
2025-09-14 02:01:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:01:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:01:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:01:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2200MB
2025-09-14 02:01:50 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 29.405933380126953, 'test_avg_loss': 0.7351483345031739, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-14 02:01:50 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 69, 'val_loss': 47.54908752441406, 'val_avg_loss': 0.6891172104987545, 'val_seen': 69, 'val_correct': 43, 'val_acc': 0.6231884057971014}
2025-09-14 02:01:50 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 29.405933380126953, 'test_avg_loss': 0.7351483345031739, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-14 02:01:50 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #8', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 8.854312300682068, 'test_avg_loss': 0.8854312300682068, 'test_seen': 10, 'test_correct': 4, 'test_acc': 0.4}}
2025-09-14 02:01:50 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #8', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 29.405933380126953, 'test_avg_loss': 0.7351483345031739, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}}
2025-09-14 02:01:50 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 29.405933380126953, 'test_avg_loss': 0.7351483345031739, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}, metrics={'val_total': 69, 'val_loss': 47.54908752441406, 'val_avg_loss': 0.6891172104987545, 'val_seen': 69, 'val_correct': 43, 'val_acc': 0.6231884057971014, 'test_total': 40, 'test_loss': 29.405933380126953, 'test_avg_loss': 0.7351483345031739, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-14 02:01:50 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 02:01:50 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:01:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-14 02:01:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:01:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:01:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-14 02:01:55 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=121.849228, avg_loss=0.648134, seen=188, correct=114, accuracy=0.606383
2025-09-14 02:01:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:01:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:01:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:01:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2200MB
2025-09-14 02:01:57 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 188, 'val_loss': 121.84922790527344, 'val_avg_loss': 0.648134190985497, 'val_seen': 188, 'val_correct': 114, 'val_acc': 0.6063829787234043}
2025-09-14 02:01:57 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 02:01:57 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 188, 'val_loss': 121.84922790527344, 'val_avg_loss': 0.648134190985497, 'val_seen': 188, 'val_correct': 114, 'val_acc': 0.6063829787234043}
2025-09-14 02:01:57 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #9', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 47, 'val_loss': 32.644951581954956, 'val_avg_loss': 0.6945734379139352, 'val_seen': 47, 'val_correct': 28, 'val_acc': 0.5957446808510638}}
2025-09-14 02:01:57 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #9', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 188, 'val_loss': 121.84922790527344, 'val_avg_loss': 0.648134190985497, 'val_seen': 188, 'val_correct': 114, 'val_acc': 0.6063829787234043}}
2025-09-14 02:01:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:01:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:01:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:01:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:01:57 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.066219, avg_loss=0.676655, seen=40, correct=23, accuracy=0.575000
2025-09-14 02:01:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:01:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:01:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:01:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2336MB allocated=2200MB
2025-09-14 02:01:59 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 27.066219329833984, 'test_avg_loss': 0.6766554832458496, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 02:01:59 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 188, 'val_loss': 121.84922790527344, 'val_avg_loss': 0.648134190985497, 'val_seen': 188, 'val_correct': 114, 'val_acc': 0.6063829787234043}
2025-09-14 02:01:59 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 27.066219329833984, 'test_avg_loss': 0.6766554832458496, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 02:01:59 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #9', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 5.87447988986969, 'test_avg_loss': 0.587447988986969, 'test_seen': 10, 'test_correct': 7, 'test_acc': 0.7}}
2025-09-14 02:01:59 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #9', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 27.066219329833984, 'test_avg_loss': 0.6766554832458496, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}}
2025-09-14 02:01:59 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 27.066219329833984, 'test_avg_loss': 0.6766554832458496, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}, metrics={'val_total': 188, 'val_loss': 121.84922790527344, 'val_avg_loss': 0.648134190985497, 'val_seen': 188, 'val_correct': 114, 'val_acc': 0.6063829787234043, 'test_total': 40, 'test_loss': 27.066219329833984, 'test_avg_loss': 0.6766554832458496, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 02:01:59 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 02:01:59 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:01:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-09-14 02:01:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:01:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=32, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:02:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-09-14 02:02:00 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=63, loss_sum=39.544437, avg_loss=0.627689, seen=63, correct=42, accuracy=0.666667
2025-09-14 02:02:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:02:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:02:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:02:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2200MB
2025-09-14 02:02:02 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 63, 'val_loss': 39.544437408447266, 'val_avg_loss': 0.6276894826737661, 'val_seen': 63, 'val_correct': 42, 'val_acc': 0.6666666666666666}
2025-09-14 02:02:02 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 02:02:02 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 63, 'val_loss': 39.544437408447266, 'val_avg_loss': 0.6276894826737661, 'val_seen': 63, 'val_correct': 42, 'val_acc': 0.6666666666666666}
2025-09-14 02:02:02 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #10', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 16, 'val_loss': 10.180190026760101, 'val_avg_loss': 0.6362618766725063, 'val_seen': 16, 'val_correct': 11, 'val_acc': 0.6875}}
2025-09-14 02:02:02 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #10', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 63, 'val_loss': 39.544437408447266, 'val_avg_loss': 0.6276894826737661, 'val_seen': 63, 'val_correct': 42, 'val_acc': 0.6666666666666666}}
2025-09-14 02:02:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:02:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:02:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:02:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:02:03 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.624001, avg_loss=0.590600, seen=40, correct=30, accuracy=0.750000
2025-09-14 02:02:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:02:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:02:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:02:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2200MB
2025-09-14 02:02:04 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 23.624000549316406, 'test_avg_loss': 0.5906000137329102, 'test_seen': 40, 'test_correct': 30, 'test_acc': 0.75}
2025-09-14 02:02:04 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 63, 'val_loss': 39.544437408447266, 'val_avg_loss': 0.6276894826737661, 'val_seen': 63, 'val_correct': 42, 'val_acc': 0.6666666666666666}
2025-09-14 02:02:04 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 23.624000549316406, 'test_avg_loss': 0.5906000137329102, 'test_seen': 40, 'test_correct': 30, 'test_acc': 0.75}
2025-09-14 02:02:04 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #10', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.622900724411011, 'test_avg_loss': 0.662290072441101, 'test_seen': 10, 'test_correct': 8, 'test_acc': 0.8}}
2025-09-14 02:02:04 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #10', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 23.624000549316406, 'test_avg_loss': 0.5906000137329102, 'test_seen': 40, 'test_correct': 30, 'test_acc': 0.75}}
2025-09-14 02:02:04 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 23.624000549316406, 'test_avg_loss': 0.5906000137329102, 'test_seen': 40, 'test_correct': 30, 'test_acc': 0.75}, metrics={'val_total': 63, 'val_loss': 39.544437408447266, 'val_avg_loss': 0.6276894826737661, 'val_seen': 63, 'val_correct': 42, 'val_acc': 0.6666666666666666, 'test_total': 40, 'test_loss': 23.624000549316406, 'test_avg_loss': 0.5906000137329102, 'test_seen': 40, 'test_correct': 30, 'test_acc': 0.75}
2025-09-14 02:02:04 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 02:02:04 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:02:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-09-14 02:02:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:02:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=16, num_train_batch_last_epoch=200, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:02:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-14 02:02:06 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=32, loss_sum=19.319059, avg_loss=0.603721, seen=32, correct=22, accuracy=0.687500
2025-09-14 02:02:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:02:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:02:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:02:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2200MB
2025-09-14 02:02:07 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 32, 'val_loss': 19.319059371948242, 'val_avg_loss': 0.6037206053733826, 'val_seen': 32, 'val_correct': 22, 'val_acc': 0.6875}
2025-09-14 02:02:07 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 02:02:07 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 32, 'val_loss': 19.319059371948242, 'val_avg_loss': 0.6037206053733826, 'val_seen': 32, 'val_correct': 22, 'val_acc': 0.6875}
2025-09-14 02:02:07 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #11', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 8, 'val_loss': 4.894798934459686, 'val_avg_loss': 0.6118498668074608, 'val_seen': 8, 'val_correct': 4, 'val_acc': 0.5}}
2025-09-14 02:02:07 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #11', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 32, 'val_loss': 19.319059371948242, 'val_avg_loss': 0.6037206053733826, 'val_seen': 32, 'val_correct': 22, 'val_acc': 0.6875}}
2025-09-14 02:02:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:02:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:02:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:02:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:02:08 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.513611, avg_loss=0.637840, seen=40, correct=21, accuracy=0.525000
2025-09-14 02:02:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:02:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:02:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:02:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2200MB
2025-09-14 02:02:09 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 25.51361083984375, 'test_avg_loss': 0.6378402709960938, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-14 02:02:09 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 32, 'val_loss': 19.319059371948242, 'val_avg_loss': 0.6037206053733826, 'val_seen': 32, 'val_correct': 22, 'val_acc': 0.6875}
2025-09-14 02:02:09 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 25.51361083984375, 'test_avg_loss': 0.6378402709960938, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-14 02:02:09 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #11', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.5487165451049805, 'test_avg_loss': 0.654871654510498, 'test_seen': 10, 'test_correct': 4, 'test_acc': 0.4}}
2025-09-14 02:02:09 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #11', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 25.51361083984375, 'test_avg_loss': 0.6378402709960938, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}}
2025-09-14 02:02:09 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 25.51361083984375, 'test_avg_loss': 0.6378402709960938, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}, metrics={'val_total': 32, 'val_loss': 19.319059371948242, 'val_avg_loss': 0.6037206053733826, 'val_seen': 32, 'val_correct': 22, 'val_acc': 0.6875, 'test_total': 40, 'test_loss': 25.51361083984375, 'test_avg_loss': 0.6378402709960938, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-14 02:02:09 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 02:02:09 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:02:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-09-14 02:02:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:02:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=69, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:02:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-09-14 02:02:13 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=137, loss_sum=89.471024, avg_loss=0.653073, seen=137, correct=85, accuracy=0.620438
2025-09-14 02:02:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:02:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:02:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:02:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2200MB
2025-09-14 02:02:14 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 137, 'val_loss': 89.47102355957031, 'val_avg_loss': 0.6530731646683965, 'val_seen': 137, 'val_correct': 85, 'val_acc': 0.6204379562043796}
2025-09-14 02:02:14 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 02:02:14 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 137, 'val_loss': 89.47102355957031, 'val_avg_loss': 0.6530731646683965, 'val_seen': 137, 'val_correct': 85, 'val_acc': 0.6204379562043796}
2025-09-14 02:02:14 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #12', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 35, 'val_loss': 24.408268928527832, 'val_avg_loss': 0.6973791122436523, 'val_seen': 35, 'val_correct': 20, 'val_acc': 0.5714285714285714}}
2025-09-14 02:02:14 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #12', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 137, 'val_loss': 89.47102355957031, 'val_avg_loss': 0.6530731646683965, 'val_seen': 137, 'val_correct': 85, 'val_acc': 0.6204379562043796}}
2025-09-14 02:02:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:02:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:02:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:02:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:02:16 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.184029, avg_loss=0.629601, seen=40, correct=27, accuracy=0.675000
2025-09-14 02:02:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:02:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:02:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:02:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2200MB
2025-09-14 02:02:17 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 25.18402862548828, 'test_avg_loss': 0.629600715637207, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-14 02:02:17 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 137, 'val_loss': 89.47102355957031, 'val_avg_loss': 0.6530731646683965, 'val_seen': 137, 'val_correct': 85, 'val_acc': 0.6204379562043796}
2025-09-14 02:02:17 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 25.18402862548828, 'test_avg_loss': 0.629600715637207, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-14 02:02:17 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #12', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 4.602640807628632, 'test_avg_loss': 0.4602640807628632, 'test_seen': 10, 'test_correct': 8, 'test_acc': 0.8}}
2025-09-14 02:02:17 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #12', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 25.18402862548828, 'test_avg_loss': 0.629600715637207, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}}
2025-09-14 02:02:17 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 25.18402862548828, 'test_avg_loss': 0.629600715637207, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}, metrics={'val_total': 137, 'val_loss': 89.47102355957031, 'val_avg_loss': 0.6530731646683965, 'val_seen': 137, 'val_correct': 85, 'val_acc': 0.6204379562043796, 'test_total': 40, 'test_loss': 25.18402862548828, 'test_avg_loss': 0.629600715637207, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-14 02:02:17 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 02:02:17 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:02:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-09-14 02:02:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:02:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=36, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:02:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-14 02:02:19 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=72, loss_sum=47.685097, avg_loss=0.662293, seen=72, correct=48, accuracy=0.666667
2025-09-14 02:02:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:02:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:02:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:02:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2200MB
2025-09-14 02:02:21 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 72, 'val_loss': 47.685096740722656, 'val_avg_loss': 0.6622930102878146, 'val_seen': 72, 'val_correct': 48, 'val_acc': 0.6666666666666666}
2025-09-14 02:02:21 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 02:02:21 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 72, 'val_loss': 47.685096740722656, 'val_avg_loss': 0.6622930102878146, 'val_seen': 72, 'val_correct': 48, 'val_acc': 0.6666666666666666}
2025-09-14 02:02:21 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #13', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 18, 'val_loss': 11.932443529367447, 'val_avg_loss': 0.6629135294093026, 'val_seen': 18, 'val_correct': 11, 'val_acc': 0.6111111111111112}}
2025-09-14 02:02:21 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #13', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 72, 'val_loss': 47.685096740722656, 'val_avg_loss': 0.6622930102878146, 'val_seen': 72, 'val_correct': 48, 'val_acc': 0.6666666666666666}}
2025-09-14 02:02:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:02:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:02:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:02:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:02:22 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.354694, avg_loss=0.733867, seen=40, correct=25, accuracy=0.625000
2025-09-14 02:02:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:02:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:02:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:02:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2200MB
2025-09-14 02:02:23 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 29.354694366455078, 'test_avg_loss': 0.7338673591613769, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-14 02:02:23 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 72, 'val_loss': 47.685096740722656, 'val_avg_loss': 0.6622930102878146, 'val_seen': 72, 'val_correct': 48, 'val_acc': 0.6666666666666666}
2025-09-14 02:02:23 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 29.354694366455078, 'test_avg_loss': 0.7338673591613769, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-14 02:02:23 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #13', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 7.334519624710083, 'test_avg_loss': 0.7334519624710083, 'test_seen': 10, 'test_correct': 6, 'test_acc': 0.6}}
2025-09-14 02:02:23 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #13', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 29.354694366455078, 'test_avg_loss': 0.7338673591613769, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}}
2025-09-14 02:02:23 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 29.354694366455078, 'test_avg_loss': 0.7338673591613769, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}, metrics={'val_total': 72, 'val_loss': 47.685096740722656, 'val_avg_loss': 0.6622930102878146, 'val_seen': 72, 'val_correct': 48, 'val_acc': 0.6666666666666666, 'test_total': 40, 'test_loss': 29.354694366455078, 'test_avg_loss': 0.7338673591613769, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-14 02:02:23 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 02:02:23 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:02:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-09-14 02:02:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:02:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=80, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:02:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-09-14 02:02:28 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=160, loss_sum=105.877228, avg_loss=0.661733, seen=160, correct=100, accuracy=0.625000
2025-09-14 02:02:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:02:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:02:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:02:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2200MB
2025-09-14 02:02:29 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 160, 'val_loss': 105.87722778320312, 'val_avg_loss': 0.6617326736450195, 'val_seen': 160, 'val_correct': 100, 'val_acc': 0.625}
2025-09-14 02:02:29 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 02:02:29 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 160, 'val_loss': 105.87722778320312, 'val_avg_loss': 0.6617326736450195, 'val_seen': 160, 'val_correct': 100, 'val_acc': 0.625}
2025-09-14 02:02:29 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #14', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 40, 'val_loss': 25.35115087032318, 'val_avg_loss': 0.6337787717580795, 'val_seen': 40, 'val_correct': 22, 'val_acc': 0.55}}
2025-09-14 02:02:29 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #14', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 160, 'val_loss': 105.87722778320312, 'val_avg_loss': 0.6617326736450195, 'val_seen': 160, 'val_correct': 100, 'val_acc': 0.625}}
2025-09-14 02:02:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:02:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:02:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:02:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:02:30 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.502337, avg_loss=0.587558, seen=40, correct=29, accuracy=0.725000
2025-09-14 02:02:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:02:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:02:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:02:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2200MB
2025-09-14 02:02:31 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 23.502336502075195, 'test_avg_loss': 0.5875584125518799, 'test_seen': 40, 'test_correct': 29, 'test_acc': 0.725}
2025-09-14 02:02:31 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 160, 'val_loss': 105.87722778320312, 'val_avg_loss': 0.6617326736450195, 'val_seen': 160, 'val_correct': 100, 'val_acc': 0.625}
2025-09-14 02:02:31 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 23.502336502075195, 'test_avg_loss': 0.5875584125518799, 'test_seen': 40, 'test_correct': 29, 'test_acc': 0.725}
2025-09-14 02:02:31 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #14', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 5.494668364524841, 'test_avg_loss': 0.5494668364524842, 'test_seen': 10, 'test_correct': 7, 'test_acc': 0.7}}
2025-09-14 02:02:31 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #14', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 23.502336502075195, 'test_avg_loss': 0.5875584125518799, 'test_seen': 40, 'test_correct': 29, 'test_acc': 0.725}}
2025-09-14 02:02:31 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 23.502336502075195, 'test_avg_loss': 0.5875584125518799, 'test_seen': 40, 'test_correct': 29, 'test_acc': 0.725}, metrics={'val_total': 160, 'val_loss': 105.87722778320312, 'val_avg_loss': 0.6617326736450195, 'val_seen': 160, 'val_correct': 100, 'val_acc': 0.625, 'test_total': 40, 'test_loss': 23.502336502075195, 'test_avg_loss': 0.5875584125518799, 'test_seen': 40, 'test_correct': 29, 'test_acc': 0.725}
2025-09-14 02:02:31 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 02:02:31 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:02:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:02:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:02:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:02:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:02:37 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=132.835938, avg_loss=0.664180, seen=200, correct=123, accuracy=0.615000
2025-09-14 02:02:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:02:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:02:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:02:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2200MB
2025-09-14 02:02:39 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 200, 'val_loss': 132.8359375, 'val_avg_loss': 0.6641796875, 'val_seen': 200, 'val_correct': 123, 'val_acc': 0.615}
2025-09-14 02:02:39 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 02:02:39 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 200, 'val_loss': 132.8359375, 'val_avg_loss': 0.6641796875, 'val_seen': 200, 'val_correct': 123, 'val_acc': 0.615}
2025-09-14 02:02:39 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #15', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 50, 'val_loss': 31.135334134101868, 'val_avg_loss': 0.6227066826820373, 'val_seen': 50, 'val_correct': 35, 'val_acc': 0.7}}
2025-09-14 02:02:39 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #15', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 200, 'val_loss': 132.8359375, 'val_avg_loss': 0.6641796875, 'val_seen': 200, 'val_correct': 123, 'val_acc': 0.615}}
2025-09-14 02:02:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:02:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:02:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:02:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:02:39 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.665230, avg_loss=0.691631, seen=40, correct=23, accuracy=0.575000
2025-09-14 02:02:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:02:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:02:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:02:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2200MB
2025-09-14 02:02:41 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 27.66522979736328, 'test_avg_loss': 0.691630744934082, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 02:02:41 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 200, 'val_loss': 132.8359375, 'val_avg_loss': 0.6641796875, 'val_seen': 200, 'val_correct': 123, 'val_acc': 0.615}
2025-09-14 02:02:41 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 27.66522979736328, 'test_avg_loss': 0.691630744934082, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 02:02:41 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #15', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 5.926028549671173, 'test_avg_loss': 0.5926028549671173, 'test_seen': 10, 'test_correct': 6, 'test_acc': 0.6}}
2025-09-14 02:02:41 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #15', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 27.66522979736328, 'test_avg_loss': 0.691630744934082, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}}
2025-09-14 02:02:41 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 27.66522979736328, 'test_avg_loss': 0.691630744934082, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}, metrics={'val_total': 200, 'val_loss': 132.8359375, 'val_avg_loss': 0.6641796875, 'val_seen': 200, 'val_correct': 123, 'val_acc': 0.615, 'test_total': 40, 'test_loss': 27.66522979736328, 'test_avg_loss': 0.691630744934082, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 02:02:41 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 02:02:41 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:02:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-09-14 02:02:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:02:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:02:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 02:02:46 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=136, loss_sum=89.325020, avg_loss=0.656802, seen=136, correct=78, accuracy=0.573529
2025-09-14 02:02:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:02:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:02:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:02:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2200MB
2025-09-14 02:02:47 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 136, 'val_loss': 89.32501983642578, 'val_avg_loss': 0.6568016164443072, 'val_seen': 136, 'val_correct': 78, 'val_acc': 0.5735294117647058}
2025-09-14 02:02:47 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 02:02:47 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 136, 'val_loss': 89.32501983642578, 'val_avg_loss': 0.6568016164443072, 'val_seen': 136, 'val_correct': 78, 'val_acc': 0.5735294117647058}
2025-09-14 02:02:47 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #16', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 34, 'val_loss': 21.819040954113007, 'val_avg_loss': 0.6417364986503825, 'val_seen': 34, 'val_correct': 24, 'val_acc': 0.7058823529411765}}
2025-09-14 02:02:47 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #16', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 136, 'val_loss': 89.32501983642578, 'val_avg_loss': 0.6568016164443072, 'val_seen': 136, 'val_correct': 78, 'val_acc': 0.5735294117647058}}
2025-09-14 02:02:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:02:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:02:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:02:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:02:48 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.957201, avg_loss=0.648930, seen=40, correct=28, accuracy=0.700000
2025-09-14 02:02:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:02:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:02:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:02:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2200MB
2025-09-14 02:02:49 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 25.95720100402832, 'test_avg_loss': 0.648930025100708, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}
2025-09-14 02:02:49 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 136, 'val_loss': 89.32501983642578, 'val_avg_loss': 0.6568016164443072, 'val_seen': 136, 'val_correct': 78, 'val_acc': 0.5735294117647058}
2025-09-14 02:02:49 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 25.95720100402832, 'test_avg_loss': 0.648930025100708, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}
2025-09-14 02:02:49 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #16', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.548501431941986, 'test_avg_loss': 0.6548501431941987, 'test_seen': 10, 'test_correct': 7, 'test_acc': 0.7}}
2025-09-14 02:02:49 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #16', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 25.95720100402832, 'test_avg_loss': 0.648930025100708, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}}
2025-09-14 02:02:49 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 25.95720100402832, 'test_avg_loss': 0.648930025100708, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}, metrics={'val_total': 136, 'val_loss': 89.32501983642578, 'val_avg_loss': 0.6568016164443072, 'val_seen': 136, 'val_correct': 78, 'val_acc': 0.5735294117647058, 'test_total': 40, 'test_loss': 25.95720100402832, 'test_avg_loss': 0.648930025100708, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}
2025-09-14 02:02:49 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 02:02:49 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:02:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:02:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:02:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:02:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:02:56 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=126.486465, avg_loss=0.632432, seen=200, correct=133, accuracy=0.665000
2025-09-14 02:02:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:02:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:02:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:02:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2200MB
2025-09-14 02:02:58 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 200, 'val_loss': 126.48646545410156, 'val_avg_loss': 0.6324323272705078, 'val_seen': 200, 'val_correct': 133, 'val_acc': 0.665}
2025-09-14 02:02:58 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 02:02:58 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 200, 'val_loss': 126.48646545410156, 'val_avg_loss': 0.6324323272705078, 'val_seen': 200, 'val_correct': 133, 'val_acc': 0.665}
2025-09-14 02:02:58 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #17', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 50, 'val_loss': 29.811615765094757, 'val_avg_loss': 0.5962323153018951, 'val_seen': 50, 'val_correct': 36, 'val_acc': 0.72}}
2025-09-14 02:02:58 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #17', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 200, 'val_loss': 126.48646545410156, 'val_avg_loss': 0.6324323272705078, 'val_seen': 200, 'val_correct': 133, 'val_acc': 0.665}}
2025-09-14 02:02:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:02:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:02:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:02:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:02:59 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.683052, avg_loss=0.692076, seen=40, correct=25, accuracy=0.625000
2025-09-14 02:02:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:02:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:03:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:03:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2200MB
2025-09-14 02:03:00 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 27.68305206298828, 'test_avg_loss': 0.692076301574707, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-14 02:03:00 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 200, 'val_loss': 126.48646545410156, 'val_avg_loss': 0.6324323272705078, 'val_seen': 200, 'val_correct': 133, 'val_acc': 0.665}
2025-09-14 02:03:00 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 27.68305206298828, 'test_avg_loss': 0.692076301574707, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-14 02:03:00 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #17', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 8.311540007591248, 'test_avg_loss': 0.8311540007591247, 'test_seen': 10, 'test_correct': 6, 'test_acc': 0.6}}
2025-09-14 02:03:00 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #17', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 27.68305206298828, 'test_avg_loss': 0.692076301574707, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}}
2025-09-14 02:03:00 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 27.68305206298828, 'test_avg_loss': 0.692076301574707, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}, metrics={'val_total': 200, 'val_loss': 126.48646545410156, 'val_avg_loss': 0.6324323272705078, 'val_seen': 200, 'val_correct': 133, 'val_acc': 0.665, 'test_total': 40, 'test_loss': 27.68305206298828, 'test_avg_loss': 0.692076301574707, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-14 02:03:00 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 02:03:01 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:03:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-09-14 02:03:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:03:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:03:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 02:03:05 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=135, loss_sum=91.472649, avg_loss=0.677575, seen=135, correct=79, accuracy=0.585185
2025-09-14 02:03:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:03:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:03:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:03:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2200MB
2025-09-14 02:03:06 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 135, 'val_loss': 91.47264862060547, 'val_avg_loss': 0.6775751749674479, 'val_seen': 135, 'val_correct': 79, 'val_acc': 0.5851851851851851}
2025-09-14 02:03:06 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 02:03:06 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 135, 'val_loss': 91.47264862060547, 'val_avg_loss': 0.6775751749674479, 'val_seen': 135, 'val_correct': 79, 'val_acc': 0.5851851851851851}
2025-09-14 02:03:06 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #18', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 34, 'val_loss': 22.04405027627945, 'val_avg_loss': 0.6483544198905721, 'val_seen': 34, 'val_correct': 21, 'val_acc': 0.6176470588235294}}
2025-09-14 02:03:06 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #18', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 135, 'val_loss': 91.47264862060547, 'val_avg_loss': 0.6775751749674479, 'val_seen': 135, 'val_correct': 79, 'val_acc': 0.5851851851851851}}
2025-09-14 02:03:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:03:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:03:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:03:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:03:08 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.947540, avg_loss=0.623689, seen=40, correct=23, accuracy=0.575000
2025-09-14 02:03:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:03:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:03:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:03:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2200MB
2025-09-14 02:03:09 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 24.947540283203125, 'test_avg_loss': 0.6236885070800782, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 02:03:09 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 135, 'val_loss': 91.47264862060547, 'val_avg_loss': 0.6775751749674479, 'val_seen': 135, 'val_correct': 79, 'val_acc': 0.5851851851851851}
2025-09-14 02:03:09 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 24.947540283203125, 'test_avg_loss': 0.6236885070800782, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 02:03:09 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #18', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 5.491641581058502, 'test_avg_loss': 0.5491641581058502, 'test_seen': 10, 'test_correct': 7, 'test_acc': 0.7}}
2025-09-14 02:03:09 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #18', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 24.947540283203125, 'test_avg_loss': 0.6236885070800782, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}}
2025-09-14 02:03:09 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 24.947540283203125, 'test_avg_loss': 0.6236885070800782, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}, metrics={'val_total': 135, 'val_loss': 91.47264862060547, 'val_avg_loss': 0.6775751749674479, 'val_seen': 135, 'val_correct': 79, 'val_acc': 0.5851851851851851, 'test_total': 40, 'test_loss': 24.947540283203125, 'test_avg_loss': 0.6236885070800782, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 02:03:09 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 02:03:09 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:03:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-14 02:03:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:03:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:03:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 02:03:12 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=69.261154, avg_loss=0.629647, seen=110, correct=67, accuracy=0.609091
2025-09-14 02:03:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:03:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:03:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:03:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2200MB
2025-09-14 02:03:14 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 110, 'val_loss': 69.26115417480469, 'val_avg_loss': 0.629646856134588, 'val_seen': 110, 'val_correct': 67, 'val_acc': 0.6090909090909091}
2025-09-14 02:03:14 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 02:03:14 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 110, 'val_loss': 69.26115417480469, 'val_avg_loss': 0.629646856134588, 'val_seen': 110, 'val_correct': 67, 'val_acc': 0.6090909090909091}
2025-09-14 02:03:14 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #19', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 28, 'val_loss': 17.23792850971222, 'val_avg_loss': 0.6156403039182935, 'val_seen': 28, 'val_correct': 18, 'val_acc': 0.6428571428571429}}
2025-09-14 02:03:14 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #19', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 110, 'val_loss': 69.26115417480469, 'val_avg_loss': 0.629646856134588, 'val_seen': 110, 'val_correct': 67, 'val_acc': 0.6090909090909091}}
2025-09-14 02:03:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:03:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:03:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:03:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:03:15 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.898016, avg_loss=0.772450, seen=40, correct=23, accuracy=0.575000
2025-09-14 02:03:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:03:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:03:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:03:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2200MB
2025-09-14 02:03:16 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 30.89801597595215, 'test_avg_loss': 0.7724503993988037, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 02:03:16 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 110, 'val_loss': 69.26115417480469, 'val_avg_loss': 0.629646856134588, 'val_seen': 110, 'val_correct': 67, 'val_acc': 0.6090909090909091}
2025-09-14 02:03:16 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 30.89801597595215, 'test_avg_loss': 0.7724503993988037, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 02:03:16 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #19', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 7.80921745300293, 'test_avg_loss': 0.780921745300293, 'test_seen': 10, 'test_correct': 6, 'test_acc': 0.6}}
2025-09-14 02:03:16 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #19', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 30.89801597595215, 'test_avg_loss': 0.7724503993988037, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}}
2025-09-14 02:03:16 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 30.89801597595215, 'test_avg_loss': 0.7724503993988037, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}, metrics={'val_total': 110, 'val_loss': 69.26115417480469, 'val_avg_loss': 0.629646856134588, 'val_seen': 110, 'val_correct': 67, 'val_acc': 0.6090909090909091, 'test_total': 40, 'test_loss': 30.89801597595215, 'test_avg_loss': 0.7724503993988037, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 02:03:16 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 02:03:16 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:03:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-09-14 02:03:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:03:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=63, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:03:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-09-14 02:03:21 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=126, loss_sum=87.442879, avg_loss=0.693991, seen=126, correct=76, accuracy=0.603175
2025-09-14 02:03:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:03:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:03:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:03:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2200MB
2025-09-14 02:03:22 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 126, 'val_loss': 87.44287872314453, 'val_avg_loss': 0.6939911009773375, 'val_seen': 126, 'val_correct': 76, 'val_acc': 0.6031746031746031}
2025-09-14 02:03:22 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 02:03:22 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 126, 'val_loss': 87.44287872314453, 'val_avg_loss': 0.6939911009773375, 'val_seen': 126, 'val_correct': 76, 'val_acc': 0.6031746031746031}
2025-09-14 02:03:22 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #20', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 32, 'val_loss': 20.441312789916992, 'val_avg_loss': 0.638791024684906, 'val_seen': 32, 'val_correct': 20, 'val_acc': 0.625}}
2025-09-14 02:03:22 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #20', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 126, 'val_loss': 87.44287872314453, 'val_avg_loss': 0.6939911009773375, 'val_seen': 126, 'val_correct': 76, 'val_acc': 0.6031746031746031}}
2025-09-14 02:03:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:03:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:03:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:03:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:03:24 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.069031, avg_loss=0.626726, seen=40, correct=25, accuracy=0.625000
2025-09-14 02:03:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:03:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:03:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:03:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2200MB
2025-09-14 02:03:25 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 25.06903076171875, 'test_avg_loss': 0.6267257690429687, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-14 02:03:25 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 126, 'val_loss': 87.44287872314453, 'val_avg_loss': 0.6939911009773375, 'val_seen': 126, 'val_correct': 76, 'val_acc': 0.6031746031746031}
2025-09-14 02:03:25 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 25.06903076171875, 'test_avg_loss': 0.6267257690429687, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-14 02:03:25 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #20', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.602296233177185, 'test_avg_loss': 0.6602296233177185, 'test_seen': 10, 'test_correct': 5, 'test_acc': 0.5}}
2025-09-14 02:03:25 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #20', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 25.06903076171875, 'test_avg_loss': 0.6267257690429687, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}}
2025-09-14 02:03:25 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 25.06903076171875, 'test_avg_loss': 0.6267257690429687, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}, metrics={'val_total': 126, 'val_loss': 87.44287872314453, 'val_avg_loss': 0.6939911009773375, 'val_seen': 126, 'val_correct': 76, 'val_acc': 0.6031746031746031, 'test_total': 40, 'test_loss': 25.06903076171875, 'test_avg_loss': 0.6267257690429687, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-14 02:03:25 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 02:03:25 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:03:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-09-14 02:03:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:03:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=77, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:03:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-09-14 02:03:30 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=153, loss_sum=103.441376, avg_loss=0.676087, seen=153, correct=90, accuracy=0.588235
2025-09-14 02:03:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:03:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:03:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:03:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2200MB
2025-09-14 02:03:31 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 153, 'val_loss': 103.44137573242188, 'val_avg_loss': 0.6760874230877246, 'val_seen': 153, 'val_correct': 90, 'val_acc': 0.5882352941176471}
2025-09-14 02:03:31 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 02:03:31 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 153, 'val_loss': 103.44137573242188, 'val_avg_loss': 0.6760874230877246, 'val_seen': 153, 'val_correct': 90, 'val_acc': 0.5882352941176471}
2025-09-14 02:03:31 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #21', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 39, 'val_loss': 23.519360184669495, 'val_avg_loss': 0.6030605175556281, 'val_seen': 39, 'val_correct': 28, 'val_acc': 0.717948717948718}}
2025-09-14 02:03:31 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #21', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 153, 'val_loss': 103.44137573242188, 'val_avg_loss': 0.6760874230877246, 'val_seen': 153, 'val_correct': 90, 'val_acc': 0.5882352941176471}}
2025-09-14 02:03:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:03:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:03:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:03:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:03:32 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.796127, avg_loss=0.744903, seen=40, correct=24, accuracy=0.600000
2025-09-14 02:03:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:03:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:03:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:03:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2200MB
2025-09-14 02:03:33 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 29.796127319335938, 'test_avg_loss': 0.7449031829833984, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-14 02:03:33 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 153, 'val_loss': 103.44137573242188, 'val_avg_loss': 0.6760874230877246, 'val_seen': 153, 'val_correct': 90, 'val_acc': 0.5882352941176471}
2025-09-14 02:03:33 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 29.796127319335938, 'test_avg_loss': 0.7449031829833984, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-14 02:03:33 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #21', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.118758738040924, 'test_avg_loss': 0.6118758738040924, 'test_seen': 10, 'test_correct': 8, 'test_acc': 0.8}}
2025-09-14 02:03:33 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #21', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 29.796127319335938, 'test_avg_loss': 0.7449031829833984, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}}
2025-09-14 02:03:33 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 29.796127319335938, 'test_avg_loss': 0.7449031829833984, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}, metrics={'val_total': 153, 'val_loss': 103.44137573242188, 'val_avg_loss': 0.6760874230877246, 'val_seen': 153, 'val_correct': 90, 'val_acc': 0.5882352941176471, 'test_total': 40, 'test_loss': 29.796127319335938, 'test_avg_loss': 0.7449031829833984, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-14 02:03:33 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 02:03:34 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:03:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 02:03:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:03:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:03:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 02:03:35 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=6.726081, avg_loss=0.611462, seen=11, correct=8, accuracy=0.727273
2025-09-14 02:03:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:03:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:03:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:03:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2200MB
2025-09-14 02:03:37 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 11, 'val_loss': 6.726081371307373, 'val_avg_loss': 0.6114619428461249, 'val_seen': 11, 'val_correct': 8, 'val_acc': 0.7272727272727273}
2025-09-14 02:03:37 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 02:03:37 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 11, 'val_loss': 6.726081371307373, 'val_avg_loss': 0.6114619428461249, 'val_seen': 11, 'val_correct': 8, 'val_acc': 0.7272727272727273}
2025-09-14 02:03:37 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #22', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 3, 'val_loss': 1.55143541097641, 'val_avg_loss': 0.5171451369921366, 'val_seen': 3, 'val_correct': 3, 'val_acc': 1.0}}
2025-09-14 02:03:37 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #22', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 11, 'val_loss': 6.726081371307373, 'val_avg_loss': 0.6114619428461249, 'val_seen': 11, 'val_correct': 8, 'val_acc': 0.7272727272727273}}
2025-09-14 02:03:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:03:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:03:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:03:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:03:38 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.364103, avg_loss=0.759103, seen=40, correct=19, accuracy=0.475000
2025-09-14 02:03:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:03:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:03:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:03:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2200MB
2025-09-14 02:03:39 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 30.364103317260742, 'test_avg_loss': 0.7591025829315186, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-14 02:03:39 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 11, 'val_loss': 6.726081371307373, 'val_avg_loss': 0.6114619428461249, 'val_seen': 11, 'val_correct': 8, 'val_acc': 0.7272727272727273}
2025-09-14 02:03:39 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 30.364103317260742, 'test_avg_loss': 0.7591025829315186, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-14 02:03:39 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #22', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 7.839238524436951, 'test_avg_loss': 0.7839238524436951, 'test_seen': 10, 'test_correct': 4, 'test_acc': 0.4}}
2025-09-14 02:03:39 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #22', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 30.364103317260742, 'test_avg_loss': 0.7591025829315186, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}}
2025-09-14 02:03:39 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 30.364103317260742, 'test_avg_loss': 0.7591025829315186, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}, metrics={'val_total': 11, 'val_loss': 6.726081371307373, 'val_avg_loss': 0.6114619428461249, 'val_seen': 11, 'val_correct': 8, 'val_acc': 0.7272727272727273, 'test_total': 40, 'test_loss': 30.364103317260742, 'test_avg_loss': 0.7591025829315186, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-14 02:03:39 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 02:03:39 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:03:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-09-14 02:03:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:03:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=15, num_train_batch_last_epoch=200, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:03:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-14 02:03:41 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=30, loss_sum=17.936758, avg_loss=0.597892, seen=30, correct=22, accuracy=0.733333
2025-09-14 02:03:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:03:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:03:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:03:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2200MB
2025-09-14 02:03:42 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 30, 'val_loss': 17.936758041381836, 'val_avg_loss': 0.5978919347127278, 'val_seen': 30, 'val_correct': 22, 'val_acc': 0.7333333333333333}
2025-09-14 02:03:42 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 02:03:42 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 30, 'val_loss': 17.936758041381836, 'val_avg_loss': 0.5978919347127278, 'val_seen': 30, 'val_correct': 22, 'val_acc': 0.7333333333333333}
2025-09-14 02:03:42 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #23', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 8, 'val_loss': 5.328504800796509, 'val_avg_loss': 0.6660631000995636, 'val_seen': 8, 'val_correct': 4, 'val_acc': 0.5}}
2025-09-14 02:03:42 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #23', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 30, 'val_loss': 17.936758041381836, 'val_avg_loss': 0.5978919347127278, 'val_seen': 30, 'val_correct': 22, 'val_acc': 0.7333333333333333}}
2025-09-14 02:03:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:03:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:03:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:03:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:03:43 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.355257, avg_loss=0.633881, seen=40, correct=23, accuracy=0.575000
2025-09-14 02:03:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:03:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:03:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:03:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2200MB
2025-09-14 02:03:44 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 25.355257034301758, 'test_avg_loss': 0.6338814258575439, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 02:03:44 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 30, 'val_loss': 17.936758041381836, 'val_avg_loss': 0.5978919347127278, 'val_seen': 30, 'val_correct': 22, 'val_acc': 0.7333333333333333}
2025-09-14 02:03:44 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 25.355257034301758, 'test_avg_loss': 0.6338814258575439, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 02:03:44 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #23', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.0005422830581665, 'test_avg_loss': 0.6000542283058167, 'test_seen': 10, 'test_correct': 6, 'test_acc': 0.6}}
2025-09-14 02:03:44 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #23', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 25.355257034301758, 'test_avg_loss': 0.6338814258575439, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}}
2025-09-14 02:03:44 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 25.355257034301758, 'test_avg_loss': 0.6338814258575439, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}, metrics={'val_total': 30, 'val_loss': 17.936758041381836, 'val_avg_loss': 0.5978919347127278, 'val_seen': 30, 'val_correct': 22, 'val_acc': 0.7333333333333333, 'test_total': 40, 'test_loss': 25.355257034301758, 'test_avg_loss': 0.6338814258575439, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 02:03:44 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 02:03:44 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:03:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:03:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:03:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:03:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:03:50 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=134.321289, avg_loss=0.671606, seen=200, correct=126, accuracy=0.630000
2025-09-14 02:03:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:03:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:03:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:03:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2200MB
2025-09-14 02:03:51 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 200, 'val_loss': 134.3212890625, 'val_avg_loss': 0.6716064453125, 'val_seen': 200, 'val_correct': 126, 'val_acc': 0.63}
2025-09-14 02:03:51 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 02:03:51 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 200, 'val_loss': 134.3212890625, 'val_avg_loss': 0.6716064453125, 'val_seen': 200, 'val_correct': 126, 'val_acc': 0.63}
2025-09-14 02:03:51 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #24', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 50, 'val_loss': 29.785039484500885, 'val_avg_loss': 0.5957007896900177, 'val_seen': 50, 'val_correct': 35, 'val_acc': 0.7}}
2025-09-14 02:03:51 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #24', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 200, 'val_loss': 134.3212890625, 'val_avg_loss': 0.6716064453125, 'val_seen': 200, 'val_correct': 126, 'val_acc': 0.63}}
2025-09-14 02:03:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:03:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:03:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:03:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:03:52 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.357374, avg_loss=0.733934, seen=40, correct=21, accuracy=0.525000
2025-09-14 02:03:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:03:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:03:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:03:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2200MB
2025-09-14 02:03:53 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 29.35737419128418, 'test_avg_loss': 0.7339343547821044, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-14 02:03:53 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 200, 'val_loss': 134.3212890625, 'val_avg_loss': 0.6716064453125, 'val_seen': 200, 'val_correct': 126, 'val_acc': 0.63}
2025-09-14 02:03:53 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 29.35737419128418, 'test_avg_loss': 0.7339343547821044, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-14 02:03:53 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #24', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.570501148700714, 'test_avg_loss': 0.6570501148700714, 'test_seen': 10, 'test_correct': 6, 'test_acc': 0.6}}
2025-09-14 02:03:53 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #24', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 29.35737419128418, 'test_avg_loss': 0.7339343547821044, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}}
2025-09-14 02:03:53 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 29.35737419128418, 'test_avg_loss': 0.7339343547821044, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}, metrics={'val_total': 200, 'val_loss': 134.3212890625, 'val_avg_loss': 0.6716064453125, 'val_seen': 200, 'val_correct': 126, 'val_acc': 0.63, 'test_total': 40, 'test_loss': 29.35737419128418, 'test_avg_loss': 0.7339343547821044, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-14 02:03:53 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 02:03:53 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:03:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:03:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:03:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:03:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:03:59 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=141.218323, avg_loss=0.706092, seen=200, correct=102, accuracy=0.510000
2025-09-14 02:03:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:03:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:04:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:04:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2200MB
2025-09-14 02:04:01 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 200, 'val_loss': 141.21832275390625, 'val_avg_loss': 0.7060916137695312, 'val_seen': 200, 'val_correct': 102, 'val_acc': 0.51}
2025-09-14 02:04:01 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 02:04:01 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 200, 'val_loss': 141.21832275390625, 'val_avg_loss': 0.7060916137695312, 'val_seen': 200, 'val_correct': 102, 'val_acc': 0.51}
2025-09-14 02:04:01 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #25', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 50, 'val_loss': 35.71499127149582, 'val_avg_loss': 0.7142998254299164, 'val_seen': 50, 'val_correct': 24, 'val_acc': 0.48}}
2025-09-14 02:04:01 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #25', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 200, 'val_loss': 141.21832275390625, 'val_avg_loss': 0.7060916137695312, 'val_seen': 200, 'val_correct': 102, 'val_acc': 0.51}}
2025-09-14 02:04:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:04:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:04:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:04:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:04:03 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=34.815933, avg_loss=0.870398, seen=40, correct=20, accuracy=0.500000
2025-09-14 02:04:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:04:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:04:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:04:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2200MB
2025-09-14 02:04:04 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 34.81593322753906, 'test_avg_loss': 0.8703983306884766, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-14 02:04:04 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 200, 'val_loss': 141.21832275390625, 'val_avg_loss': 0.7060916137695312, 'val_seen': 200, 'val_correct': 102, 'val_acc': 0.51}
2025-09-14 02:04:04 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 34.81593322753906, 'test_avg_loss': 0.8703983306884766, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-14 02:04:04 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #25', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 8.26791000366211, 'test_avg_loss': 0.8267910003662109, 'test_seen': 10, 'test_correct': 6, 'test_acc': 0.6}}
2025-09-14 02:04:04 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #25', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 34.81593322753906, 'test_avg_loss': 0.8703983306884766, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}}
2025-09-14 02:04:04 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 34.81593322753906, 'test_avg_loss': 0.8703983306884766, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}, metrics={'val_total': 200, 'val_loss': 141.21832275390625, 'val_avg_loss': 0.7060916137695312, 'val_seen': 200, 'val_correct': 102, 'val_acc': 0.51, 'test_total': 40, 'test_loss': 34.81593322753906, 'test_avg_loss': 0.8703983306884766, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-14 02:04:04 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 02:04:04 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:04:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-09-14 02:04:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:04:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=81, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:04:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-09-14 02:04:10 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=161, loss_sum=101.685844, avg_loss=0.631589, seen=161, correct=111, accuracy=0.689441
2025-09-14 02:04:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:04:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:04:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:04:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2200MB
2025-09-14 02:04:11 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 161, 'val_loss': 101.68584442138672, 'val_avg_loss': 0.6315890957850107, 'val_seen': 161, 'val_correct': 111, 'val_acc': 0.6894409937888198}
2025-09-14 02:04:11 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 02:04:11 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 161, 'val_loss': 101.68584442138672, 'val_avg_loss': 0.6315890957850107, 'val_seen': 161, 'val_correct': 111, 'val_acc': 0.6894409937888198}
2025-09-14 02:04:11 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #26', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 41, 'val_loss': 24.96247708797455, 'val_avg_loss': 0.608840904584745, 'val_seen': 41, 'val_correct': 27, 'val_acc': 0.6585365853658537}}
2025-09-14 02:04:11 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #26', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 161, 'val_loss': 101.68584442138672, 'val_avg_loss': 0.6315890957850107, 'val_seen': 161, 'val_correct': 111, 'val_acc': 0.6894409937888198}}
2025-09-14 02:04:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:04:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:04:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:04:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:04:13 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.463200, avg_loss=0.611580, seen=40, correct=28, accuracy=0.700000
2025-09-14 02:04:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:04:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:04:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:04:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2200MB
2025-09-14 02:04:14 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 24.463199615478516, 'test_avg_loss': 0.6115799903869629, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}
2025-09-14 02:04:14 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 161, 'val_loss': 101.68584442138672, 'val_avg_loss': 0.6315890957850107, 'val_seen': 161, 'val_correct': 111, 'val_acc': 0.6894409937888198}
2025-09-14 02:04:14 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 24.463199615478516, 'test_avg_loss': 0.6115799903869629, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}
2025-09-14 02:04:14 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #26', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 5.8063984811306, 'test_avg_loss': 0.58063984811306, 'test_seen': 10, 'test_correct': 7, 'test_acc': 0.7}}
2025-09-14 02:04:14 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #26', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 24.463199615478516, 'test_avg_loss': 0.6115799903869629, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}}
2025-09-14 02:04:14 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 24.463199615478516, 'test_avg_loss': 0.6115799903869629, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}, metrics={'val_total': 161, 'val_loss': 101.68584442138672, 'val_avg_loss': 0.6315890957850107, 'val_seen': 161, 'val_correct': 111, 'val_acc': 0.6894409937888198, 'test_total': 40, 'test_loss': 24.463199615478516, 'test_avg_loss': 0.6115799903869629, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}
2025-09-14 02:04:14 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 02:04:14 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:04:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-09-14 02:04:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:04:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=62, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:04:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-09-14 02:04:18 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=123, loss_sum=81.497147, avg_loss=0.662578, seen=123, correct=72, accuracy=0.585366
2025-09-14 02:04:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:04:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:04:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:04:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2200MB
2025-09-14 02:04:19 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 123, 'val_loss': 81.49714660644531, 'val_avg_loss': 0.6625784276946773, 'val_seen': 123, 'val_correct': 72, 'val_acc': 0.5853658536585366}
2025-09-14 02:04:19 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 02:04:19 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 123, 'val_loss': 81.49714660644531, 'val_avg_loss': 0.6625784276946773, 'val_seen': 123, 'val_correct': 72, 'val_acc': 0.5853658536585366}
2025-09-14 02:04:19 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #27', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 31, 'val_loss': 18.418301671743393, 'val_avg_loss': 0.5941387636046256, 'val_seen': 31, 'val_correct': 21, 'val_acc': 0.6774193548387096}}
2025-09-14 02:04:19 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #27', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 123, 'val_loss': 81.49714660644531, 'val_avg_loss': 0.6625784276946773, 'val_seen': 123, 'val_correct': 72, 'val_acc': 0.5853658536585366}}
2025-09-14 02:04:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:04:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:04:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:04:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:04:21 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.136164, avg_loss=0.703404, seen=40, correct=19, accuracy=0.475000
2025-09-14 02:04:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:04:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:04:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:04:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2200MB
2025-09-14 02:04:22 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 28.13616371154785, 'test_avg_loss': 0.7034040927886963, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-14 02:04:22 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 123, 'val_loss': 81.49714660644531, 'val_avg_loss': 0.6625784276946773, 'val_seen': 123, 'val_correct': 72, 'val_acc': 0.5853658536585366}
2025-09-14 02:04:22 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 28.13616371154785, 'test_avg_loss': 0.7034040927886963, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-14 02:04:22 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #27', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.845880508422852, 'test_avg_loss': 0.6845880508422851, 'test_seen': 10, 'test_correct': 5, 'test_acc': 0.5}}
2025-09-14 02:04:22 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #27', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 28.13616371154785, 'test_avg_loss': 0.7034040927886963, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}}
2025-09-14 02:04:22 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 28.13616371154785, 'test_avg_loss': 0.7034040927886963, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}, metrics={'val_total': 123, 'val_loss': 81.49714660644531, 'val_avg_loss': 0.6625784276946773, 'val_seen': 123, 'val_correct': 72, 'val_acc': 0.5853658536585366, 'test_total': 40, 'test_loss': 28.13616371154785, 'test_avg_loss': 0.7034040927886963, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-14 02:04:22 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 02:04:22 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:04:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-09-14 02:04:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:04:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=38, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:04:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-14 02:04:24 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=75, loss_sum=54.192154, avg_loss=0.722562, seen=75, correct=37, accuracy=0.493333
2025-09-14 02:04:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:04:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:04:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:04:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2200MB
2025-09-14 02:04:25 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 75, 'val_loss': 54.19215393066406, 'val_avg_loss': 0.7225620524088542, 'val_seen': 75, 'val_correct': 37, 'val_acc': 0.49333333333333335}
2025-09-14 02:04:25 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 02:04:25 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 75, 'val_loss': 54.19215393066406, 'val_avg_loss': 0.7225620524088542, 'val_seen': 75, 'val_correct': 37, 'val_acc': 0.49333333333333335}
2025-09-14 02:04:25 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #28', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 19, 'val_loss': 15.985178112983704, 'val_avg_loss': 0.8413251638412476, 'val_seen': 19, 'val_correct': 7, 'val_acc': 0.3684210526315789}}
2025-09-14 02:04:25 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #28', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 75, 'val_loss': 54.19215393066406, 'val_avg_loss': 0.7225620524088542, 'val_seen': 75, 'val_correct': 37, 'val_acc': 0.49333333333333335}}
2025-09-14 02:04:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:04:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:04:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:04:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:04:27 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.103092, avg_loss=0.702577, seen=40, correct=23, accuracy=0.575000
2025-09-14 02:04:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:04:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:04:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:04:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2200MB
2025-09-14 02:04:28 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 28.103092193603516, 'test_avg_loss': 0.7025773048400878, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 02:04:28 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 75, 'val_loss': 54.19215393066406, 'val_avg_loss': 0.7225620524088542, 'val_seen': 75, 'val_correct': 37, 'val_acc': 0.49333333333333335}
2025-09-14 02:04:28 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 28.103092193603516, 'test_avg_loss': 0.7025773048400878, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 02:04:28 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #28', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 7.065112054347992, 'test_avg_loss': 0.7065112054347992, 'test_seen': 10, 'test_correct': 6, 'test_acc': 0.6}}
2025-09-14 02:04:28 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #28', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 28.103092193603516, 'test_avg_loss': 0.7025773048400878, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}}
2025-09-14 02:04:28 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 28.103092193603516, 'test_avg_loss': 0.7025773048400878, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}, metrics={'val_total': 75, 'val_loss': 54.19215393066406, 'val_avg_loss': 0.7225620524088542, 'val_seen': 75, 'val_correct': 37, 'val_acc': 0.49333333333333335, 'test_total': 40, 'test_loss': 28.103092193603516, 'test_avg_loss': 0.7025773048400878, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 02:04:28 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 02:04:28 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:04:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:04:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:04:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:04:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:04:34 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=138.284454, avg_loss=0.691422, seen=200, correct=113, accuracy=0.565000
2025-09-14 02:04:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:04:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:04:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:04:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2200MB
2025-09-14 02:04:35 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 200, 'val_loss': 138.28445434570312, 'val_avg_loss': 0.6914222717285157, 'val_seen': 200, 'val_correct': 113, 'val_acc': 0.565}
2025-09-14 02:04:35 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 02:04:35 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 200, 'val_loss': 138.28445434570312, 'val_avg_loss': 0.6914222717285157, 'val_seen': 200, 'val_correct': 113, 'val_acc': 0.565}
2025-09-14 02:04:35 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #29', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 50, 'val_loss': 35.52138561010361, 'val_avg_loss': 0.7104277122020721, 'val_seen': 50, 'val_correct': 26, 'val_acc': 0.52}}
2025-09-14 02:04:35 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #29', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 200, 'val_loss': 138.28445434570312, 'val_avg_loss': 0.6914222717285157, 'val_seen': 200, 'val_correct': 113, 'val_acc': 0.565}}
2025-09-14 02:04:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:04:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:04:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:04:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:04:37 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=22.510845, avg_loss=0.562771, seen=40, correct=26, accuracy=0.650000
2025-09-14 02:04:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:04:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:04:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:04:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2200MB
2025-09-14 02:04:38 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 22.510845184326172, 'test_avg_loss': 0.5627711296081543, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-14 02:04:38 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 200, 'val_loss': 138.28445434570312, 'val_avg_loss': 0.6914222717285157, 'val_seen': 200, 'val_correct': 113, 'val_acc': 0.565}
2025-09-14 02:04:38 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 22.510845184326172, 'test_avg_loss': 0.5627711296081543, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-14 02:04:38 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #29', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 7.771138548851013, 'test_avg_loss': 0.7771138548851013, 'test_seen': 10, 'test_correct': 5, 'test_acc': 0.5}}
2025-09-14 02:04:38 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #29', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 22.510845184326172, 'test_avg_loss': 0.5627711296081543, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}}
2025-09-14 02:04:38 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 22.510845184326172, 'test_avg_loss': 0.5627711296081543, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}, metrics={'val_total': 200, 'val_loss': 138.28445434570312, 'val_avg_loss': 0.6914222717285157, 'val_seen': 200, 'val_correct': 113, 'val_acc': 0.565, 'test_total': 40, 'test_loss': 22.510845184326172, 'test_avg_loss': 0.5627711296081543, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-14 02:04:38 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 02:04:38 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:04:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-09-14 02:04:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:04:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=85, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:04:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-09-14 02:04:44 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=170, loss_sum=110.708176, avg_loss=0.651225, seen=170, correct=108, accuracy=0.635294
2025-09-14 02:04:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:04:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:04:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:04:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2200MB
2025-09-14 02:04:45 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 170, 'val_loss': 110.70817565917969, 'val_avg_loss': 0.651224562701057, 'val_seen': 170, 'val_correct': 108, 'val_acc': 0.6352941176470588}
2025-09-14 02:04:45 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 02:04:45 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 170, 'val_loss': 110.70817565917969, 'val_avg_loss': 0.651224562701057, 'val_seen': 170, 'val_correct': 108, 'val_acc': 0.6352941176470588}
2025-09-14 02:04:45 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #30', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 43, 'val_loss': 28.326733231544495, 'val_avg_loss': 0.6587612379428952, 'val_seen': 43, 'val_correct': 26, 'val_acc': 0.6046511627906976}}
2025-09-14 02:04:45 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #30', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 170, 'val_loss': 110.70817565917969, 'val_avg_loss': 0.651224562701057, 'val_seen': 170, 'val_correct': 108, 'val_acc': 0.6352941176470588}}
2025-09-14 02:04:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:04:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:04:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:04:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:04:46 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.693569, avg_loss=0.642339, seen=40, correct=25, accuracy=0.625000
2025-09-14 02:04:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:04:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:04:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:04:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2200MB
2025-09-14 02:04:47 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 25.69356918334961, 'test_avg_loss': 0.6423392295837402, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-14 02:04:47 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 170, 'val_loss': 110.70817565917969, 'val_avg_loss': 0.651224562701057, 'val_seen': 170, 'val_correct': 108, 'val_acc': 0.6352941176470588}
2025-09-14 02:04:47 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 25.69356918334961, 'test_avg_loss': 0.6423392295837402, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-14 02:04:47 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #30', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 7.307033479213715, 'test_avg_loss': 0.7307033479213715, 'test_seen': 10, 'test_correct': 5, 'test_acc': 0.5}}
2025-09-14 02:04:47 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #30', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 25.69356918334961, 'test_avg_loss': 0.6423392295837402, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}}
2025-09-14 02:04:47 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 25.69356918334961, 'test_avg_loss': 0.6423392295837402, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}, metrics={'val_total': 170, 'val_loss': 110.70817565917969, 'val_avg_loss': 0.651224562701057, 'val_seen': 170, 'val_correct': 108, 'val_acc': 0.6352941176470588, 'test_total': 40, 'test_loss': 25.69356918334961, 'test_avg_loss': 0.6423392295837402, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-14 02:04:47 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 02:04:47 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:04:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-09-14 02:04:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:04:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=97, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:04:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-09-14 02:04:54 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=193, loss_sum=134.772720, avg_loss=0.698304, seen=193, correct=110, accuracy=0.569948
2025-09-14 02:04:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:04:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:04:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:04:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2200MB
2025-09-14 02:04:55 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 193, 'val_loss': 134.77272033691406, 'val_avg_loss': 0.698304250450332, 'val_seen': 193, 'val_correct': 110, 'val_acc': 0.5699481865284974}
2025-09-14 02:04:55 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 02:04:55 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 193, 'val_loss': 134.77272033691406, 'val_avg_loss': 0.698304250450332, 'val_seen': 193, 'val_correct': 110, 'val_acc': 0.5699481865284974}
2025-09-14 02:04:55 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #31', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 49, 'val_loss': 34.00784534215927, 'val_avg_loss': 0.6940376600440668, 'val_seen': 49, 'val_correct': 31, 'val_acc': 0.6326530612244898}}
2025-09-14 02:04:55 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #31', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 193, 'val_loss': 134.77272033691406, 'val_avg_loss': 0.698304250450332, 'val_seen': 193, 'val_correct': 110, 'val_acc': 0.5699481865284974}}
2025-09-14 02:04:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:04:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:04:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:04:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:04:57 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.675898, avg_loss=0.666897, seen=40, correct=26, accuracy=0.650000
2025-09-14 02:04:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:04:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:04:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:04:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2200MB
2025-09-14 02:04:58 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 26.6758975982666, 'test_avg_loss': 0.6668974399566651, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-14 02:04:58 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 193, 'val_loss': 134.77272033691406, 'val_avg_loss': 0.698304250450332, 'val_seen': 193, 'val_correct': 110, 'val_acc': 0.5699481865284974}
2025-09-14 02:04:58 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 26.6758975982666, 'test_avg_loss': 0.6668974399566651, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-14 02:04:58 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #31', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 8.420492172241211, 'test_avg_loss': 0.8420492172241211, 'test_seen': 10, 'test_correct': 5, 'test_acc': 0.5}}
2025-09-14 02:04:58 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #31', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 26.6758975982666, 'test_avg_loss': 0.6668974399566651, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}}
2025-09-14 02:04:58 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 26.6758975982666, 'test_avg_loss': 0.6668974399566651, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}, metrics={'val_total': 193, 'val_loss': 134.77272033691406, 'val_avg_loss': 0.698304250450332, 'val_seen': 193, 'val_correct': 110, 'val_acc': 0.5699481865284974, 'test_total': 40, 'test_loss': 26.6758975982666, 'test_avg_loss': 0.6668974399566651, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-14 02:04:58 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 02:04:58 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:04:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-09-14 02:04:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:04:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=56, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:05:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 02:05:01 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=112, loss_sum=67.657616, avg_loss=0.604086, seen=112, correct=79, accuracy=0.705357
2025-09-14 02:05:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:05:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:05:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:05:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2200MB
2025-09-14 02:05:03 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 112, 'val_loss': 67.6576156616211, 'val_avg_loss': 0.6040858541216169, 'val_seen': 112, 'val_correct': 79, 'val_acc': 0.7053571428571429}
2025-09-14 02:05:03 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 02:05:03 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 112, 'val_loss': 67.6576156616211, 'val_avg_loss': 0.6040858541216169, 'val_seen': 112, 'val_correct': 79, 'val_acc': 0.7053571428571429}
2025-09-14 02:05:03 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #32', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 28, 'val_loss': 15.582014232873917, 'val_avg_loss': 0.5565005083169255, 'val_seen': 28, 'val_correct': 22, 'val_acc': 0.7857142857142857}}
2025-09-14 02:05:03 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #32', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 112, 'val_loss': 67.6576156616211, 'val_avg_loss': 0.6040858541216169, 'val_seen': 112, 'val_correct': 79, 'val_acc': 0.7053571428571429}}
2025-09-14 02:05:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:05:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:05:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:05:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:05:04 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.239391, avg_loss=0.730985, seen=40, correct=24, accuracy=0.600000
2025-09-14 02:05:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:05:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:05:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:05:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2200MB
2025-09-14 02:05:05 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 29.239391326904297, 'test_avg_loss': 0.7309847831726074, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-14 02:05:05 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 112, 'val_loss': 67.6576156616211, 'val_avg_loss': 0.6040858541216169, 'val_seen': 112, 'val_correct': 79, 'val_acc': 0.7053571428571429}
2025-09-14 02:05:05 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 29.239391326904297, 'test_avg_loss': 0.7309847831726074, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-14 02:05:05 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #32', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 7.654135823249817, 'test_avg_loss': 0.7654135823249817, 'test_seen': 10, 'test_correct': 6, 'test_acc': 0.6}}
2025-09-14 02:05:05 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #32', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 29.239391326904297, 'test_avg_loss': 0.7309847831726074, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}}
2025-09-14 02:05:05 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 29.239391326904297, 'test_avg_loss': 0.7309847831726074, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}, metrics={'val_total': 112, 'val_loss': 67.6576156616211, 'val_avg_loss': 0.6040858541216169, 'val_seen': 112, 'val_correct': 79, 'val_acc': 0.7053571428571429, 'test_total': 40, 'test_loss': 29.239391326904297, 'test_avg_loss': 0.7309847831726074, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-14 02:05:05 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 02:05:05 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:05:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-09-14 02:05:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:05:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=37, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:05:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-14 02:05:08 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=74, loss_sum=49.657257, avg_loss=0.671044, seen=74, correct=48, accuracy=0.648649
2025-09-14 02:05:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:05:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:05:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:05:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2200MB
2025-09-14 02:05:09 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 74, 'val_loss': 49.657257080078125, 'val_avg_loss': 0.6710440145956503, 'val_seen': 74, 'val_correct': 48, 'val_acc': 0.6486486486486487}
2025-09-14 02:05:09 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 02:05:09 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 74, 'val_loss': 49.657257080078125, 'val_avg_loss': 0.6710440145956503, 'val_seen': 74, 'val_correct': 48, 'val_acc': 0.6486486486486487}
2025-09-14 02:05:09 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #33', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 19, 'val_loss': 13.366454124450684, 'val_avg_loss': 0.7034975854974044, 'val_seen': 19, 'val_correct': 11, 'val_acc': 0.5789473684210527}}
2025-09-14 02:05:09 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #33', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 74, 'val_loss': 49.657257080078125, 'val_avg_loss': 0.6710440145956503, 'val_seen': 74, 'val_correct': 48, 'val_acc': 0.6486486486486487}}
2025-09-14 02:05:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:05:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:05:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:05:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:05:10 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.514992, avg_loss=0.687875, seen=40, correct=23, accuracy=0.575000
2025-09-14 02:05:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:05:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:05:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:05:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2200MB
2025-09-14 02:05:11 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 27.514991760253906, 'test_avg_loss': 0.6878747940063477, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 02:05:11 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 74, 'val_loss': 49.657257080078125, 'val_avg_loss': 0.6710440145956503, 'val_seen': 74, 'val_correct': 48, 'val_acc': 0.6486486486486487}
2025-09-14 02:05:11 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 27.514991760253906, 'test_avg_loss': 0.6878747940063477, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 02:05:11 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #33', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 7.031862378120422, 'test_avg_loss': 0.7031862378120423, 'test_seen': 10, 'test_correct': 6, 'test_acc': 0.6}}
2025-09-14 02:05:11 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #33', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 27.514991760253906, 'test_avg_loss': 0.6878747940063477, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}}
2025-09-14 02:05:11 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 27.514991760253906, 'test_avg_loss': 0.6878747940063477, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}, metrics={'val_total': 74, 'val_loss': 49.657257080078125, 'val_avg_loss': 0.6710440145956503, 'val_seen': 74, 'val_correct': 48, 'val_acc': 0.6486486486486487, 'test_total': 40, 'test_loss': 27.514991760253906, 'test_avg_loss': 0.6878747940063477, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 02:05:11 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 02:05:11 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:05:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:05:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:05:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:05:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:05:18 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=125.822350, avg_loss=0.629112, seen=200, correct=133, accuracy=0.665000
2025-09-14 02:05:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:05:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:05:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:05:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2326MB allocated=2200MB
2025-09-14 02:05:20 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 200, 'val_loss': 125.82234954833984, 'val_avg_loss': 0.6291117477416992, 'val_seen': 200, 'val_correct': 133, 'val_acc': 0.665}
2025-09-14 02:05:20 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 02:05:20 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 200, 'val_loss': 125.82234954833984, 'val_avg_loss': 0.6291117477416992, 'val_seen': 200, 'val_correct': 133, 'val_acc': 0.665}
2025-09-14 02:05:20 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #34', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 50, 'val_loss': 29.32210785150528, 'val_avg_loss': 0.5864421570301056, 'val_seen': 50, 'val_correct': 35, 'val_acc': 0.7}}
2025-09-14 02:05:20 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #34', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 200, 'val_loss': 125.82234954833984, 'val_avg_loss': 0.6291117477416992, 'val_seen': 200, 'val_correct': 133, 'val_acc': 0.665}}
2025-09-14 02:05:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:05:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:05:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:05:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:05:21 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.595667, avg_loss=0.639892, seen=40, correct=26, accuracy=0.650000
2025-09-14 02:05:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:05:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:05:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:05:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2326MB allocated=2200MB
2025-09-14 02:05:22 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 25.595666885375977, 'test_avg_loss': 0.6398916721343995, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-14 02:05:22 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 200, 'val_loss': 125.82234954833984, 'val_avg_loss': 0.6291117477416992, 'val_seen': 200, 'val_correct': 133, 'val_acc': 0.665}
2025-09-14 02:05:22 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 25.595666885375977, 'test_avg_loss': 0.6398916721343995, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-14 02:05:22 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #34', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.460256934165955, 'test_avg_loss': 0.6460256934165954, 'test_seen': 10, 'test_correct': 7, 'test_acc': 0.7}}
2025-09-14 02:05:22 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #34', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 25.595666885375977, 'test_avg_loss': 0.6398916721343995, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}}
2025-09-14 02:05:22 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 25.595666885375977, 'test_avg_loss': 0.6398916721343995, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}, metrics={'val_total': 200, 'val_loss': 125.82234954833984, 'val_avg_loss': 0.6291117477416992, 'val_seen': 200, 'val_correct': 133, 'val_acc': 0.665, 'test_total': 40, 'test_loss': 25.595666885375977, 'test_avg_loss': 0.6398916721343995, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-14 02:05:22 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 02:05:23 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:05:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:05:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:05:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:05:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:05:28 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=128.092834, avg_loss=0.640464, seen=200, correct=127, accuracy=0.635000
2025-09-14 02:05:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:05:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:05:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:05:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2326MB allocated=2200MB
2025-09-14 02:05:29 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 200, 'val_loss': 128.09283447265625, 'val_avg_loss': 0.6404641723632812, 'val_seen': 200, 'val_correct': 127, 'val_acc': 0.635}
2025-09-14 02:05:29 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 02:05:29 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 200, 'val_loss': 128.09283447265625, 'val_avg_loss': 0.6404641723632812, 'val_seen': 200, 'val_correct': 127, 'val_acc': 0.635}
2025-09-14 02:05:29 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #35', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 50, 'val_loss': 34.35484990477562, 'val_avg_loss': 0.6870969980955124, 'val_seen': 50, 'val_correct': 29, 'val_acc': 0.58}}
2025-09-14 02:05:29 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #35', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 200, 'val_loss': 128.09283447265625, 'val_avg_loss': 0.6404641723632812, 'val_seen': 200, 'val_correct': 127, 'val_acc': 0.635}}
2025-09-14 02:05:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:05:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:05:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:05:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:05:30 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.443249, avg_loss=0.686081, seen=40, correct=22, accuracy=0.550000
2025-09-14 02:05:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:05:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:05:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:05:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2326MB allocated=2200MB
2025-09-14 02:05:31 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 27.443248748779297, 'test_avg_loss': 0.6860812187194825, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-14 02:05:31 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 200, 'val_loss': 128.09283447265625, 'val_avg_loss': 0.6404641723632812, 'val_seen': 200, 'val_correct': 127, 'val_acc': 0.635}
2025-09-14 02:05:31 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 27.443248748779297, 'test_avg_loss': 0.6860812187194825, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-14 02:05:31 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #35', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 5.1427321434021, 'test_avg_loss': 0.51427321434021, 'test_seen': 10, 'test_correct': 8, 'test_acc': 0.8}}
2025-09-14 02:05:31 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #35', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 27.443248748779297, 'test_avg_loss': 0.6860812187194825, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}}
2025-09-14 02:05:31 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 27.443248748779297, 'test_avg_loss': 0.6860812187194825, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}, metrics={'val_total': 200, 'val_loss': 128.09283447265625, 'val_avg_loss': 0.6404641723632812, 'val_seen': 200, 'val_correct': 127, 'val_acc': 0.635, 'test_total': 40, 'test_loss': 27.443248748779297, 'test_avg_loss': 0.6860812187194825, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-14 02:05:31 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 02:05:31 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:05:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-09-14 02:05:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:05:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=27, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:05:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-09-14 02:05:33 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=54, loss_sum=37.048122, avg_loss=0.686076, seen=54, correct=30, accuracy=0.555556
2025-09-14 02:05:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:05:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:05:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:05:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2200MB
2025-09-14 02:05:34 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 54, 'val_loss': 37.04812240600586, 'val_avg_loss': 0.6860763408519603, 'val_seen': 54, 'val_correct': 30, 'val_acc': 0.5555555555555556}
2025-09-14 02:05:34 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 02:05:34 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 54, 'val_loss': 37.04812240600586, 'val_avg_loss': 0.6860763408519603, 'val_seen': 54, 'val_correct': 30, 'val_acc': 0.5555555555555556}
2025-09-14 02:05:34 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #36', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 14, 'val_loss': 9.171395480632782, 'val_avg_loss': 0.6550996771880558, 'val_seen': 14, 'val_correct': 7, 'val_acc': 0.5}}
2025-09-14 02:05:34 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #36', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 54, 'val_loss': 37.04812240600586, 'val_avg_loss': 0.6860763408519603, 'val_seen': 54, 'val_correct': 30, 'val_acc': 0.5555555555555556}}
2025-09-14 02:05:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:05:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:05:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:05:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:05:36 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.292452, avg_loss=0.582311, seen=40, correct=27, accuracy=0.675000
2025-09-14 02:05:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:05:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:05:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:05:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2200MB
2025-09-14 02:05:37 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 23.292451858520508, 'test_avg_loss': 0.5823112964630127, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-14 02:05:37 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 54, 'val_loss': 37.04812240600586, 'val_avg_loss': 0.6860763408519603, 'val_seen': 54, 'val_correct': 30, 'val_acc': 0.5555555555555556}
2025-09-14 02:05:37 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 23.292451858520508, 'test_avg_loss': 0.5823112964630127, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-14 02:05:37 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #36', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.107160210609436, 'test_avg_loss': 0.6107160210609436, 'test_seen': 10, 'test_correct': 8, 'test_acc': 0.8}}
2025-09-14 02:05:37 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #36', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 23.292451858520508, 'test_avg_loss': 0.5823112964630127, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}}
2025-09-14 02:05:37 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 23.292451858520508, 'test_avg_loss': 0.5823112964630127, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}, metrics={'val_total': 54, 'val_loss': 37.04812240600586, 'val_avg_loss': 0.6860763408519603, 'val_seen': 54, 'val_correct': 30, 'val_acc': 0.5555555555555556, 'test_total': 40, 'test_loss': 23.292451858520508, 'test_avg_loss': 0.5823112964630127, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-14 02:05:37 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 02:05:37 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:05:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:05:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:05:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:05:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:05:43 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=138.589188, avg_loss=0.692946, seen=200, correct=121, accuracy=0.605000
2025-09-14 02:05:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:05:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:05:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:05:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2200MB
2025-09-14 02:05:45 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 200, 'val_loss': 138.5891876220703, 'val_avg_loss': 0.6929459381103515, 'val_seen': 200, 'val_correct': 121, 'val_acc': 0.605}
2025-09-14 02:05:45 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 02:05:45 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 200, 'val_loss': 138.5891876220703, 'val_avg_loss': 0.6929459381103515, 'val_seen': 200, 'val_correct': 121, 'val_acc': 0.605}
2025-09-14 02:05:45 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #37', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 50, 'val_loss': 40.29245203733444, 'val_avg_loss': 0.8058490407466888, 'val_seen': 50, 'val_correct': 25, 'val_acc': 0.5}}
2025-09-14 02:05:45 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #37', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 200, 'val_loss': 138.5891876220703, 'val_avg_loss': 0.6929459381103515, 'val_seen': 200, 'val_correct': 121, 'val_acc': 0.605}}
2025-09-14 02:05:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:05:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:05:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:05:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:05:47 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.346579, avg_loss=0.633664, seen=40, correct=27, accuracy=0.675000
2025-09-14 02:05:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:05:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:05:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:05:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2200MB
2025-09-14 02:05:48 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 25.34657859802246, 'test_avg_loss': 0.6336644649505615, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-14 02:05:48 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 200, 'val_loss': 138.5891876220703, 'val_avg_loss': 0.6929459381103515, 'val_seen': 200, 'val_correct': 121, 'val_acc': 0.605}
2025-09-14 02:05:48 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 25.34657859802246, 'test_avg_loss': 0.6336644649505615, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-14 02:05:48 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #37', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 5.663764774799347, 'test_avg_loss': 0.5663764774799347, 'test_seen': 10, 'test_correct': 7, 'test_acc': 0.7}}
2025-09-14 02:05:48 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #37', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 25.34657859802246, 'test_avg_loss': 0.6336644649505615, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}}
2025-09-14 02:05:48 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 25.34657859802246, 'test_avg_loss': 0.6336644649505615, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}, metrics={'val_total': 200, 'val_loss': 138.5891876220703, 'val_avg_loss': 0.6929459381103515, 'val_seen': 200, 'val_correct': 121, 'val_acc': 0.605, 'test_total': 40, 'test_loss': 25.34657859802246, 'test_avg_loss': 0.6336644649505615, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-14 02:05:48 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 02:05:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:05:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:05:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:05:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:05:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:05:55 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=132.833359, avg_loss=0.664167, seen=200, correct=125, accuracy=0.625000
2025-09-14 02:05:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:05:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:05:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:05:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2200MB
2025-09-14 02:05:56 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 200, 'val_loss': 132.83335876464844, 'val_avg_loss': 0.6641667938232422, 'val_seen': 200, 'val_correct': 125, 'val_acc': 0.625}
2025-09-14 02:05:56 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 02:05:56 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 200, 'val_loss': 132.83335876464844, 'val_avg_loss': 0.6641667938232422, 'val_seen': 200, 'val_correct': 125, 'val_acc': 0.625}
2025-09-14 02:05:56 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #38', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 50, 'val_loss': 35.75039607286453, 'val_avg_loss': 0.7150079214572906, 'val_seen': 50, 'val_correct': 30, 'val_acc': 0.6}}
2025-09-14 02:05:56 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #38', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 200, 'val_loss': 132.83335876464844, 'val_avg_loss': 0.6641667938232422, 'val_seen': 200, 'val_correct': 125, 'val_acc': 0.625}}
2025-09-14 02:05:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:05:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:05:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:05:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:05:58 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.673285, avg_loss=0.666832, seen=40, correct=25, accuracy=0.625000
2025-09-14 02:05:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:05:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:05:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:05:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2200MB
2025-09-14 02:05:59 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 26.67328453063965, 'test_avg_loss': 0.6668321132659912, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-14 02:05:59 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 200, 'val_loss': 132.83335876464844, 'val_avg_loss': 0.6641667938232422, 'val_seen': 200, 'val_correct': 125, 'val_acc': 0.625}
2025-09-14 02:05:59 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 26.67328453063965, 'test_avg_loss': 0.6668321132659912, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-14 02:05:59 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #38', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 7.8995338678359985, 'test_avg_loss': 0.7899533867835998, 'test_seen': 10, 'test_correct': 6, 'test_acc': 0.6}}
2025-09-14 02:05:59 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #38', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 26.67328453063965, 'test_avg_loss': 0.6668321132659912, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}}
2025-09-14 02:05:59 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 26.67328453063965, 'test_avg_loss': 0.6668321132659912, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}, metrics={'val_total': 200, 'val_loss': 132.83335876464844, 'val_avg_loss': 0.6641667938232422, 'val_seen': 200, 'val_correct': 125, 'val_acc': 0.625, 'test_total': 40, 'test_loss': 26.67328453063965, 'test_avg_loss': 0.6668321132659912, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-14 02:05:59 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 02:05:59 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:06:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-14 02:06:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:06:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:06:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-14 02:06:01 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=61.092705, avg_loss=0.736057, seen=83, correct=44, accuracy=0.530120
2025-09-14 02:06:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:06:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:06:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:06:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2200MB
2025-09-14 02:06:03 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 83, 'val_loss': 61.09270477294922, 'val_avg_loss': 0.7360566840114364, 'val_seen': 83, 'val_correct': 44, 'val_acc': 0.5301204819277109}
2025-09-14 02:06:03 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 02:06:03 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 83, 'val_loss': 61.09270477294922, 'val_avg_loss': 0.7360566840114364, 'val_seen': 83, 'val_correct': 44, 'val_acc': 0.5301204819277109}
2025-09-14 02:06:03 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #39', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 21, 'val_loss': 18.47469860315323, 'val_avg_loss': 0.8797475525311061, 'val_seen': 21, 'val_correct': 8, 'val_acc': 0.38095238095238093}}
2025-09-14 02:06:03 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #39', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 83, 'val_loss': 61.09270477294922, 'val_avg_loss': 0.7360566840114364, 'val_seen': 83, 'val_correct': 44, 'val_acc': 0.5301204819277109}}
2025-09-14 02:06:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:06:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:06:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:06:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:06:04 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.948551, avg_loss=0.748714, seen=40, correct=23, accuracy=0.575000
2025-09-14 02:06:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:06:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:06:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:06:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2200MB
2025-09-14 02:06:05 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 29.948551177978516, 'test_avg_loss': 0.7487137794494629, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 02:06:05 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 83, 'val_loss': 61.09270477294922, 'val_avg_loss': 0.7360566840114364, 'val_seen': 83, 'val_correct': 44, 'val_acc': 0.5301204819277109}
2025-09-14 02:06:05 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 29.948551177978516, 'test_avg_loss': 0.7487137794494629, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 02:06:05 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #39', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 8.161215752363205, 'test_avg_loss': 0.8161215752363205, 'test_seen': 10, 'test_correct': 5, 'test_acc': 0.5}}
2025-09-14 02:06:05 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #39', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 29.948551177978516, 'test_avg_loss': 0.7487137794494629, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}}
2025-09-14 02:06:05 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 29.948551177978516, 'test_avg_loss': 0.7487137794494629, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}, metrics={'val_total': 83, 'val_loss': 61.09270477294922, 'val_avg_loss': 0.7360566840114364, 'val_seen': 83, 'val_correct': 44, 'val_acc': 0.5301204819277109, 'test_total': 40, 'test_loss': 29.948551177978516, 'test_avg_loss': 0.7487137794494629, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 02:06:05 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 02:06:06 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:06:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:06:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:06:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:06:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:06:12 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=126.999008, avg_loss=0.634995, seen=200, correct=125, accuracy=0.625000
2025-09-14 02:06:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:06:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:06:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:06:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2200MB
2025-09-14 02:06:13 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 200, 'val_loss': 126.99900817871094, 'val_avg_loss': 0.6349950408935547, 'val_seen': 200, 'val_correct': 125, 'val_acc': 0.625}
2025-09-14 02:06:13 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 02:06:13 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 200, 'val_loss': 126.99900817871094, 'val_avg_loss': 0.6349950408935547, 'val_seen': 200, 'val_correct': 125, 'val_acc': 0.625}
2025-09-14 02:06:13 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #40', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 50, 'val_loss': 32.57090324163437, 'val_avg_loss': 0.6514180648326874, 'val_seen': 50, 'val_correct': 29, 'val_acc': 0.58}}
2025-09-14 02:06:13 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #40', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 200, 'val_loss': 126.99900817871094, 'val_avg_loss': 0.6349950408935547, 'val_seen': 200, 'val_correct': 125, 'val_acc': 0.625}}
2025-09-14 02:06:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:06:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:06:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:06:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:06:14 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.684296, avg_loss=0.667107, seen=40, correct=24, accuracy=0.600000
2025-09-14 02:06:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:06:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:06:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:06:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2200MB
2025-09-14 02:06:15 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 26.684295654296875, 'test_avg_loss': 0.6671073913574219, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-14 02:06:15 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 200, 'val_loss': 126.99900817871094, 'val_avg_loss': 0.6349950408935547, 'val_seen': 200, 'val_correct': 125, 'val_acc': 0.625}
2025-09-14 02:06:15 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 26.684295654296875, 'test_avg_loss': 0.6671073913574219, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-14 02:06:15 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #40', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.03571754693985, 'test_avg_loss': 0.6035717546939849, 'test_seen': 10, 'test_correct': 5, 'test_acc': 0.5}}
2025-09-14 02:06:15 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #40', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 26.684295654296875, 'test_avg_loss': 0.6671073913574219, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}}
2025-09-14 02:06:15 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 26.684295654296875, 'test_avg_loss': 0.6671073913574219, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}, metrics={'val_total': 200, 'val_loss': 126.99900817871094, 'val_avg_loss': 0.6349950408935547, 'val_seen': 200, 'val_correct': 125, 'val_acc': 0.625, 'test_total': 40, 'test_loss': 26.684295654296875, 'test_avg_loss': 0.6671073913574219, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-14 02:06:15 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 02:06:16 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:06:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-09-14 02:06:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:06:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=60, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:06:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-09-14 02:06:19 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=119, loss_sum=80.252548, avg_loss=0.674391, seen=119, correct=70, accuracy=0.588235
2025-09-14 02:06:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:06:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:06:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:06:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2200MB
2025-09-14 02:06:21 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 119, 'val_loss': 80.25254821777344, 'val_avg_loss': 0.6743911614938944, 'val_seen': 119, 'val_correct': 70, 'val_acc': 0.5882352941176471}
2025-09-14 02:06:21 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 02:06:21 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 119, 'val_loss': 80.25254821777344, 'val_avg_loss': 0.6743911614938944, 'val_seen': 119, 'val_correct': 70, 'val_acc': 0.5882352941176471}
2025-09-14 02:06:21 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #41', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 30, 'val_loss': 16.12539917230606, 'val_avg_loss': 0.5375133057435354, 'val_seen': 30, 'val_correct': 25, 'val_acc': 0.8333333333333334}}
2025-09-14 02:06:21 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #41', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 119, 'val_loss': 80.25254821777344, 'val_avg_loss': 0.6743911614938944, 'val_seen': 119, 'val_correct': 70, 'val_acc': 0.5882352941176471}}
2025-09-14 02:06:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:06:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:06:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:06:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:06:22 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.367504, avg_loss=0.684188, seen=40, correct=22, accuracy=0.550000
2025-09-14 02:06:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:06:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:06:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:06:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2200MB
2025-09-14 02:06:23 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 27.367504119873047, 'test_avg_loss': 0.6841876029968261, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-14 02:06:23 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 119, 'val_loss': 80.25254821777344, 'val_avg_loss': 0.6743911614938944, 'val_seen': 119, 'val_correct': 70, 'val_acc': 0.5882352941176471}
2025-09-14 02:06:23 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 27.367504119873047, 'test_avg_loss': 0.6841876029968261, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-14 02:06:23 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #41', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.997580289840698, 'test_avg_loss': 0.6997580289840698, 'test_seen': 10, 'test_correct': 7, 'test_acc': 0.7}}
2025-09-14 02:06:23 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #41', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 27.367504119873047, 'test_avg_loss': 0.6841876029968261, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}}
2025-09-14 02:06:23 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 27.367504119873047, 'test_avg_loss': 0.6841876029968261, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}, metrics={'val_total': 119, 'val_loss': 80.25254821777344, 'val_avg_loss': 0.6743911614938944, 'val_seen': 119, 'val_correct': 70, 'val_acc': 0.5882352941176471, 'test_total': 40, 'test_loss': 27.367504119873047, 'test_avg_loss': 0.6841876029968261, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-14 02:06:23 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 02:06:24 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:06:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:06:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:06:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:06:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:06:30 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=126.397827, avg_loss=0.631989, seen=200, correct=128, accuracy=0.640000
2025-09-14 02:06:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:06:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:06:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:06:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2200MB
2025-09-14 02:06:31 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 200, 'val_loss': 126.3978271484375, 'val_avg_loss': 0.6319891357421875, 'val_seen': 200, 'val_correct': 128, 'val_acc': 0.64}
2025-09-14 02:06:31 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 02:06:31 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 200, 'val_loss': 126.3978271484375, 'val_avg_loss': 0.6319891357421875, 'val_seen': 200, 'val_correct': 128, 'val_acc': 0.64}
2025-09-14 02:06:31 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #42', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 50, 'val_loss': 34.3772594332695, 'val_avg_loss': 0.68754518866539, 'val_seen': 50, 'val_correct': 28, 'val_acc': 0.56}}
2025-09-14 02:06:31 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #42', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 200, 'val_loss': 126.3978271484375, 'val_avg_loss': 0.6319891357421875, 'val_seen': 200, 'val_correct': 128, 'val_acc': 0.64}}
2025-09-14 02:06:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:06:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:06:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:06:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:06:33 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.930984, avg_loss=0.673275, seen=40, correct=23, accuracy=0.575000
2025-09-14 02:06:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:06:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:06:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:06:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2200MB
2025-09-14 02:06:34 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 26.930984497070312, 'test_avg_loss': 0.6732746124267578, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 02:06:34 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 200, 'val_loss': 126.3978271484375, 'val_avg_loss': 0.6319891357421875, 'val_seen': 200, 'val_correct': 128, 'val_acc': 0.64}
2025-09-14 02:06:34 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 26.930984497070312, 'test_avg_loss': 0.6732746124267578, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 02:06:34 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #42', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.793021142482758, 'test_avg_loss': 0.6793021142482758, 'test_seen': 10, 'test_correct': 7, 'test_acc': 0.7}}
2025-09-14 02:06:34 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #42', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 26.930984497070312, 'test_avg_loss': 0.6732746124267578, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}}
2025-09-14 02:06:34 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 26.930984497070312, 'test_avg_loss': 0.6732746124267578, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}, metrics={'val_total': 200, 'val_loss': 126.3978271484375, 'val_avg_loss': 0.6319891357421875, 'val_seen': 200, 'val_correct': 128, 'val_acc': 0.64, 'test_total': 40, 'test_loss': 26.930984497070312, 'test_avg_loss': 0.6732746124267578, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 02:06:34 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 02:06:34 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:06:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-09-14 02:06:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:06:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=45, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:06:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-09-14 02:06:37 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=89, loss_sum=62.607426, avg_loss=0.703454, seen=89, correct=51, accuracy=0.573034
2025-09-14 02:06:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:06:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:06:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:06:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2200MB
2025-09-14 02:06:38 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 89, 'val_loss': 62.607425689697266, 'val_avg_loss': 0.7034542212325535, 'val_seen': 89, 'val_correct': 51, 'val_acc': 0.5730337078651685}
2025-09-14 02:06:38 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 02:06:38 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 89, 'val_loss': 62.607425689697266, 'val_avg_loss': 0.7034542212325535, 'val_seen': 89, 'val_correct': 51, 'val_acc': 0.5730337078651685}
2025-09-14 02:06:38 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #43', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 23, 'val_loss': 17.438606321811676, 'val_avg_loss': 0.7582002748613772, 'val_seen': 23, 'val_correct': 13, 'val_acc': 0.5652173913043478}}
2025-09-14 02:06:38 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #43', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 89, 'val_loss': 62.607425689697266, 'val_avg_loss': 0.7034542212325535, 'val_seen': 89, 'val_correct': 51, 'val_acc': 0.5730337078651685}}
2025-09-14 02:06:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:06:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:06:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:06:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:06:39 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.712900, avg_loss=0.642823, seen=40, correct=26, accuracy=0.650000
2025-09-14 02:06:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:06:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:06:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:06:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2200MB
2025-09-14 02:06:40 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 25.712900161743164, 'test_avg_loss': 0.6428225040435791, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-14 02:06:40 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 89, 'val_loss': 62.607425689697266, 'val_avg_loss': 0.7034542212325535, 'val_seen': 89, 'val_correct': 51, 'val_acc': 0.5730337078651685}
2025-09-14 02:06:40 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 25.712900161743164, 'test_avg_loss': 0.6428225040435791, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-14 02:06:40 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #43', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.17145311832428, 'test_avg_loss': 0.6171453118324279, 'test_seen': 10, 'test_correct': 6, 'test_acc': 0.6}}
2025-09-14 02:06:40 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #43', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 25.712900161743164, 'test_avg_loss': 0.6428225040435791, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}}
2025-09-14 02:06:40 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 25.712900161743164, 'test_avg_loss': 0.6428225040435791, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}, metrics={'val_total': 89, 'val_loss': 62.607425689697266, 'val_avg_loss': 0.7034542212325535, 'val_seen': 89, 'val_correct': 51, 'val_acc': 0.5730337078651685, 'test_total': 40, 'test_loss': 25.712900161743164, 'test_avg_loss': 0.6428225040435791, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-14 02:06:40 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 02:06:40 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:06:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:06:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:06:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:06:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:06:45 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=142.911621, avg_loss=0.714558, seen=200, correct=109, accuracy=0.545000
2025-09-14 02:06:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:06:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:06:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:06:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2200MB
2025-09-14 02:06:46 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 200, 'val_loss': 142.91162109375, 'val_avg_loss': 0.71455810546875, 'val_seen': 200, 'val_correct': 109, 'val_acc': 0.545}
2025-09-14 02:06:46 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 02:06:46 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 200, 'val_loss': 142.91162109375, 'val_avg_loss': 0.71455810546875, 'val_seen': 200, 'val_correct': 109, 'val_acc': 0.545}
2025-09-14 02:06:46 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #44', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 50, 'val_loss': 39.23704034090042, 'val_avg_loss': 0.7847408068180084, 'val_seen': 50, 'val_correct': 24, 'val_acc': 0.48}}
2025-09-14 02:06:46 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #44', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 200, 'val_loss': 142.91162109375, 'val_avg_loss': 0.71455810546875, 'val_seen': 200, 'val_correct': 109, 'val_acc': 0.545}}
2025-09-14 02:06:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:06:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:06:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:06:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:06:48 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.927763, avg_loss=0.648194, seen=40, correct=23, accuracy=0.575000
2025-09-14 02:06:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:06:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:06:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:06:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2200MB
2025-09-14 02:06:49 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 25.927762985229492, 'test_avg_loss': 0.6481940746307373, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 02:06:49 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 200, 'val_loss': 142.91162109375, 'val_avg_loss': 0.71455810546875, 'val_seen': 200, 'val_correct': 109, 'val_acc': 0.545}
2025-09-14 02:06:49 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 25.927762985229492, 'test_avg_loss': 0.6481940746307373, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 02:06:49 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #44', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.412199378013611, 'test_avg_loss': 0.6412199378013611, 'test_seen': 10, 'test_correct': 6, 'test_acc': 0.6}}
2025-09-14 02:06:49 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #44', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 25.927762985229492, 'test_avg_loss': 0.6481940746307373, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}}
2025-09-14 02:06:49 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 25.927762985229492, 'test_avg_loss': 0.6481940746307373, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}, metrics={'val_total': 200, 'val_loss': 142.91162109375, 'val_avg_loss': 0.71455810546875, 'val_seen': 200, 'val_correct': 109, 'val_acc': 0.545, 'test_total': 40, 'test_loss': 25.927762985229492, 'test_avg_loss': 0.6481940746307373, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 02:06:49 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 02:06:49 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:06:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-09-14 02:06:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:06:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=50, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:06:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-09-14 02:06:52 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=100, loss_sum=63.832306, avg_loss=0.638323, seen=100, correct=59, accuracy=0.590000
2025-09-14 02:06:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:06:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:06:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:06:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2200MB
2025-09-14 02:06:53 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 100, 'val_loss': 63.832305908203125, 'val_avg_loss': 0.6383230590820312, 'val_seen': 100, 'val_correct': 59, 'val_acc': 0.59}
2025-09-14 02:06:53 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 02:06:53 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 100, 'val_loss': 63.832305908203125, 'val_avg_loss': 0.6383230590820312, 'val_seen': 100, 'val_correct': 59, 'val_acc': 0.59}
2025-09-14 02:06:53 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #45', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 25, 'val_loss': 14.8637475669384, 'val_avg_loss': 0.594549902677536, 'val_seen': 25, 'val_correct': 17, 'val_acc': 0.68}}
2025-09-14 02:06:53 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #45', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 100, 'val_loss': 63.832305908203125, 'val_avg_loss': 0.6383230590820312, 'val_seen': 100, 'val_correct': 59, 'val_acc': 0.59}}
2025-09-14 02:06:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:06:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:06:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:06:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:06:55 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.878376, avg_loss=0.721959, seen=40, correct=22, accuracy=0.550000
2025-09-14 02:06:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:06:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:06:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:06:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2200MB
2025-09-14 02:06:56 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 28.878376007080078, 'test_avg_loss': 0.721959400177002, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-14 02:06:56 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 100, 'val_loss': 63.832305908203125, 'val_avg_loss': 0.6383230590820312, 'val_seen': 100, 'val_correct': 59, 'val_acc': 0.59}
2025-09-14 02:06:56 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 28.878376007080078, 'test_avg_loss': 0.721959400177002, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-14 02:06:56 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #45', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 8.26654577255249, 'test_avg_loss': 0.826654577255249, 'test_seen': 10, 'test_correct': 6, 'test_acc': 0.6}}
2025-09-14 02:06:56 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #45', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 28.878376007080078, 'test_avg_loss': 0.721959400177002, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}}
2025-09-14 02:06:56 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 28.878376007080078, 'test_avg_loss': 0.721959400177002, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}, metrics={'val_total': 100, 'val_loss': 63.832305908203125, 'val_avg_loss': 0.6383230590820312, 'val_seen': 100, 'val_correct': 59, 'val_acc': 0.59, 'test_total': 40, 'test_loss': 28.878376007080078, 'test_avg_loss': 0.721959400177002, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-14 02:06:56 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 02:06:56 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:06:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-14 02:06:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:06:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:06:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 02:06:59 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=79.188599, avg_loss=0.719896, seen=110, correct=59, accuracy=0.536364
2025-09-14 02:06:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:06:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:07:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:07:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2200MB
2025-09-14 02:07:01 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 110, 'val_loss': 79.1885986328125, 'val_avg_loss': 0.7198963512073864, 'val_seen': 110, 'val_correct': 59, 'val_acc': 0.5363636363636364}
2025-09-14 02:07:01 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 02:07:01 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 110, 'val_loss': 79.1885986328125, 'val_avg_loss': 0.7198963512073864, 'val_seen': 110, 'val_correct': 59, 'val_acc': 0.5363636363636364}
2025-09-14 02:07:01 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #46', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 28, 'val_loss': 19.59674447774887, 'val_avg_loss': 0.6998837313481739, 'val_seen': 28, 'val_correct': 16, 'val_acc': 0.5714285714285714}}
2025-09-14 02:07:01 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #46', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 110, 'val_loss': 79.1885986328125, 'val_avg_loss': 0.7198963512073864, 'val_seen': 110, 'val_correct': 59, 'val_acc': 0.5363636363636364}}
2025-09-14 02:07:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:07:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:07:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:07:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:07:03 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=21.790783, avg_loss=0.544770, seen=40, correct=30, accuracy=0.750000
2025-09-14 02:07:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:07:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:07:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:07:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2200MB
2025-09-14 02:07:04 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 21.790782928466797, 'test_avg_loss': 0.54476957321167, 'test_seen': 40, 'test_correct': 30, 'test_acc': 0.75}
2025-09-14 02:07:04 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 110, 'val_loss': 79.1885986328125, 'val_avg_loss': 0.7198963512073864, 'val_seen': 110, 'val_correct': 59, 'val_acc': 0.5363636363636364}
2025-09-14 02:07:04 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 21.790782928466797, 'test_avg_loss': 0.54476957321167, 'test_seen': 40, 'test_correct': 30, 'test_acc': 0.75}
2025-09-14 02:07:04 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #46', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 4.238343358039856, 'test_avg_loss': 0.4238343358039856, 'test_seen': 10, 'test_correct': 9, 'test_acc': 0.9}}
2025-09-14 02:07:04 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #46', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 21.790782928466797, 'test_avg_loss': 0.54476957321167, 'test_seen': 40, 'test_correct': 30, 'test_acc': 0.75}}
2025-09-14 02:07:04 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 21.790782928466797, 'test_avg_loss': 0.54476957321167, 'test_seen': 40, 'test_correct': 30, 'test_acc': 0.75}, metrics={'val_total': 110, 'val_loss': 79.1885986328125, 'val_avg_loss': 0.7198963512073864, 'val_seen': 110, 'val_correct': 59, 'val_acc': 0.5363636363636364, 'test_total': 40, 'test_loss': 21.790782928466797, 'test_avg_loss': 0.54476957321167, 'test_seen': 40, 'test_correct': 30, 'test_acc': 0.75}
2025-09-14 02:07:04 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 02:07:04 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:07:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-09-14 02:07:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:07:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=74, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:07:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-14 02:07:08 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=147, loss_sum=103.766289, avg_loss=0.705893, seen=147, correct=81, accuracy=0.551020
2025-09-14 02:07:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:07:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:07:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:07:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2200MB
2025-09-14 02:07:10 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 147, 'val_loss': 103.76628875732422, 'val_avg_loss': 0.7058931207981239, 'val_seen': 147, 'val_correct': 81, 'val_acc': 0.5510204081632653}
2025-09-14 02:07:10 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 02:07:10 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 147, 'val_loss': 103.76628875732422, 'val_avg_loss': 0.7058931207981239, 'val_seen': 147, 'val_correct': 81, 'val_acc': 0.5510204081632653}
2025-09-14 02:07:10 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #47', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 37, 'val_loss': 26.055459320545197, 'val_avg_loss': 0.7042016032579783, 'val_seen': 37, 'val_correct': 21, 'val_acc': 0.5675675675675675}}
2025-09-14 02:07:10 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #47', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 147, 'val_loss': 103.76628875732422, 'val_avg_loss': 0.7058931207981239, 'val_seen': 147, 'val_correct': 81, 'val_acc': 0.5510204081632653}}
2025-09-14 02:07:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:07:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:07:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:07:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:07:11 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.400660, avg_loss=0.635016, seen=40, correct=28, accuracy=0.700000
2025-09-14 02:07:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:07:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:07:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:07:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2200MB
2025-09-14 02:07:13 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 25.400659561157227, 'test_avg_loss': 0.6350164890289307, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}
2025-09-14 02:07:13 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 147, 'val_loss': 103.76628875732422, 'val_avg_loss': 0.7058931207981239, 'val_seen': 147, 'val_correct': 81, 'val_acc': 0.5510204081632653}
2025-09-14 02:07:13 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 25.400659561157227, 'test_avg_loss': 0.6350164890289307, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}
2025-09-14 02:07:13 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #47', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 5.353911757469177, 'test_avg_loss': 0.5353911757469177, 'test_seen': 10, 'test_correct': 9, 'test_acc': 0.9}}
2025-09-14 02:07:13 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #47', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 25.400659561157227, 'test_avg_loss': 0.6350164890289307, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}}
2025-09-14 02:07:13 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 25.400659561157227, 'test_avg_loss': 0.6350164890289307, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}, metrics={'val_total': 147, 'val_loss': 103.76628875732422, 'val_avg_loss': 0.7058931207981239, 'val_seen': 147, 'val_correct': 81, 'val_acc': 0.5510204081632653, 'test_total': 40, 'test_loss': 25.400659561157227, 'test_avg_loss': 0.6350164890289307, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}
2025-09-14 02:07:13 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 02:07:13 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:07:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-09-14 02:07:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:07:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=23, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:07:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-09-14 02:07:14 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=46, loss_sum=34.515648, avg_loss=0.750340, seen=46, correct=21, accuracy=0.456522
2025-09-14 02:07:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:07:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:07:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:07:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2200MB
2025-09-14 02:07:16 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 46, 'val_loss': 34.515647888183594, 'val_avg_loss': 0.7503401714822521, 'val_seen': 46, 'val_correct': 21, 'val_acc': 0.45652173913043476}
2025-09-14 02:07:16 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 02:07:16 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 46, 'val_loss': 34.515647888183594, 'val_avg_loss': 0.7503401714822521, 'val_seen': 46, 'val_correct': 21, 'val_acc': 0.45652173913043476}
2025-09-14 02:07:16 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #48', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 12, 'val_loss': 8.970346331596375, 'val_avg_loss': 0.7475288609663645, 'val_seen': 12, 'val_correct': 6, 'val_acc': 0.5}}
2025-09-14 02:07:16 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #48', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 46, 'val_loss': 34.515647888183594, 'val_avg_loss': 0.7503401714822521, 'val_seen': 46, 'val_correct': 21, 'val_acc': 0.45652173913043476}}
2025-09-14 02:07:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:07:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:07:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:07:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:07:18 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.375139, avg_loss=0.759378, seen=40, correct=18, accuracy=0.450000
2025-09-14 02:07:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:07:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:07:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:07:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2200MB
2025-09-14 02:07:19 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 30.375139236450195, 'test_avg_loss': 0.7593784809112549, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-14 02:07:19 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 46, 'val_loss': 34.515647888183594, 'val_avg_loss': 0.7503401714822521, 'val_seen': 46, 'val_correct': 21, 'val_acc': 0.45652173913043476}
2025-09-14 02:07:19 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 30.375139236450195, 'test_avg_loss': 0.7593784809112549, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-14 02:07:19 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #48', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 8.093395471572876, 'test_avg_loss': 0.8093395471572876, 'test_seen': 10, 'test_correct': 4, 'test_acc': 0.4}}
2025-09-14 02:07:19 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #48', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 30.375139236450195, 'test_avg_loss': 0.7593784809112549, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}}
2025-09-14 02:07:19 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 30.375139236450195, 'test_avg_loss': 0.7593784809112549, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}, metrics={'val_total': 46, 'val_loss': 34.515647888183594, 'val_avg_loss': 0.7503401714822521, 'val_seen': 46, 'val_correct': 21, 'val_acc': 0.45652173913043476, 'test_total': 40, 'test_loss': 30.375139236450195, 'test_avg_loss': 0.7593784809112549, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-14 02:07:19 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 02:07:19 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:07:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-09-14 02:07:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:07:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=66, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:07:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-09-14 02:07:23 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=132, loss_sum=83.874619, avg_loss=0.635414, seen=132, correct=84, accuracy=0.636364
2025-09-14 02:07:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:07:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:07:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:07:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2200MB
2025-09-14 02:07:25 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 132, 'val_loss': 83.87461853027344, 'val_avg_loss': 0.6354137767444957, 'val_seen': 132, 'val_correct': 84, 'val_acc': 0.6363636363636364}
2025-09-14 02:07:25 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 02:07:25 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 132, 'val_loss': 83.87461853027344, 'val_avg_loss': 0.6354137767444957, 'val_seen': 132, 'val_correct': 84, 'val_acc': 0.6363636363636364}
2025-09-14 02:07:25 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #49', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 33, 'val_loss': 17.157131642103195, 'val_avg_loss': 0.5199130800637332, 'val_seen': 33, 'val_correct': 25, 'val_acc': 0.7575757575757576}}
2025-09-14 02:07:25 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #49', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 132, 'val_loss': 83.87461853027344, 'val_avg_loss': 0.6354137767444957, 'val_seen': 132, 'val_correct': 84, 'val_acc': 0.6363636363636364}}
2025-09-14 02:07:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:07:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:07:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:07:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:07:26 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.589750, avg_loss=0.764744, seen=40, correct=20, accuracy=0.500000
2025-09-14 02:07:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:07:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:07:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:07:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2200MB
2025-09-14 02:07:27 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 30.589750289916992, 'test_avg_loss': 0.7647437572479248, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-14 02:07:27 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 132, 'val_loss': 83.87461853027344, 'val_avg_loss': 0.6354137767444957, 'val_seen': 132, 'val_correct': 84, 'val_acc': 0.6363636363636364}
2025-09-14 02:07:27 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 30.589750289916992, 'test_avg_loss': 0.7647437572479248, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-14 02:07:27 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #49', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 8.107812523841858, 'test_avg_loss': 0.8107812523841857, 'test_seen': 10, 'test_correct': 5, 'test_acc': 0.5}}
2025-09-14 02:07:27 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #49', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 30.589750289916992, 'test_avg_loss': 0.7647437572479248, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}}
2025-09-14 02:07:27 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 30.589750289916992, 'test_avg_loss': 0.7647437572479248, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}, metrics={'val_total': 132, 'val_loss': 83.87461853027344, 'val_avg_loss': 0.6354137767444957, 'val_seen': 132, 'val_correct': 84, 'val_acc': 0.6363636363636364, 'test_total': 40, 'test_loss': 30.589750289916992, 'test_avg_loss': 0.7647437572479248, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-14 02:07:27 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 02:07:28 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:07:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-09-14 02:07:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:07:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:07:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 02:07:32 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=133, loss_sum=95.461922, avg_loss=0.717759, seen=133, correct=74, accuracy=0.556391
2025-09-14 02:07:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:07:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:07:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:07:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2200MB
2025-09-14 02:07:34 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 133, 'val_loss': 95.46192169189453, 'val_avg_loss': 0.7177588097134927, 'val_seen': 133, 'val_correct': 74, 'val_acc': 0.556390977443609}
2025-09-14 02:07:34 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 02:07:34 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 133, 'val_loss': 95.46192169189453, 'val_avg_loss': 0.7177588097134927, 'val_seen': 133, 'val_correct': 74, 'val_acc': 0.556390977443609}
2025-09-14 02:07:34 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #50', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 34, 'val_loss': 25.12318307161331, 'val_avg_loss': 0.7389171491650974, 'val_seen': 34, 'val_correct': 16, 'val_acc': 0.47058823529411764}}
2025-09-14 02:07:34 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #50', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 133, 'val_loss': 95.46192169189453, 'val_avg_loss': 0.7177588097134927, 'val_seen': 133, 'val_correct': 74, 'val_acc': 0.556390977443609}}
2025-09-14 02:07:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:07:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:07:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:07:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:07:35 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.753803, avg_loss=0.768845, seen=40, correct=18, accuracy=0.450000
2025-09-14 02:07:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:07:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:07:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:07:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2200MB
2025-09-14 02:07:37 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 30.753803253173828, 'test_avg_loss': 0.7688450813293457, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-14 02:07:37 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 133, 'val_loss': 95.46192169189453, 'val_avg_loss': 0.7177588097134927, 'val_seen': 133, 'val_correct': 74, 'val_acc': 0.556390977443609}
2025-09-14 02:07:37 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 30.753803253173828, 'test_avg_loss': 0.7688450813293457, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-14 02:07:37 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #50', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 8.167646646499634, 'test_avg_loss': 0.8167646646499633, 'test_seen': 10, 'test_correct': 3, 'test_acc': 0.3}}
2025-09-14 02:07:37 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #50', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 30.753803253173828, 'test_avg_loss': 0.7688450813293457, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}}
2025-09-14 02:07:37 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 30.753803253173828, 'test_avg_loss': 0.7688450813293457, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}, metrics={'val_total': 133, 'val_loss': 95.46192169189453, 'val_avg_loss': 0.7177588097134927, 'val_seen': 133, 'val_correct': 74, 'val_acc': 0.556390977443609, 'test_total': 40, 'test_loss': 30.753803253173828, 'test_avg_loss': 0.7688450813293457, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-14 02:07:37 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 02:07:37 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:07:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-14 02:07:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:07:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:07:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-14 02:07:40 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=56.474255, avg_loss=0.680413, seen=83, correct=55, accuracy=0.662651
2025-09-14 02:07:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:07:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:07:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:07:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2200MB
2025-09-14 02:07:41 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 83, 'val_loss': 56.4742546081543, 'val_avg_loss': 0.680412706122341, 'val_seen': 83, 'val_correct': 55, 'val_acc': 0.6626506024096386}
2025-09-14 02:07:41 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 02:07:41 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 83, 'val_loss': 56.4742546081543, 'val_avg_loss': 0.680412706122341, 'val_seen': 83, 'val_correct': 55, 'val_acc': 0.6626506024096386}
2025-09-14 02:07:41 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #51', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 21, 'val_loss': 16.780643939971924, 'val_avg_loss': 0.7990782828558058, 'val_seen': 21, 'val_correct': 12, 'val_acc': 0.5714285714285714}}
2025-09-14 02:07:41 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #51', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 83, 'val_loss': 56.4742546081543, 'val_avg_loss': 0.680412706122341, 'val_seen': 83, 'val_correct': 55, 'val_acc': 0.6626506024096386}}
2025-09-14 02:07:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:07:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:07:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:07:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:07:42 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.343769, avg_loss=0.683594, seen=40, correct=27, accuracy=0.675000
2025-09-14 02:07:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:07:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:07:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:07:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2200MB
2025-09-14 02:07:43 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 27.343769073486328, 'test_avg_loss': 0.6835942268371582, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-14 02:07:43 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 83, 'val_loss': 56.4742546081543, 'val_avg_loss': 0.680412706122341, 'val_seen': 83, 'val_correct': 55, 'val_acc': 0.6626506024096386}
2025-09-14 02:07:43 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 27.343769073486328, 'test_avg_loss': 0.6835942268371582, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-14 02:07:43 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #51', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.810078859329224, 'test_avg_loss': 0.6810078859329224, 'test_seen': 10, 'test_correct': 7, 'test_acc': 0.7}}
2025-09-14 02:07:43 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #51', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 27.343769073486328, 'test_avg_loss': 0.6835942268371582, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}}
2025-09-14 02:07:43 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 27.343769073486328, 'test_avg_loss': 0.6835942268371582, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}, metrics={'val_total': 83, 'val_loss': 56.4742546081543, 'val_avg_loss': 0.680412706122341, 'val_seen': 83, 'val_correct': 55, 'val_acc': 0.6626506024096386, 'test_total': 40, 'test_loss': 27.343769073486328, 'test_avg_loss': 0.6835942268371582, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-14 02:07:43 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 02:07:43 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:07:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-14 02:07:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:07:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:07:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-14 02:07:50 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=128.952789, avg_loss=0.685919, seen=188, correct=107, accuracy=0.569149
2025-09-14 02:07:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:07:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:07:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:07:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2200MB
2025-09-14 02:07:51 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 188, 'val_loss': 128.95278930664062, 'val_avg_loss': 0.6859190920565991, 'val_seen': 188, 'val_correct': 107, 'val_acc': 0.5691489361702128}
2025-09-14 02:07:51 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 02:07:51 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 188, 'val_loss': 128.95278930664062, 'val_avg_loss': 0.6859190920565991, 'val_seen': 188, 'val_correct': 107, 'val_acc': 0.5691489361702128}
2025-09-14 02:07:51 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #52', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 47, 'val_loss': 30.23387509584427, 'val_avg_loss': 0.6432739382094526, 'val_seen': 47, 'val_correct': 31, 'val_acc': 0.6595744680851063}}
2025-09-14 02:07:51 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #52', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 188, 'val_loss': 128.95278930664062, 'val_avg_loss': 0.6859190920565991, 'val_seen': 188, 'val_correct': 107, 'val_acc': 0.5691489361702128}}
2025-09-14 02:07:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:07:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:07:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:07:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:07:52 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.148201, avg_loss=0.653705, seen=40, correct=22, accuracy=0.550000
2025-09-14 02:07:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:07:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:07:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:07:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2200MB
2025-09-14 02:07:53 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 26.14820098876953, 'test_avg_loss': 0.6537050247192383, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-14 02:07:53 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 188, 'val_loss': 128.95278930664062, 'val_avg_loss': 0.6859190920565991, 'val_seen': 188, 'val_correct': 107, 'val_acc': 0.5691489361702128}
2025-09-14 02:07:53 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 26.14820098876953, 'test_avg_loss': 0.6537050247192383, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-14 02:07:53 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #52', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 8.199673593044281, 'test_avg_loss': 0.8199673593044281, 'test_seen': 10, 'test_correct': 3, 'test_acc': 0.3}}
2025-09-14 02:07:53 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #52', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 26.14820098876953, 'test_avg_loss': 0.6537050247192383, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}}
2025-09-14 02:07:53 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 26.14820098876953, 'test_avg_loss': 0.6537050247192383, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}, metrics={'val_total': 188, 'val_loss': 128.95278930664062, 'val_avg_loss': 0.6859190920565991, 'val_seen': 188, 'val_correct': 107, 'val_acc': 0.5691489361702128, 'test_total': 40, 'test_loss': 26.14820098876953, 'test_avg_loss': 0.6537050247192383, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-14 02:07:53 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 02:07:53 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:07:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:07:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:07:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:07:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:07:59 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=134.291992, avg_loss=0.671460, seen=200, correct=119, accuracy=0.595000
2025-09-14 02:07:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:07:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:08:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:08:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2200MB
2025-09-14 02:08:01 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 200, 'val_loss': 134.2919921875, 'val_avg_loss': 0.6714599609375, 'val_seen': 200, 'val_correct': 119, 'val_acc': 0.595}
2025-09-14 02:08:01 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 02:08:01 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 200, 'val_loss': 134.2919921875, 'val_avg_loss': 0.6714599609375, 'val_seen': 200, 'val_correct': 119, 'val_acc': 0.595}
2025-09-14 02:08:01 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #53', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 50, 'val_loss': 32.87399709224701, 'val_avg_loss': 0.6574799418449402, 'val_seen': 50, 'val_correct': 31, 'val_acc': 0.62}}
2025-09-14 02:08:01 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #53', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 200, 'val_loss': 134.2919921875, 'val_avg_loss': 0.6714599609375, 'val_seen': 200, 'val_correct': 119, 'val_acc': 0.595}}
2025-09-14 02:08:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:08:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:08:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:08:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:08:03 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.882519, avg_loss=0.647063, seen=40, correct=27, accuracy=0.675000
2025-09-14 02:08:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:08:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139614689951744 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:08:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:08:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2200MB
2025-09-14 02:08:04 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 25.882518768310547, 'test_avg_loss': 0.6470629692077636, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-14 02:08:04 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 200, 'val_loss': 134.2919921875, 'val_avg_loss': 0.6714599609375, 'val_seen': 200, 'val_correct': 119, 'val_acc': 0.595}
2025-09-14 02:08:04 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 25.882518768310547, 'test_avg_loss': 0.6470629692077636, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-14 02:08:04 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #53', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 8.181304216384888, 'test_avg_loss': 0.8181304216384888, 'test_seen': 10, 'test_correct': 5, 'test_acc': 0.5}}
2025-09-14 02:08:04 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #53', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 25.882518768310547, 'test_avg_loss': 0.6470629692077636, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}}
2025-09-14 02:08:04 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 25.882518768310547, 'test_avg_loss': 0.6470629692077636, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}, metrics={'val_total': 200, 'val_loss': 134.2919921875, 'val_avg_loss': 0.6714599609375, 'val_seen': 200, 'val_correct': 119, 'val_acc': 0.595, 'test_total': 40, 'test_loss': 25.882518768310547, 'test_avg_loss': 0.6470629692077636, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-14 02:08:04 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 02:08:04 (federatedscope.core.workers.server:772) INFO: {'Role': 'Server #', 'Round': 0, 'Results_weighted_avg': {'val_total': 124.9622641509434, 'val_loss': 104.17429437202466, 'val_avg_loss': 0.6684229447008997, 'val_acc': 0.6053148120187226, 'test_total': 40.0, 'test_loss': 27.122783049097603, 'test_avg_loss': 0.67806957622744, 'test_acc': 0.6037735849056604}, 'Results_avg': {'val_total': 124.9622641509434, 'val_loss': 83.52764458026526, 'val_avg_loss': 0.6711406678320743, 'val_acc': 0.6082372904729341, 'test_total': 40.0, 'test_loss': 27.122783049097603, 'test_avg_loss': 0.6780695762274401, 'test_acc': 0.6037735849056604}, 'Results_fairness': {'val_total': 124.9622641509434, 'test_total': 40.0, 'val_loss_std': 41.54965018508213, 'val_loss_bottom_decile': 19.319059371948242, 'val_loss_top_decile': 134.3212890625, 'val_loss_min': 6.4696855545043945, 'val_loss_max': 142.91162109375, 'val_loss_bottom10%': 10.101961231231689, 'val_loss_top10%': 138.34959920247397, 'val_loss_cos1': 0.8953436007693857, 'val_loss_entropy': 3.8229931081565938, 'val_avg_loss_std': 0.050710634252388496, 'val_avg_loss_bottom_decile': 0.6276894826737661, 'val_avg_loss_top_decile': 0.7177588097134927, 'val_avg_loss_min': 0.5881532322276722, 'val_avg_loss_max': 0.9308722235939719, 'val_avg_loss_bottom10%': 0.6010627138563049, 'val_avg_loss_top10%': 0.7629143820695656, 'val_avg_loss_cos1': 0.9971575931644598, 'val_avg_loss_entropy': 3.967578559719989, 'val_acc_std': 0.07004104039774604, 'val_acc_bottom_decile': 0.5363636363636364, 'val_acc_top_decile': 0.6875, 'val_acc_min': 0.36363636363636365, 'val_acc_max': 0.8181818181818182, 'val_acc_bottom10%': 0.4707223836055685, 'val_acc_top10%': 0.7268476692389737, 'val_acc_cos1': 0.9934349821108267, 'val_acc_entropy': 3.9634784680271276, 'test_loss_std': 2.5961626868281336, 'test_loss_bottom_decile': 24.36602210998535, 'test_loss_top_decile': 30.589750289916992, 'test_loss_min': 21.790782928466797, 'test_loss_max': 34.81593322753906, 'test_loss_bottom10%': 22.944083404541015, 'test_loss_top10%': 31.700374285380047, 'test_loss_cos1': 0.9954501876845755, 'test_loss_entropy': 3.965756193767814, 'test_avg_loss_std': 0.06490406717070334, 'test_avg_loss_bottom_decile': 0.6091505527496338, 'test_avg_loss_top_decile': 0.7647437572479248, 'test_avg_loss_min': 0.54476957321167, 'test_avg_loss_max': 0.8703983306884766, 'test_avg_loss_bottom10%': 0.5736020851135254, 'test_avg_loss_top10%': 0.7925093571345011, 'test_avg_loss_cos1': 0.9954501876845754, 'test_avg_loss_entropy': 3.96575619378082, 'test_acc_std': 0.08029959033459118, 'test_acc_bottom_decile': 0.5, 'test_acc_top_decile': 0.7, 'test_acc_min': 0.45, 'test_acc_max': 0.775, 'test_acc_bottom10%': 0.4600000000000001, 'test_acc_top10%': 0.7374999999999999, 'test_acc_cos1': 0.9912716271941607, 'test_acc_entropy': 3.961394447318901}}
2025-09-14 02:08:04 (root:790) INFO: Find new best result: {'client_best_individual': {'val_loss': 6.4696855545043945, 'val_total': 11.0, 'val_avg_loss': 0.5881532322276722, 'val_acc': 0.8181818181818182, 'test_total': 40.0, 'test_loss': 21.790782928466797, 'test_avg_loss': 0.54476957321167, 'test_acc': 0.775}}
2025-09-14 02:08:04 (root:790) INFO: Find new best result: {'client_best_individual': {'val_loss': 6.4696855545043945, 'val_total': 11.0, 'val_avg_loss': 0.5881532322276722, 'val_acc': 0.8181818181818182, 'test_total': 40.0, 'test_loss': 21.790782928466797, 'test_avg_loss': 0.54476957321167, 'test_acc': 0.775}, 'client_summarized_weighted_avg': {'val_loss': 104.17429437202466, 'val_total': 124.9622641509434, 'val_avg_loss': 0.6684229447008997, 'val_acc': 0.6053148120187226, 'test_total': 40.0, 'test_loss': 27.122783049097603, 'test_avg_loss': 0.67806957622744, 'test_acc': 0.6037735849056604}}
2025-09-14 02:08:04 (root:790) INFO: Find new best result: {'client_best_individual': {'val_loss': 6.4696855545043945, 'val_total': 11.0, 'val_avg_loss': 0.5881532322276722, 'val_acc': 0.8181818181818182, 'test_total': 40.0, 'test_loss': 21.790782928466797, 'test_avg_loss': 0.54476957321167, 'test_acc': 0.775}, 'client_summarized_weighted_avg': {'val_loss': 104.17429437202466, 'val_total': 124.9622641509434, 'val_avg_loss': 0.6684229447008997, 'val_acc': 0.6053148120187226, 'test_total': 40.0, 'test_loss': 27.122783049097603, 'test_avg_loss': 0.67806957622744, 'test_acc': 0.6037735849056604}, 'client_summarized_avg': {'val_loss': 83.52764458026526, 'val_total': 124.9622641509434, 'val_avg_loss': 0.6711406678320743, 'val_acc': 0.6082372904729341, 'test_total': 40.0, 'test_loss': 27.122783049097603, 'test_avg_loss': 0.6780695762274401, 'test_acc': 0.6037735849056604}}
2025-09-14 02:08:04 (root:790) INFO: Find new best result: {'client_best_individual': {'val_loss': 6.4696855545043945, 'val_total': 11.0, 'val_avg_loss': 0.5881532322276722, 'val_acc': 0.8181818181818182, 'test_total': 40.0, 'test_loss': 21.790782928466797, 'test_avg_loss': 0.54476957321167, 'test_acc': 0.775}, 'client_summarized_weighted_avg': {'val_loss': 104.17429437202466, 'val_total': 124.9622641509434, 'val_avg_loss': 0.6684229447008997, 'val_acc': 0.6053148120187226, 'test_total': 40.0, 'test_loss': 27.122783049097603, 'test_avg_loss': 0.67806957622744, 'test_acc': 0.6037735849056604}, 'client_summarized_avg': {'val_loss': 83.52764458026526, 'val_total': 124.9622641509434, 'val_avg_loss': 0.6711406678320743, 'val_acc': 0.6082372904729341, 'test_total': 40.0, 'test_loss': 27.122783049097603, 'test_avg_loss': 0.6780695762274401, 'test_acc': 0.6037735849056604}, 'client_summarized_fairness': {'val_loss_entropy': 3.8229931081565938, 'val_loss_cos1': 0.8953436007693857, 'val_loss_top10%': 138.34959920247397, 'val_loss_bottom10%': 10.101961231231689, 'val_loss_max': 142.91162109375, 'val_loss_min': 6.4696855545043945, 'val_loss_top_decile': 134.3212890625, 'val_loss_bottom_decile': 19.319059371948242, 'val_loss_std': 41.54965018508213, 'val_total': 124.9622641509434, 'test_total': 40.0, 'val_avg_loss_std': 0.050710634252388496, 'val_avg_loss_bottom_decile': 0.6276894826737661, 'val_avg_loss_top_decile': 0.7177588097134927, 'val_avg_loss_min': 0.5881532322276722, 'val_avg_loss_max': 0.9308722235939719, 'val_avg_loss_bottom10%': 0.6010627138563049, 'val_avg_loss_top10%': 0.7629143820695656, 'val_avg_loss_cos1': 0.9971575931644598, 'val_avg_loss_entropy': 3.967578559719989, 'val_acc_std': 0.07004104039774604, 'val_acc_bottom_decile': 0.5363636363636364, 'val_acc_top_decile': 0.6875, 'val_acc_min': 0.36363636363636365, 'val_acc_max': 0.8181818181818182, 'val_acc_bottom10%': 0.4707223836055685, 'val_acc_top10%': 0.7268476692389737, 'val_acc_cos1': 0.9934349821108267, 'val_acc_entropy': 3.9634784680271276, 'test_loss_std': 2.5961626868281336, 'test_loss_bottom_decile': 24.36602210998535, 'test_loss_top_decile': 30.589750289916992, 'test_loss_min': 21.790782928466797, 'test_loss_max': 34.81593322753906, 'test_loss_bottom10%': 22.944083404541015, 'test_loss_top10%': 31.700374285380047, 'test_loss_cos1': 0.9954501876845755, 'test_loss_entropy': 3.965756193767814, 'test_avg_loss_std': 0.06490406717070334, 'test_avg_loss_bottom_decile': 0.6091505527496338, 'test_avg_loss_top_decile': 0.7647437572479248, 'test_avg_loss_min': 0.54476957321167, 'test_avg_loss_max': 0.8703983306884766, 'test_avg_loss_bottom10%': 0.5736020851135254, 'test_avg_loss_top10%': 0.7925093571345011, 'test_avg_loss_cos1': 0.9954501876845754, 'test_avg_loss_entropy': 3.96575619378082, 'test_acc_std': 0.08029959033459118, 'test_acc_bottom_decile': 0.5, 'test_acc_top_decile': 0.7, 'test_acc_min': 0.45, 'test_acc_max': 0.775, 'test_acc_bottom10%': 0.4600000000000001, 'test_acc_top10%': 0.7374999999999999, 'test_acc_cos1': 0.9912716271941607, 'test_acc_entropy': 3.961394447318901}}
2025-09-14 02:08:04 (federatedscope.core.workers.server:518) INFO: Server: Final evaluation is finished! Starting merging results.
2025-09-14 02:08:04 (federatedscope.core.workers.server:644) INFO: {'Role': 'Server #', 'Round': 'Final', 'Results_raw': {'client_best_individual': {'val_loss': 6.4696855545043945, 'val_total': 11.0, 'val_avg_loss': 0.5881532322276722, 'val_acc': 0.8181818181818182, 'test_total': 40.0, 'test_loss': 21.790782928466797, 'test_avg_loss': 0.54476957321167, 'test_acc': 0.775}, 'client_summarized_weighted_avg': {'val_loss': 104.17429437202466, 'val_total': 124.9622641509434, 'val_avg_loss': 0.6684229447008997, 'val_acc': 0.6053148120187226, 'test_total': 40.0, 'test_loss': 27.122783049097603, 'test_avg_loss': 0.67806957622744, 'test_acc': 0.6037735849056604}, 'client_summarized_avg': {'val_loss': 83.52764458026526, 'val_total': 124.9622641509434, 'val_avg_loss': 0.6711406678320743, 'val_acc': 0.6082372904729341, 'test_total': 40.0, 'test_loss': 27.122783049097603, 'test_avg_loss': 0.6780695762274401, 'test_acc': 0.6037735849056604}, 'client_summarized_fairness': {'val_loss_entropy': 3.8229931081565938, 'val_loss_cos1': 0.8953436007693857, 'val_loss_top10%': 138.34959920247397, 'val_loss_bottom10%': 10.101961231231689, 'val_loss_max': 142.91162109375, 'val_loss_min': 6.4696855545043945, 'val_loss_top_decile': 134.3212890625, 'val_loss_bottom_decile': 19.319059371948242, 'val_loss_std': 41.54965018508213, 'val_total': 124.9622641509434, 'test_total': 40.0, 'val_avg_loss_std': 0.050710634252388496, 'val_avg_loss_bottom_decile': 0.6276894826737661, 'val_avg_loss_top_decile': 0.7177588097134927, 'val_avg_loss_min': 0.5881532322276722, 'val_avg_loss_max': 0.9308722235939719, 'val_avg_loss_bottom10%': 0.6010627138563049, 'val_avg_loss_top10%': 0.7629143820695656, 'val_avg_loss_cos1': 0.9971575931644598, 'val_avg_loss_entropy': 3.967578559719989, 'val_acc_std': 0.07004104039774604, 'val_acc_bottom_decile': 0.5363636363636364, 'val_acc_top_decile': 0.6875, 'val_acc_min': 0.36363636363636365, 'val_acc_max': 0.8181818181818182, 'val_acc_bottom10%': 0.4707223836055685, 'val_acc_top10%': 0.7268476692389737, 'val_acc_cos1': 0.9934349821108267, 'val_acc_entropy': 3.9634784680271276, 'test_loss_std': 2.5961626868281336, 'test_loss_bottom_decile': 24.36602210998535, 'test_loss_top_decile': 30.589750289916992, 'test_loss_min': 21.790782928466797, 'test_loss_max': 34.81593322753906, 'test_loss_bottom10%': 22.944083404541015, 'test_loss_top10%': 31.700374285380047, 'test_loss_cos1': 0.9954501876845755, 'test_loss_entropy': 3.965756193767814, 'test_avg_loss_std': 0.06490406717070334, 'test_avg_loss_bottom_decile': 0.6091505527496338, 'test_avg_loss_top_decile': 0.7647437572479248, 'test_avg_loss_min': 0.54476957321167, 'test_avg_loss_max': 0.8703983306884766, 'test_avg_loss_bottom10%': 0.5736020851135254, 'test_avg_loss_top10%': 0.7925093571345011, 'test_avg_loss_cos1': 0.9954501876845754, 'test_avg_loss_entropy': 3.96575619378082, 'test_acc_std': 0.08029959033459118, 'test_acc_bottom_decile': 0.5, 'test_acc_top_decile': 0.7, 'test_acc_min': 0.45, 'test_acc_max': 0.775, 'test_acc_bottom10%': 0.4600000000000001, 'test_acc_top10%': 0.7374999999999999, 'test_acc_cos1': 0.9912716271941607, 'test_acc_entropy': 3.961394447318901}}}
2025-09-14 02:08:04 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #1', 'Round': 1, 'Results_raw': {'val_total': 146, 'val_loss': 97.2881851196289, 'val_avg_loss': 0.6663574323262254, 'val_acc': 0.6164383561643836, 'test_total': 40, 'test_loss': 24.495136260986328, 'test_avg_loss': 0.6123784065246582, 'test_acc': 0.775}}
2025-09-14 02:08:04 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #2', 'Round': 1, 'Results_raw': {'val_total': 11, 'val_loss': 10.239594459533691, 'val_avg_loss': 0.9308722235939719, 'val_acc': 0.36363636363636365, 'test_total': 40, 'test_loss': 27.791149139404297, 'test_avg_loss': 0.6947787284851075, 'test_acc': 0.6}}
2025-09-14 02:08:04 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #3', 'Round': 1, 'Results_raw': {'val_total': 36, 'val_loss': 24.887300491333008, 'val_avg_loss': 0.691313902537028, 'val_acc': 0.6111111111111112, 'test_total': 40, 'test_loss': 27.284481048583984, 'test_avg_loss': 0.6821120262145997, 'test_acc': 0.6}}
2025-09-14 02:08:04 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #4', 'Round': 1, 'Results_raw': {'val_total': 11, 'val_loss': 6.4696855545043945, 'val_avg_loss': 0.5881532322276722, 'val_acc': 0.8181818181818182, 'test_total': 40, 'test_loss': 30.768991470336914, 'test_avg_loss': 0.7692247867584229, 'test_acc': 0.45}}
2025-09-14 02:08:04 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #5', 'Round': 1, 'Results_raw': {'val_total': 14, 'val_loss': 9.137686729431152, 'val_avg_loss': 0.6526919092450824, 'val_acc': 0.6428571428571429, 'test_total': 40, 'test_loss': 24.36602210998535, 'test_avg_loss': 0.6091505527496338, 'test_acc': 0.725}}
2025-09-14 02:08:04 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #6', 'Round': 1, 'Results_raw': {'val_total': 134, 'val_loss': 86.74491119384766, 'val_avg_loss': 0.6473500835361765, 'val_acc': 0.6194029850746269, 'test_total': 40, 'test_loss': 32.37575149536133, 'test_avg_loss': 0.8093937873840332, 'test_acc': 0.525}}
2025-09-14 02:08:04 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #7', 'Round': 1, 'Results_raw': {'val_total': 57, 'val_loss': 38.973934173583984, 'val_avg_loss': 0.6837532311155085, 'val_acc': 0.5964912280701754, 'test_total': 40, 'test_loss': 24.442901611328125, 'test_avg_loss': 0.6110725402832031, 'test_acc': 0.7}}
2025-09-14 02:08:04 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #8', 'Round': 1, 'Results_raw': {'val_total': 69, 'val_loss': 47.54908752441406, 'val_avg_loss': 0.6891172104987545, 'val_acc': 0.6231884057971014, 'test_total': 40, 'test_loss': 29.405933380126953, 'test_avg_loss': 0.7351483345031739, 'test_acc': 0.55}}
2025-09-14 02:08:04 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #9', 'Round': 1, 'Results_raw': {'val_total': 188, 'val_loss': 121.84922790527344, 'val_avg_loss': 0.648134190985497, 'val_acc': 0.6063829787234043, 'test_total': 40, 'test_loss': 27.066219329833984, 'test_avg_loss': 0.6766554832458496, 'test_acc': 0.575}}
2025-09-14 02:08:04 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #10', 'Round': 1, 'Results_raw': {'val_total': 63, 'val_loss': 39.544437408447266, 'val_avg_loss': 0.6276894826737661, 'val_acc': 0.6666666666666666, 'test_total': 40, 'test_loss': 23.624000549316406, 'test_avg_loss': 0.5906000137329102, 'test_acc': 0.75}}
2025-09-14 02:08:04 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #11', 'Round': 1, 'Results_raw': {'val_total': 32, 'val_loss': 19.319059371948242, 'val_avg_loss': 0.6037206053733826, 'val_acc': 0.6875, 'test_total': 40, 'test_loss': 25.51361083984375, 'test_avg_loss': 0.6378402709960938, 'test_acc': 0.525}}
2025-09-14 02:08:04 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #12', 'Round': 1, 'Results_raw': {'val_total': 137, 'val_loss': 89.47102355957031, 'val_avg_loss': 0.6530731646683965, 'val_acc': 0.6204379562043796, 'test_total': 40, 'test_loss': 25.18402862548828, 'test_avg_loss': 0.629600715637207, 'test_acc': 0.675}}
2025-09-14 02:08:04 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #13', 'Round': 1, 'Results_raw': {'val_total': 72, 'val_loss': 47.685096740722656, 'val_avg_loss': 0.6622930102878146, 'val_acc': 0.6666666666666666, 'test_total': 40, 'test_loss': 29.354694366455078, 'test_avg_loss': 0.7338673591613769, 'test_acc': 0.625}}
2025-09-14 02:08:04 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #14', 'Round': 1, 'Results_raw': {'val_total': 160, 'val_loss': 105.87722778320312, 'val_avg_loss': 0.6617326736450195, 'val_acc': 0.625, 'test_total': 40, 'test_loss': 23.502336502075195, 'test_avg_loss': 0.5875584125518799, 'test_acc': 0.725}}
2025-09-14 02:08:04 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #15', 'Round': 1, 'Results_raw': {'val_total': 200, 'val_loss': 132.8359375, 'val_avg_loss': 0.6641796875, 'val_acc': 0.615, 'test_total': 40, 'test_loss': 27.66522979736328, 'test_avg_loss': 0.691630744934082, 'test_acc': 0.575}}
2025-09-14 02:08:04 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #16', 'Round': 1, 'Results_raw': {'val_total': 136, 'val_loss': 89.32501983642578, 'val_avg_loss': 0.6568016164443072, 'val_acc': 0.5735294117647058, 'test_total': 40, 'test_loss': 25.95720100402832, 'test_avg_loss': 0.648930025100708, 'test_acc': 0.7}}
2025-09-14 02:08:04 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #17', 'Round': 1, 'Results_raw': {'val_total': 200, 'val_loss': 126.48646545410156, 'val_avg_loss': 0.6324323272705078, 'val_acc': 0.665, 'test_total': 40, 'test_loss': 27.68305206298828, 'test_avg_loss': 0.692076301574707, 'test_acc': 0.625}}
2025-09-14 02:08:04 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #18', 'Round': 1, 'Results_raw': {'val_total': 135, 'val_loss': 91.47264862060547, 'val_avg_loss': 0.6775751749674479, 'val_acc': 0.5851851851851851, 'test_total': 40, 'test_loss': 24.947540283203125, 'test_avg_loss': 0.6236885070800782, 'test_acc': 0.575}}
2025-09-14 02:08:04 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #19', 'Round': 1, 'Results_raw': {'val_total': 110, 'val_loss': 69.26115417480469, 'val_avg_loss': 0.629646856134588, 'val_acc': 0.6090909090909091, 'test_total': 40, 'test_loss': 30.89801597595215, 'test_avg_loss': 0.7724503993988037, 'test_acc': 0.575}}
2025-09-14 02:08:04 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #20', 'Round': 1, 'Results_raw': {'val_total': 126, 'val_loss': 87.44287872314453, 'val_avg_loss': 0.6939911009773375, 'val_acc': 0.6031746031746031, 'test_total': 40, 'test_loss': 25.06903076171875, 'test_avg_loss': 0.6267257690429687, 'test_acc': 0.625}}
2025-09-14 02:08:04 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #21', 'Round': 1, 'Results_raw': {'val_total': 153, 'val_loss': 103.44137573242188, 'val_avg_loss': 0.6760874230877246, 'val_acc': 0.5882352941176471, 'test_total': 40, 'test_loss': 29.796127319335938, 'test_avg_loss': 0.7449031829833984, 'test_acc': 0.6}}
2025-09-14 02:08:04 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #22', 'Round': 1, 'Results_raw': {'val_total': 11, 'val_loss': 6.726081371307373, 'val_avg_loss': 0.6114619428461249, 'val_acc': 0.7272727272727273, 'test_total': 40, 'test_loss': 30.364103317260742, 'test_avg_loss': 0.7591025829315186, 'test_acc': 0.475}}
2025-09-14 02:08:04 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #23', 'Round': 1, 'Results_raw': {'val_total': 30, 'val_loss': 17.936758041381836, 'val_avg_loss': 0.5978919347127278, 'val_acc': 0.7333333333333333, 'test_total': 40, 'test_loss': 25.355257034301758, 'test_avg_loss': 0.6338814258575439, 'test_acc': 0.575}}
2025-09-14 02:08:04 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #24', 'Round': 1, 'Results_raw': {'val_total': 200, 'val_loss': 134.3212890625, 'val_avg_loss': 0.6716064453125, 'val_acc': 0.63, 'test_total': 40, 'test_loss': 29.35737419128418, 'test_avg_loss': 0.7339343547821044, 'test_acc': 0.525}}
2025-09-14 02:08:04 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #25', 'Round': 1, 'Results_raw': {'val_total': 200, 'val_loss': 141.21832275390625, 'val_avg_loss': 0.7060916137695312, 'val_acc': 0.51, 'test_total': 40, 'test_loss': 34.81593322753906, 'test_avg_loss': 0.8703983306884766, 'test_acc': 0.5}}
2025-09-14 02:08:04 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #26', 'Round': 1, 'Results_raw': {'val_total': 161, 'val_loss': 101.68584442138672, 'val_avg_loss': 0.6315890957850107, 'val_acc': 0.6894409937888198, 'test_total': 40, 'test_loss': 24.463199615478516, 'test_avg_loss': 0.6115799903869629, 'test_acc': 0.7}}
2025-09-14 02:08:04 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #27', 'Round': 1, 'Results_raw': {'val_total': 123, 'val_loss': 81.49714660644531, 'val_avg_loss': 0.6625784276946773, 'val_acc': 0.5853658536585366, 'test_total': 40, 'test_loss': 28.13616371154785, 'test_avg_loss': 0.7034040927886963, 'test_acc': 0.475}}
2025-09-14 02:08:04 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #28', 'Round': 1, 'Results_raw': {'val_total': 75, 'val_loss': 54.19215393066406, 'val_avg_loss': 0.7225620524088542, 'val_acc': 0.49333333333333335, 'test_total': 40, 'test_loss': 28.103092193603516, 'test_avg_loss': 0.7025773048400878, 'test_acc': 0.575}}
2025-09-14 02:08:04 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #29', 'Round': 1, 'Results_raw': {'val_total': 200, 'val_loss': 138.28445434570312, 'val_avg_loss': 0.6914222717285157, 'val_acc': 0.565, 'test_total': 40, 'test_loss': 22.510845184326172, 'test_avg_loss': 0.5627711296081543, 'test_acc': 0.65}}
2025-09-14 02:08:04 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #30', 'Round': 1, 'Results_raw': {'val_total': 170, 'val_loss': 110.70817565917969, 'val_avg_loss': 0.651224562701057, 'val_acc': 0.6352941176470588, 'test_total': 40, 'test_loss': 25.69356918334961, 'test_avg_loss': 0.6423392295837402, 'test_acc': 0.625}}
2025-09-14 02:08:04 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #31', 'Round': 1, 'Results_raw': {'val_total': 193, 'val_loss': 134.77272033691406, 'val_avg_loss': 0.698304250450332, 'val_acc': 0.5699481865284974, 'test_total': 40, 'test_loss': 26.6758975982666, 'test_avg_loss': 0.6668974399566651, 'test_acc': 0.65}}
2025-09-14 02:08:04 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #32', 'Round': 1, 'Results_raw': {'val_total': 112, 'val_loss': 67.6576156616211, 'val_avg_loss': 0.6040858541216169, 'val_acc': 0.7053571428571429, 'test_total': 40, 'test_loss': 29.239391326904297, 'test_avg_loss': 0.7309847831726074, 'test_acc': 0.6}}
2025-09-14 02:08:04 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #33', 'Round': 1, 'Results_raw': {'val_total': 74, 'val_loss': 49.657257080078125, 'val_avg_loss': 0.6710440145956503, 'val_acc': 0.6486486486486487, 'test_total': 40, 'test_loss': 27.514991760253906, 'test_avg_loss': 0.6878747940063477, 'test_acc': 0.575}}
2025-09-14 02:08:04 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #34', 'Round': 1, 'Results_raw': {'val_total': 200, 'val_loss': 125.82234954833984, 'val_avg_loss': 0.6291117477416992, 'val_acc': 0.665, 'test_total': 40, 'test_loss': 25.595666885375977, 'test_avg_loss': 0.6398916721343995, 'test_acc': 0.65}}
2025-09-14 02:08:04 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #35', 'Round': 1, 'Results_raw': {'val_total': 200, 'val_loss': 128.09283447265625, 'val_avg_loss': 0.6404641723632812, 'val_acc': 0.635, 'test_total': 40, 'test_loss': 27.443248748779297, 'test_avg_loss': 0.6860812187194825, 'test_acc': 0.55}}
2025-09-14 02:08:04 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #36', 'Round': 1, 'Results_raw': {'val_total': 54, 'val_loss': 37.04812240600586, 'val_avg_loss': 0.6860763408519603, 'val_acc': 0.5555555555555556, 'test_total': 40, 'test_loss': 23.292451858520508, 'test_avg_loss': 0.5823112964630127, 'test_acc': 0.675}}
2025-09-14 02:08:04 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #37', 'Round': 1, 'Results_raw': {'val_total': 200, 'val_loss': 138.5891876220703, 'val_avg_loss': 0.6929459381103515, 'val_acc': 0.605, 'test_total': 40, 'test_loss': 25.34657859802246, 'test_avg_loss': 0.6336644649505615, 'test_acc': 0.675}}
2025-09-14 02:08:04 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #38', 'Round': 1, 'Results_raw': {'val_total': 200, 'val_loss': 132.83335876464844, 'val_avg_loss': 0.6641667938232422, 'val_acc': 0.625, 'test_total': 40, 'test_loss': 26.67328453063965, 'test_avg_loss': 0.6668321132659912, 'test_acc': 0.625}}
2025-09-14 02:08:04 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #39', 'Round': 1, 'Results_raw': {'val_total': 83, 'val_loss': 61.09270477294922, 'val_avg_loss': 0.7360566840114364, 'val_acc': 0.5301204819277109, 'test_total': 40, 'test_loss': 29.948551177978516, 'test_avg_loss': 0.7487137794494629, 'test_acc': 0.575}}
2025-09-14 02:08:04 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #40', 'Round': 1, 'Results_raw': {'val_total': 200, 'val_loss': 126.99900817871094, 'val_avg_loss': 0.6349950408935547, 'val_acc': 0.625, 'test_total': 40, 'test_loss': 26.684295654296875, 'test_avg_loss': 0.6671073913574219, 'test_acc': 0.6}}
2025-09-14 02:08:04 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #41', 'Round': 1, 'Results_raw': {'val_total': 119, 'val_loss': 80.25254821777344, 'val_avg_loss': 0.6743911614938944, 'val_acc': 0.5882352941176471, 'test_total': 40, 'test_loss': 27.367504119873047, 'test_avg_loss': 0.6841876029968261, 'test_acc': 0.55}}
2025-09-14 02:08:04 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #42', 'Round': 1, 'Results_raw': {'val_total': 200, 'val_loss': 126.3978271484375, 'val_avg_loss': 0.6319891357421875, 'val_acc': 0.64, 'test_total': 40, 'test_loss': 26.930984497070312, 'test_avg_loss': 0.6732746124267578, 'test_acc': 0.575}}
2025-09-14 02:08:04 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #43', 'Round': 1, 'Results_raw': {'val_total': 89, 'val_loss': 62.607425689697266, 'val_avg_loss': 0.7034542212325535, 'val_acc': 0.5730337078651685, 'test_total': 40, 'test_loss': 25.712900161743164, 'test_avg_loss': 0.6428225040435791, 'test_acc': 0.65}}
2025-09-14 02:08:04 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #44', 'Round': 1, 'Results_raw': {'val_total': 200, 'val_loss': 142.91162109375, 'val_avg_loss': 0.71455810546875, 'val_acc': 0.545, 'test_total': 40, 'test_loss': 25.927762985229492, 'test_avg_loss': 0.6481940746307373, 'test_acc': 0.575}}
2025-09-14 02:08:04 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #45', 'Round': 1, 'Results_raw': {'val_total': 100, 'val_loss': 63.832305908203125, 'val_avg_loss': 0.6383230590820312, 'val_acc': 0.59, 'test_total': 40, 'test_loss': 28.878376007080078, 'test_avg_loss': 0.721959400177002, 'test_acc': 0.55}}
2025-09-14 02:08:04 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #46', 'Round': 1, 'Results_raw': {'val_total': 110, 'val_loss': 79.1885986328125, 'val_avg_loss': 0.7198963512073864, 'val_acc': 0.5363636363636364, 'test_total': 40, 'test_loss': 21.790782928466797, 'test_avg_loss': 0.54476957321167, 'test_acc': 0.75}}
2025-09-14 02:08:04 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #47', 'Round': 1, 'Results_raw': {'val_total': 147, 'val_loss': 103.76628875732422, 'val_avg_loss': 0.7058931207981239, 'val_acc': 0.5510204081632653, 'test_total': 40, 'test_loss': 25.400659561157227, 'test_avg_loss': 0.6350164890289307, 'test_acc': 0.7}}
2025-09-14 02:08:04 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #48', 'Round': 1, 'Results_raw': {'val_total': 46, 'val_loss': 34.515647888183594, 'val_avg_loss': 0.7503401714822521, 'val_acc': 0.45652173913043476, 'test_total': 40, 'test_loss': 30.375139236450195, 'test_avg_loss': 0.7593784809112549, 'test_acc': 0.45}}
2025-09-14 02:08:04 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #49', 'Round': 1, 'Results_raw': {'val_total': 132, 'val_loss': 83.87461853027344, 'val_avg_loss': 0.6354137767444957, 'val_acc': 0.6363636363636364, 'test_total': 40, 'test_loss': 30.589750289916992, 'test_avg_loss': 0.7647437572479248, 'test_acc': 0.5}}
2025-09-14 02:08:04 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #50', 'Round': 1, 'Results_raw': {'val_total': 133, 'val_loss': 95.46192169189453, 'val_avg_loss': 0.7177588097134927, 'val_acc': 0.556390977443609, 'test_total': 40, 'test_loss': 30.753803253173828, 'test_avg_loss': 0.7688450813293457, 'test_acc': 0.45}}
2025-09-14 02:08:04 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #51', 'Round': 1, 'Results_raw': {'val_total': 83, 'val_loss': 56.4742546081543, 'val_avg_loss': 0.680412706122341, 'val_acc': 0.6626506024096386, 'test_total': 40, 'test_loss': 27.343769073486328, 'test_avg_loss': 0.6835942268371582, 'test_acc': 0.675}}
2025-09-14 02:08:04 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #52', 'Round': 1, 'Results_raw': {'val_total': 188, 'val_loss': 128.95278930664062, 'val_avg_loss': 0.6859190920565991, 'val_acc': 0.5691489361702128, 'test_total': 40, 'test_loss': 26.14820098876953, 'test_avg_loss': 0.6537050247192383, 'test_acc': 0.55}}
2025-09-14 02:08:04 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #53', 'Round': 1, 'Results_raw': {'val_total': 200, 'val_loss': 134.2919921875, 'val_avg_loss': 0.6714599609375, 'val_acc': 0.595, 'test_total': 40, 'test_loss': 25.882518768310547, 'test_avg_loss': 0.6470629692077636, 'test_acc': 0.675}}
2025-09-14 02:08:04 (federatedscope.core.monitors.monitor:268) INFO: In worker #0, the system-related metrics are: {'id': 0, 'fl_end_time_minutes': 214.29861058333333, 'total_model_size': 0, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 92208, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 02:08:04 (federatedscope.core.workers.client:842) INFO: ================= client 1 received finish message =================
2025-09-14 02:08:04 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:08:04 (federatedscope.core.monitors.monitor:268) INFO: In worker #1, the system-related metrics are: {'id': 1, 'fl_end_time_minutes': 214.29969681666668, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367544, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 02:08:04 (federatedscope.core.workers.client:842) INFO: ================= client 2 received finish message =================
2025-09-14 02:08:04 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:08:04 (federatedscope.core.monitors.monitor:268) INFO: In worker #2, the system-related metrics are: {'id': 2, 'fl_end_time_minutes': 214.23709418333334, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367544, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 02:08:04 (federatedscope.core.workers.client:842) INFO: ================= client 3 received finish message =================
2025-09-14 02:08:04 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:08:04 (federatedscope.core.monitors.monitor:268) INFO: In worker #3, the system-related metrics are: {'id': 3, 'fl_end_time_minutes': 214.19764271666665, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367552, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 02:08:04 (federatedscope.core.workers.client:842) INFO: ================= client 4 received finish message =================
2025-09-14 02:08:04 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:08:04 (federatedscope.core.monitors.monitor:268) INFO: In worker #4, the system-related metrics are: {'id': 4, 'fl_end_time_minutes': 214.15757878333335, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367552, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 02:08:04 (federatedscope.core.workers.client:842) INFO: ================= client 5 received finish message =================
2025-09-14 02:08:04 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:08:04 (federatedscope.core.monitors.monitor:268) INFO: In worker #5, the system-related metrics are: {'id': 5, 'fl_end_time_minutes': 214.11720726666667, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367552, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 02:08:04 (federatedscope.core.workers.client:842) INFO: ================= client 6 received finish message =================
2025-09-14 02:08:04 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:08:04 (federatedscope.core.monitors.monitor:268) INFO: In worker #6, the system-related metrics are: {'id': 6, 'fl_end_time_minutes': 214.07686918333332, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367552, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 02:08:04 (federatedscope.core.workers.client:842) INFO: ================= client 7 received finish message =================
2025-09-14 02:08:04 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:08:04 (federatedscope.core.monitors.monitor:268) INFO: In worker #7, the system-related metrics are: {'id': 7, 'fl_end_time_minutes': 214.03618113333332, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367544, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 02:08:04 (federatedscope.core.workers.client:842) INFO: ================= client 8 received finish message =================
2025-09-14 02:08:04 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:08:04 (federatedscope.core.monitors.monitor:268) INFO: In worker #8, the system-related metrics are: {'id': 8, 'fl_end_time_minutes': 213.99587675, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367544, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 02:08:04 (federatedscope.core.workers.client:842) INFO: ================= client 9 received finish message =================
2025-09-14 02:08:04 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:08:04 (federatedscope.core.monitors.monitor:268) INFO: In worker #9, the system-related metrics are: {'id': 9, 'fl_end_time_minutes': 213.95546151666667, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367544, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 02:08:04 (federatedscope.core.workers.client:842) INFO: ================= client 10 received finish message =================
2025-09-14 02:08:05 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:08:05 (federatedscope.core.monitors.monitor:268) INFO: In worker #10, the system-related metrics are: {'id': 10, 'fl_end_time_minutes': 213.91524735, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367544, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 02:08:05 (federatedscope.core.workers.client:842) INFO: ================= client 11 received finish message =================
2025-09-14 02:08:05 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:08:05 (federatedscope.core.monitors.monitor:268) INFO: In worker #11, the system-related metrics are: {'id': 11, 'fl_end_time_minutes': 213.8750766, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367544, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 02:08:05 (federatedscope.core.workers.client:842) INFO: ================= client 12 received finish message =================
2025-09-14 02:08:05 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:08:05 (federatedscope.core.monitors.monitor:268) INFO: In worker #12, the system-related metrics are: {'id': 12, 'fl_end_time_minutes': 213.83491581666667, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367544, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 02:08:05 (federatedscope.core.workers.client:842) INFO: ================= client 13 received finish message =================
2025-09-14 02:08:05 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:08:05 (federatedscope.core.monitors.monitor:268) INFO: In worker #13, the system-related metrics are: {'id': 13, 'fl_end_time_minutes': 213.79415966666664, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367552, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 02:08:05 (federatedscope.core.workers.client:842) INFO: ================= client 14 received finish message =================
2025-09-14 02:08:05 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:08:05 (federatedscope.core.monitors.monitor:268) INFO: In worker #14, the system-related metrics are: {'id': 14, 'fl_end_time_minutes': 213.74967523333333, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367544, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 02:08:05 (federatedscope.core.workers.client:842) INFO: ================= client 15 received finish message =================
2025-09-14 02:08:05 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:08:05 (federatedscope.core.monitors.monitor:268) INFO: In worker #15, the system-related metrics are: {'id': 15, 'fl_end_time_minutes': 213.70847065, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367544, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 02:08:05 (federatedscope.core.workers.client:842) INFO: ================= client 16 received finish message =================
2025-09-14 02:08:05 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:08:05 (federatedscope.core.monitors.monitor:268) INFO: In worker #16, the system-related metrics are: {'id': 16, 'fl_end_time_minutes': 213.66792693333335, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367544, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 02:08:05 (federatedscope.core.workers.client:842) INFO: ================= client 17 received finish message =================
2025-09-14 02:08:05 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:08:05 (federatedscope.core.monitors.monitor:268) INFO: In worker #17, the system-related metrics are: {'id': 17, 'fl_end_time_minutes': 213.62762023333335, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367544, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 02:08:05 (federatedscope.core.workers.client:842) INFO: ================= client 18 received finish message =================
2025-09-14 02:08:05 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:08:05 (federatedscope.core.monitors.monitor:268) INFO: In worker #18, the system-related metrics are: {'id': 18, 'fl_end_time_minutes': 213.58735328333333, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367544, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 02:08:05 (federatedscope.core.workers.client:842) INFO: ================= client 19 received finish message =================
2025-09-14 02:08:05 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:08:05 (federatedscope.core.monitors.monitor:268) INFO: In worker #19, the system-related metrics are: {'id': 19, 'fl_end_time_minutes': 213.54741958333335, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367544, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 02:08:05 (federatedscope.core.workers.client:842) INFO: ================= client 20 received finish message =================
2025-09-14 02:08:05 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:08:05 (federatedscope.core.monitors.monitor:268) INFO: In worker #20, the system-related metrics are: {'id': 20, 'fl_end_time_minutes': 213.49020215000002, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367552, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 02:08:05 (federatedscope.core.workers.client:842) INFO: ================= client 21 received finish message =================
2025-09-14 02:08:05 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:08:05 (federatedscope.core.monitors.monitor:268) INFO: In worker #21, the system-related metrics are: {'id': 21, 'fl_end_time_minutes': 213.45089436666666, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367544, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 02:08:05 (federatedscope.core.workers.client:842) INFO: ================= client 22 received finish message =================
2025-09-14 02:08:05 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:08:05 (federatedscope.core.monitors.monitor:268) INFO: In worker #22, the system-related metrics are: {'id': 22, 'fl_end_time_minutes': 213.41157488333334, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367552, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 02:08:05 (federatedscope.core.workers.client:842) INFO: ================= client 23 received finish message =================
2025-09-14 02:08:05 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:08:05 (federatedscope.core.monitors.monitor:268) INFO: In worker #23, the system-related metrics are: {'id': 23, 'fl_end_time_minutes': 213.37101955, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367544, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 02:08:05 (federatedscope.core.workers.client:842) INFO: ================= client 24 received finish message =================
2025-09-14 02:08:05 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:08:05 (federatedscope.core.monitors.monitor:268) INFO: In worker #24, the system-related metrics are: {'id': 24, 'fl_end_time_minutes': 213.32854716666668, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367544, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 02:08:05 (federatedscope.core.workers.client:842) INFO: ================= client 25 received finish message =================
2025-09-14 02:08:05 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:08:05 (federatedscope.core.monitors.monitor:268) INFO: In worker #25, the system-related metrics are: {'id': 25, 'fl_end_time_minutes': 213.28487706666667, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367552, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 02:08:05 (federatedscope.core.workers.client:842) INFO: ================= client 26 received finish message =================
2025-09-14 02:08:05 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:08:05 (federatedscope.core.monitors.monitor:268) INFO: In worker #26, the system-related metrics are: {'id': 26, 'fl_end_time_minutes': 213.24423326666664, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367544, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 02:08:05 (federatedscope.core.workers.client:842) INFO: ================= client 27 received finish message =================
2025-09-14 02:08:05 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:08:05 (federatedscope.core.monitors.monitor:268) INFO: In worker #27, the system-related metrics are: {'id': 27, 'fl_end_time_minutes': 213.2036047, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367552, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 02:08:05 (federatedscope.core.workers.client:842) INFO: ================= client 28 received finish message =================
2025-09-14 02:08:05 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:08:05 (federatedscope.core.monitors.monitor:268) INFO: In worker #28, the system-related metrics are: {'id': 28, 'fl_end_time_minutes': 213.16269316666666, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367544, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 02:08:05 (federatedscope.core.workers.client:842) INFO: ================= client 29 received finish message =================
2025-09-14 02:08:05 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:08:05 (federatedscope.core.monitors.monitor:268) INFO: In worker #29, the system-related metrics are: {'id': 29, 'fl_end_time_minutes': 213.12184573333334, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367552, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 02:08:05 (federatedscope.core.workers.client:842) INFO: ================= client 30 received finish message =================
2025-09-14 02:08:06 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:08:06 (federatedscope.core.monitors.monitor:268) INFO: In worker #30, the system-related metrics are: {'id': 30, 'fl_end_time_minutes': 213.0814319, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367544, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 02:08:06 (federatedscope.core.workers.client:842) INFO: ================= client 31 received finish message =================
2025-09-14 02:08:06 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:08:06 (federatedscope.core.monitors.monitor:268) INFO: In worker #31, the system-related metrics are: {'id': 31, 'fl_end_time_minutes': 213.04091348333336, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367552, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 02:08:06 (federatedscope.core.workers.client:842) INFO: ================= client 32 received finish message =================
2025-09-14 02:08:06 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:08:06 (federatedscope.core.monitors.monitor:268) INFO: In worker #32, the system-related metrics are: {'id': 32, 'fl_end_time_minutes': 212.99980663333332, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367544, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 02:08:06 (federatedscope.core.workers.client:842) INFO: ================= client 33 received finish message =================
2025-09-14 02:08:06 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:08:06 (federatedscope.core.monitors.monitor:268) INFO: In worker #33, the system-related metrics are: {'id': 33, 'fl_end_time_minutes': 212.95876744999998, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367552, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 02:08:06 (federatedscope.core.workers.client:842) INFO: ================= client 34 received finish message =================
2025-09-14 02:08:06 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:08:06 (federatedscope.core.monitors.monitor:268) INFO: In worker #34, the system-related metrics are: {'id': 34, 'fl_end_time_minutes': 212.91628916666664, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367544, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 02:08:06 (federatedscope.core.workers.client:842) INFO: ================= client 35 received finish message =================
2025-09-14 02:08:06 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:08:06 (federatedscope.core.monitors.monitor:268) INFO: In worker #35, the system-related metrics are: {'id': 35, 'fl_end_time_minutes': 212.87147431666665, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367544, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 02:08:06 (federatedscope.core.workers.client:842) INFO: ================= client 36 received finish message =================
2025-09-14 02:08:06 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:08:06 (federatedscope.core.monitors.monitor:268) INFO: In worker #36, the system-related metrics are: {'id': 36, 'fl_end_time_minutes': 212.83029571666665, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367544, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 02:08:06 (federatedscope.core.workers.client:842) INFO: ================= client 37 received finish message =================
2025-09-14 02:08:06 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:08:06 (federatedscope.core.monitors.monitor:268) INFO: In worker #37, the system-related metrics are: {'id': 37, 'fl_end_time_minutes': 212.78931215, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367552, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 02:08:06 (federatedscope.core.workers.client:842) INFO: ================= client 38 received finish message =================
2025-09-14 02:08:06 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:08:06 (federatedscope.core.monitors.monitor:268) INFO: In worker #38, the system-related metrics are: {'id': 38, 'fl_end_time_minutes': 212.748068, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367544, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 02:08:06 (federatedscope.core.workers.client:842) INFO: ================= client 39 received finish message =================
2025-09-14 02:08:06 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:08:06 (federatedscope.core.monitors.monitor:268) INFO: In worker #39, the system-related metrics are: {'id': 39, 'fl_end_time_minutes': 212.7068686833333, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367544, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 02:08:06 (federatedscope.core.workers.client:842) INFO: ================= client 40 received finish message =================
2025-09-14 02:08:06 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:08:06 (federatedscope.core.monitors.monitor:268) INFO: In worker #40, the system-related metrics are: {'id': 40, 'fl_end_time_minutes': 212.64850446666665, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367552, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 02:08:06 (federatedscope.core.workers.client:842) INFO: ================= client 41 received finish message =================
2025-09-14 02:08:06 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:08:06 (federatedscope.core.monitors.monitor:268) INFO: In worker #41, the system-related metrics are: {'id': 41, 'fl_end_time_minutes': 212.60768388333332, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367544, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 02:08:06 (federatedscope.core.workers.client:842) INFO: ================= client 42 received finish message =================
2025-09-14 02:08:06 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:08:06 (federatedscope.core.monitors.monitor:268) INFO: In worker #42, the system-related metrics are: {'id': 42, 'fl_end_time_minutes': 212.56665941666668, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367544, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 02:08:06 (federatedscope.core.workers.client:842) INFO: ================= client 43 received finish message =================
2025-09-14 02:08:06 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:08:06 (federatedscope.core.monitors.monitor:268) INFO: In worker #43, the system-related metrics are: {'id': 43, 'fl_end_time_minutes': 212.52564921666666, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367544, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 02:08:06 (federatedscope.core.workers.client:842) INFO: ================= client 44 received finish message =================
2025-09-14 02:08:06 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:08:06 (federatedscope.core.monitors.monitor:268) INFO: In worker #44, the system-related metrics are: {'id': 44, 'fl_end_time_minutes': 212.48496016666667, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367544, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 02:08:06 (federatedscope.core.workers.client:842) INFO: ================= client 45 received finish message =================
2025-09-14 02:08:06 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:08:06 (federatedscope.core.monitors.monitor:268) INFO: In worker #45, the system-related metrics are: {'id': 45, 'fl_end_time_minutes': 212.44085993333334, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367544, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 02:08:06 (federatedscope.core.workers.client:842) INFO: ================= client 46 received finish message =================
2025-09-14 02:08:06 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:08:06 (federatedscope.core.monitors.monitor:268) INFO: In worker #46, the system-related metrics are: {'id': 46, 'fl_end_time_minutes': 212.39867596666667, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367544, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 02:08:06 (federatedscope.core.workers.client:842) INFO: ================= client 47 received finish message =================
2025-09-14 02:08:06 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:08:06 (federatedscope.core.monitors.monitor:268) INFO: In worker #47, the system-related metrics are: {'id': 47, 'fl_end_time_minutes': 212.35792535000002, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367552, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 02:08:06 (federatedscope.core.workers.client:842) INFO: ================= client 48 received finish message =================
2025-09-14 02:08:06 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:08:06 (federatedscope.core.monitors.monitor:268) INFO: In worker #48, the system-related metrics are: {'id': 48, 'fl_end_time_minutes': 212.31750735, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367544, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 02:08:06 (federatedscope.core.workers.client:842) INFO: ================= client 49 received finish message =================
2025-09-14 02:08:06 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:08:06 (federatedscope.core.monitors.monitor:268) INFO: In worker #49, the system-related metrics are: {'id': 49, 'fl_end_time_minutes': 212.2767861, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367544, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 02:08:06 (federatedscope.core.workers.client:842) INFO: ================= client 50 received finish message =================
2025-09-14 02:08:07 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:08:07 (federatedscope.core.monitors.monitor:268) INFO: In worker #50, the system-related metrics are: {'id': 50, 'fl_end_time_minutes': 212.23620203333334, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367544, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 02:08:07 (federatedscope.core.workers.client:842) INFO: ================= client 51 received finish message =================
2025-09-14 02:08:07 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:08:07 (federatedscope.core.monitors.monitor:268) INFO: In worker #51, the system-related metrics are: {'id': 51, 'fl_end_time_minutes': 212.19554795000002, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367544, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 02:08:07 (federatedscope.core.workers.client:842) INFO: ================= client 52 received finish message =================
2025-09-14 02:08:07 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:08:07 (federatedscope.core.monitors.monitor:268) INFO: In worker #52, the system-related metrics are: {'id': 52, 'fl_end_time_minutes': 212.15469130000002, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367544, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 02:08:07 (federatedscope.core.workers.client:842) INFO: ================= client 53 received finish message =================
2025-09-14 02:08:07 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-14 02:08:07 (federatedscope.core.monitors.monitor:268) INFO: In worker #53, the system-related metrics are: {'id': 53, 'fl_end_time_minutes': 212.11361926666666, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367544, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 02:08:07 (federatedscope.core.monitors.monitor:441) INFO: We will compress the file eval_results.raw into a .gz file, and delete the old one
2025-09-14 02:08:07 (federatedscope.core.monitors.monitor:359) INFO: After merging the system metrics from all works, we got avg: defaultdict(None, {'id': 'sys_avg', 'sys_avg/fl_end_time_minutes': 213.22255696728396, 'sys_avg/total_model_size': '478.65M', 'sys_avg/total_flops': '0.0', 'sys_avg/total_upload_bytes': '0.0', 'sys_avg/total_download_bytes': '1.28M', 'sys_avg/global_convergence_round': 0.0, 'sys_avg/local_convergence_round': 0.0, 'sys_avg/global_convergence_time_minutes': 0.0, 'sys_avg/local_convergence_time_minutes': 0.0})
2025-09-14 02:08:07 (federatedscope.core.monitors.monitor:360) INFO: After merging the system metrics from all works, we got std: defaultdict(None, {'id': 'sys_std', 'sys_std/fl_end_time_minutes': 0.6522255290477569, 'sys_std/total_model_size': '65.75M', 'sys_std/total_flops': '0.0', 'sys_std/total_upload_bytes': '0.0', 'sys_std/total_download_bytes': '167.91K', 'sys_std/global_convergence_round': 0.0, 'sys_std/local_convergence_round': 0.0, 'sys_std/global_convergence_time_minutes': 0.0, 'sys_std/local_convergence_time_minutes': 0.0})
